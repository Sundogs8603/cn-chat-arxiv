<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.06949</link><description>&lt;p&gt;
HyperDreamBooth&#65306;&#29992;&#20110;&#24555;&#36895;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06949
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#20010;&#20307;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#20010;&#24615;&#21270;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27599;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#26102;&#38388;&#25237;&#20837;&#65292;&#20026;&#27599;&#20010;&#20027;&#39064;&#23384;&#20648;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#20250;&#23545;&#23384;&#20648;&#23481;&#37327;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDreamBooth-&#19968;&#31181;&#33021;&#22815;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#26377;&#25928;&#29983;&#25104;&#19968;&#32452;&#20010;&#24615;&#21270;&#26435;&#37325;&#30340;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26435;&#37325;&#32452;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#25645;&#37197;&#24555;&#36895;&#24494;&#35843;&#65292;HyperDreamBooth&#33021;&#22815;&#20197;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#29983;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20027;&#39064;&#32454;&#33410;&#21516;&#26102;&#20063;&#20445;&#25345;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#32422;50&#20493;&#20307;&#29616;&#20102;&#38754;&#37096;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#29702;&#35770;&#32852;&#31995;&#65292;&#23558;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21464;&#25442;&#21644;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#20204;&#26159;&#31561;&#20215;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#25351;&#20986;&#20102;&#20165;&#20165;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06941</link><description>&lt;p&gt;
&#20851;&#20110;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations. (arXiv:2307.06941v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#29702;&#35770;&#32852;&#31995;&#65292;&#23558;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21464;&#25442;&#21644;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#20204;&#26159;&#31561;&#20215;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#25351;&#20986;&#20102;&#20165;&#20165;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#20013;&#20004;&#31181;&#26368;&#21463;&#27426;&#36814;&#30340;&#35299;&#37322;&#31867;&#22411;&#26159;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20004;&#31867;&#26041;&#27861;&#19968;&#30452;&#22312;&#29420;&#31435;&#22320;&#36827;&#34892;&#30740;&#31350;&#65292;&#32780;&#23545;&#23427;&#20204;&#30340;&#35843;&#21644;&#21482;&#26377;&#23569;&#25968;&#35797;&#22270;&#26159;&#32463;&#39564;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#65288;&#20027;&#35201;&#20851;&#27880;&#20294;&#19981;&#38480;&#20110;SHAP&#65289;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20043;&#38388;&#30340;&#26126;&#30830;&#29702;&#35770;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;Shapley&#20540;&#30340;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#26377;&#25928;&#21464;&#25442;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#35777;&#26126;&#23427;&#20204;&#23454;&#38469;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#31561;&#20215;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;Shapley&#20540;&#20197;&#22806;&#30340;&#21338;&#24328;&#35770;&#35299;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#31561;&#20215;&#26465;&#20214;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21482;&#31616;&#21333;&#22320;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantita
&lt;/p&gt;</description></item><item><title>FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.06933</link><description>&lt;p&gt;
FDAPT: &#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06933
&lt;/p&gt;
&lt;p&gt;
FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;DAPT&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26356;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FDAPT&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FDAPT&#22312;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#37117;&#33021;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20923;&#32467;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FFDAPT&#65289;&#12290;FFDAPT&#24179;&#22343;&#25552;&#39640;&#20102;12.1%&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;FDAPT&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#19968;&#33324;&#24615;&#33021;&#27874;&#21160;&#20445;&#25345;&#22312;1%&#20197;&#19979;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#24037;&#20316;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL) can enhance model adaptation by leveraging more sensitive and distributed data while preserving data privacy. However, few studies have focused on this method. Therefore, we conduct the first comprehensive empirical study to evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We demonstrate that FDAPT can maintain competitive downstream task performance to the centralized baseline in both IID and non-IID situations. Furthermore, we propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and exhibits similar downstream task performance to standard FDAPT, with general performance fluctuations remaining less than 1%. Finally, through a critical evaluation of our work, we identify promising future research directions for this new research area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#20013;&#26377;&#25928;&#22320;&#20351;&#29992;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#25110;&#20808;&#21069;&#20449;&#24687;&#30340;&#20381;&#36182;&#12290;&#24341;&#20837;&#20102;&#23545;&#27604;&#24230;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#21644;&#21487;&#32534;&#36753;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#26631;&#35760;&#25512;&#21521;&#26368;&#36817;&#30340;&#29616;&#26377;&#26631;&#35760;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.06925</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#36890;&#29992;&#30340;&#35843;&#20248;&#32534;&#30721;&#22120;&#29992;&#20110;&#24555;&#36895;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models. (arXiv:2307.06925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06925
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#20013;&#26377;&#25928;&#22320;&#20351;&#29992;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#25110;&#20808;&#21069;&#20449;&#24687;&#30340;&#20381;&#36182;&#12290;&#24341;&#20837;&#20102;&#23545;&#27604;&#24230;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#21644;&#21487;&#32534;&#36753;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#26631;&#35760;&#25512;&#21521;&#26368;&#36817;&#30340;&#29616;&#26377;&#26631;&#35760;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#20010;&#24615;&#21270;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#23558;&#33258;&#24049;&#30340;&#35270;&#35273;&#27010;&#24565;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#26469;&#25351;&#23548;&#21019;&#36896;&#24615;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;T2I&#20010;&#24615;&#21270;&#30340;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#23545;&#22810;&#20010;&#22270;&#20687;&#21644;&#38271;&#26102;&#38388;&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#37117;&#23616;&#38480;&#20110;&#21333;&#19968;&#39046;&#22495;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#22810;&#26679;&#21270;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#25110;&#20851;&#20110;&#20010;&#24615;&#21270;&#27010;&#24565;&#30340;&#20808;&#21069;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24230;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#25345;&#23545;&#30446;&#26631;&#27010;&#24565;&#29305;&#24449;&#30340;&#39640;&#20445;&#30495;&#24230;&#65292;&#21516;&#26102;&#20351;&#39044;&#27979;&#30340;&#23884;&#20837;&#20445;&#25345;&#25509;&#36817;&#28508;&#22312;&#31354;&#38388;&#30340;&#21487;&#32534;&#36753;&#21306;&#22495;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#26631;&#35760;&#25512;&#21521;&#20854;&#26368;&#36817;&#30340;&#29616;&#26377;CLIP&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#26631;&#35760;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are 
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06915</link><description>&lt;p&gt;
&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;: &#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality. (arXiv:2307.06915v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#29616;&#20195;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#31616;&#21333;&#21644;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#22312;&#19981;&#21516;&#30340;&#24773;&#22659;&#19979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#24179;&#22343;&#26041;&#26696;&#26469;&#21152;&#36895;SGD&#30340;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#29992;&#20110;SGD&#30340;&#36890;&#29992;&#24179;&#22343;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31867;&#21152;&#26435;&#24179;&#22343;SGD&#35299;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#23637;&#29616;&#20986;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#26368;&#20248;&#26435;&#37325;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) is one of the simplest and most popular algorithms in modern statistical and machine learning due to its computational and memory efficiency. Various averaging schemes have been proposed to accelerate the convergence of SGD in different settings. In this paper, we explore a general averaging scheme for SGD. Specifically, we establish the asymptotic normality of a broad range of weighted averaged SGD solutions and provide asymptotically valid online inference approaches. Furthermore, we propose an adaptive averaging scheme that exhibits both optimal statistical rate and favorable non-asymptotic convergence, drawing insights from the optimal weight for the linear model in terms of non-asymptotic mean squared error (MSE).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.06913</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#25581;&#31034;&#29420;&#29305;&#30340;&#27010;&#24565;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06913
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#27169;&#22411;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26356;&#26131;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#20687;&#32032;&#26174;&#33879;&#24615;&#31561;&#29305;&#24449;&#24402;&#22240;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#23450;&#20041;&#35299;&#37322;&#20998;&#26512;&#30340;&#27010;&#24565;&#20250;&#21463;&#21040;&#29992;&#25143;&#23545;&#27010;&#24565;&#26399;&#26395;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20998;&#35299;&#19968;&#20010;&#23618;&#30340;&#28508;&#22312;&#31354;&#38388;&#25104;&#22855;&#24322;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#23545;&#20854;&#36827;&#34892;&#31934;&#28860;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#39640;&#26041;&#24046;&#26041;&#21521;&#19978;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#24182;&#25351;&#21521;&#35821;&#20041;&#19978;&#29420;&#29305;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26131;&#20110;&#29702;&#35299;&#30340;&#65292;&#20855;&#26377;&#19968;&#33268;&#24615;&#65292;&#24182;&#19982;&#25152;&#38656;&#20219;&#21153;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06887</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06887
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#26159;&#31070;&#32463;&#32593;&#32476;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#22914;&#20309;&#20197;&#21450;&#20026;&#20309;&#21457;&#29983;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#21487;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#25110;&#38543;&#26426;&#29305;&#24449;&#33539;&#20363;&#20013;&#24494;&#19981;&#36275;&#36947;&#30340;&#29305;&#24449;&#23398;&#20064;&#30340;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#32463;&#24120;&#22320;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#20123;&#20808;&#21069;&#30340;&#20998;&#26512;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#26368;&#24120;&#35265;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;min-max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;Extra-gradient&#31639;&#27861;&#21457;&#25955;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06886</link><description>&lt;p&gt;
Min-Max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Min-Max Optimization under Delays. (arXiv:2307.06886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06886
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;min-max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;Extra-gradient&#31639;&#27861;&#21457;&#25955;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#24310;&#36831;&#21644;&#24322;&#27493;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#22242;&#38431;&#24191;&#27867;&#20998;&#26512;&#20102;&#20855;&#26377;&#24310;&#36831;&#26799;&#24230;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26080;&#31867;&#20284;&#30340;&#29702;&#35770;&#21487;&#29992;&#20110;min-max&#20248;&#21270;&#65292;&#36825;&#20010;&#35805;&#39064;&#30001;&#20110;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#21338;&#24328;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#24102;&#26377;&#24310;&#36831;&#26799;&#24230;&#26356;&#26032;&#30340;&#26631;&#20934;min-max&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#65288;&#32463;&#39564;&#24615;&#22320;&#65289;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;&#20687;Extra-gradient (EG) &#36825;&#26679;&#30340;&#26480;&#20986;&#31639;&#27861;&#22312;&#31616;&#21333;&#23454;&#20363;&#19978;&#21457;&#25955;&#65292;&#32780;&#22312;&#27809;&#26377;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;EG&#21487;&#20197;&#20445;&#35777;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#26377;&#24517;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#30456;&#24212;&#22320;&#65292;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477; - &#19978;&#21319; (GDA)&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge on simple instances for which \texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\texttt{G
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20462;&#25913;&#27010;&#29575;&#25110;&#22870;&#21169;&#26102;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#20445;&#25345;&#20540;&#20989;&#25968;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#24182;&#19988;&#36825;&#20010;&#25361;&#25112;&#19982;&#29366;&#24577;&#25968;&#30446;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.06877</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The complexity of non-stationary reinforcement learning. (arXiv:2307.06877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06877
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20462;&#25913;&#27010;&#29575;&#25110;&#22870;&#21169;&#26102;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#20445;&#25345;&#20540;&#20989;&#25968;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#24182;&#19988;&#36825;&#20010;&#25361;&#25112;&#19982;&#29366;&#24577;&#25968;&#30446;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#34987;&#35748;&#20026;&#26159;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#22797;&#26434;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#24688;&#22909;&#25429;&#25417;&#21040;&#20102;&#36825;&#20010;&#25361;&#25112;&#65306;&#20462;&#25913;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#19968;&#20010;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#27010;&#29575;&#25110;&#22870;&#21169;&#65292;&#38656;&#35201;&#33457;&#36153;&#20960;&#20046;&#19982;&#29366;&#24577;&#25968;&#30446;&#19968;&#26679;&#22810;&#30340;&#26102;&#38388;&#26469;&#21450;&#26102;&#26356;&#26032;&#20540;&#20989;&#25968;&#65292;&#38500;&#38750;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;(SETH)&#26159;&#38169;&#35823;&#30340;&#65307;SETH&#26159;P&#8800;NP&#29468;&#24819;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#21152;&#24378;&#29256;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#30446;&#21069;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#29366;&#24577;&#25968;&#30446;&#36890;&#24120;&#26159;&#22825;&#25991;&#25968;&#23383;&#32423;&#21035;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20165;&#20165;"&#28155;&#21152;"&#19968;&#20010;&#26032;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\textit{adding}$ a new state-action pair is considerably easier to implement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20559;&#35265;&#20998;&#26512;&#26469;&#24110;&#21161;&#22320;&#26041;&#25919;&#24220;&#35782;&#21035;&#26377;&#21487;&#33021;&#38656;&#35201;&#26089;&#26399;&#25588;&#21161;&#30340;&#23478;&#24237;&#12290;&#22312;&#26500;&#24314;&#27169;&#22411;&#26102;&#65292;&#34429;&#28982;&#27169;&#22411;&#23637;&#31034;&#20102;&#35782;&#21035;&#24180;&#36731;&#20154;&#38656;&#35201;&#24178;&#39044;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19979;&#20063;&#20135;&#29983;&#20102;&#24456;&#22810;&#38169;&#35823;&#30340;&#27491;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.06871</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20559;&#35265;&#20998;&#26512;&#26469;&#35782;&#21035;&#22320;&#26041;&#25919;&#24220;&#30340;&#26089;&#26399;&#25588;&#21161;&#36716;&#20171;
&lt;/p&gt;
&lt;p&gt;
Identifying Early Help Referrals For Local Authorities With Machine Learning And Bias Analysis. (arXiv:2307.06871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20559;&#35265;&#20998;&#26512;&#26469;&#24110;&#21161;&#22320;&#26041;&#25919;&#24220;&#35782;&#21035;&#26377;&#21487;&#33021;&#38656;&#35201;&#26089;&#26399;&#25588;&#21161;&#30340;&#23478;&#24237;&#12290;&#22312;&#26500;&#24314;&#27169;&#22411;&#26102;&#65292;&#34429;&#28982;&#27169;&#22411;&#23637;&#31034;&#20102;&#35782;&#21035;&#24180;&#36731;&#20154;&#38656;&#35201;&#24178;&#39044;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19979;&#20063;&#20135;&#29983;&#20102;&#24456;&#22810;&#38169;&#35823;&#30340;&#27491;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#26684;&#20848;&#30340;&#22320;&#26041;&#25919;&#24220;&#65288;&#22914;&#33713;&#26031;&#29305;&#37089;&#22996;&#21592;&#20250;&#65289;&#25552;&#20379;&#26089;&#26399;&#25588;&#21161;&#26381;&#21153;&#65292;&#21487;&#22312;&#24180;&#36731;&#20154;&#38754;&#20020;&#26080;&#27861;&#20165;&#38752;&#26222;&#36941;&#26381;&#21153;&#65288;&#22914;&#23398;&#26657;&#65289;&#25903;&#25345;&#30340;&#22256;&#38590;&#26102;&#25552;&#20379;&#25588;&#21161;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#24110;&#21161;&#19987;&#23478;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#36827;&#34892;&#26089;&#26399;&#25588;&#21161;&#35780;&#20272;&#21644;&#25903;&#25345;&#30340;&#23478;&#24237;&#12290;&#33713;&#26031;&#29305;&#37089;&#22996;&#21592;&#20250;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;14360&#26465;18&#23681;&#20197;&#19979;&#24180;&#36731;&#20154;&#35760;&#24405;&#30340;&#21311;&#21517;&#21270;&#25968;&#25454;&#38598;&#12290;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#26500;&#24314;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#20197;&#39564;&#35777;&#21644;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24212;&#29992;&#20102;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#26469;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#23613;&#31649;&#27169;&#22411;&#23637;&#31034;&#20102;&#35782;&#21035;&#38656;&#35201;&#24178;&#39044;&#25110;&#26089;&#26399;&#25588;&#21161;&#30340;&#24180;&#36731;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#20135;&#29983;&#20102;&#22823;&#37327;&#38169;&#35823;&#30340;&#27491;&#20363;&#65292;&#29305;&#21035;&#26159;&#24403;&#20351;&#29992;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#26500;&#24314;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools. This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support. LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18. The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models. Bias mitigation techniques were applied to improve the fairness of these models. During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced dat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.06870</link><description>&lt;p&gt;
&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#38271;&#26102;&#38388;&#20869;&#37096;&#32626;&#22312;&#23478;&#20013;&#30340;&#26426;&#22120;&#20154;&#38754;&#20020;&#30528;&#30495;&#27491;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#12290;&#20316;&#20026;&#26426;&#22120;&#20154;&#23547;&#27714;&#20026;&#29992;&#25143;&#25552;&#20379;&#24110;&#21161;&#65292;&#23427;&#24212;&#35813;&#21033;&#29992;&#20219;&#20309;&#31215;&#32047;&#30340;&#32463;&#39564;&#26469;&#25913;&#36827;&#33258;&#24049;&#30340;&#30693;&#35782;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#29087;&#32451;&#30340;&#21161;&#25163;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#36825;&#31181;&#24773;&#20917;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#24314;&#27169;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#65292;&#20026;&#35268;&#21010;&#22120;&#29983;&#25104;&#20505;&#36873;&#36830;&#32493;&#21442;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#25968;&#25454;&#22914;&#20309;&#22312;&#20219;&#21153;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#20915;&#23450;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#22312;&#32447;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#65292;&#36825;&#20123;&#20219;&#21153;&#20316;&#20026;&#27599;&#20010;&#27169;&#22411;&#23545;&#29366;&#24577;&#30340;&#29702;&#35299;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.
&lt;/p&gt;</description></item><item><title>AnuraSet&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#26032;&#28909;&#24102;&#34521;&#31867;&#21483;&#22768;&#36827;&#34892;&#35782;&#21035;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;27&#23567;&#26102;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#35760;&#24405;&#65292;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#35299;&#20915;&#34521;&#31867;&#21483;&#22768;&#35782;&#21035;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06860</link><description>&lt;p&gt;
AnuraSet&#65306;&#19968;&#20221;&#29992;&#20110;&#23545;&#34987;&#21160;&#22768;&#23398;&#30417;&#27979;&#20013;&#26032;&#28909;&#24102;&#34521;&#31867;&#21483;&#22768;&#35782;&#21035;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AnuraSet: A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring. (arXiv:2307.06860v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06860
&lt;/p&gt;
&lt;p&gt;
AnuraSet&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#26032;&#28909;&#24102;&#34521;&#31867;&#21483;&#22768;&#36827;&#34892;&#35782;&#21035;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;27&#23567;&#26102;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#35760;&#24405;&#65292;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#35299;&#20915;&#34521;&#31867;&#21483;&#22768;&#35782;&#21035;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21464;&#21270;&#39044;&#35745;&#23558;&#24341;&#21457;&#34521;&#31867;&#22768;&#23398;&#34892;&#20026;&#30340;&#36716;&#21464;&#65292;&#21487;&#20197;&#36890;&#36807;&#34987;&#21160;&#22768;&#23398;&#30417;&#27979;&#36827;&#34892;&#30740;&#31350;&#12290;&#20102;&#35299;&#21483;&#22768;&#34892;&#20026;&#30340;&#21464;&#21270;&#38656;&#35201;&#35782;&#21035;&#34521;&#31867;&#29289;&#31181;&#65292;&#30001;&#20110;&#26032;&#28909;&#24102;&#38899;&#26223;&#30340;&#29305;&#27530;&#29305;&#28857;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#22810;&#29289;&#31181;&#34521;&#31867;&#21483;&#22768;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#34987;&#21160;&#22768;&#23398;&#30417;&#27979;&#35760;&#24405;&#30340;42&#20010;&#19981;&#21516;&#29289;&#31181;&#30340;&#19987;&#23478;&#27880;&#37322;&#32452;&#25104;&#65292;&#24635;&#35745;27&#23567;&#26102;&#12290;&#25105;&#20204;&#21521;&#20844;&#20247;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#35775;&#38382;&#65292;&#21253;&#25324;&#21407;&#22987;&#35760;&#24405;&#12289;&#23454;&#39564;&#35774;&#32622;&#20195;&#30721;&#20197;&#21450;&#19968;&#39033;&#32454;&#31890;&#24230;&#20998;&#31867;&#38382;&#39064;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#65292;&#40723;&#21169;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#35299;&#20915;&#34521;&#31867;&#21483;&#22768;&#35782;&#21035;&#38382;&#39064;&#20197;&#25512;&#36827;&#20445;&#25252;&#25919;&#31574;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#21644;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;GitHub&#23384;&#20648;&#24211;https://github.com/soundclim/anuraset &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global change is predicted to induce shifts in anuran acoustic behavior, which can be studied through passive acoustic monitoring (PAM). Understanding changes in calling behavior requires the identification of anuran species, which is challenging due to the particular characteristics of neotropical soundscapes. In this paper, we introduce a large-scale multi-species dataset of anuran amphibians calls recorded by PAM, that comprises 27 hours of expert annotations for 42 different species from two Brazilian biomes. We provide open access to the dataset, including the raw recordings, experimental setup code, and a benchmark with a baseline model of the fine-grained categorization problem. Additionally, we highlight the challenges of the dataset to encourage machine learning researchers to solve the problem of anuran call identification towards conservation policy. All our experiments and resources can be found on our GitHub repository https://github.com/soundclim/anuraset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#23558;&#19981;&#21516;&#22122;&#22768;&#27169;&#22411;&#20197;&#19981;&#21516;&#24133;&#24230;&#28155;&#21152;&#21040;CNN&#26550;&#26500;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#22122;&#22768;&#27880;&#20837;&#30340;&#26032;&#21551;&#21457;&#21644;&#24314;&#35758;&#65292;&#26377;&#21161;&#20110;&#20248;&#21270;&#22270;&#20687;&#20998;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.06855</link><description>&lt;p&gt;
&#35757;&#32451;CNN&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#32473;&#22270;&#20687;&#27880;&#20837;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation in Training CNNs: Injecting Noise to Images. (arXiv:2307.06855v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#23558;&#19981;&#21516;&#22122;&#22768;&#27169;&#22411;&#20197;&#19981;&#21516;&#24133;&#24230;&#28155;&#21152;&#21040;CNN&#26550;&#26500;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#22122;&#22768;&#27880;&#20837;&#30340;&#26032;&#21551;&#21457;&#21644;&#24314;&#35758;&#65292;&#26377;&#21161;&#20110;&#20248;&#21270;&#22270;&#20687;&#20998;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#27880;&#20837;&#26159;&#25968;&#25454;&#22686;&#24378;&#30340;&#19968;&#20010;&#22522;&#26412;&#24037;&#20855;&#65292;&#28982;&#32780;&#30446;&#21069;&#36824;&#27809;&#26377;&#24191;&#27867;&#25509;&#21463;&#30340;&#31243;&#24207;&#23558;&#20854;&#19982;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23558;&#19981;&#21516;&#22122;&#22768;&#27169;&#22411;&#20197;&#19981;&#21516;&#30340;&#24133;&#24230;&#28155;&#21152;&#21040;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#19978;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#36827;&#34892;&#27604;&#36739;&#65292;&#20351;&#29992;&#32467;&#26500;&#30456;&#20284;&#24230;&#65288;SSIM&#65289;&#24230;&#37327;&#26041;&#27861;&#32473;&#20986;&#20855;&#26377;&#19981;&#21516;&#23494;&#24230;&#20989;&#25968;&#30340;&#22122;&#22768;&#27169;&#22411;&#30456;&#21516;&#30340;&#24133;&#24230;&#27700;&#24179;&#12290;&#22522;&#26412;&#32467;&#26524;&#19968;&#33268;&#22320;&#31526;&#21512;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22823;&#37096;&#20998;&#24120;&#35782;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#20851;&#20110;&#22122;&#22768;&#27880;&#20837;&#30340;&#26032;&#21551;&#21457;&#21644;&#24314;&#35758;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#23558;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#22270;&#20687;&#20998;&#31867;&#30340;&#26368;&#20339;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise injection is a fundamental tool for data augmentation, and yet there is no widely accepted procedure to incorporate it with learning frameworks. This study analyzes the effects of adding or applying different noise models of varying magnitudes to Convolutional Neural Network (CNN) architectures. Noise models that are distributed with different density functions are given common magnitude levels via Structural Similarity (SSIM) metric in order to create an appropriate ground for comparison. The basic results are conforming with the most of the common notions in machine learning, and also introduce some novel heuristics and recommendations on noise injection. The new approaches will provide better understanding on optimal learning procedures for image classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#24352;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24341;&#20837;&#24352;&#37327;QR&#20998;&#35299;&#26469;&#21152;&#24555;&#32593;&#32476;&#24310;&#36831;&#20272;&#35745;&#38382;&#39064;&#30340;&#24352;&#37327;&#23436;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06848</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Leverage Sampling&#21644;Tensor QR&#20998;&#35299;&#36827;&#34892;&#32593;&#32476;&#24310;&#36831;&#20272;&#35745;&#30340;&#24352;&#37327;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion via Leverage Sampling and Tensor QR Decomposition for Network Latency Estimation. (arXiv:2307.06848v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#24352;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24341;&#20837;&#24352;&#37327;QR&#20998;&#35299;&#26469;&#21152;&#24555;&#32593;&#32476;&#24310;&#36831;&#20272;&#35745;&#38382;&#39064;&#30340;&#24352;&#37327;&#23436;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#24310;&#36831;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#19988;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#21033;&#29992;&#32593;&#32476;&#33410;&#28857;&#30340;&#25968;&#25454;&#32467;&#26500;&#24418;&#25104;&#30697;&#38453;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#32500;&#24230;&#26500;&#24314;&#20102;&#24352;&#37327;&#27169;&#22411;&#65292;&#23558;&#25972;&#20010;&#38382;&#39064;&#24402;&#32467;&#20026;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#24352;&#37327;Leverage Sampling&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;Tensor QR&#20998;&#35299;&#26469;&#21152;&#24555;&#24352;&#37327;&#23436;&#25104;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#26367;&#25442;&#20026;&#24352;&#37327;CSVD-QR&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#36895;&#30340;&#24352;&#37327;Levarage Sampling&#12290;&#20026;&#20102;&#21152;&#24555;&#19981;&#23436;&#20840;&#24352;&#37327;&#30340;&#23436;&#25104;&#36895;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24352;&#37327;&#30340;$L_{2,1}$-norm&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#24352;&#37327;&#26680;&#33539;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;Tensor QR&#20998;&#35299;&#24341;&#20837;&#21040;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the network latency estimation, which has been an important metric for network performance. However, a large scale of network latency estimation requires a lot of computing time. Therefore, we propose a new method that is much faster and maintains high accuracy. The data structure of network nodes can form a matrix, and the tensor model can be formed by introducing the time dimension. Thus, the entire problem can be be summarized as a tensor completion problem. The main idea of our method is improving the tensor leverage sampling strategy and introduce tensor QR decomposition into tensor completion. To achieve faster tensor leverage sampling, we replace tensor singular decomposition (t-SVD) with tensor CSVD-QR to appoximate t-SVD. To achieve faster completion for incomplete tensor, we use the tensor $L_{2,1}$-norm rather than traditional tensor nuclear norm. Furthermore, we introduce tensor QR decomposition into alternating direction method of multipliers (AD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21160;&#24577;&#21644;&#28789;&#27963;&#22320;&#25805;&#20316;5G&#22810;MAP&#32593;&#32476;&#12290;&#22312;&#39640;&#23618;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#20849;&#35782;&#30830;&#23450;MAPs&#30340;&#25968;&#37327;&#24182;&#32771;&#34385;&#32593;&#32476;&#33258;&#31649;&#29702;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#20302;&#23618;&#65292;&#20351;&#29992;&#19968;&#20010;&#21452;&#27880;&#24847;&#21147;&#30340;DRL&#27169;&#22411;&#26469;&#31649;&#29702;MAPs&#30340;&#20301;&#32622;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#26426;&#21046;&#26469;&#35757;&#32451;&#21644;&#20849;&#20139;&#27599;&#20010;MAP&#30340;&#20301;&#32622;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#20989;&#25968;&#20849;&#21516;&#20248;&#21270;MAPs&#30340;&#20301;&#32622;&#21644;&#22238;&#20256;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2307.06842</link><description>&lt;p&gt;
&#38754;&#21521;&#21160;&#24577;&#21644;&#28789;&#27963;&#30340;5G&#22810;MAP&#32593;&#32476;&#25805;&#20316;&#30340;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Agent Deep Reinforcement Learning for Dynamic and Flexible 3D Operation of 5G Multi-MAP Networks. (arXiv:2307.06842v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21160;&#24577;&#21644;&#28789;&#27963;&#22320;&#25805;&#20316;5G&#22810;MAP&#32593;&#32476;&#12290;&#22312;&#39640;&#23618;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#20849;&#35782;&#30830;&#23450;MAPs&#30340;&#25968;&#37327;&#24182;&#32771;&#34385;&#32593;&#32476;&#33258;&#31649;&#29702;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#20302;&#23618;&#65292;&#20351;&#29992;&#19968;&#20010;&#21452;&#27880;&#24847;&#21147;&#30340;DRL&#27169;&#22411;&#26469;&#31649;&#29702;MAPs&#30340;&#20301;&#32622;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#26426;&#21046;&#26469;&#35757;&#32451;&#21644;&#20849;&#20139;&#27599;&#20010;MAP&#30340;&#20301;&#32622;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#20989;&#25968;&#20849;&#21516;&#20248;&#21270;MAPs&#30340;&#20301;&#32622;&#21644;&#22238;&#20256;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;5G&#32593;&#32476;&#20013;&#39640;&#25928;&#31649;&#29702;&#31227;&#21160;&#25509;&#20837;&#28857;(MAPs)&#65292;MAPs&#26159;&#26080;&#20154;&#26426;(UAV)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#32771;&#34385;&#32508;&#21512;&#25509;&#20837;-&#22238;&#20256;(IAB)&#32422;&#26463;&#26469;&#21160;&#24577;&#37325;&#37197;&#32622;&#32593;&#32476;&#12290;&#39640;&#23618;&#20915;&#31574;&#36807;&#31243;&#36890;&#36807;&#20849;&#35782;&#30830;&#23450;MAPs&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#36807;&#31243;&#26469;&#32771;&#34385;&#32593;&#32476;&#33258;&#31649;&#29702;&#20013;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#20302;&#23618;&#65292;MAPs&#20351;&#29992;&#22522;&#20110;&#21452;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#27169;&#22411;&#36827;&#34892;&#20301;&#32622;&#31649;&#29702;&#65292;&#40723;&#21169;&#21512;&#20316;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#20302;&#23618;&#35757;&#32451;&#21644;&#20849;&#20139;&#27599;&#20010;MAP&#30340;&#19968;&#20010;&#20301;&#32622;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#30446;&#26631;&#22870;&#21169;&#20989;&#25968;&#20849;&#21516;&#20248;&#21270;MAPs&#30340;&#20301;&#32622;&#21644;&#22238;&#20256;&#36830;&#25509;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;MAP&#20301;&#32622;&#23545;&#26080;&#32447;&#22238;&#20256;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the efficient management of Mobile Access Points (MAPs), which are Unmanned Aerial Vehicles (UAV), in 5G networks. We propose a two-level hierarchical architecture, which dynamically reconfigures the network while considering Integrated Access-Backhaul (IAB) constraints. The high-layer decision process determines the number of MAPs through consensus, and we develop a joint optimization process to account for co-dependence in network self-management. In the low-layer, MAPs manage their placement using a double-attention based Deep Reinforcement Learning (DRL) model that encourages cooperation without retraining. To improve generalization and reduce complexity, we propose a federated mechanism for training and sharing one placement model for every MAP in the low-layer. Additionally, we jointly optimize the placement and backhaul connectivity of MAPs using a multi-objective reward function, considering the impact of varying MAP placement on wireless backhaul connectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#35770;&#25991;&#39046;&#22495;&#20013;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;11&#20010;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#26088;&#22312;&#25913;&#36827;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06840</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#28151;&#21512;&#26684;&#32593;&#21355;&#26143;&#21644;&#27979;&#37327;&#38477;&#27700;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Ensemble learning for blending gridded satellite and gauge-measured precipitation data. (arXiv:2307.06840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#35770;&#25991;&#39046;&#22495;&#20013;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;11&#20010;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#26088;&#22312;&#25913;&#36827;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25913;&#21892;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#24120;&#24120;&#20351;&#29992;&#22238;&#24402;&#31639;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22320;&#38754;&#30340;&#27979;&#37327;&#26159;&#22240;&#21464;&#37327;&#65292;&#21355;&#26143;&#25968;&#25454;&#26159;&#39044;&#27979;&#21464;&#37327;&#65292;&#36824;&#26377;&#22320;&#24418;&#22240;&#32032;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#23558;&#22810;&#20010;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#36275;&#22815;&#25968;&#37327;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#20197;&#25552;&#39640;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#22823;&#35268;&#27169;&#27604;&#36739;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#25972;&#20010;&#32654;&#22269;&#21644;15&#24180;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#24191;&#27867;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#36825;&#20010;&#29305;&#23450;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;11&#20010;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;PERSIANN&#65288;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36965;&#24863;&#20449;&#24687;&#20272;&#35745;&#38477;&#27700;&#65289;&#21644;IMERG&#65288;&#22810;&#26143;&#32852;&#21512;&#21453;&#28436;&#20272;&#35745;&#38477;&#27700;&#65289;&#30340;&#26376;&#24230;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression algorithms are regularly used for improving the accuracy of satellite precipitation products. In this context, ground-based measurements are the dependent variable and the satellite data are the predictor variables, together with topography factors. Alongside this, it is increasingly recognised in many fields that combinations of algorithms through ensemble learning can lead to substantial predictive performance improvements. Still, a sufficient number of ensemble learners for improving the accuracy of satellite precipitation products and their large-scale comparison are currently missing from the literature. In this work, we fill this specific gap by proposing 11 new ensemble learners in the field and by extensively comparing them for the entire contiguous United States and for a 15-year period. We use monthly data from the PERSIANN (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals f
&lt;/p&gt;</description></item><item><title>PC-Droid&#26159;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#21644;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#25552;&#20379;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06836</link><description>&lt;p&gt;
PC-Droid: &#26356;&#24555;&#30340;&#25193;&#25955;&#36895;&#24230;&#21644;&#25913;&#36827;&#30340;&#31890;&#23376;&#20113;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
PC-Droid: Faster diffusion and improved quality for particle cloud generation. (arXiv:2307.06836v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06836
&lt;/p&gt;
&lt;p&gt;
PC-Droid&#26159;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#21644;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#25552;&#20379;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;PC-JeDi&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PC-Droid&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#24133;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21943;&#27880;&#31890;&#23376;&#20113;&#12290;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#12289;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#24182;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20004;&#31181;&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#28508;&#21147;&#26469;&#30740;&#31350;&#29983;&#25104;&#36895;&#24230;&#21644;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26356;&#24555;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#36229;&#36234;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29983;&#25104;&#26102;&#38388;&#27604;PC-JeDi&#24555;&#19978;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the success of PC-JeDi we introduce PC-Droid, a substantially improved diffusion model for the generation of jet particle clouds. By leveraging a new diffusion formulation, studying more recent integration solvers, and training on all jet types simultaneously, we are able to achieve state-of-the-art performance for all types of jets across all evaluation metrics. We study the trade-off between generation speed and quality by comparing two attention based architectures, as well as the potential of consistency distillation to reduce the number of diffusion steps. Both the faster architecture and consistency models demonstrate performance surpassing many competing models, with generation time up to two orders of magnitude faster than PC-JeDi.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38647;&#36798;&#36741;&#21161;&#21160;&#24577;&#38459;&#25377;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#21160;&#24577;&#25143;&#22806;&#29615;&#22659;&#20013;&#27627;&#31859;&#27874;&#21644;&#22826;&#36203;&#20857;&#32593;&#32476;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#38459;&#25377;&#29366;&#24577;&#21644;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#20027;&#21160;&#20999;&#25442;&#25110;&#27874;&#26463;&#20999;&#25442;&#20943;&#23569;&#24310;&#36831;&#65292;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.06834</link><description>&lt;p&gt;
&#22312;&#32852;&#21512;mmWave&#32593;&#32476;&#20013;&#25552;&#39640;&#21487;&#38752;&#24615;&#65306;&#19968;&#31181;&#20351;&#29992;&#38647;&#36798;&#36741;&#21161;&#21160;&#24577;&#38459;&#25377;&#35782;&#21035;&#30340;&#23454;&#29992;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Enhancing Reliability in Federated mmWave Networks: A Practical and Scalable Solution using Radar-Aided Dynamic Blockage Recognition. (arXiv:2307.06834v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38647;&#36798;&#36741;&#21161;&#21160;&#24577;&#38459;&#25377;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#21160;&#24577;&#25143;&#22806;&#29615;&#22659;&#20013;&#27627;&#31859;&#27874;&#21644;&#22826;&#36203;&#20857;&#32593;&#32476;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#38459;&#25377;&#29366;&#24577;&#21644;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#20027;&#21160;&#20999;&#25442;&#25110;&#27874;&#26463;&#20999;&#25442;&#20943;&#23569;&#24310;&#36831;&#65292;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#25143;&#22806;&#29615;&#22659;&#20013;&#25552;&#39640;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#21644;&#22826;&#36203;&#20857;&#65288;THz&#65289;&#32593;&#32476;&#26381;&#21153;&#21487;&#38752;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#35270;&#36317;&#65288;LoS&#65289;&#36830;&#25509;&#24456;&#23481;&#26131;&#34987;&#31227;&#21160;&#30340;&#38556;&#30861;&#29289;&#65288;&#22914;&#20154;&#21644;&#36710;&#36742;&#65289;&#25171;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#38647;&#36798;&#36741;&#21161;&#21160;&#24577;&#38459;&#22622;&#35782;&#21035;&#65288;RaDaR&#65289;&#65292;&#21033;&#29992;&#38647;&#36798;&#27979;&#37327;&#21644;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21452;&#36755;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#38459;&#22622;&#29366;&#24577;&#21644;&#26102;&#38388;&#12290;&#36825;&#21487;&#20197;&#30830;&#23450;&#20027;&#21160;&#20999;&#25442;&#65288;PHO&#65289;&#25110;&#27874;&#26463;&#20999;&#25442;&#30340;&#26368;&#20339;&#28857;&#65292;&#20174;&#32780;&#20943;&#23569;5G&#26032;&#26080;&#32447;&#30005;&#31243;&#24207;&#24341;&#20837;&#30340;&#24310;&#36831;&#65292;&#24182;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20307;&#39564;&#65288;QoE&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#38647;&#36798;&#20256;&#24863;&#22120;&#26469;&#30417;&#25511;&#21644;&#36319;&#36394;&#29289;&#20307;&#30340;&#36816;&#21160;&#65292;&#29983;&#25104;&#26377;&#29992;&#20110;&#22330;&#26223;&#20998;&#26512;&#21644;&#39044;&#27979;&#30340;&#36317;&#31163;-&#35282;&#24230;&#21644;&#36317;&#31163;-&#36895;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces a new method to improve the dependability of millimeter-wave (mmWave) and terahertz (THz) network services in dynamic outdoor environments. In these settings, line-of-sight (LoS) connections are easily interrupted by moving obstacles like humans and vehicles. The proposed approach, coined as Radar-aided Dynamic blockage Recognition (RaDaR), leverages radar measurements and federated learning (FL) to train a dual-output neural network (NN) model capable of simultaneously predicting blockage status and time. This enables determining the optimal point for proactive handover (PHO) or beam switching, thereby reducing the latency introduced by 5G new radio procedures and ensuring high quality of experience (QoE). The framework employs radar sensors to monitor and track objects movement, generating range-angle and range-velocity maps that are useful for scene analysis and predictions. Moreover, FL provides additional benefits such as privacy protection, scalability, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;&#29926;&#22622;&#23572;&#26364;&#21644;&#21345;&#20195;&#32435;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#23450;&#29702;&#29992;&#20110;&#22788;&#29702;&#19982;&#20284;&#28982;&#20989;&#25968;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#32467;&#26524;&#23545;&#20110;&#24037;&#31243;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.06831</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#23450;&#29702;&#29992;&#20110;&#19978;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
A Novel Bayes' Theorem for Upper Probabilities. (arXiv:2307.06831v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;&#29926;&#22622;&#23572;&#26364;&#21644;&#21345;&#20195;&#32435;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#23450;&#29702;&#29992;&#20110;&#22788;&#29702;&#19982;&#20284;&#28982;&#20989;&#25968;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#32467;&#26524;&#23545;&#20110;&#24037;&#31243;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20182;&#20204;1990&#24180;&#30340;&#24320;&#21019;&#24615;&#35770;&#25991;&#20013;&#65292;&#29926;&#22622;&#23572;&#26364;&#21644;&#21345;&#20195;&#32435;&#24314;&#31435;&#20102;&#22312;&#20808;&#39564;&#27010;&#29575;&#20301;&#20110;&#27010;&#29575;&#31867;&#21035;P&#65292;&#19988;&#20284;&#28982;&#20989;&#25968;&#26159;&#31934;&#30830;&#20989;&#25968;&#26102;&#65292;&#21487;&#27979;&#38598;A&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#27010;&#29575;&#30340;&#19978;&#38480;&#12290;&#20182;&#20204;&#36824;&#32473;&#20986;&#20102;&#36825;&#31181;&#19978;&#38480;&#25104;&#31435;&#26102;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#22788;&#29702;&#19982;&#20284;&#28982;&#20989;&#25968;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#24341;&#20837;&#20182;&#20204;&#32467;&#26524;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#24403;&#20808;&#39564;&#27010;&#29575;&#21644;&#20284;&#28982;&#20989;&#25968;&#37117;&#23646;&#20110;&#19968;&#32452;&#27010;&#29575;&#26102;&#30340;&#21518;&#39564;&#27010;&#29575;&#19978;&#38480;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#36825;&#31181;&#19978;&#38480;&#25104;&#20026;&#31561;&#24335;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20010;&#32467;&#26524;&#26412;&#36523;&#24456;&#26377;&#36259;&#65292;&#24182;&#19988;&#26377;&#21487;&#33021;&#24212;&#29992;&#20110;&#24037;&#31243;&#39046;&#22495;&#65288;&#20363;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their seminal 1990 paper, Wasserman and Kadane establish an upper bound for the Bayes' posterior probability of a measurable set $A$, when the prior lies in a class of probability measures $\mathcal{P}$ and the likelihood is precise. They also give a sufficient condition for such upper bound to hold with equality. In this paper, we introduce a generalization of their result by additionally addressing uncertainty related to the likelihood. We give an upper bound for the posterior probability when both the prior and the likelihood belong to a set of probabilities. Furthermore, we give a sufficient condition for this upper bound to become an equality. This result is interesting on its own, and has the potential of being applied to various fields of engineering (e.g. model predictive control), machine learning, and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#24120;&#35265;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#24605;&#24819;&#12289;&#29702;&#35770;&#22522;&#30784;&#21644;&#26041;&#27861;&#20851;&#31995;&#31561;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#21457;&#23637;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06825</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#24120;&#35265;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Causal Framework to Unify Common Domain Generalization Approaches. (arXiv:2307.06825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#24120;&#35265;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#24605;&#24819;&#12289;&#29702;&#35770;&#22522;&#30784;&#21644;&#26041;&#27861;&#20851;&#31995;&#31561;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#21457;&#23637;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#25351;&#23398;&#20064;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19982;&#35757;&#32451;&#39046;&#22495;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#26032;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#23427;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#24050;&#25552;&#20986;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#19981;&#21516;&#30340;&#26041;&#27861;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20351;&#24471;&#24456;&#38590;&#23545;&#36825;&#20010;&#39046;&#22495;&#26377;&#19968;&#20010;&#25972;&#20307;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#24182;&#22312;&#26694;&#26550;&#20013;&#23545;&#24120;&#35265;&#30340;DG&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#20197;&#19979;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65306;(1)&#27599;&#20010;DG&#26041;&#27861;&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20160;&#20040;&#65311;(2)&#29702;&#35770;&#19978;&#20026;&#20160;&#20040;&#26399;&#26395;&#23427;&#25913;&#21892;&#23545;&#26032;&#39046;&#22495;&#30340;&#25512;&#24191;&#33021;&#21147;&#65311;(3)&#19981;&#21516;&#30340;DG&#26041;&#27861;&#20043;&#38388;&#22914;&#20309;&#30456;&#20851;&#65292;&#26377;&#20160;&#20040;&#30456;&#23545;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65311;&#36890;&#36807;&#25552;&#20379;&#23545;DG&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20013;&#30340;&#21407;&#21017;&#24182;&#24320;&#21457;&#20986;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s). It is a fundamental problem in machine learning and has attracted much attention in recent years. A large number of approaches have been proposed. Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area. In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework. Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method? (2) Why is it expected to improve generalization to new domains theoretically? (3) How are different DG methods related to each other and what are relative advantages and limitations? By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective
&lt;/p&gt;</description></item><item><title>CLAIMED&#26159;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#26500;&#24314;&#21487;&#37325;&#29992;&#30340;&#36816;&#31639;&#31526;&#21644;&#21487;&#25193;&#23637;&#30340;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06824</link><description>&lt;p&gt;
CLAIMED -- &#22312;&#31185;&#23398;&#21152;&#36895;&#21457;&#29616;&#20013;&#26500;&#24314;&#31895;&#31890;&#24230;&#36816;&#31639;&#31526;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science. (arXiv:2307.06824v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06824
&lt;/p&gt;
&lt;p&gt;
CLAIMED&#26159;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#26500;&#24314;&#21487;&#37325;&#29992;&#30340;&#36816;&#31639;&#31526;&#21644;&#21487;&#25193;&#23637;&#30340;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#65292;&#37325;&#22797;&#24615;&#21644;&#37325;&#29992;&#24615;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#31185;&#23398;&#23478;&#22312;&#20174;&#25968;&#25454;&#21040;&#20986;&#29256;&#30340;&#36807;&#31243;&#20013;&#26377;&#30528;&#20016;&#23500;&#30340;&#32463;&#39564;&#12290;&#23613;&#31649;&#19968;&#20123;&#20986;&#29256;&#28192;&#36947;&#35201;&#27714;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#33719;&#24471;&#65292;&#20294;&#37325;&#26032;&#36816;&#34892;&#21644;&#39564;&#35777;&#23454;&#39564;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#26631;&#20934;&#12290;&#22240;&#27492;&#65292;&#37325;&#29992;&#29616;&#26377;&#30340;&#31185;&#23398;&#25968;&#25454;&#22788;&#29702;&#20195;&#30721;&#26469;&#33258;&#26368;&#26032;&#30740;&#31350;&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#25105;&#20204;&#24341;&#20837;CLAIMED&#65292;&#23427;&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#35299;&#20915;&#20102;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#38382;&#39064;&#30340;&#31185;&#23398;&#30740;&#31350;&#23454;&#36341;&#12290;CLAIMED&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25903;&#25345;&#31185;&#23398;&#23478;&#20174;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#31185;&#23398;&#36816;&#31639;&#31526;&#24211;&#20013;&#37325;&#26032;&#32452;&#21512;&#24037;&#20316;&#27969;&#31243;&#65292;&#26469;&#26500;&#24314;&#21487;&#37325;&#29992;&#30340;&#36816;&#31639;&#31526;&#21644;&#21487;&#25193;&#23637;&#30340;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#23454;&#29616;&#65292;&#20294;CLAIMED&#26159;&#32534;&#31243;&#35821;&#35328;&#12289;&#31185;&#23398;&#24211;&#21644;&#25191;&#34892;&#29615;&#22659;&#26080;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern data-driven science, reproducibility and reusability are key challenges. Scientists are well skilled in the process from data to publication. Although some publication channels require source code and data to be made accessible, rerunning and verifying experiments is usually hard due to a lack of standards. Therefore, reusing existing scientific data processing code from state-of-the-art research is hard as well. This is why we introduce CLAIMED, which has a proven track record in scientific research for addressing the repeatability and reusability issues in modern data-driven science. CLAIMED is a framework to build reusable operators and scalable scientific workflows by supporting the scientist to draw from previous work by re-composing workflows from existing libraries of coarse-grained scientific operators. Although various implementations exist, CLAIMED is programming language, scientific library, and execution environment agnostic.
&lt;/p&gt;</description></item><item><title>TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2307.06822</link><description>&lt;p&gt;
TinyMetaFed: &#39640;&#25928;&#30340;&#29992;&#20110;TinyML&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06822
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning (TinyML)&#39046;&#22495;&#22312;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#65289;&#19978;&#23454;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#20351;TinyML&#24212;&#29992;&#21463;&#30410;&#12290;&#32852;&#37030;&#20803;&#23398;&#20064;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#31572;&#26696;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;TinyML&#30828;&#20214;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#28304;&#12289;&#38544;&#31169;&#21644;&#36890;&#20449;&#38480;&#21046;&#32780;&#19981;&#23454;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TinyMetaFed&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;TinyMetaFed&#20419;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#26032;&#35774;&#22791;&#19978;&#24555;&#36895;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#37096;&#20998;&#26412;&#22320;&#37325;&#26500;&#21644;Top-P%&#36873;&#25321;&#24615;&#36890;&#20449;&#25552;&#20379;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23398;&#20064;&#25968;&#23383;&#21518;&#21521;&#20256;&#25773;&#65288;LDBP&#65289;&#26469;&#22343;&#34913;&#21452;&#26497;&#21270;&#20809;&#32420;&#20256;&#36755;&#20013;&#30340;&#33394;&#25955;&#31649;&#29702;&#65288;DM&#65289;&#38142;&#36335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21333;&#20449;&#36947;&#21644;WDM&#20256;&#36755;&#20013;&#65292;LDBP&#30456;&#27604;&#20110;&#32447;&#24615;&#22343;&#34913;&#21644;DBP&#20998;&#21035;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20449;&#22122;&#27604;&#25913;&#21892;&#21644;Q&#22240;&#23376;&#22686;&#30410;&#12290;&#27492;&#22806;&#65292;&#39057;&#22495;&#23454;&#29616;LDBP&#21644;DBP&#22312;&#22797;&#26434;&#24615;&#19978;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.06821</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#25968;&#23383;&#21518;&#21521;&#20256;&#25773;&#22312;&#33394;&#25955;&#31649;&#29702;&#31995;&#32479;&#20013;&#36827;&#34892;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Equalization in Dispersion-Managed Systems Using Learned Digital Back-Propagation. (arXiv:2307.06821v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23398;&#20064;&#25968;&#23383;&#21518;&#21521;&#20256;&#25773;&#65288;LDBP&#65289;&#26469;&#22343;&#34913;&#21452;&#26497;&#21270;&#20809;&#32420;&#20256;&#36755;&#20013;&#30340;&#33394;&#25955;&#31649;&#29702;&#65288;DM&#65289;&#38142;&#36335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21333;&#20449;&#36947;&#21644;WDM&#20256;&#36755;&#20013;&#65292;LDBP&#30456;&#27604;&#20110;&#32447;&#24615;&#22343;&#34913;&#21644;DBP&#20998;&#21035;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20449;&#22122;&#27604;&#25913;&#21892;&#21644;Q&#22240;&#23376;&#22686;&#30410;&#12290;&#27492;&#22806;&#65292;&#39057;&#22495;&#23454;&#29616;LDBP&#21644;DBP&#22312;&#22797;&#26434;&#24615;&#19978;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23398;&#20064;&#25968;&#23383;&#21518;&#21521;&#20256;&#25773;&#65288;LDBP&#65289;&#26469;&#22343;&#34913;&#21452;&#26497;&#21270;&#20809;&#32420;&#20256;&#36755;&#20013;&#30340;&#33394;&#25955;&#31649;&#29702;&#65288;DM&#65289;&#38142;&#36335;&#12290; LDBP&#26159;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;DBP&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;WDM&#21452;&#26497;&#21270;&#20809;&#32420;&#20256;&#36755;&#31995;&#32479;&#20013;&#35780;&#20272;&#20102;DBP&#21644;LDBP&#65292;&#35813;&#31995;&#32479;&#20197;&#27599;&#20010;&#36890;&#36947;256 Gbit / s&#30340;&#27604;&#29305;&#29575;&#36816;&#34892;&#65292;&#20855;&#26377;&#20026;2016 km&#38142;&#36335;&#35774;&#35745;&#30340;15&#65285;&#21097;&#20313;&#33394;&#25955;&#30340;&#33394;&#25955;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21333;&#20449;&#36947;&#20256;&#36755;&#20013;&#65292;LDBP&#20998;&#21035;&#27604;&#32447;&#24615;&#22343;&#34913;&#21644;DBP&#23454;&#29616;&#20102;6.3 dB&#21644;2.5 dB&#30340;&#26377;&#25928;&#20449;&#22122;&#27604;&#25913;&#21892;&#12290;&#22312;WDM&#20256;&#36755;&#20013;&#65292;&#30456;&#24212;&#30340;$Q$&#22240;&#23376;&#22686;&#30410;&#20998;&#21035;&#20026;1.1 dB&#21644;0.4 dB&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#39057;&#22495;&#23454;&#29616;LDBP&#21644;DBP&#22312;&#22797;&#26434;&#24615;&#26041;&#38754;&#27604;&#26102;&#22495;&#23454;&#29616;&#26356;&#26377;&#20248;&#21183;&#12290;&#36825;&#20123;&#21457;&#29616;&#35777;&#26126;&#20102;LDBP&#22312;&#22343;&#34913;&#33394;&#25955;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the use of the learned digital back-propagation (LDBP) for equalizing dual-polarization fiber-optic transmission in dispersion-managed (DM) links. LDBP is a deep neural network that optimizes the parameters of DBP using the stochastic gradient descent. We evaluate DBP and LDBP in a simulated WDM dual-polarization fiber transmission system operating at the bitrate of 256 Gbit/s per channel, with a dispersion map designed for a 2016 km link with 15% residual dispersion. Our results show that in single-channel transmission, LDBP achieves an effective signal-to-noise ratio improvement of 6.3 dB and 2.5 dB, respectively, over linear equalization and DBP. In WDM transmission, the corresponding $Q$-factor gains are 1.1 dB and 0.4 dB, respectively. Additionally, we conduct a complexity analysis, which reveals that a frequency-domain implementation of LDBP and DBP is more favorable in terms of complexity than the time-domain implementation. These findings demonstra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#38750;&#32447;&#24615;&#21442;&#25968;&#27169;&#22411;&#38477;&#38454;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#38469;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06816</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#23618;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#38750;&#32447;&#24615;&#21442;&#25968;&#27169;&#22411;&#38477;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder. (arXiv:2307.06816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#38750;&#32447;&#24615;&#21442;&#25968;&#27169;&#22411;&#38477;&#38454;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#38469;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#21442;&#25968;&#27169;&#22411;&#38477;&#38454;&#65288;MOR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;LSH-VAE&#65289;&#65292;&#33021;&#22815;&#38024;&#23545;&#20855;&#26377;&#22823;&#37327;&#33258;&#30001;&#24230;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#21442;&#25968;&#25554;&#20540;&#30340;&#38750;&#32447;&#24615;MOR&#12290;LSH-VAE&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#25913;&#21464;&#25552;&#39640;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65306;&#20998;&#23618;&#28145;&#24230;&#32467;&#26500;&#21644;&#28151;&#21512;&#21152;&#26435;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;&#38750;&#32447;&#24615;MOR&#26041;&#27861;&#12289;&#33258;&#32534;&#30721;&#22120;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30456;&#27604;&#65292;&#36825;&#20123;&#25913;&#36827;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;LSH-VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28508;&#22312;&#27969;&#24418;&#29699;&#32447;&#24615;&#25554;&#20540;&#30340;&#21442;&#25968;MOR&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#19977;&#20010;&#38750;&#32447;&#24615;&#21644;&#22810;&#29289;&#29702;&#21160;&#24577;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A data-driven parametric model order reduction (MOR) method using a deep artificial neural network is proposed. The present network, which is the least-squares hierarchical variational autoencoder (LSH-VAE), is capable of performing nonlinear MOR for the parametric interpolation of a nonlinear dynamic system with a significant number of degrees of freedom. LSH-VAE exploits two major changes to the existing networks: a hierarchical deep structure and a hybrid weighted, probabilistic loss function. The enhancements result in a significantly improved accuracy and stability compared against the conventional nonlinear MOR methods, autoencoder, and variational autoencoder. Upon LSH-VAE, a parametric MOR framework is presented based on the spherically linear interpolation of the latent manifold. The present framework is validated and evaluated on three nonlinear and multiphysics dynamic systems. First, the present framework is evaluated on the fluid-structure interaction benchmark problem to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#21644;&#29983;&#25104;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06797</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#30340;&#24555;&#36895;&#19988;&#21151;&#33021;&#24615;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics. (arXiv:2307.06797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#21644;&#29983;&#25104;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65288;&#22914;&#20154;&#21475;&#22522;&#22240;&#32452;&#23398;&#12289;RNA&#25110;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#65289;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#26631;&#31614;&#29305;&#23450;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#28151;&#21512;&#25928;&#29575;&#20302;&#19979;&#32780;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#24433;&#21709;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#22686;&#21152;&#20102;&#29983;&#25104;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21033;&#29992;&#38750;&#24179;&#34913;&#25928;&#24212;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26679;&#26412;&#30340;&#27491;&#30830;&#20998;&#31867;&#33021;&#21147;&#65292;&#24182;&#21482;&#38656;&#23569;&#25968;&#20960;&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20854;&#25104;&#21151;&#24212;&#29992;&#20110;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#24471;&#21040;&#35777;&#26126;&#65306;&#25163;&#20889;&#25968;&#23383;&#65292;&#25353;&#22823;&#38470;&#36215;&#28304;&#20998;&#31867;&#30340;&#20154;&#31867;&#22522;&#22240;&#32452;&#31361;&#21464;&#65292;&#37238;&#34507;&#30333;&#23478;&#26063;&#30340;&#21151;&#33021;&#24207;&#21015;&#65292;&#20197;&#21450;&#29305;&#23450;&#20998;&#31867;&#27861;&#30340;&#21516;&#28304;RNA&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we address the challenge of using energy-based models to produce high-quality, label-specific data in complex structured datasets, such as population genetics, RNA or protein sequences data. Traditional training methods encounter difficulties due to inefficient Markov chain Monte Carlo mixing, which affects the diversity of synthetic data and increases generation times. To address these issues, we use a novel training algorithm that exploits non-equilibrium effects. This approach, applied on the Restricted Boltzmann Machine, improves the model's ability to correctly classify samples and generate high-quality synthetic data in only a few sampling steps. The effectiveness of this method is demonstrated by its successful application to four different types of data: handwritten digits, mutations of human genomes classified by continental origin, functionally characterized sequences of an enzyme protein family, and homologous RNA sequences from specific taxonomies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#32420;&#30340;&#35270;&#35273;&#21644;&#35302;&#35273;&#20256;&#24863;&#36827;&#34892;&#35010;&#32541;&#23450;&#20301;&#21644;&#26816;&#27979;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35010;&#32541;&#25506;&#27979;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#22120;&#21644;&#35302;&#35273;&#25968;&#25454;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06784</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20351;&#29992;&#35270;&#35273;&#21644;&#35302;&#35273;&#20256;&#24863;&#36827;&#34892;&#34920;&#38754;&#25506;&#27979;&#20197;&#23454;&#29616;&#35010;&#32541;&#30340;&#26816;&#27979;&#21644;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation. (arXiv:2307.06784v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#32420;&#30340;&#35270;&#35273;&#21644;&#35302;&#35273;&#20256;&#24863;&#36827;&#34892;&#35010;&#32541;&#23450;&#20301;&#21644;&#26816;&#27979;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35010;&#32541;&#25506;&#27979;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#22120;&#21644;&#35302;&#35273;&#25968;&#25454;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#32420;&#30340;&#35270;&#35273;&#21644;&#35302;&#35273;&#20998;&#26512;&#30340;&#35010;&#32541;&#23450;&#20301;&#21644;&#26816;&#27979;&#31639;&#27861;&#12290;&#21033;&#29992;&#20809;&#32420;&#21046;&#25104;&#30340;&#25351;&#29366;&#20256;&#24863;&#22120;&#29992;&#20110;&#25968;&#25454;&#37319;&#38598;&#65292;&#20197;&#25910;&#38598;&#20998;&#26512;&#21644;&#23454;&#39564;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#26816;&#27979;&#35010;&#32541;&#30340;&#21487;&#33021;&#20301;&#32622;&#65292;&#20351;&#29992;&#25668;&#20687;&#22836;&#23545;&#29615;&#22659;&#36827;&#34892;&#25195;&#25551;&#65292;&#24182;&#36816;&#34892;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#12290;&#19968;&#26086;&#26816;&#27979;&#21040;&#35010;&#32541;&#65292;&#20174;&#35010;&#32541;&#30340;&#39592;&#26550;&#21270;&#29256;&#26412;&#21019;&#24314;&#19968;&#20010;&#20840;&#36830;&#25509;&#22270;&#12290;&#28982;&#21518;&#37319;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#35745;&#31639;&#25506;&#27979;&#35010;&#32541;&#30340;&#26368;&#30701;&#36335;&#24452;&#65292;&#29992;&#20110;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#24050;&#24320;&#21457;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#36816;&#21160;&#35268;&#21010;&#22120;&#23558;&#35010;&#32541;&#21010;&#20998;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#28982;&#21518;&#36880;&#20010;&#25506;&#27979;&#12290;&#28982;&#21518;&#65292;&#26426;&#26800;&#33218;&#24320;&#22987;&#25506;&#27979;&#24182;&#25191;&#34892;&#35302;&#35273;&#25968;&#25454;&#20998;&#31867;&#65292;&#20197;&#30830;&#35748;&#35813;&#20301;&#32622;&#26159;&#21542;&#30495;&#30340;&#23384;&#22312;&#35010;&#32541;&#65292;&#25110;&#32773;&#21482;&#26159;&#35270;&#35273;&#31639;&#27861;&#30340;&#35823;&#25253;&#12290;&#22914;&#26524;&#26816;&#27979;&#21040;&#35010;&#32541;&#65292;&#21017;&#36824;&#36827;&#34892;&#35010;&#32541;&#23450;&#37327;&#29305;&#24449;&#21270;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics. A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments. To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm. Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack. A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator. The motion planner divides the crack into multiple nodes which are then explored individually. Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm. If a crack is detected, also the le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KM-MAML&#30340;&#22810;&#27169;&#24577;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#28436;&#21270;&#33021;&#21147;&#21644;&#36229;&#32593;&#32476;&#29983;&#25104;&#27169;&#24577;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20197;&#29992;&#20110;&#22810;&#31181;&#21644;&#26410;&#30693;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2307.06771</link><description>&lt;p&gt;
&#23558;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;MRI&#37325;&#24314;&#25512;&#24191;&#21040;&#22810;&#31181;&#21644;&#26410;&#30693;&#23545;&#27604;&#24230;&#30340;&#20803;&#23398;&#20064;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalizing Supervised Deep Learning MRI Reconstruction to Multiple and Unseen Contrasts using Meta-Learning Hypernetworks. (arXiv:2307.06771v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KM-MAML&#30340;&#22810;&#27169;&#24577;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#28436;&#21270;&#33021;&#21147;&#21644;&#36229;&#32593;&#32476;&#29983;&#25104;&#27169;&#24577;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20197;&#29992;&#20110;&#22810;&#31181;&#21644;&#26410;&#30693;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#25805;&#20316;&#65292;&#24182;&#26377;&#21161;&#20110;&#25512;&#36827;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#27492;&#22806;&#65292;&#20803;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#29992;&#20110;&#21508;&#31181;&#22270;&#20687;&#20219;&#21153;&#30340;&#20849;&#20139;&#21644;&#21028;&#21035;&#26435;&#37325;&#26469;&#22686;&#24378;&#22270;&#20687;&#20219;&#21153;&#30340;&#30693;&#35782;&#25512;&#24191;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#35797;&#22270;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#38598;&#21512;&#65292;&#36825;&#21487;&#33021;&#23545;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#35828;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#28436;&#21270;&#33021;&#21147;&#26469;&#21253;&#25324;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#37319;&#38598;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31216;&#20026;KM-MAML&#65288;&#22522;&#20110;&#26680;&#35843;&#21046;&#30340;&#22810;&#27169;&#24577;&#20803;&#23398;&#20064;&#65289;&#65292;&#20855;&#26377;&#36827;&#21270;&#30340;&#36229;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#27169;&#24577;&#29305;&#23450;&#30340;&#26435;&#37325;&#12290;&#36825;&#20123;&#26435;&#37325;&#36890;&#36807;&#37325;&#26032;&#26657;&#20934;&#22522;&#32593;&#32476;&#30340;&#27599;&#20010;&#26680;&#65292;&#20026;&#22810;&#20010;&#27169;&#24335;&#25552;&#20379;&#27169;&#24335;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning has recently been an emerging data-efficient learning technique for various medical imaging operations and has helped advance contemporary deep learning models. Furthermore, meta-learning enhances the knowledge generalization of the imaging tasks by learning both shared and discriminative weights for various configurations of imaging tasks. However, existing meta-learning models attempt to learn a single set of weight initializations of a neural network that might be restrictive for multimodal data. This work aims to develop a multimodal meta-learning model for image reconstruction, which augments meta-learning with evolutionary capabilities to encompass diverse acquisition settings of multimodal data. Our proposed model called KM-MAML (Kernel Modulation-based Multimodal Meta-Learning), has hypernetworks that evolve to generate mode-specific weights. These weights provide the mode-specific inductive bias for multiple modes by re-calibrating each kernel of the base network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21307;&#30103;&#39046;&#22495;&#30340;&#20154;&#21475;&#22270;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#36890;&#36807;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#36827;&#34892;&#23457;&#26680;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36825;&#19968;&#29305;&#23450;&#24046;&#20998;&#38544;&#31169;&#24212;&#29992;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#22270;&#30340;&#21516;&#36136;&#24615;&#31243;&#24230;&#19982;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06760</link><description>&lt;p&gt;
&#21307;&#30103;&#20154;&#21475;&#22270;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#19982;&#25928;&#29992;&#26435;&#34913;&#65306;&#26469;&#33258;&#24046;&#20998;&#38544;&#31169;&#21644;&#22270;&#32467;&#26500;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs: Insights from Differential Privacy and Graph Structure. (arXiv:2307.06760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21307;&#30103;&#39046;&#22495;&#30340;&#20154;&#21475;&#22270;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#36890;&#36807;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#36827;&#34892;&#23457;&#26680;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36825;&#19968;&#29305;&#23450;&#24046;&#20998;&#38544;&#31169;&#24212;&#29992;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#22270;&#30340;&#21516;&#36136;&#24615;&#31243;&#24230;&#19982;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21307;&#30103;&#39046;&#22495;&#30340;&#20154;&#21475;&#22270;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;&#22312;&#19981;&#21516;&#38544;&#31169;&#27700;&#24179;&#19979;&#30340;&#38544;&#31169;&#19982;&#25928;&#29992;&#30340;&#26435;&#34913;&#65292;&#24182;&#36890;&#36807;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#36827;&#34892;&#23457;&#26680;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36825;&#19968;&#29305;&#23450;&#24046;&#20998;&#38544;&#31169;&#24212;&#29992;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24213;&#23618;&#22270;&#32467;&#26500;&#19982;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#22270;&#30340;&#21516;&#36136;&#24615;&#31243;&#24230;&#19982;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#22240;&#32032;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate an empirical investigation into differentially private graph neural networks on population graphs from the medical domain by examining privacy-utility trade-offs at different privacy levels on both real-world and synthetic datasets and performing auditing through membership inference attacks. Our findings highlight the potential and the challenges of this specific DP application area. Moreover, we find evidence that the underlying graph structure constitutes a potential factor for larger performance gaps by showing a correlation between the degree of graph homophily and the accuracy of the trained model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Cramer&#36317;&#31163;&#65292;&#35813;&#36317;&#31163;&#20989;&#25968;&#22312;&#22810;&#20803;&#24773;&#20917;&#19979;&#20855;&#26377;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#19988;&#26131;&#20110;&#35745;&#31639;&#21644;&#23454;&#29616;&#65292;&#24182;&#22312;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.06753</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Cramer&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent. (arXiv:2307.06753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Cramer&#36317;&#31163;&#65292;&#35813;&#36317;&#31163;&#20989;&#25968;&#22312;&#22810;&#20803;&#24773;&#20917;&#19979;&#20855;&#26377;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#19988;&#26131;&#20110;&#35745;&#31639;&#21644;&#23454;&#29616;&#65292;&#24182;&#22312;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20197;&#20854;&#34920;&#36798;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#38395;&#21517;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32479;&#35745;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#24050;&#30693;&#31639;&#27861;&#21487;&#20197;&#25311;&#21512;&#25110;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20123;&#21253;&#25324;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#21644;&#20998;&#21106;Wasserstein&#36317;&#31163;&#12290;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#20860;&#23481;&#30340;&#31639;&#27861;&#26356;&#23569;&#65292;&#36825;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#24120;&#35265;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#32500;&#24773;&#20917;&#19979;&#20004;&#20010;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sliced Cramer 2&#36317;&#31163;&#30340;&#36317;&#31163;&#20989;&#25968;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#33324;&#30340;&#22810;&#20803;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#35768;&#22810;&#20808;&#21069;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#22312;&#19968;&#32500;&#24773;&#20917;&#19979;&#20855;&#26377;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65288;&#20363;&#22914;PyTorch&#65289;&#36827;&#34892;&#26131;&#20110;&#35745;&#31639;&#21644;&#23454;&#29616;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning. Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning. However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance. Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.  In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\'er 2-distance for learning general multivariate GMMs. Our approach has several advantages over many previous methods. First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06742</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach. (arXiv:2307.06742v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#32676;&#19968;&#20307;&#21270;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#22478;&#38469;&#26053;&#34892;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#36890;&#36807;&#23454;&#26045;&#38656;&#27714;&#21709;&#24212;&#24615;&#22686;&#24378;&#25514;&#26045;&#65292;&#26377;&#26395;&#21319;&#32423;&#20256;&#32479;&#30340;&#22478;&#38469;&#23458;&#36710;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#32447;&#25805;&#20316;&#21463;&#21040;&#36710;&#36742;&#36164;&#28304;&#20998;&#37197;&#21644;&#25340;&#36710;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#20043;&#38388;&#32806;&#21512;&#24615;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22312;&#32447;&#36710;&#38431;&#31649;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#26694;&#26550;&#30340;&#19978;&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#23553;&#24314;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21327;&#21516;&#20998;&#37197;&#38386;&#32622;&#36710;&#36742;&#21040;&#19981;&#21516;&#30340;&#22478;&#38469;&#32447;&#36335;&#65292;&#32780;&#22312;&#19979;&#23618;&#65292;&#21017;&#20351;&#29992;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#26356;&#26032;&#36710;&#36742;&#30340;&#36335;&#32447;&#12290;&#22522;&#20110;&#20013;&#22269;&#21414;&#38376;&#21450;&#20854;&#21608;&#36793;&#22478;&#24066;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integrated development of city clusters has given rise to an increasing demand for intercity travel. Intercity ride-pooling service exhibits considerable potential in upgrading traditional intercity bus services by implementing demand-responsive enhancements. Nevertheless, its online operations suffer the inherent complexities due to the coupling of vehicle resource allocation among cities and pooled-ride vehicle routing. To tackle these challenges, this study proposes a two-level framework designed to facilitate online fleet management. Specifically, a novel multi-agent feudal reinforcement learning model is proposed at the upper level of the framework to cooperatively assign idle vehicles to different intercity lines, while the lower level updates the routes of vehicles using an adaptive large neighborhood search heuristic. Numerical studies based on the realistic dataset of Xiamen and its surrounding cities in China show that the proposed framework effectively mitigates the supp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPR-Net&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#22810;&#23610;&#24230;&#21382;&#21490;&#24207;&#21015;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#27169;&#24335;&#37325;&#29616;&#30340;&#20808;&#39564;&#30693;&#35782;&#26500;&#24314;&#27169;&#24335;&#25193;&#23637;&#39044;&#27979;&#26041;&#27861;&#65292;&#26368;&#21518;&#20351;&#29992;&#21453;&#21367;&#31215;&#36816;&#31639;&#23558;&#26410;&#26469;&#27169;&#24335;&#37325;&#24314;&#20026;&#26410;&#26469;&#24207;&#21015;&#12290;MPR-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#24182;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.06736</link><description>&lt;p&gt;
MPR-Net: &#22810;&#23610;&#24230;&#27169;&#24335;&#37325;&#29616;&#24341;&#23548;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting. (arXiv:2307.06736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPR-Net&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#22810;&#23610;&#24230;&#21382;&#21490;&#24207;&#21015;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#27169;&#24335;&#37325;&#29616;&#30340;&#20808;&#39564;&#30693;&#35782;&#26500;&#24314;&#27169;&#24335;&#25193;&#23637;&#39044;&#27979;&#26041;&#27861;&#65292;&#26368;&#21518;&#20351;&#29992;&#21453;&#21367;&#31215;&#36816;&#31639;&#23558;&#26410;&#26469;&#27169;&#24335;&#37325;&#24314;&#20026;&#26410;&#26469;&#24207;&#21015;&#12290;MPR-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#24182;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#22266;&#26377;&#30340;&#25361;&#25112;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#30740;&#31350;&#30340;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#21382;&#21490;&#24207;&#21015;&#20013;&#30340;&#26377;&#25928;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#39044;&#27979;&#12290;&#22522;&#20110;&#28857;&#23545;&#28857;&#36830;&#25509;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;Transformer&#26550;&#26500;&#30340;&#20808;&#36827;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#65292;&#20294;&#20854;&#36741;&#21161;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26500;&#22266;&#26377;&#22320;&#30772;&#22351;&#20102;&#26102;&#38388;&#39034;&#24207;&#65292;&#38477;&#20302;&#20102;&#20449;&#24687;&#21033;&#29992;&#29575;&#65292;&#24182;&#20351;&#39044;&#27979;&#36807;&#31243;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;MPR-Net&#12290;&#23427;&#39318;&#20808;&#20351;&#29992;&#21367;&#31215;&#36816;&#31639;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#22810;&#23610;&#24230;&#21382;&#21490;&#24207;&#21015;&#27169;&#24335;&#65292;&#28982;&#21518;&#22522;&#20110;&#27169;&#24335;&#37325;&#29616;&#30340;&#20808;&#39564;&#30693;&#35782;&#26500;&#24314;&#19968;&#31181;&#27169;&#24335;&#25193;&#23637;&#39044;&#27979;&#26041;&#27861;&#65292;&#26368;&#21518;&#20351;&#29992;&#21453;&#21367;&#31215;&#36816;&#31639;&#23558;&#26410;&#26469;&#27169;&#24335;&#37325;&#24314;&#20026;&#26410;&#26469;&#24207;&#21015;&#12290;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;MPR-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#24182;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting has received wide interest from existing research due to its broad applications and inherent challenging. The research challenge lies in identifying effective patterns in historical series and applying them to future forecasting. Advanced models based on point-wise connected MLP and Transformer architectures have strong fitting power, but their secondary computational complexity limits practicality. Additionally, those structures inherently disrupt the temporal order, reducing the information utilization and making the forecasting process uninterpretable. To solve these problems, this paper proposes a forecasting model, MPR-Net. It first adaptively decomposes multi-scale historical series patterns using convolution operation, then constructs a pattern extension forecasting method based on the prior knowledge of pattern reproduction, and finally reconstructs future patterns into future series using deconvolution operation. By leveraging the temporal dependencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#36924;&#36817;&#27604;&#30340;&#23545;&#25968;&#32423;&#21035;&#28145;&#24230;&#24182;&#34892;&#31639;&#27861;&#26469;&#35299;&#20915;&#20851;&#32852;&#32858;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06723</link><description>&lt;p&gt;
&#22312;&#23545;&#25968;&#32423;&#21035;&#30340;&#36718;&#25968;&#20013;&#25171;&#30772;&#20851;&#32852;&#32858;&#31867;&#30340;3&#22240;&#23376;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Breaking 3-Factor Approximation for Correlation Clustering in Polylogarithmic Rounds. (arXiv:2307.06723v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#36924;&#36817;&#27604;&#30340;&#23545;&#25968;&#32423;&#21035;&#28145;&#24230;&#24182;&#34892;&#31639;&#27861;&#26469;&#35299;&#20915;&#20851;&#32852;&#32858;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20851;&#32852;&#32858;&#31867;&#38382;&#39064;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#20854;&#20013;&#27599;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#23454;&#20307;&#34987;&#26631;&#35760;&#20026;&#30456;&#20284;&#25110;&#19981;&#30456;&#20284;&#12290;&#30446;&#26631;&#26159;&#23558;&#23454;&#20307;&#20998;&#25104;&#32858;&#31867;&#65292;&#20197;&#26368;&#23567;&#21270;&#19982;&#26631;&#31614;&#30340;&#19981;&#19968;&#33268;&#25968;&#37327;&#12290;&#30446;&#21069;&#65292;&#25152;&#26377;&#39640;&#25928;&#30340;&#24182;&#34892;&#31639;&#27861;&#30340;&#36924;&#36817;&#27604;&#33267;&#23569;&#20026;3&#12290;&#19982;&#22810;&#39033;&#24335;&#26102;&#38388;&#39034;&#24207;&#31639;&#27861;[CLN22]&#33719;&#24471;&#30340;$1.994+\epsilon$&#27604;&#29575;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#36924;&#36817;&#27604;&#30340;&#23545;&#25968;&#32423;&#21035;&#28145;&#24230;&#24182;&#34892;&#31639;&#27861;&#65292;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#35745;&#31639;&#20986;&#19968;&#20010;$(2.4+\epsilon)$-&#36817;&#20284;&#35299;&#65292;&#24182;&#20351;&#29992;$\tilde{O}(m^{1.5})$&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;$\tilde{O}(m^{1.5})$&#26102;&#38388;&#39034;&#24207;&#31639;&#27861;&#21644;&#19968;&#20010;&#20855;&#26377;$\tilde{O}(m^{1.5})$&#24635;&#20869;&#23384;&#30340;&#23545;&#25968;&#32423;&#21035;&#36718;&#25968;&#20122;&#32447;&#24615;&#20869;&#23384;MPC&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;Awerbuch&#65292;Khandekar&#21644;Rao&#30340;[AKR12]&#21463;&#38271;&#24230;&#32422;&#26463;&#30340;m&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study parallel algorithms for the correlation clustering problem, where every pair of two different entities is labeled with similar or dissimilar. The goal is to partition the entities into clusters to minimize the number of disagreements with the labels. Currently, all efficient parallel algorithms have an approximation ratio of at least 3. In comparison with the $1.994+\epsilon$ ratio achieved by polynomial-time sequential algorithms [CLN22], a significant gap exists.  We propose the first poly-logarithmic depth parallel algorithm that achieves a better approximation ratio than 3. Specifically, our algorithm computes a $(2.4+\epsilon)$-approximate solution and uses $\tilde{O}(m^{1.5})$ work. Additionally, it can be translated into a $\tilde{O}(m^{1.5})$-time sequential algorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with $\tilde{O}(m^{1.5})$ total memory.  Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12] length-constrained m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#31574;&#30053;&#21644;&#22870;&#21169;&#20272;&#35745;&#22120;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#23398;&#20064;&#22312;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06721</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#23548;&#21521;&#24335;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#34920;&#29616;&#20248;&#31168;&#65311;&#29702;&#35299;&#23545;&#25239;&#23398;&#20064;&#21450;&#20854;&#26367;&#20195;&#26041;&#27861;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative. (arXiv:2307.06721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#31574;&#30053;&#21644;&#22870;&#21169;&#20272;&#35745;&#22120;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#23398;&#20064;&#22312;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31574;&#30053;&#26159;&#26681;&#25454;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#30340;&#24403;&#21069;&#29366;&#24577;&#30830;&#23450;&#31995;&#32479;&#21160;&#20316;&#30340;&#20851;&#38190;&#12290;&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064; (RL) &#24050;&#25104;&#20026;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064; (DPL) &#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#22522;&#20110; RL &#30340; DPL &#20013;&#65292;&#26681;&#25454;&#22870;&#21169;&#26356;&#26032;&#23545;&#35805;&#31574;&#30053;&#12290;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#29366;&#24577;&#21160;&#20316;&#23545;&#32452;&#21512;&#30340;&#22810;&#39046;&#22495;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#22330;&#26223;&#65292;&#31934;&#32454;&#26500;&#24314;&#20687;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#30340;&#22870;&#21169;&#26469;&#26377;&#25928;&#25351;&#23548;&#23545;&#35805;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#31181;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#22870;&#21169;&#30340;&#26041;&#24335;&#26159;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064; (AL) &#21516;&#26102;&#35757;&#32451;&#22870;&#21169;&#20272;&#35745;&#22120;&#21644;&#23545;&#35805;&#31574;&#30053;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#22266;&#26377;&#30340;&#23545;&#25239;&#23398;&#20064;&#38382;&#39064;&#65288;&#20363;&#22914;&#27169;&#24335;&#22349;&#32553;&#65289;&#20063;&#21313;&#20998;&#26840;&#25163;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#23545;&#35805;&#31574;&#30053;&#21644;&#22870;&#21169;&#20272;&#35745;&#22120;&#30340;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#39318;&#20808;&#30830;&#23450;&#20102; AL &#22312; DPL &#20013;&#30340;&#20316;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#22522;&#20110;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.06713</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#26657;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#20808;&#39564;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#27491;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22823;&#37327;&#26080;&#30417;&#30563;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#26657;&#20934;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#26041;&#27861;&#36827;&#34892;&#36866;&#24212;&#20197;&#25191;&#34892;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#23558;LLM&#35270;&#20026;&#40657;&#30418;&#65292;&#22312;&#27169;&#22411;&#23631;&#38556;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#26657;&#20934;&#27169;&#22411;&#21518;&#39564;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#25552;&#31034;&#35757;&#32451;&#26679;&#26412;&#21644;&#26080;&#36866;&#24212;&#25968;&#25454;&#19979;&#30340;&#26657;&#20934;&#26041;&#27861;&#20013;&#20248;&#20110;&#26410;&#36866;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#33410;&#28857;&#25490;&#24207;&#12289;&#26680;&#20989;&#25968;&#21644;&#22270;&#23884;&#20837;&#30340;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;GRAN&#20248;&#20110;GraphRNN&#30340;&#20248;&#21183;&#65292;&#24182;&#20026;&#23567;&#22411;&#22270;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;GraphRNN&#25913;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#33410;&#28857;&#29305;&#24449;&#21021;&#22987;&#21270;&#30340;&#26368;&#20339;&#23454;&#36341;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2307.06709</link><description>&lt;p&gt;
GRAN&#20248;&#20110;GraphRNN&#65306;&#22522;&#20110;&#33410;&#28857;&#25490;&#24207;&#12289;&#26680;&#20989;&#25968;&#21644;&#22270;&#23884;&#20837;&#30340;&#24230;&#37327;&#29992;&#20110;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators. (arXiv:2307.06709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#33410;&#28857;&#25490;&#24207;&#12289;&#26680;&#20989;&#25968;&#21644;&#22270;&#23884;&#20837;&#30340;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;GRAN&#20248;&#20110;GraphRNN&#30340;&#20248;&#21183;&#65292;&#24182;&#20026;&#23567;&#22411;&#22270;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;GraphRNN&#25913;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#33410;&#28857;&#29305;&#24449;&#21021;&#22987;&#21270;&#30340;&#26368;&#20339;&#23454;&#36341;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#25552;&#20986;&#65292;&#24182;&#22312;&#33647;&#29289;&#21457;&#29616;&#12289;&#36947;&#36335;&#32593;&#32476;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#31243;&#24207;&#32508;&#21512;&#31561;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22270;&#38754;&#20020;&#30528;&#29702;&#35770;&#19978;&#30340;&#25361;&#25112;&#65292;&#22914;&#21516;&#26500;&#34920;&#31034;&#65292;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22256;&#38590;&#12290;&#22312;&#24212;&#29992;&#39046;&#22495;&#20013;&#65292;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#65311;&#25105;&#20204;&#23545;&#20998;&#24067;&#22270;&#19981;&#21464;&#37327;&#30340;&#26680;&#20989;&#25968;&#24230;&#37327;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#22312;&#22270;&#23884;&#20837;&#31354;&#38388;&#20013;&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#24418;&#21644;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#24230;&#37327;&#12290;&#22522;&#20110;&#27969;&#24418;&#30340;&#24230;&#37327;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#26469;&#27604;&#36739;&#20004;&#20010;&#33879;&#21517;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;GraphRNN&#21644;GRAN&#65292;&#24182;&#25581;&#31034;&#20102;&#33410;&#28857;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;GRAN&#20248;&#20110;GraphRNN&#65292;&#32780;&#19988;&#25105;&#20204;&#25552;&#20986;&#30340;&#20351;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#25490;&#24207;&#30340;GraphRNN&#36866;&#29992;&#20110;&#23567;&#22411;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#33410;&#28857;&#29305;&#24449;&#21021;&#22987;&#21270;&#30340;&#26368;&#20339;&#23454;&#36341;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of generative models for graphs have been proposed. They are used in drug discovery, road networks, neural architecture search, and program synthesis. Generating graphs has theoretical challenges, such as isomorphic representations -- evaluating how well a generative model performs is difficult. Which model to choose depending on the application domain?  We extensively study kernel-based metrics on distributions of graph invariants and manifold-based and kernel-based metrics in graph embedding space. Manifold-based metrics outperform kernel-based metrics in embedding space. We use these metrics to compare GraphRNN and GRAN, two well-known generative models for graphs, and unveil the influence of node orderings. It shows the superiority of GRAN over GraphRNN - further, our proposed adaptation of GraphRNN with a depth-first search ordering is effective for small-sized graphs.  A guideline on good practices regarding dataset selection and node feature initialization is prov
&lt;/p&gt;</description></item><item><title>S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06701</link><description>&lt;p&gt;
S-HR-VQVAE: &#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06701
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23558;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#26469;&#35299;&#20915;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;S-HR-VQVAE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;HR-VQVAE&#22312;&#23545;&#38745;&#27490;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#20869;&#22312;&#33021;&#21147;&#21644;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#21450;ST-PixelCNN&#22788;&#29702;&#26102;&#31354;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292; S-HR-VQVAE&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23398;&#20064;&#26102;&#31354;&#20449;&#24687;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#28040;&#38500;&#27169;&#31946;&#39044;&#27979;&#21644;&#38544;&#24335;&#24314;&#27169;&#29289;&#29702;&#29305;&#24615;&#12290;&#23545;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#19982;&#39030;&#32423;&#35270;&#39057;&#39044;&#27979;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
&lt;/p&gt;</description></item><item><title>IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06698</link><description>&lt;p&gt;
IntelliGraphs: &#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06698
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#25991;&#29486;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#19981;&#20165;&#20165;&#26159;&#38142;&#25509;&#30340;&#38598;&#21512;&#65292;&#36824;&#20855;&#26377;&#20854;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#12290;&#35821;&#20041;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26597;&#35810;&#22238;&#31572;&#25110;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23376;&#22270;&#25512;&#26029;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#24517;&#39035;&#29983;&#25104;&#21487;&#33021;&#30340;&#24182;&#19988;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IntelliGraphs&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#38598;&#21512;&#12290;IntelliGraphs&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#20256;&#32479;KGE&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#35821;&#20041;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#22522;&#20934;&#23558;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#27979;&#35797;&#24179;&#21488;&#19978;&#36827;&#34892;&#23884;&#20837;&#24335;SRAM&#32769;&#21270;&#20998;&#26512;&#65292;&#21457;&#29616;&#36890;&#36807;SRAM&#21021;&#22987;&#21270;&#20559;&#24046;&#21644;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#25351;&#26631;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#20351;&#29992;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.06693</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#27979;&#35797;&#24179;&#21488;&#19978;&#36827;&#34892;&#23884;&#20837;&#24335;SRAM&#32769;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ageing Analysis of Embedded SRAM on a Large-Scale Testbed Using Machine Learning. (arXiv:2307.06693v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06693
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#27979;&#35797;&#24179;&#21488;&#19978;&#36827;&#34892;&#23884;&#20837;&#24335;SRAM&#32769;&#21270;&#20998;&#26512;&#65292;&#21457;&#29616;&#36890;&#36807;SRAM&#21021;&#22987;&#21270;&#20559;&#24046;&#21644;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#25351;&#26631;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#20351;&#29992;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32769;&#21270;&#26816;&#27979;&#21644;&#25925;&#38556;&#39044;&#27979;&#22312;&#35768;&#22810;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#37096;&#32626;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#37096;&#32626;&#22312;&#37326;&#22806;&#26080;&#20154;&#30475;&#31649;&#22320;&#36816;&#34892;&#22810;&#24180;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#25968;&#37327;&#24040;&#22823;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#33324;&#29992;&#36884;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;154&#20010;&#26495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;SRAM&#32769;&#21270;&#23454;&#35777;&#20998;&#26512;&#12290;&#20174;&#27599;&#20010;&#33410;&#28857;&#22312;&#21551;&#21160;&#26102;&#23481;&#26131;&#25910;&#38598;&#21040;&#30340;SRAM&#21021;&#22987;&#21270;&#20559;&#24046;&#20986;&#21457;&#65292;&#25105;&#20204;&#24212;&#29992;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#25351;&#26631;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#35813;&#33410;&#28857;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#32769;&#21270;&#24433;&#21709;&#26159;&#24494;&#22937;&#30340;&#65292;&#20294;&#25105;&#20204;&#30340;&#25351;&#26631;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#20351;&#29992;&#26102;&#38388;&#65292;&#22238;&#24402;&#22120;&#30340;R2&#20998;&#25968;&#20026;0.77&#65292;&#24179;&#22343;&#35823;&#24046;&#20026;24%&#65292;&#24212;&#29992;&#20845;&#20010;&#26376;&#20998;&#36776;&#29575;&#30340;&#20998;&#31867;&#22120;&#30340;F1&#20998;&#25968;&#36229;&#36807;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;
Ageing detection and failure prediction are essential in many Internet of Things (IoT) deployments, which operate huge quantities of embedded devices unattended in the field for years. In this paper, we present a large-scale empirical analysis of natural SRAM wear-out using 154 boards from a general-purpose testbed. Starting from SRAM initialization bias, which each node can easily collect at startup, we apply various metrics for feature extraction and experiment with common machine learning methods to predict the age of operation for this node. Our findings indicate that even though ageing impacts are subtle, our indicators can well estimate usage times with an $R^2$ score of 0.77 and a mean error of 24% using regressors, and with an F1 score above 0.6 for classifiers applying a six-months resolution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Aeolus Ocean&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#20154;&#27700;&#38754;&#33322;&#34892;&#22120;&#33258;&#20027;COLREG&#21512;&#35268;&#23548;&#33322;&#30340;&#20223;&#30495;&#29615;&#22659;&#12290;&#36890;&#36807;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#22797;&#21046;&#30495;&#23454;&#30340;&#25805;&#20316;&#26465;&#20214;&#65292;&#21487;&#20197;&#20026;DRL&#21644;CV&#31639;&#27861;&#30340;&#24320;&#21457;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.06688</link><description>&lt;p&gt;
Aeolus Ocean -- &#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#28023;&#27915;&#29289;&#20307;&#26816;&#27979;&#30340;&#26080;&#20154;&#27700;&#38754;&#33322;&#34892;&#22120;&#33258;&#20027;COLREG&#21512;&#35268;&#23548;&#33322;&#30340;&#20223;&#30495;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Aeolus Ocean -- A simulation environment for the autonomous COLREG-compliant navigation of Unmanned Surface Vehicles using Deep Reinforcement Learning and Maritime Object Detection. (arXiv:2307.06688v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Aeolus Ocean&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#20154;&#27700;&#38754;&#33322;&#34892;&#22120;&#33258;&#20027;COLREG&#21512;&#35268;&#23548;&#33322;&#30340;&#20223;&#30495;&#29615;&#22659;&#12290;&#36890;&#36807;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#22797;&#21046;&#30495;&#23454;&#30340;&#25805;&#20316;&#26465;&#20214;&#65292;&#21487;&#20197;&#20026;DRL&#21644;CV&#31639;&#27861;&#30340;&#24320;&#21457;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28023;&#36816;&#39046;&#22495;&#65292;&#26397;&#30528;&#26080;&#20154;&#27700;&#38754;&#33322;&#34892;&#22120;&#65288;USV&#65289;&#30340;&#33322;&#34892;&#33258;&#20027;&#21270;&#21457;&#23637;&#65292;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23433;&#20840;&#24615;&#24182;&#20943;&#23569;&#36816;&#33829;&#25104;&#26412;&#65292;&#36824;&#21487;&#20197;&#20026;&#28023;&#27915;&#30740;&#31350;&#12289;&#25506;&#32034;&#21644;&#30417;&#27979;&#25552;&#20379;&#19968;&#31995;&#21015;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; USV &#25511;&#21046;&#31995;&#32479;&#24517;&#39035;&#33021;&#22815;&#23433;&#20840;&#21487;&#38752;&#22320;&#36981;&#23432;&#28023;&#19978;&#36991;&#30896;&#22269;&#38469;&#35268;&#23450;&#65288;COLREGs&#65289;&#65292;&#22312;&#19982;&#20854;&#20182;&#33337;&#21482;&#30456;&#36935;&#26102;&#25353;&#32473;&#23450;&#33322;&#28857;&#33322;&#34892;&#65292;&#21516;&#26102;&#21463;&#21040;&#30333;&#22825;&#25110;&#22812;&#38388;&#30340;&#30495;&#23454;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#21487;&#33021;&#30340;&#22810;&#31181;&#22330;&#26223;&#65292;&#24517;&#39035;&#26377;&#19968;&#20010;&#33021;&#22815;&#22797;&#21046;USVs&#23558;&#36935;&#21040;&#30340;&#30495;&#23454;&#25805;&#20316;&#26465;&#20214;&#30340;&#34394;&#25311;&#29615;&#22659;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20043;&#21069;&#36827;&#34892;&#27169;&#25311;&#12290;&#36825;&#31181;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#26159;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#31639;&#27861;&#29992;&#20110;&#24320;&#21457;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heading towards navigational autonomy in unmanned surface vehicles (USVs) in the maritime sector can fundamentally lead towards safer waters as well as reduced operating costs, while also providing a range of exciting new capabilities for oceanic research, exploration and monitoring. However, achieving such a goal is challenging. USV control systems must, safely and reliably, be able to adhere to the international regulations for preventing collisions at sea (COLREGs) in encounters with other vessels as they navigate to a given waypoint while being affected by realistic weather conditions, either during the day or at night. To deal with the multitude of possible scenarios, it is critical to have a virtual environment that is able to replicate the realistic operating conditions USVs will encounter, before they can be implemented in the real world. Such "digital twins" form the foundations upon which Deep Reinforcement Learning (DRL) and Computer Vision (CV) algorithms can be used to dev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#39044;&#27979;&#23454;&#26102;&#31227;&#21160;&#32593;&#32476;&#20013;VoIP&#27969;&#37327;&#30340;&#20851;&#38190;QoS/QoE&#25551;&#36848;&#31526;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#36816;&#33829;&#21830;&#20248;&#21270;&#32593;&#32476;&#35268;&#21010;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.06645</link><description>&lt;p&gt;
&#23454;&#26102;&#31227;&#21160;&#32593;&#32476;&#20013;VoIP&#27969;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series characterization and forecasting of VoIP traffic in real mobile networks. (arXiv:2307.06645v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#39044;&#27979;&#23454;&#26102;&#31227;&#21160;&#32593;&#32476;&#20013;VoIP&#27969;&#37327;&#30340;&#20851;&#38190;QoS/QoE&#25551;&#36848;&#31526;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#36816;&#33829;&#21830;&#20248;&#21270;&#32593;&#32476;&#35268;&#21010;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#31227;&#21160;&#32593;&#32476;&#20013;&#23454;&#26102;&#27969;&#37327;&#65288;&#20363;&#22914;VoIP&#65289;&#30340;&#34892;&#20026;&#65292;&#21487;&#20197;&#24110;&#21161;&#36816;&#33829;&#21830;&#26356;&#22909;&#22320;&#35268;&#21010;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#24182;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;VoIP&#27969;&#37327;&#22312;&#23454;&#38469;&#31227;&#21160;&#29615;&#22659;&#20013;&#20851;&#38190;QoS/QoE&#25551;&#36848;&#31526;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65288;&#20854;&#20013;&#19968;&#20123;&#22312;&#25216;&#26415;&#25991;&#29486;&#20013;&#34987;&#24573;&#30053;&#65289;&#12290;&#35813;&#38382;&#39064;&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#21487;&#20197;&#21457;&#29616;&#21644;&#24314;&#27169;&#21508;&#31181;&#25551;&#36848;&#31526;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#24182;&#39044;&#27979;&#23427;&#20204;&#22312;&#26410;&#26469;&#26102;&#26399;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#36716;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#21521;&#37327;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65289;&#36827;&#34892;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#36824;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#36741;&#21161;&#20998;&#26512;&#65288;&#24179;&#31283;&#24615;&#65292;&#27491;&#20132;&#33033;&#20914;&#21709;&#24212;&#31561;&#65289;&#21487;&#20197;&#21457;&#29616;VoIP&#27969;&#37327;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the behavior of real-time traffic (e.g., VoIP) in mobility scenarios could help the operators to better plan their network infrastructures and to optimize the allocation of resources. Accordingly, in this work the authors propose a forecasting analysis of crucial QoS/QoE descriptors (some of which neglected in the technical literature) of VoIP traffic in a real mobile environment. The problem is formulated in terms of a multivariate time series analysis. Such a formalization allows to discover and model the temporal relationships among various descriptors and to forecast their behaviors for future periods. Techniques such as Vector Autoregressive models and machine learning (deep-based and tree-based) approaches are employed and compared in terms of performance and time complexity, by reframing the multivariate time series problem into a supervised learning one. Moreover, a series of auxiliary analyses (stationarity, orthogonal impulse responses, etc.) are performed to disco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22343;&#21248;&#25910;&#25947;&#30028;&#38480;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#29366;&#24577;-of-the-art&#19978;&#30028;&#19982;&#19979;&#30028;&#20043;&#38388;&#30340;&#31354;&#32570;&#12290;</title><link>http://arxiv.org/abs/2307.06644</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#8220;fat-shattering&#8221;&#32500;&#24230;&#30340;&#25913;&#36827;&#30340;&#22343;&#21248;&#25910;&#25947;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
An Improved Uniform Convergence Bound with Fat-Shattering Dimension. (arXiv:2307.06644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22343;&#21248;&#25910;&#25947;&#30028;&#38480;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#29366;&#24577;-of-the-art&#19978;&#30028;&#19982;&#19979;&#30028;&#20043;&#38388;&#30340;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;fat-shattering&#8221;&#32500;&#24230;&#21051;&#30011;&#20102;&#23454;&#20540;&#20989;&#25968;&#30340;&#22343;&#21248;&#25910;&#25947;&#29305;&#24615;&#12290;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#19978;&#30028;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#23384;&#22312;&#19968;&#20010;&#20056;&#27861;&#24179;&#26041;&#23545;&#25968;&#22240;&#23376;&#65292;&#19982;&#24050;&#26377;&#30340;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#31354;&#32570;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22343;&#21248;&#25910;&#25947;&#30028;&#38480;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fat-shattering dimension characterizes the uniform convergence property of real-valued functions. The state-of-the-art upper bounds feature a multiplicative squared logarithmic factor on the sample complexity, leaving an open gap with the existing lower bound. We provide an improved uniform convergence bound that closes this gap.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#35782;&#21035;&#26234;&#33021;&#20307;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#22238;&#24402;&#21644;&#24341;&#20837;&#25429;&#25417;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#20391;&#20449;&#24687;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#30340;&#30701;&#26242;&#36816;&#34892;&#20013;&#20165;&#20165;5&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24674;&#22797;&#30495;&#23454;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2307.06640</link><description>&lt;p&gt;
&#21457;&#29616;&#26234;&#33021;&#20307;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#30340;&#23398;&#20064;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering How Agents Learn Using Few Data. (arXiv:2307.06640v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#35782;&#21035;&#26234;&#33021;&#20307;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#22238;&#24402;&#21644;&#24341;&#20837;&#25429;&#25417;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#20391;&#20449;&#24687;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#30340;&#30701;&#26242;&#36816;&#34892;&#20013;&#20165;&#20165;5&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24674;&#22797;&#30495;&#23454;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#26159;&#35774;&#35745;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#20204;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#32463;&#39564;&#21644;&#36807;&#21435;&#30340;&#20132;&#20114;&#20013;&#33258;&#20027;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#35782;&#21035;&#35268;&#23450;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#20010;&#31995;&#32479;&#36712;&#36857;&#30340;&#30701;&#26242;&#31361;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22810;&#39033;&#24335;&#22238;&#24402;&#35782;&#21035;&#26234;&#33021;&#20307;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#19978;&#36890;&#36807;&#24341;&#20837;&#25429;&#25417;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22522;&#26412;&#20551;&#35774;&#25110;&#26399;&#26395;&#30340;&#20391;&#20449;&#24687;&#32422;&#26463;&#26469;&#34917;&#20607;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#21644;&#20248;&#21270;&#32422;&#26463;&#30340;&#24179;&#26041;&#21644;&#35745;&#31639;&#26469;&#23454;&#26045;&#36825;&#20123;&#32422;&#26463;&#65292;&#20174;&#32780;&#24471;&#21040;&#36234;&#26469;&#36234;&#20934;&#30830;&#30340;&#26234;&#33021;&#20307;&#21160;&#21147;&#23398;&#36817;&#20284;&#23618;&#27425;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#30340;&#30701;&#26242;&#36816;&#34892;&#20013;&#30340;5&#20010;&#26679;&#26412;&#65292;&#23601;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#21160;&#21147;&#23398;&#65292;&#21253;&#25324;&#22343;&#34913;&#36873;&#25321;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning algorithms are an essential tool for designing multi-agent systems, as they enable agents to autonomously learn from their experience and past interactions. In this work, we propose a theoretical and algorithmic framework for real-time identification of the learning dynamics that govern agent behavior using a short burst of a single system trajectory. Our method identifies agent dynamics through polynomial regression, where we compensate for limited data by incorporating side-information constraints that capture fundamental assumptions or expectations about agent behavior. These constraints are enforced computationally using sum-of-squares optimization, leading to a hierarchy of increasingly better approximations of the true agent dynamics. Extensive experiments demonstrated that our approach, using only 5 samples from a short run of a single trajectory, accurately recovers the true dynamics across various benchmarks, including equilibrium selection and predictio
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#38024;&#23545;&#22270;&#30693;&#35782;&#33976;&#39311;&#30340;&#26080;&#26694;&#26550;KD&#26694;&#26550;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22270;&#26694;&#26550;&#20998;&#35299;&#25552;&#20379;&#30340;&#22810;&#23610;&#24230;&#22270;&#30693;&#35782;&#65292;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#65292;&#24182;&#20855;&#26377;&#32531;&#35299;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06631</link><description>&lt;p&gt;
&#26080;&#26694;&#22270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Frameless Graph Knowledge Distillation. (arXiv:2307.06631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#38024;&#23545;&#22270;&#30693;&#35782;&#33976;&#39311;&#30340;&#26080;&#26694;&#26550;KD&#26694;&#26550;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22270;&#26694;&#26550;&#20998;&#35299;&#25552;&#20379;&#30340;&#22810;&#23610;&#24230;&#22270;&#30693;&#35782;&#65292;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#65292;&#24182;&#20855;&#26377;&#32531;&#35299;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;Knowledge Distillation&#65292;KD&#65289;&#24050;&#26174;&#31034;&#20986;&#22312;&#23558;&#22797;&#26434;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#31616;&#21333;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20174;&#32780;&#21487;&#20197;&#39640;&#25928;&#22320;&#23436;&#25104;&#32321;&#37325;&#30340;&#23398;&#20064;&#20219;&#21153;&#32780;&#19981;&#22833;&#21435;&#22826;&#22810;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#23558;KD&#26426;&#21046;&#24212;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;GNNs&#65289;&#65292;&#20197;&#36890;&#36807;&#23398;&#29983;&#27169;&#22411;&#21152;&#36895;&#27169;&#22411;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20110;KD&#30340;GNNs&#22312;&#23398;&#29983;&#27169;&#22411;&#20013;&#21033;&#29992;MLP&#20316;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#26469;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#27969;&#31243;&#65292;&#32780;&#19981;&#32771;&#34385;&#25945;&#24072;&#27169;&#22411;&#30340;&#22270;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#23610;&#24230;GNNs&#30340;KD&#26694;&#26550;&#65292;&#21363;&#22270;&#26694;&#26550;&#65288;graph framelet&#65289;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22270;&#26694;&#26550;&#20998;&#35299;&#25552;&#20379;&#30340;&#22810;&#23610;&#24230;&#22270;&#30693;&#35782;&#65292;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#65292;&#24182;&#20855;&#26377;&#32531;&#35299;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has shown great potential for transferring knowledge from a complex teacher model to a simple student model in which the heavy learning task can be accomplished efficiently and without losing too much prediction accuracy. Recently, many attempts have been made by applying the KD mechanism to the graph representation learning models such as graph neural networks (GNNs) to accelerate the model's inference speed via student models. However, many existing KD-based GNNs utilize MLP as a universal approximator in the student model to imitate the teacher model's process without considering the graph knowledge from the teacher model. In this work, we provide a KD-based framework on multi-scaled GNNs, known as graph framelet, and prove that by adequately utilizing the graph knowledge in a multi-scaled manner provided by graph framelet decomposition, the student model is capable of adapting both homophilic and heterophilic graphs and has the potential of alleviating t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30740;&#31350;&#20102;&#32463;&#20856;&#21644;&#37327;&#23376;&#36890;&#20449;&#20013;&#30340;&#37327;&#23376;&#36890;&#36947;&#32534;&#30721;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#36890;&#36947;&#27169;&#22411;&#19979;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#20026;&#25512;&#36827;&#37327;&#23376;&#36890;&#20449;&#31995;&#32479;&#30740;&#31350;&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36890;&#20449;&#23481;&#37327;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.06622</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#37327;&#23376;&#36890;&#36947;&#32534;&#30721;&#30340;&#37327;&#23376;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Quantum Autoencoders for Learning Quantum Channel Codes. (arXiv:2307.06622v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30740;&#31350;&#20102;&#32463;&#20856;&#21644;&#37327;&#23376;&#36890;&#20449;&#20013;&#30340;&#37327;&#23376;&#36890;&#36947;&#32534;&#30721;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#36890;&#36947;&#27169;&#22411;&#19979;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#20026;&#25512;&#36827;&#37327;&#23376;&#36890;&#20449;&#31995;&#32479;&#30740;&#31350;&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36890;&#20449;&#23481;&#37327;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#32463;&#20856;&#21644;&#37327;&#23376;&#36890;&#20449;&#20013;&#30340;&#24212;&#29992;&#65292;&#28041;&#21450;&#19981;&#21516;&#37327;&#23376;&#27604;&#29305;&#36890;&#36947;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#21644;&#28789;&#27963;&#30340;&#36890;&#36947;&#22122;&#22768;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#29983;&#25104;&#37327;&#23376;&#36890;&#36947;&#32534;&#30721;&#24182;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#25506;&#32034;&#20102;&#32463;&#20856;&#12289;&#36741;&#21161;&#32416;&#32544;&#21644;&#37327;&#23376;&#36890;&#20449;&#22330;&#26223;&#12290;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#37327;&#23376;&#36890;&#36947;&#27169;&#22411;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#36827;&#37327;&#23376;&#36890;&#20449;&#31995;&#32479;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#22312;&#35843;&#21046;&#32422;&#26463;&#12289;&#21508;&#31181;&#36890;&#20449;&#35774;&#32622;&#21644;&#19981;&#21516;&#36890;&#36947;&#27169;&#22411;&#19979;&#30340;&#23481;&#37327;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the application of quantum machine learning techniques for classical and quantum communication across different qubit channel models. By employing parameterized quantum circuits and a flexible channel noise model, we develop a machine learning framework to generate quantum channel codes and evaluate their effectiveness. We explore classical, entanglement-assisted, and quantum communication scenarios within our framework. Applying it to various quantum channel models as proof of concept, we demonstrate strong performance in each case. Our results highlight the potential of quantum machine learning in advancing research on quantum communication systems, enabling a better understanding of capacity bounds under modulation constraints, various communication settings, and diverse channel models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#38752;&#37327;&#21270;&#12289;&#26377;&#38480;&#26102;&#38388;&#21327;&#20316;&#21327;&#35758;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#26469;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06620</link><description>&lt;p&gt;
&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#19982;&#37327;&#21270;&#26377;&#38480;&#26102;&#38388;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Online Distributed Learning with Quantized Finite-Time Coordination. (arXiv:2307.06620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#38752;&#37327;&#21270;&#12289;&#26377;&#38480;&#26102;&#38388;&#21327;&#20316;&#21327;&#35758;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#26469;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#26159;&#25351;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#19968;&#32452;&#20195;&#29702;&#38656;&#35201;&#21512;&#20316;&#22320;&#20174;&#27969;&#25968;&#25454;&#20013;&#35757;&#32451;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#32852;&#37030;&#23398;&#20064;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#32780;&#20165;&#20381;&#38752;&#20195;&#29702;&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#12290;&#35813;&#26041;&#27861;&#32463;&#24120;&#29992;&#20110;&#25968;&#25454;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#25110;&#25104;&#26412;&#21407;&#22240;&#19981;&#33021;&#31227;&#21160;&#21040;&#38598;&#20013;&#20301;&#32622;&#30340;&#22330;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#23569;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#37327;&#21270;&#30340;&#12289;&#26377;&#38480;&#26102;&#38388;&#30340;&#21327;&#20316;&#21327;&#35758;&#26469;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20801;&#35768;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#12290;&#38543;&#26426;&#26799;&#24230;&#26159;&#20351;&#29992;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#30340;&#38543;&#26426;&#25277;&#26679;&#23376;&#38598;&#35745;&#31639;&#30340;&#65292;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider online distributed learning problems. Online distributed learning refers to the process of training learning models on distributed data sources. In our setting a set of agents need to cooperatively train a learning model from streaming data. Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents. This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons. In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models. Furthermore, our algorithm allows for the use of stochastic gradients during local training. Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;IMM&#28388;&#27874;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27979;&#37327;&#25968;&#25454;&#21363;&#21487;&#20248;&#21270;&#28388;&#27874;&#22120;&#30340;&#21442;&#25968;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#20351;&#29992;&#30495;&#20540;&#21442;&#25968;&#21270;&#30340;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.06618</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;IMM&#28388;&#27874;&#22120;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning IMM Filter Parameters from Measurements using Gradient Descent. (arXiv:2307.06618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;IMM&#28388;&#27874;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27979;&#37327;&#25968;&#25454;&#21363;&#21487;&#20248;&#21270;&#28388;&#27874;&#22120;&#30340;&#21442;&#25968;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#20351;&#29992;&#30495;&#20540;&#21442;&#25968;&#21270;&#30340;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34701;&#21512;&#21644;&#36319;&#36394;&#31639;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#20381;&#36182;&#20110;&#26082;&#25551;&#36848;&#20256;&#24863;&#22120;&#31995;&#32479;&#21448;&#21487;&#20197;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;&#23613;&#31649;&#35843;&#25972;&#36825;&#20123;&#21464;&#37327;&#23545;&#20110;&#20256;&#24863;&#22120;&#31995;&#32479;&#26469;&#35828;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#65292;&#20294;&#22312;&#36319;&#36394;&#30446;&#26631;&#30340;&#20869;&#22312;&#21442;&#25968;&#22312;&#31995;&#32479;&#37096;&#32626;&#20043;&#21069;&#29978;&#33267;&#21487;&#33021;&#23436;&#20840;&#19981;&#21487;&#35266;&#27979;&#12290;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;&#20256;&#24863;&#22120;&#31995;&#32479;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#30340;&#25968;&#37327;&#33258;&#28982;&#22686;&#21152;&#65292;&#38656;&#35201;&#33258;&#21160;&#20248;&#21270;&#27169;&#22411;&#21464;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#20165;&#20351;&#29992;&#27979;&#37327;&#26469;&#20248;&#21270;&#20132;&#20114;&#22810;&#27169;&#22411;&#65288;IMM&#65289;&#28388;&#27874;&#22120;&#30340;&#21442;&#25968;&#65292;&#22240;&#27492;&#26080;&#38656;&#20219;&#20309;&#22522;&#30784;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#35780;&#20272;&#20102;&#32467;&#26524;&#26041;&#27861;&#65292;&#32467;&#26524;&#26041;&#27861;&#25104;&#21151;&#21305;&#37197;&#20102;&#20351;&#29992;&#22522;&#30784;&#30495;&#20540;&#21442;&#25968;&#21270;&#30340;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of data fusion and tracking algorithms often depends on parameters that not only describe the sensor system, but can also be task-specific. While for the sensor system tuning these variables is time-consuming and mostly requires expert knowledge, intrinsic parameters of targets under track can even be completely unobservable until the system is deployed. With state-of-the-art sensor systems growing more and more complex, the number of parameters naturally increases, necessitating the automatic optimization of the model variables. In this paper, the parameters of an interacting multiple model (IMM) filter are optimized solely using measurements, thus without necessity for any ground-truth data. The resulting method is evaluated through an ablation study on simulated data, where the trained model manages to match the performance of a filter parametrized with ground-truth values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.06608</link><description>&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#24341;&#20837;&#65306;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#20102;&#26368;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#30340;&#28508;&#21147;&#21644;&#28789;&#27963;&#24615;&#32570;&#20047;&#35748;&#35782;&#12290;&#21463;&#21040;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#20852;&#36259;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;1&#65289;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#20197;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65307;2&#65289;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#21019;&#26032;&#24605;&#24819;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#36873;&#25321;&#20195;&#29702;&#27169;&#22411;&#30340;&#20004;&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#22522;&#30784;&#27169;&#22411;&#26159;&#36825;&#19968;&#35282;&#33394;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30683;&#30462;&#22320;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#36825;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25105;&#20204;&#24402;&#22240;&#20110;&#32570;&#20047;&#19978;&#36848;&#25351;&#23548;&#21407;&#21017;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;H-&#20284;&#28982;&#27861;&#36827;&#34892;&#35757;&#32451;&#21644;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21253;&#21547;&#20010;&#20307;&#29305;&#23450;&#33030;&#24369;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2307.06581</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21322;&#21442;&#25968;&#33030;&#24369;&#24615;&#27169;&#22411;&#21450;&#20854;H-&#20284;&#28982;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks for Semiparametric Frailty Models via H-likelihood. (arXiv:2307.06581v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;H-&#20284;&#28982;&#27861;&#36827;&#34892;&#35757;&#32451;&#21644;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21253;&#21547;&#20010;&#20307;&#29305;&#23450;&#33030;&#24369;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#27979;&#32676;&#38598;&#21270;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20285;&#39532;&#33030;&#24369;&#24615;&#27169;&#22411;&#65288;DNN-FM&#65289;&#12290;&#35813;&#27169;&#22411;&#30340;&#19968;&#20010;&#20248;&#21183;&#26159;&#36890;&#36807;&#26032;&#30340;H-&#20284;&#28982;&#20989;&#25968;&#30340;&#32852;&#21512;&#26368;&#22823;&#21270;&#65292;&#20026;&#22266;&#23450;&#21442;&#25968;&#25552;&#20379;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#24182;&#20026;&#38543;&#26426;&#33030;&#24615;&#25552;&#20379;&#20102;&#26368;&#20339;&#26080;&#20559;&#39044;&#27979;&#22120;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;DNN-FM&#36890;&#36807;&#20351;&#29992;&#36127;&#38754;&#21078;&#26512;&#30340;H-&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#21078;&#26512;&#38750;&#21442;&#25968;&#22522;&#32447;&#39118;&#38505;&#26469;&#26500;&#36896;&#12290;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;&#65292;&#21253;&#21547;&#20010;&#20307;&#29305;&#23450;&#33030;&#24369;&#24615;&#26377;&#21161;&#20110;&#25913;&#21892;&#22522;&#20110;DNN&#30340;Cox&#27169;&#22411;&#65288;DNN-Cox&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
For prediction of clustered time-to-event data, we propose a new deep neural network based gamma frailty model (DNN-FM). An advantage of the proposed model is that the joint maximization of the new h-likelihood provides maximum likelihood estimators for fixed parameters and best unbiased predictors for random frailties. Thus, the proposed DNN-FM is trained by using a negative profiled h-likelihood as a loss function, constructed by profiling out the non-parametric baseline hazard. Experimental studies show that the proposed method enhances the prediction performance of the existing methods. A real data analysis shows that the inclusion of subject-specific frailties helps to improve prediction of the DNN based Cox model (DNN-Cox).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20122;&#32447;&#24615;&#28608;&#27963;&#31070;&#32463;&#20803;&#35782;&#21035;&#23454;&#29616;&#39640;&#25928;&#30340;SGD&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#31639;&#27861;&#25910;&#25947;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(M^2/\epsilon^2)$&#12290;</title><link>http://arxiv.org/abs/2307.06565</link><description>&lt;p&gt;
&#36890;&#36807;&#20122;&#32447;&#24615;&#28608;&#27963;&#31070;&#32463;&#20803;&#35782;&#21035;&#23454;&#29616;&#39640;&#25928;&#30340;SGD&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification. (arXiv:2307.06565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06565
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20122;&#32447;&#24615;&#28608;&#27963;&#31070;&#32463;&#20803;&#35782;&#21035;&#23454;&#29616;&#39640;&#25928;&#30340;SGD&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#31639;&#27861;&#25910;&#25947;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(M^2/\epsilon^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#25910;&#25947;&#20445;&#35777;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#26159;&#19968;&#20010;&#22522;&#26412;&#19988;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38745;&#24577;&#30340;&#21322;&#31354;&#38388;&#25253;&#21578;&#25968;&#25454;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#20840;&#36830;&#25509;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23454;&#29616;&#24179;&#31227;ReLU&#28608;&#27963;&#65292;&#20197;&#36890;&#36807;&#20960;&#20309;&#25628;&#32034;&#22312;&#20122;&#32447;&#24615;&#26102;&#38388;&#20869;&#36827;&#34892;&#28608;&#27963;&#31070;&#32463;&#20803;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;$O(M^2/\epsilon^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20869;&#25910;&#25947;&#65292;&#20854;&#20013;$M$&#26159;&#31995;&#25968;&#33539;&#25968;&#19978;&#30028;&#65292;$\epsilon$&#26159;&#35823;&#24046;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been widely used in many fields, but the model training process usually consumes massive computational resources and time. Therefore, designing an efficient neural network training method with a provable convergence guarantee is a fundamental and important research question. In this paper, we present a static half-space report data structure that consists of a fully connected two-layer neural network for shifted ReLU activation to enable activated neuron identification in sublinear time via geometric search. We also prove that our algorithm can converge in $O(M^2/\epsilon^2)$ time with network size quadratic in the coefficient norm upper bound $M$ and error term $\epsilon$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06564</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#35302;&#21457;&#24178;&#39044;&#26469;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22686;&#21152;&#27491;&#38754;&#26696;&#20363;&#32467;&#26524;&#30340;&#27010;&#29575;&#12290;&#36825;&#20123;&#24178;&#39044;&#26159;&#26681;&#25454;&#24178;&#39044;&#31574;&#30053;&#35302;&#21457;&#30340;&#12290;&#24378;&#21270;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#36890;&#36807;&#35797;&#38169;&#23398;&#20064;&#24178;&#39044;&#31574;&#30053;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#20551;&#35774;&#21487;&#29992;&#20110;&#25191;&#34892;&#24178;&#39044;&#30340;&#36164;&#28304;&#25968;&#37327;&#26159;&#26080;&#38480;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#36164;&#28304;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#22256;&#22659;&#26159;&#22522;&#20110;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#30340;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#26469;&#35302;&#21457;&#24178;&#39044;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#23545;&#24178;&#39044;&#30340;&#24517;&#35201;&#24615;&#25110;&#25928;&#26524;&#23384;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23558;&#26377;&#38480;&#30340;&#36164;&#28304;&#29992;&#20110;&#24178;&#39044;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes. These interventions are triggered according to an intervention policy. Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error. Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice. This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization. Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37329;&#23646;&#27687;&#21270;&#29289;&#20256;&#24863;&#22120;&#38453;&#21015;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOCs&#65289;&#65292;&#19988;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#22238;&#24402;&#20998;&#26512;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06556</link><description>&lt;p&gt;
&#22522;&#20110;&#37329;&#23646;&#27687;&#21270;&#29289;&#30340;&#27668;&#20307;&#20256;&#24863;&#22120;&#38453;&#21015;&#22312;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;VOCs&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex Mixtures using Machine Learning. (arXiv:2307.06556v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37329;&#23646;&#27687;&#21270;&#29289;&#20256;&#24863;&#22120;&#38453;&#21015;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOCs&#65289;&#65292;&#19988;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#22238;&#24402;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOCs&#65289;&#30340;&#26816;&#27979;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30142;&#30149;&#26089;&#26399;&#26816;&#27979;&#30340;&#21487;&#34892;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19977;&#20010;&#37329;&#23646;&#27687;&#21270;&#29289;&#30005;&#26497;&#30340;&#20256;&#24863;&#22120;&#38453;&#21015;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#28151;&#21512;&#29289;&#20013;&#35782;&#21035;&#20986;&#22235;&#31181;&#19981;&#21516;&#30340;VOCs&#12290;&#37329;&#23646;&#27687;&#21270;&#29289;&#20256;&#24863;&#22120;&#38453;&#21015;&#32463;&#36807;&#19981;&#21516;VOC&#27987;&#24230;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#20057;&#37255;&#12289;&#19993;&#37230;&#12289;&#30002;&#33519;&#21644;&#27695;&#20223;&#12290;&#20174;&#21333;&#19968;&#27668;&#20307;&#21644;&#28151;&#21512;&#29289;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#20915;&#31574;&#26641;&#12289;&#32447;&#24615;&#22238;&#24402;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;KNN&#21644;RF&#22312;&#23545;&#27668;&#20307;&#28151;&#21512;&#29289;&#20013;&#30340;&#19981;&#21516;&#21464;&#21270;&#21270;&#23398;&#21697;&#36827;&#34892;&#20998;&#31867;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;99%&#12290;&#22312;&#22238;&#24402;&#20998;&#26512;&#20013;&#65292;KNN&#30340;&#32467;&#26524;&#26368;&#22909;&#65292;R2&#20540;&#36229;&#36807;0.99&#65292;LOD&#20540;&#20026;0.012&#12289;0.015&#12289;0.014&#21644;0.025 PPM&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of Volatile Organic Compounds (VOCs) from the breath is becoming a viable route for the early detection of diseases non-invasively. This paper presents a sensor array with three metal oxide electrodes that can use machine learning methods to identify four distinct VOCs in a mixture. The metal oxide sensor array was subjected to various VOC concentrations, including ethanol, acetone, toluene and chloroform. The dataset obtained from individual gases and their mixtures were analyzed using multiple machine learning algorithms, such as Random Forest (RF), K-Nearest Neighbor (KNN), Decision Tree, Linear Regression, Logistic Regression, Naive Bayes, Linear Discriminant Analysis, Artificial Neural Network, and Support Vector Machine. KNN and RF have shown more than 99% accuracy in classifying different varying chemicals in the gas mixtures. In regression analysis, KNN has delivered the best results with R2 value of more than 0.99 and LOD of 0.012, 0.015, 0.014 and 0.025 PPM for pred
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.06555</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36924;&#36817;&#65306;&#20174;ReLU&#21040;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#28608;&#27963;&#20989;&#25968;&#38598;&#21512;A&#65292;&#21253;&#25324;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#12289;LeakyReLU&#12289;ReLU^2&#12289;ELU&#12289;SELU&#12289;Softplus&#12289;GELU&#12289;SiLU&#12289;Swish&#12289;Mish&#12289;Sigmoid&#12289;Tanh&#12289;Arctan&#12289;Softsign&#12289;dSiLU&#21644;SRS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#24847;&#28608;&#27963;&#20989;&#25968;varrho&#8712;A&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#24471;&#22823;&#37096;&#20998;&#23545;&#20110;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#33021;&#22815;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65292;&#23613;&#31649;&#38656;&#35201;&#31245;&#22823;&#30340;&#24120;&#25968;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#25928;&#30340;&#32534;&#30721;-&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#23454;&#29616;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#23450;&#20301;&#32954;&#32467;&#33410;&#30340;&#30446;&#26631;&#65292;&#36991;&#20813;&#20102;&#38477;&#37319;&#26679;&#23548;&#33268;&#30340;&#20449;&#21495;&#20002;&#22833;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#26694;&#26550;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06547</link><description>&lt;p&gt;
&#20351;&#29992;&#27531;&#24046;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#20840;&#20998;&#36776;&#29575;&#32954;&#32467;&#33410;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Full-resolution Lung Nodule Segmentation from Chest X-ray Images using Residual Encoder-Decoder Networks. (arXiv:2307.06547v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#25928;&#30340;&#32534;&#30721;-&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#23454;&#29616;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#23450;&#20301;&#32954;&#32467;&#33410;&#30340;&#30446;&#26631;&#65292;&#36991;&#20813;&#20102;&#38477;&#37319;&#26679;&#23548;&#33268;&#30340;&#20449;&#21495;&#20002;&#22833;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#26694;&#26550;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#26089;&#26399;&#35786;&#26029;&#19982;&#33391;&#22909;&#30340;&#39044;&#21518;&#30456;&#20851;&#12290;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#24265;&#20215;&#30340;&#32954;&#30284;&#35786;&#26029;&#25104;&#20687;&#26041;&#24335;&#12290;&#20351;&#29992;CXR&#24456;&#38590;&#21306;&#20998;&#21487;&#30097;&#32467;&#33410;&#19982;&#34880;&#31649;&#21644;&#39592;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20986;&#20102;&#36741;&#21161;&#20154;&#31867;&#25918;&#23556;&#31185;&#21307;&#24072;&#36827;&#34892;&#35813;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#20027;&#35201;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#38477;&#37319;&#26679;&#22270;&#20687;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12289;&#26410;&#32463;&#35777;&#23454;&#30340;&#27867;&#21270;&#26041;&#27861;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#25928;&#30340;&#32534;&#30721;-&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#26469;&#23450;&#20301;&#32954;&#32467;&#33410;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#20197;&#36991;&#20813;&#22240;&#38477;&#37319;&#26679;&#32780;&#23548;&#33268;&#30340;&#20449;&#21495;&#20002;&#22833;&#12290;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20351;&#29992;JSRT&#32954;&#32467;&#33410;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#29992;&#20110;&#23450;&#20301;&#26469;&#33258;&#29420;&#31435;&#30340;&#22806;&#37096;CXR&#25968;&#25454;&#38598;&#30340;&#32954;&#32467;&#33410;&#12290;&#20351;&#29992;&#33258;&#21160;&#21270;&#26694;&#26550;&#27979;&#37327;&#20102;&#28789;&#25935;&#24230;&#21644;&#20551;&#38451;&#24615;&#29575;&#65292;&#20197;&#28040;&#38500;&#20219;&#20309;&#35266;&#23519;&#32773;&#20027;&#35266;&#24615;&#12290;&#36825;&#20123;&#23454;&#39564;&#21487;&#20197;&#23454;&#29616;&#32954;&#32467;&#33410;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is the leading cause of cancer death and early diagnosis is associated with a positive prognosis. Chest X-ray (CXR) provides an inexpensive imaging mode for lung cancer diagnosis. Suspicious nodules are difficult to distinguish from vascular and bone structures using CXR. Computer vision has previously been proposed to assist human radiologists in this task, however, leading studies use down-sampled images and computationally expensive methods with unproven generalization. Instead, this study localizes lung nodules using efficient encoder-decoder neural networks that process full resolution images to avoid any signal loss resulting from down-sampling. Encoder-decoder networks are trained and tested using the JSRT lung nodule dataset. The networks are used to localize lung nodules from an independent external CXR dataset. Sensitivity and false positive rates are measured using an automated framework to eliminate any observer subjectivity. These experiments allow for the dete
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#38388;&#35270;&#37326;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;&#30701;&#20110;&#23454;&#38469;&#20540;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#21487;&#20197;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21628;&#21505;&#22312;IRL&#20013;&#21516;&#26102;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#12290;</title><link>http://arxiv.org/abs/2307.06541</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effective Horizon of Inverse Reinforcement Learning. (arXiv:2307.06541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#38388;&#35270;&#37326;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;&#30701;&#20110;&#23454;&#38469;&#20540;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#21487;&#20197;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21628;&#21505;&#22312;IRL&#20013;&#21516;&#26102;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#32473;&#23450;&#26102;&#38388;&#35270;&#37326;&#30340;&#65288;&#21069;&#21521;&#65289;&#24378;&#21270;&#23398;&#20064;&#25110;&#35268;&#21010;&#26469;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35813;&#31574;&#30053;&#19982;&#19987;&#23478;&#28436;&#31034;&#21305;&#37197;&#12290;&#26102;&#38388;&#35270;&#37326;&#22312;&#30830;&#23450;&#22870;&#21169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;IRL&#31639;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#27604;&#22320;&#38754;&#23454;&#38469;&#20540;&#26356;&#30701;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#36890;&#24120;&#33021;&#26356;&#24555;&#22320;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;&#27492;&#29616;&#35937;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#24182;&#32473;&#20986;&#20102;&#35299;&#37322;&#65306;&#26102;&#38388;&#35270;&#37326;&#25511;&#21046;&#20102;&#24341;&#21457;&#31574;&#30053;&#31867;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#36825;&#19968;&#20998;&#26512;&#20026;IRL&#30340;&#26377;&#25928;&#35270;&#37326;&#36873;&#25321;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#12290;&#23427;&#20063;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;IRL&#20844;&#24335;&#65306;&#19982;&#20165;&#20855;&#26377;&#32473;&#23450;&#35270;&#37326;&#30340;&#22870;&#21169;&#30456;&#27604;&#65292;&#20849;&#21516;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#35270;&#37326;&#26356;&#21152;&#33258;&#28982;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimate and the computational efficiency of IRL algorithms. Interestingly, an effective time horizon shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis leads to a principled choice of the effective horizon for IRL. It also prompts us to reexamine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon together rather than the reward alone with a given horizon. Our experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24494;&#21338;&#25968;&#25454;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#32422;0.73&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;CNN&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12289;&#24066;&#22330;&#30740;&#31350;&#21644;&#25919;&#31574;&#30740;&#31350;&#31561;&#39046;&#22495;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.06540</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#21338;&#24773;&#24863;&#20998;&#26512;&#65306;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach. (arXiv:2307.06540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24494;&#21338;&#25968;&#25454;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#32422;0.73&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;CNN&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12289;&#24066;&#22330;&#30740;&#31350;&#21644;&#25919;&#31574;&#30740;&#31350;&#31561;&#39046;&#22495;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#26469;&#33258;&#24494;&#21338;&#30340;119,988&#26465;&#21407;&#22987;&#25512;&#25991;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#26159;&#20174;&#30334;&#24230;&#30340;PaddlePaddle AI&#24179;&#21488;&#33719;&#21462;&#30340;&#65292;&#32463;&#36807;&#20102;&#31934;&#32454;&#30340;&#39044;&#22788;&#29702;&#12289;&#20998;&#35789;&#21644;&#24773;&#24863;&#26631;&#31614;&#20998;&#31867;&#12290;&#21033;&#29992;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;CNN&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#31867;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;&#32422;0.73&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#26174;&#31034;&#20102;&#23545;&#27491;&#38754;&#12289;&#20013;&#24615;&#21644;&#36127;&#38754;&#24773;&#24863;&#30340;&#24179;&#34913;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;CNN&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12289;&#24066;&#22330;&#30740;&#31350;&#21644;&#25919;&#31574;&#30740;&#31350;&#31561;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;&#23436;&#25972;&#30340;&#23454;&#39564;&#20869;&#23481;&#21644;&#20195;&#30721;&#24050;&#22312;Kaggle&#25968;&#25454;&#24179;&#21488;&#19978;&#20844;&#24320;&#25552;&#20379;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP). The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels. A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification. The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments. The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies. The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development. Future work ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20197;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#31639;&#27861;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#27809;&#26377;&#32452;&#20214;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#24182;&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#32858;&#31867;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#21487;&#20197;&#22312;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#19979;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.06538</link><description>&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#19982;&#25511;&#21046;&#29702;&#35770;&#30340;&#32467;&#21512;&#65306;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems. (arXiv:2307.06538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20197;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#31639;&#27861;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#27809;&#26377;&#32452;&#20214;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#24182;&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#32858;&#31867;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#21487;&#20197;&#22312;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#19979;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Chen&#21644;Poor&#24320;&#22987;&#30740;&#31350;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#34429;&#28982;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#24050;&#32463;&#22312;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#20351;&#29992;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#25311;&#21512;&#25110;&#32773;&#23545;&#25968;&#25454;&#20013;&#34920;&#31034;&#30340;&#22522;&#30784;&#23376;&#32676;&#20307;&#26377;&#26356;&#20016;&#23500;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32452;&#20214;&#26080;&#24378;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#19982;&#36712;&#36857;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#32858;&#31867;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#35266;&#23519;&#65292;&#21363;&#32463;&#20856;&#30340;&#20309;-&#21345;&#23572;&#26364;&#31639;&#27861;&#26159;&#23398;&#20064;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#29616;&#20195;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#30340;&#36817;&#20146;&#12290;&#36825;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25805;&#20316;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models. This gives us a playbook for how to extend it to work with more complicated generative models.
&lt;/p&gt;</description></item><item><title>DSV&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39564;&#35777;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#36873;&#25321;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#31163;&#32676;&#28857;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06534</link><description>&lt;p&gt;
DSV:&#33258;&#30417;&#30563;&#31163;&#32676;&#28857;&#27169;&#22411;&#36873;&#25321;&#30340;&#23545;&#40784;&#39564;&#35777;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection. (arXiv:2307.06534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06534
&lt;/p&gt;
&lt;p&gt;
DSV&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39564;&#35777;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#36873;&#25321;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#31163;&#32676;&#28857;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#29983;&#25104;&#20869;&#37096;&#30417;&#30563;&#20449;&#21495;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#19978;&#26377;&#25928;&#12290;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#38754;&#20020;&#33719;&#21462;&#30495;&#23454;&#26631;&#31614;&#30340;&#39640;&#25104;&#26412;&#65292;&#22240;&#27492;&#21487;&#20197;&#20174;SSL&#20013;&#22823;&#22823;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#30340;&#36229;&#21442;&#25968;&#23545;&#22522;&#20110;SSL&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#30446;&#21069;&#36824;&#27809;&#26377;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DSV&#65288;&#19981;&#19968;&#33268;&#24615;&#21644;&#21487;&#20998;&#31163;&#24615;&#39564;&#35777;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39564;&#35777;&#25439;&#22833;&#65292;&#29992;&#20110;&#36873;&#25321;&#20855;&#26377;&#26377;&#25928;&#21442;&#25968;&#30340;&#39640;&#24615;&#33021;&#26816;&#27979;&#27169;&#22411;&#12290; DSV&#36890;&#36807;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#20998;&#21035;&#36817;&#20284;&#20102;&#27979;&#35797;&#25968;&#25454;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#21487;&#20998;&#31163;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;DSV&#36827;&#34892;&#35780;&#20272;&#21487;&#20197;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#23545;&#40784;&#24615;&#30340;&#26377;&#25928;SSAD&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has proven effective in solving various problems by generating internal supervisory signals. Unsupervised anomaly detection, which faces the high cost of obtaining true labels, is an area that can greatly benefit from SSL. However, recent literature suggests that tuning the hyperparameters (HP) of data augmentation functions is crucial to the success of SSL-based anomaly detection (SSAD), yet a systematic method for doing so remains unknown. In this work, we propose DSV (Discordance and Separability Validation), an unsupervised validation loss to select high-performing detection models with effective augmentation HPs. DSV captures the alignment between an augmentation function and the anomaly-generating mechanism with surrogate losses, which approximate the discordance and separability of test data, respectively. As a result, the evaluation via DSV leads to selecting an effective SSAD model exhibiting better alignment, which results in high detection accu
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#33647;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#21270;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20248;&#21270;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24050;&#26377;&#22810;&#31181;&#21270;&#21512;&#29289;&#36827;&#20837;&#20102;&#20020;&#24202;&#35797;&#39564;&#12290;&#31185;&#23398;&#30028;&#24517;&#39035;&#20180;&#32454;&#23457;&#26597;&#24050;&#30693;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#21482;&#26377;&#20855;&#26377;&#36275;&#22815;&#30340;&#30495;&#23454;&#22522;&#20934;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25165;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06521</link><description>&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Drug Discovery: Are We There Yet?. (arXiv:2307.06521v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#33647;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#21270;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20248;&#21270;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24050;&#26377;&#22810;&#31181;&#21270;&#21512;&#29289;&#36827;&#20837;&#20102;&#20020;&#24202;&#35797;&#39564;&#12290;&#31185;&#23398;&#30028;&#24517;&#39035;&#20180;&#32454;&#23457;&#26597;&#24050;&#30693;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#21482;&#26377;&#20855;&#26377;&#36275;&#22815;&#30340;&#30495;&#23454;&#22522;&#20934;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25165;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#27491;&#22312;&#36866;&#24212;&#25968;&#25454;&#31185;&#23398;&#12289;&#20449;&#24687;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#26032;&#25216;&#26415;&#65292;&#20197;&#21152;&#24555;&#26377;&#25928;&#27835;&#30103;&#30340;&#24320;&#21457;&#65292;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#21160;&#29289;&#23454;&#39564;&#12290;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25913;&#21464;&#33647;&#29289;&#21457;&#29616;&#65292;&#25237;&#36164;&#32773;&#12289;&#24037;&#19994;&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#20197;&#21450;&#31435;&#27861;&#32773;&#23545;&#27492;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25104;&#21151;&#30340;&#33647;&#29289;&#21457;&#29616;&#38656;&#35201;&#20248;&#21270;&#19982;&#33647;&#29702;&#21160;&#21147;&#23398;&#12289;&#33647;&#20195;&#21160;&#21147;&#23398;&#21644;&#20020;&#24202;&#32467;&#26524;&#30456;&#20851;&#30340;&#24615;&#36136;&#12290;&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#30340;&#19977;&#20010;&#25903;&#26609;&#65306;&#30142;&#30149;&#12289;&#38774;&#28857;&#21644;&#27835;&#30103;&#27169;&#24335;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#33647;&#29289;&#12290;&#29983;&#25104;&#21270;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20248;&#21270;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20351;&#24471;&#22810;&#31181;&#21270;&#21512;&#29289;&#36827;&#20837;&#20102;&#20020;&#24202;&#35797;&#39564;&#12290;&#31185;&#23398;&#30028;&#24517;&#39035;&#20180;&#32454;&#23457;&#26597;&#24050;&#30693;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#21482;&#26377;&#20855;&#26377;&#36275;&#22815;&#30340;&#30495;&#23454;&#22522;&#20934;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25165;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug discovery is adapting to novel technologies such as data science, informatics, and artificial intelligence (AI) to accelerate effective treatment development while reducing costs and animal experiments. AI is transforming drug discovery, as indicated by increasing interest from investors, industrial and academic scientists, and legislators. Successful drug discovery requires optimizing properties related to pharmacodynamics, pharmacokinetics, and clinical outcomes. This review discusses the use of AI in the three pillars of drug discovery: diseases, targets, and therapeutic modalities, with a focus on small molecule drugs. AI technologies, such as generative chemistry, machine learning, and multi-property optimization, have enabled several compounds to enter clinical trials. The scientific community must carefully vet known information to address the reproducibility crisis. The full potential of AI in drug discovery can only be realized with sufficient ground truth and appropriate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#20174;&#19994;&#32773;&#19982;&#24037;&#20855;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#36825;&#20123;&#20114;&#21160;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#21644;&#31995;&#32479;&#24320;&#21457;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#22312;&#23398;&#20064;&#21644;&#21327;&#35843;&#23454;&#36341;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#35774;&#26045;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06518</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#21644;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;
Machine Learning practices and infrastructures. (arXiv:2307.06518v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#20174;&#19994;&#32773;&#19982;&#24037;&#20855;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#36825;&#20123;&#20114;&#21160;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#21644;&#31995;&#32479;&#24320;&#21457;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#22312;&#23398;&#20064;&#21644;&#21327;&#35843;&#23454;&#36341;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#35774;&#26045;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#22312;&#37325;&#22823;&#39046;&#22495;&#37096;&#32626;&#26102;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#33021;&#21152;&#21095;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#65292;&#21019;&#36896;&#26032;&#30340;&#27495;&#35270;&#27169;&#24335;&#65292;&#24182;&#22266;&#21270;&#36807;&#26102;&#30340;&#31038;&#20250;&#26500;&#36896;&#12290;&#22240;&#27492;&#65292;ML&#31995;&#32479;&#24320;&#21457;&#30340;&#31038;&#20250;&#32972;&#26223;&#65288;&#21363;&#32452;&#32455;&#12289;&#22242;&#38431;&#12289;&#25991;&#21270;&#65289;&#26159;AI&#20262;&#29702;&#39046;&#22495;&#21644;&#20915;&#31574;&#32773;&#36827;&#34892;&#31215;&#26497;&#30740;&#31350;&#21644;&#24178;&#39044;&#30340;&#28966;&#28857;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#24120;&#34987;&#24573;&#35270;&#30340;&#31038;&#20250;&#32972;&#26223;&#26041;&#38754;&#65306;&#20174;&#19994;&#32773;&#19982;&#20182;&#20204;&#25152;&#20381;&#36182;&#30340;&#24037;&#20855;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#36825;&#20123;&#20114;&#21160;&#22312;&#22609;&#36896;ML&#23454;&#36341;&#21644;ML&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;Stack Exchange&#35770;&#22363;&#19978;&#25552;&#20986;&#30340;&#38382;&#39064;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;ML&#23454;&#36341;&#20013;&#20351;&#29992;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#65288;&#22914;Jupyter Notebook&#21644;Google Colab&#65289;&#12290;&#25105;&#21457;&#29616;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#22312;&#23398;&#20064;&#21644;&#21327;&#35843;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#26500;&#25104;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#20934;&#30830;&#26657;&#20934;AI&#31995;&#32479;&#20013;&#30340;&#20449;&#24565;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#39640;&#21518;&#24724;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#21644;&#34917;&#25937;&#25104;&#26412;&#23545;&#20110;&#26356;&#26032;&#20915;&#31574;&#32773;&#30340;&#20449;&#24565;&#20197;&#21450;&#25152;&#25345;&#20449;&#24565;&#30340;&#24378;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23558;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20998;&#25104;&#20004;&#31867;:&#20027;&#35266;&#24615;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#22788;&#29702;&#20449;&#24565;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06513</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#25512;&#29702;&#23454;&#29616;&#20449;&#24565;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Leveraging Contextual Counterfactuals Toward Belief Calibration. (arXiv:2307.06513v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#20934;&#30830;&#26657;&#20934;AI&#31995;&#32479;&#20013;&#30340;&#20449;&#24565;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#39640;&#21518;&#24724;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#21644;&#34917;&#25937;&#25104;&#26412;&#23545;&#20110;&#26356;&#26032;&#20915;&#31574;&#32773;&#30340;&#20449;&#24565;&#20197;&#21450;&#25152;&#25345;&#20449;&#24565;&#30340;&#24378;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23558;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20998;&#25104;&#20004;&#31867;:&#20027;&#35266;&#24615;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#22788;&#29702;&#20449;&#24565;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#37319;&#38598;&#21407;&#21017;&#25110;&#27491;&#21017;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#31561;&#26041;&#24335;&#65292;&#20154;&#30340;&#20449;&#24565;&#21644;&#20215;&#20540;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;AI&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#20803;&#23545;&#40784;&#38382;&#39064;&#26159;&#20154;&#31867;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#36328;&#32676;&#20307;&#30340;&#19981;&#23545;&#40784;&#24615;&#65292;&#32780;&#19988;&#21363;&#20351;&#22312;&#20154;&#31867;&#20043;&#38388;&#65292;&#27599;&#20010;&#20449;&#24565;&#30340;&#38544;&#21547;&#24378;&#24230;&#20063;&#21487;&#33021;&#19981;&#22909;&#26657;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23581;&#35797;&#36328;&#19978;&#19979;&#25991;&#36827;&#34892;&#25512;&#24191;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39640;&#21518;&#24724;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#21644;&#34917;&#25937;&#25104;&#26412;&#23545;&#20110;&#26356;&#26032;&#20915;&#31574;&#32773;&#30340;&#20449;&#24565;&#20197;&#21450;&#25152;&#25345;&#20449;&#24565;&#30340;&#24378;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#24341;&#20837;&#21453;&#20107;&#23454;&#25512;&#29702;&#26159;&#20934;&#30830;&#26657;&#20934;&#20449;&#24565;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20998;&#20026;&#20004;&#31867;&#65306;&#20027;&#35266;&#24615;&#65288;&#21516;&#19968;&#32676;&#20307;&#20869;&#30340;&#20010;&#20307;&#38388;&#65289;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65288;&#21516;&#19968;&#20154;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#65289;
&lt;/p&gt;
&lt;p&gt;
Beliefs and values are increasingly being incorporated into our AI systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. However, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. Specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. Therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. To do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06501</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#23454;&#29616;&#20154;&#24037;&#33008;&#33146;
&lt;/p&gt;
&lt;p&gt;
Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20154;&#24037;&#33008;&#33146;(AP)&#22312;&#23454;&#29616;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#38381;&#29615;&#34880;&#31958;&#25511;&#21046;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#30340;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#30340;&#34880;&#31958;&#27979;&#37327;&#65292;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;AP&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#36890;&#36807;&#21160;&#24577;&#27169;&#22411;&#21644;&#23433;&#20840;&#32422;&#26463;&#25552;&#20379;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20294;&#20854;&#32570;&#20047;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#21463;&#21040;&#26410;&#23459;&#24067;&#30340;&#39278;&#39135;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20294;&#38754;&#20020;&#20998;&#24067;&#20559;&#31227;&#21644;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;HyCPAP&#65292;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;HyCPAP&#23558;MPC&#31574;&#30053;&#19982;&#38598;&#25104;DRL&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#24357;&#34917;&#21508;&#33258;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM). However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements. While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals. Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements. Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges. HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations. To facilitate faste
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#30340;&#40657;&#31665;&#25915;&#20987;&#26041;&#27861;&#65292;QuScore&#65292;&#23545;&#25239;&#23545;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#32806;&#21512;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;(IDLSes)&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#20102;&#35299;&#30446;&#26631;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#20943;&#23569;&#26597;&#35810;&#27425;&#25968;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.06496</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#30340;&#40657;&#31665;&#25915;&#20987;&#23545;&#25239;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems. (arXiv:2307.06496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#30340;&#40657;&#31665;&#25915;&#20987;&#26041;&#27861;&#65292;QuScore&#65292;&#23545;&#25239;&#23545;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#32806;&#21512;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;(IDLSes)&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#20102;&#35299;&#30446;&#26631;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#20943;&#23569;&#26597;&#35810;&#27425;&#25968;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#20013;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#25915;&#20987;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#23558;DNN&#27169;&#22411;&#19982;&#35299;&#37322;&#27169;&#22411;&#30456;&#32467;&#21512;&#21487;&#20197;&#22312;&#20154;&#31867;&#19987;&#23478;&#20171;&#20837;&#26102;&#25552;&#20379;&#19968;&#31181;&#23433;&#20840;&#24863;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#30830;&#23450;&#25152;&#32473;&#26679;&#26412;&#26159;&#33391;&#24615;&#30340;&#36824;&#26159;&#24694;&#24847;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;(IDLSes)&#26131;&#21463;&#24694;&#24847;&#31713;&#25913;&#30340;&#25915;&#20987;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#23545;IDLSes&#32452;&#20214;&#30340;&#35775;&#38382;&#21463;&#38480;&#65292;&#23545;&#25163;&#26356;&#38590;&#20197;&#27450;&#39575;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#25928;&#29575;&#21644;&#35780;&#20998;&#30340;&#40657;&#31665;&#25915;&#20987;IDLSes&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;QuScore&#65292;&#23427;&#19981;&#38656;&#35201;&#20102;&#35299;&#30446;&#26631;&#27169;&#22411;&#21450;&#20854;&#32806;&#21512;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;QuScore&#22522;&#20110;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#31181;&#26377;&#25928;&#30340;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#24517;&#35201;&#30340;&#26597;&#35810;&#25968;&#37327;&#20197;&#36827;&#34892;&#25104;&#21151;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are susceptible to adversarial samples in white and black-box environments. Although previous studies have shown high attack success rates, coupling DNN models with interpretation models could offer a sense of security when a human expert is involved, who can identify whether a given sample is benign or malicious. However, in white-box environments, interpretable deep learning systems (IDLSes) have been shown to be vulnerable to malicious manipulations. In black-box settings, as access to the components of IDLSes is limited, it becomes more challenging for the adversary to fool the system. In this work, we propose a Query-efficient Score-based black-box attack against IDLSes, QuScore, which requires no knowledge of the target model and its coupled interpretation model. QuScore is based on transfer-based and score-based methods by employing an effective microbial genetic algorithm. Our method is designed to reduce the number of queries necessary to carry out success
&lt;/p&gt;</description></item><item><title>&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;</title><link>http://arxiv.org/abs/2307.06483</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#23548;&#33268;&#22238;&#24402;&#20998;&#26512;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#33021;&#20462;&#22797;&#21527;&#65311;&#26159;&#30340;&#65292;&#25105;&#20204;&#33021;&#65281;
&lt;/p&gt;
&lt;p&gt;
Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06483
&lt;/p&gt;
&lt;p&gt;
&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#22120;&#65288;ACs&#65289;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#26500;&#24314;&#65292;&#21487;&#20197;&#23545;&#20174;&#25991;&#26412;&#21040;&#22270;&#29255;&#21644;&#35270;&#39057;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24050;&#32463;&#25104;&#20026;&#20256;&#25773;&#31185;&#23398;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24191;&#27867;&#27969;&#34892;&#30340;&#27979;&#37327;&#35774;&#22791;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21363;&#20351;&#26159;&#39640;&#24230;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#20063;&#20250;&#20135;&#29983;&#38169;&#35823;&#65292;&#36825;&#23548;&#33268;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#21644;&#19979;&#28216;&#20998;&#26512;&#20013;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#65292;&#38500;&#38750;&#36825;&#20123;&#20998;&#26512;&#32771;&#34385;&#21040;&#36825;&#20123;&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;SML&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#25773;&#23398;&#32773;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#12290;&#21407;&#21017;&#19978;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#39564;&#35777;&#25968;&#25454;&#65288;&#22914;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#21019;&#24314;&#30340;&#25968;&#25454;&#65289;&#26469;&#32416;&#27491;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#24182;&#20135;&#29983;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;R&#21253;misclassificationmodels&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#26469;&#25581;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#21452;&#32447;&#24615;&#23884;&#20837;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#36827;&#34892;&#22806;&#25512;&#12290;&#36825;&#20010;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#24191;&#20041;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.06457</link><description>&lt;p&gt;
&#35299;&#20915;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65306;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective. (arXiv:2307.06457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#21452;&#32447;&#24615;&#23884;&#20837;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#36827;&#34892;&#22806;&#25512;&#12290;&#36825;&#20010;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#24191;&#20041;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#33719;&#24471;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;(a)&#22312;&#27979;&#35797;&#21644;&#35757;&#32451;&#20998;&#24067;&#19979;&#65292;&#26631;&#31614;$z$&#30001;&#29305;&#24449;$(x,y)$&#30340;&#23545;&#20915;&#23450;&#65292;(b)&#35757;&#32451;&#20998;&#24067;&#28085;&#30422;&#20102;$x$&#21644;$y$&#20998;&#21035;&#30340;&#19968;&#23450;&#36793;&#32536;&#20998;&#24067;&#65292;&#20294;&#26159;(c)&#27979;&#35797;&#20998;&#24067;&#28041;&#21450;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#26410;&#28085;&#30422;&#30340;$(x,y)$&#30340;&#20135;&#21697;&#20998;&#24067;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26631;&#31614;&#30001;&#21452;&#32447;&#24615;&#23884;&#20837;&#21040;Hilbert&#31354;&#38388;$H$&#20013;&#32473;&#20986;&#30340;&#29305;&#27530;&#24773;&#20917;&#65306;$\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23545;&#22312;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#22495;&#36827;&#34892;&#22806;&#25512;&#65292;&#21363;&#23454;&#29616;&#21452;&#32447;&#24615;&#32452;&#21512;&#22806;&#25512;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#24191;&#20041;&#21270;&#65292;&#23545;&#20110;&#35813;&#24773;&#20917;&#65292;&#25152;&#26377;&#29616;&#26377;&#32467;&#26524;&#37117;&#35201;&#27714;....
&lt;/p&gt;
&lt;p&gt;
Obtaining rigorous statistical guarantees for generalization under distribution shift remains an open and active research area. We study a setting we call combinatorial distribution shift, where (a) under the test- and training-distributions, the labels $z$ are determined by pairs of features $(x,y)$, (b) the training distribution has coverage of certain marginal distributions over $x$ and $y$ separately, but (c) the test distribution involves examples from a product distribution over $(x,y)$ that is {not} covered by the training distribution. Focusing on the special case where the labels are given by bilinear embeddings into a Hilbert space $H$: $\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$, we aim to extrapolate to a test distribution domain that is $not$ covered in training, i.e., achieving bilinear combinatorial extrapolation.  Our setting generalizes a special case of matrix completion from missing-not-at-random data, for which all existing results requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25214;&#21040;&#38543;&#26426;&#26102;&#28382;&#24494;&#20998;&#21338;&#24328;&#38381;&#29615;&#32435;&#20160;&#22343;&#34913;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#29609;&#23478;&#30340;&#25511;&#21046;&#36827;&#34892;&#21442;&#25968;&#21270;&#24182;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#24067;&#26391;&#34394;&#25311;&#21338;&#24328;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.06450</link><description>&lt;p&gt;
&#38543;&#26426;&#26102;&#28382;&#24494;&#20998;&#21338;&#24328;&#65306;&#37329;&#34701;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Delay Differential Games: Financial Modeling and Machine Learning Algorithms. (arXiv:2307.06450v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25214;&#21040;&#38543;&#26426;&#26102;&#28382;&#24494;&#20998;&#21338;&#24328;&#38381;&#29615;&#32435;&#20160;&#22343;&#34913;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#29609;&#23478;&#30340;&#25511;&#21046;&#36827;&#34892;&#21442;&#25968;&#21270;&#24182;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#24067;&#26391;&#34394;&#25311;&#21338;&#24328;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25214;&#21040;&#38543;&#26426;&#26102;&#28382;&#24494;&#20998;&#21338;&#24328;&#38381;&#29615;&#32435;&#20160;&#22343;&#34913;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#36825;&#20123;&#21338;&#24328;&#22312;&#37329;&#34701;&#21644;&#32463;&#27982;&#23398;&#20013;&#24456;&#24120;&#35265;&#65292;&#22810;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#24310;&#36831;&#25928;&#24212;&#24448;&#24448;&#26159;&#27169;&#22411;&#20013;&#24076;&#26395;&#30340;&#29305;&#24449;&#65292;&#20294;&#36825;&#20063;&#20250;&#22686;&#21152;&#38382;&#39064;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#19981;&#21516;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#29609;&#23478;&#30340;&#25511;&#21046;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#24067;&#26391;&#34394;&#25311;&#21338;&#24328;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#36825;&#20123;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;&#35299;&#30340;&#37329;&#34701;&#30456;&#20851;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#26032;&#30340;&#38382;&#39064;&#24182;&#25512;&#23548;&#20986;&#23427;&#20204;&#30340;&#20998;&#26512;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a numerical methodology for finding the closed-loop Nash equilibrium of stochastic delay differential games through deep learning. These games are prevalent in finance and economics where multi-agent interaction and delayed effects are often desired features in a model, but are introduced at the expense of increased dimensionality of the problem. This increased dimensionality is especially significant as that arising from the number of players is coupled with the potential infinite dimensionality caused by the delay. Our approach involves parameterizing the controls of each player using distinct recurrent neural networks. These recurrent neural network-based controls are then trained using a modified version of Brown's fictitious play, incorporating deep learning techniques. To evaluate the effectiveness of our methodology, we test it on finance-related problems with known solutions. Furthermore, we also develop new problems and derive their analytical Nash eq
&lt;/p&gt;</description></item><item><title>&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06442</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#21327;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Collaboration in Distributed Parameter Estimation with Resource Constraints. (arXiv:2307.06442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06442
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32771;&#34385;&#36164;&#28304;&#32422;&#26463;&#21644;&#19981;&#21516;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25910;&#38598;&#30340;&#35266;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21442;&#25968;&#20272;&#35745;&#30340;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#32452;&#20256;&#24863;&#22120;/&#20195;&#29702;&#65292;&#27599;&#20010;&#20256;&#24863;&#22120;/&#20195;&#29702;&#26679;&#26412;&#26469;&#33258;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#30340;&#19981;&#21516;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#20272;&#35745;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#38416;&#36848;&#20026;&#36153;&#33293;&#23572;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;&#25110;Cramer-Rao&#30028;&#26368;&#23567;&#21270;&#65289;&#38382;&#39064;&#12290;&#24403;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#22320;&#35782;&#21035;&#20986;&#20004;&#20010;&#29305;&#23450;&#24773;&#20917;&#65306;&#65288;1&#65289;&#19981;&#33021;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#36827;&#34892;&#21327;&#20316;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#65288;2&#65289;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#28041;&#21450;&#25237;&#36164;&#26377;&#38480;&#36164;&#28304;&#20197;&#21327;&#20316;&#37319;&#26679;&#21644;&#36716;&#31227;&#24050;&#30693;&#32479;&#35745;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sensor/agent data collection and collaboration policies for parameter estimation, accounting for resource constraints and correlation between observations collected by distinct sensors/agents. Specifically, we consider a group of sensors/agents each samples from different variables of a multivariate Gaussian distribution and has different estimation objectives, and we formulate a sensor/agent's data collection and collaboration policy design problem as a Fisher information maximization (or Cramer-Rao bound minimization) problem. When the knowledge of correlation between variables is available, we analytically identify two particular scenarios: (1) where the knowledge of the correlation between samples cannot be leveraged for collaborative estimation purposes and (2) where the optimal data collection policy involves investing scarce resources to collaboratively sample and transfer information that is not of immediate interest and whose statistics are already known, with the sol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#32479;&#35745;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#36873;&#25321;&#24615;&#32972;&#26223;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#65292;&#24182;&#29992;&#20110;Belle II&#27979;&#37327;&#31232;&#26377;&#36807;&#31243;&#20013;&#33410;&#32422;&#36164;&#28304;&#24182;&#22788;&#29702;&#36807;&#28388;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.06434</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21152;&#26435;&#20107;&#20214;&#25913;&#36827;Belle II&#30340;&#36873;&#25321;&#24615;&#32972;&#26223;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Improved selective background Monte Carlo simulation at Belle II with graph attention networks and weighted events. (arXiv:2307.06434v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06434
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#32479;&#35745;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#36873;&#25321;&#24615;&#32972;&#26223;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#65292;&#24182;&#29992;&#20110;Belle II&#27979;&#37327;&#31232;&#26377;&#36807;&#31243;&#20013;&#33410;&#32422;&#36164;&#28304;&#24182;&#22788;&#29702;&#36807;&#28388;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Belle II&#27979;&#37327;&#31232;&#26377;&#36807;&#31243;&#26102;&#65292;&#38656;&#35201;&#24040;&#22823;&#30340;&#20142;&#24230;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#25311;&#26469;&#30830;&#23450;&#20449;&#21495;&#25928;&#29575;&#21644;&#32972;&#26223;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#22823;&#37096;&#20998;&#27169;&#25311;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#32972;&#26223;&#24773;&#20917;&#19979;&#65292;&#37117;&#34987;&#20107;&#20214;&#36873;&#25321;&#20002;&#24323;&#20102;&#12290;&#22240;&#27492;&#65292;&#22312;&#26089;&#26399;&#38454;&#27573;&#24341;&#20837;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#28388;&#22120;&#65292;&#20197;&#33410;&#30465;&#36164;&#28304;&#29992;&#20110;&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#20107;&#20214;&#37325;&#24314;&#65292;&#24182;&#19988;&#36824;&#25913;&#36827;&#20102;&#36807;&#28388;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#32479;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#26679;&#21644;&#37325;&#26032;&#21152;&#26435;&#65292;&#26469;&#22788;&#29702;&#36807;&#28388;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When measuring rare processes at Belle II, a huge luminosity is required, which means a large number of simulations are necessary to determine signal efficiencies and background contributions. However, this process demands high computation costs while most of the simulated data, in particular in case of background, are discarded by the event selection. Thus, filters using graph neural networks are introduced at an early stage to save the resources for the detector simulation and reconstruction of events discarded at analysis level. In our work, we improved the performance of the filters using graph attention and investigated statistical methods including sampling and reweighting to deal with the biases introduced by the filtering.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33021;&#37327;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20998;&#25968;&#35745;&#31639;&#25110;&#26114;&#36149;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;&#23454;&#29616;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#65292;&#24182;&#22312;&#23398;&#20064;&#20302;&#32500;&#25968;&#25454;&#20998;&#24067;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06431</link><description>&lt;p&gt;
&#33021;&#37327;&#24046;&#24322;&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#29420;&#31435;&#20110;&#35780;&#20998;&#30340;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. (arXiv:2307.06431v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06431
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33021;&#37327;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20998;&#25968;&#35745;&#31639;&#25110;&#26114;&#36149;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;&#23454;&#29616;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#65292;&#24182;&#22312;&#23398;&#20064;&#20302;&#32500;&#25968;&#25454;&#20998;&#24067;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#27169;&#22411;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#26222;&#21450;&#21463;&#21040;&#20102;&#35757;&#32451;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#33021;&#37327;&#24046;&#24322;&#65288;ED&#65289;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#20998;&#25968;&#30340;&#35745;&#31639;&#25110;&#26114;&#36149;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#30340;&#26497;&#38480;&#19979;&#65292;ED&#25509;&#36817;&#20110;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#65292;&#26377;&#25928;&#22320;&#22312;&#20004;&#32773;&#20043;&#38388;&#25554;&#20540;&#12290;&#22240;&#27492;&#65292;&#26368;&#23567;&#21270;ED&#20272;&#35745;&#20811;&#26381;&#20102;&#22312;&#22522;&#20110;&#20998;&#25968;&#30340;&#20272;&#35745;&#26041;&#27861;&#20013;&#36935;&#21040;&#30340;&#36817;&#35270;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#20139;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#25110;&#23545;&#27604;&#25955;&#24230;&#30456;&#27604;&#65292;ED&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#20302;&#32500;&#25968;&#25454;&#20998;&#24067;&#12290;&#23545;&#20110;&#39640;&#32500;&#22270;&#20687;&#25968;&#25454;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27969;&#24418;&#20551;&#35774;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#23545;e&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35777;&#26126;&#20102;&#33021;&#37327;&#24046;&#24322;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that ED approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum ED estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;&#12290;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#30340;&#31169;&#23494;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06422</link><description>&lt;p&gt;
&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection. (arXiv:2307.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;&#12290;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#30340;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#22312;&#35299;&#20915;&#28041;&#21450;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23454;&#38469;&#23398;&#20064;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22270;&#23398;&#20064;&#26041;&#27861;&#19981;&#20165;&#36890;&#36807;&#20854;&#27169;&#22411;&#21442;&#25968;&#65292;&#36824;&#36890;&#36807;&#20854;&#27169;&#22411;&#39044;&#27979;&#26292;&#38706;&#20102;&#25935;&#24863;&#30340;&#29992;&#25143;&#20449;&#24687;&#21644;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#20165;&#25552;&#20379;&#27169;&#22411;&#26435;&#37325;&#38544;&#31169;&#30340;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25216;&#26415;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#36825;&#23588;&#20854;&#36866;&#29992;&#20110;&#36890;&#36807;&#22270;&#21367;&#31215;&#30452;&#25509;&#21033;&#29992;&#30456;&#37051;&#33410;&#28857;&#23646;&#24615;&#36827;&#34892;&#33410;&#28857;&#39044;&#27979;&#30340;&#24773;&#20917;&#65292;&#36825;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#65288;GDP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#36866;&#29992;&#20110;&#22270;&#23398;&#20064;&#29615;&#22659;&#30340;&#24418;&#24335;&#21270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#37117;&#26159;&#21487;&#35777;&#26126;&#30340;&#31169;&#26377;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#21487;&#33021;&#23384;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25918;&#26494;&#30340;&#33410;&#28857;&#23618;&#27425;&#38544;&#31169;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning methods, such as Graph Neural Networks (GNNs) based on graph convolutions, are highly successful in solving real-world learning problems involving graph-structured data. However, graph learning methods expose sensitive user information and interactions not only through their model parameters but also through their model predictions. Consequently, standard Differential Privacy (DP) techniques that merely offer model weight privacy are inadequate. This is especially the case for node predictions that leverage neighboring node attributes directly via graph convolutions that create additional risks of privacy leakage. To address this problem, we introduce Graph Differential Privacy (GDP), a new formal DP framework tailored to graph learning settings that ensures both provably private model parameters and predictions. Furthermore, since there may be different privacy requirements for the node attributes and graph structure, we introduce a novel notion of relaxed node-level da
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#27979;&#35797;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#29305;&#24449;&#20540;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#36873;&#25321;&#36866;&#24403;&#30340;&#32467;&#26500;&#21457;&#29616;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06406</link><description>&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#27979;&#35797;&#31232;&#30095;&#24615;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Testing Sparsity Assumptions in Bayesian Networks. (arXiv:2307.06406v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#27979;&#35797;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#29305;&#24449;&#20540;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#36873;&#25321;&#36866;&#24403;&#30340;&#32467;&#26500;&#21457;&#29616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#32467;&#26500;&#21457;&#29616;&#31639;&#27861;&#36890;&#24120;&#35201;&#20040;&#23545;&#30495;&#27491;&#30340;&#24213;&#23618;&#32593;&#32476;&#31232;&#30095;&#24615;&#20570;&#20986;&#20551;&#35774;&#65292;&#35201;&#20040;&#21463;&#21040;&#35745;&#31639;&#38480;&#21046;&#32780;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#23569;&#37327;&#21464;&#37327;&#30340;&#32593;&#32476;&#12290;&#23613;&#31649;&#36825;&#20123;&#31232;&#30095;&#24615;&#20551;&#35774;&#21487;&#20197;&#37319;&#21462;&#22810;&#31181;&#24418;&#24335;&#65292;&#20294;&#36890;&#24120;&#20551;&#35774;&#38598;&#20013;&#22312;&#24213;&#23618;&#22270;&#30340;&#26368;&#22823;&#20837;&#24230;&#19978;&#38480;$\nabla_G$&#12290;Duttweiler&#31561;&#20154;&#65288;2023&#65289;&#30340;&#23450;&#29702;2&#35777;&#26126;&#20102;&#32447;&#24615;BN&#30340;&#26631;&#20934;&#21270;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;$\Omega$&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#26159;$\nabla_G$&#30340;&#19968;&#20010;&#19979;&#30028;&#12290;&#22312;&#27492;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;$\Omega$&#26679;&#26412;&#29305;&#24449;&#20540;&#30340;&#28176;&#36817;&#24615;&#36136;&#21644;&#21435;&#20559;&#36807;&#31243;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#31181;&#20551;&#35774;&#26816;&#39564;&#65292;&#21487;&#20197;&#29992;&#26469;&#30830;&#23450;BN&#30340;&#26368;&#22823;&#20837;&#24230;&#26159;&#21542;&#22823;&#20110;1&#12290;&#24314;&#35758;&#22312;&#32447;&#24615;BN&#32467;&#26500;&#21457;&#29616;&#24037;&#20316;&#27969;&#20013;&#20351;&#29992;&#27492;&#20551;&#35774;&#26816;&#39564;&#26469;&#36741;&#21161;&#36873;&#25321;&#36866;&#24403;&#30340;&#32467;&#26500;&#21457;&#29616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian network (BN) structure discovery algorithms typically either make assumptions about the sparsity of the true underlying network, or are limited by computational constraints to networks with a small number of variables. While these sparsity assumptions can take various forms, frequently the assumptions focus on an upper bound for the maximum in-degree of the underlying graph $\nabla_G$. Theorem 2 in Duttweiler et. al. (2023) demonstrates that the largest eigenvalue of the normalized inverse covariance matrix ($\Omega$) of a linear BN is a lower bound for $\nabla_G$. Building on this result, this paper provides the asymptotic properties of, and a debiasing procedure for, the sample eigenvalues of $\Omega$, leading to a hypothesis test that may be used to determine if the BN has max in-degree greater than 1. A linear BN structure discovery workflow is suggested in which the investigator uses this hypothesis test to aid in selecting an appropriate structure discovery algorithm. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;nODEs&#65289;&#27169;&#22411;&#65292;&#23558;&#20854;&#36171;&#20104;&#33258;&#36866;&#24212;&#26102;&#38388;&#23610;&#24230;&#65292;&#24182;&#31216;&#20043;&#20026;&#38376;&#25511;&#31070;&#32463;ODEs&#65288;gnODEs&#65289;&#12290;&#36890;&#36807;&#22312;&#36830;&#32493;&#35760;&#24518;&#20219;&#21153;&#20013;&#23637;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;gnODEs&#20855;&#26377;&#23398;&#20064;&#65288;&#36817;&#20284;&#65289;&#36830;&#32493;&#21560;&#24341;&#23376;&#30340;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38477;&#32500;&#21518;&#30340;gnODEs&#22312;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#21487;&#20197;&#23545;&#23398;&#20064;&#21040;&#30340;&#21560;&#24341;&#23376;&#32467;&#26500;&#36827;&#34892;&#26174;&#24615;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.06398</link><description>&lt;p&gt;
&#35757;&#32451;&#24615;&#12289;&#34920;&#36798;&#24615;&#21644;&#35299;&#37322;&#24615;&#22312;&#38376;&#25511;&#31070;&#32463;ODE&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trainability, Expressivity and Interpretability in Gated Neural ODEs. (arXiv:2307.06398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;nODEs&#65289;&#27169;&#22411;&#65292;&#23558;&#20854;&#36171;&#20104;&#33258;&#36866;&#24212;&#26102;&#38388;&#23610;&#24230;&#65292;&#24182;&#31216;&#20043;&#20026;&#38376;&#25511;&#31070;&#32463;ODEs&#65288;gnODEs&#65289;&#12290;&#36890;&#36807;&#22312;&#36830;&#32493;&#35760;&#24518;&#20219;&#21153;&#20013;&#23637;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;gnODEs&#20855;&#26377;&#23398;&#20064;&#65288;&#36817;&#20284;&#65289;&#36830;&#32493;&#21560;&#24341;&#23376;&#30340;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38477;&#32500;&#21518;&#30340;gnODEs&#22312;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#21487;&#20197;&#23545;&#23398;&#20064;&#21040;&#30340;&#21560;&#24341;&#23376;&#32467;&#26500;&#36827;&#34892;&#26174;&#24615;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21160;&#24577;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#28041;&#21450;&#21040;&#22797;&#26434;&#23384;&#20648;&#21644;&#26816;&#32034;&#30340;&#35745;&#31639;&#23545;&#20110;&#36825;&#20123;&#32593;&#32476;&#26469;&#35828;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#19981;&#23481;&#26131;&#23454;&#29616;&#25110;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#31867;&#30001;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;nODEs&#65289;&#25551;&#36848;&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#20132;&#20114;&#20316;&#29992;&#65292;&#36171;&#20104;nODEs&#33258;&#36866;&#24212;&#26102;&#38388;&#23610;&#24230;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;nODEs&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38376;&#25511;&#31070;&#32463;ODEs&#65288;gnODEs&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#38656;&#35201;&#23545;&#36830;&#32493;&#37327;&#36827;&#34892;&#35760;&#24518;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;gnODEs&#30340;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;&#36817;&#20284;&#65289;&#36830;&#32493;&#21560;&#24341;&#23376;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#38477;&#32500;&#30340;gnODEs&#22914;&#20309;&#20445;&#25345;&#20854;&#24314;&#27169;&#33021;&#21147;&#65292;&#21516;&#26102;&#22823;&#22823;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#29978;&#33267;&#20801;&#35768;&#23545;&#23398;&#20064;&#21040;&#30340;&#21560;&#24341;&#23376;&#32467;&#26500;&#36827;&#34892;&#26174;&#24615;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how the dynamics in biological and artificial neural networks implement the computations required for a task is a salient open question in machine learning and neuroscience. In particular, computations requiring complex memory storage and retrieval pose a significant challenge for these networks to implement or learn. Recently, a family of models described by neural ordinary differential equations (nODEs) has emerged as powerful dynamical neural network models capable of capturing complex dynamics. Here, we extend nODEs by endowing them with adaptive timescales using gating interactions. We refer to these as gated neural ODEs (gnODEs). Using a task that requires memory of continuous quantities, we demonstrate the inductive bias of the gnODEs to learn (approximate) continuous attractors. We further show how reduced-dimensional gnODEs retain their modeling power while greatly improving interpretability, even allowing explicit visualization of the structure of learned attrac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20197;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#20272;&#35745;&#26631;&#31614;&#65292;&#24182;&#25552;&#20986;&#36741;&#21161;&#30446;&#26631;&#26469;&#22788;&#29702;&#21512;&#25104;&#35270;&#39057;&#30340;&#20998;&#24067;&#22806;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06385</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#30340;&#26102;&#38388;&#26631;&#31614;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20197;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#20272;&#35745;&#26631;&#31614;&#65292;&#24182;&#25552;&#20986;&#36741;&#21161;&#30446;&#26631;&#26469;&#22788;&#29702;&#21512;&#25104;&#35270;&#39057;&#30340;&#20998;&#24067;&#22806;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#26159;&#25351;&#22312;&#35270;&#39057;&#20013;&#23545;&#21516;&#26102;&#21487;&#35265;&#21644;&#21487;&#21548;&#21040;&#30340;&#20107;&#20214;&#36827;&#34892;&#26102;&#38388;&#23450;&#20301;&#21644;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#38382;&#39064;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#26377;&#35270;&#39057;&#32423;&#21035;&#30340;&#20107;&#20214;&#26631;&#31614;&#65288;&#20165;&#26377;&#20107;&#20214;&#26159;&#21542;&#20986;&#29616;&#65292;&#20294;&#27809;&#26377;&#26102;&#38388;&#20301;&#32622;&#20449;&#24687;&#65289;&#21487;&#29992;&#20110;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20351;&#29992;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20197;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#20272;&#35745;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#27493;&#39588;&#30830;&#23450;&#35757;&#32451;&#35270;&#39057;&#20013;&#27599;&#20010;&#24103;&#29255;&#27573;&#30340;&#26631;&#31614;&#23376;&#38598;: (i) &#29992;&#21478;&#19968;&#20010;&#35270;&#39057;&#20013;&#19982;&#35270;&#39057;&#32423;&#21035;&#26631;&#31614;&#27809;&#26377;&#37325;&#21472;&#30340;&#24103;&#26367;&#25442;&#29255;&#27573;&#22806;&#30340;&#24103;&#65292; (ii) &#23558;&#36825;&#20010;&#21512;&#25104;&#35270;&#39057;&#36755;&#20837;&#22522;&#30784;&#27169;&#22411;&#65292;&#20165;&#25552;&#21462;&#35813;&#29255;&#27573;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#22788;&#29702;&#21512;&#25104;&#35270;&#39057;&#30340;&#20998;&#24067;&#22806;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#30446;&#26631;&#65292;&#29992;&#20110;&#24341;&#20837;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Event Localization (AVEL) is the task of temporally localizing and classifying \emph{audio-visual events}, i.e., events simultaneously visible and audible in a video. In this paper, we solve AVEL in a weakly-supervised setting, where only video-level event labels (their presence/absence, but not their locations in time) are available as supervision for training. Our idea is to use a base model to estimate labels on the training data at a finer temporal resolution than at the video level and re-train the model with these labels. I.e., we determine the subset of labels for each \emph{slice} of frames in a training video by (i) replacing the frames outside the slice with those from a second video having no overlap in video-level labels, and (ii) feeding this synthetic video into the base model to extract labels for just the slice in question. To handle the out-of-distribution nature of our synthetic videos, we propose an auxiliary objective for the base model that induces mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#20010;&#24615;&#21270;&#26469;&#25913;&#21892;PPG&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#23558;&#21407;&#22987;PPG&#20449;&#21495;&#36716;&#25442;&#20026;&#26356;&#20855;&#21306;&#20998;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#32467;&#21512;&#19977;&#31181;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23454;&#29616;&#36816;&#21160;&#26816;&#27979;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06380</link><description>&lt;p&gt;
&#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#23454;&#29616;&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;PPG&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Personalized Anomaly Detection in PPG Data using Representation Learning and Biometric Identification. (arXiv:2307.06380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#20010;&#24615;&#21270;&#26469;&#25913;&#21892;PPG&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#23558;&#21407;&#22987;PPG&#20449;&#21495;&#36716;&#25442;&#20026;&#26356;&#20855;&#21306;&#20998;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#32467;&#21512;&#19977;&#31181;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23454;&#29616;&#36816;&#21160;&#26816;&#27979;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#25551;&#35760;&#65288;PPG&#65289;&#20449;&#21495;&#36890;&#24120;&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#33719;&#21462;&#65292;&#20855;&#26377;&#25345;&#32493;&#20581;&#24247;&#30417;&#27979;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#37027;&#20123;&#20197;&#32597;&#35265;&#21644;&#24494;&#22937;&#30340;&#24515;&#33039;&#27169;&#24335;&#26469;&#34920;&#29616;&#30340;&#24515;&#33039;&#30142;&#30149;&#21487;&#33021;&#20250;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#39640;&#21464;&#24322;&#24615;&#65292;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#24378;&#20581;&#21487;&#38752;&#30340;&#24322;&#24120;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#20010;&#24615;&#21270;&#26469;&#25552;&#39640;PPG&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23558;&#21407;&#22987;PPG&#20449;&#21495;&#36716;&#25442;&#25104;&#26356;&#20855;&#21306;&#20998;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#19977;&#31181;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;&#36816;&#21160;&#26816;&#27979;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24191;&#20041;&#21644;&#20010;&#24615;&#21270;&#30340;&#24773;&#26223;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) signals, typically acquired from wearable devices, hold significant potential for continuous fitness-health monitoring. In particular, heart conditions that manifest in rare and subtle deviating heart patterns may be interesting. However, robust and reliable anomaly detection within these data remains a challenge due to the scarcity of labeled data and high inter-subject variability. This paper introduces a two-stage framework leveraging representation learning and personalization to improve anomaly detection performance in PPG data. The proposed framework first employs representation learning to transform the original PPG signals into a more discriminative and compact representation. We then apply three different unsupervised anomaly detection methods for movement detection and biometric identification. We validate our approach using two different datasets in both generalized and personalized scenarios. The results show that representation learning significa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35774;&#35745;&#21644;&#35757;&#32451;&#21327;&#35758;&#30340;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#31561;&#20215;&#36215;&#26469;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#25968;&#25454;&#38598;&#38480;&#21046;&#19979;&#20915;&#23450;PINN&#39044;&#27979;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#65292;&#20197;&#21450;&#36890;&#36807;&#21407;&#22987;&#24494;&#20998;&#26041;&#31243;&#20013;&#28304;&#39033;&#30340;&#35889;&#20998;&#35299;&#26469;&#37327;&#21270;&#32593;&#32476;&#24341;&#20837;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.06362</link><description>&lt;p&gt;
&#20855;&#26377;&#35889;&#20559;&#24046;&#21644;&#20869;&#26680;-&#20219;&#21153;&#23545;&#40784;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks. (arXiv:2307.06362v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35774;&#35745;&#21644;&#35757;&#32451;&#21327;&#35758;&#30340;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#31561;&#20215;&#36215;&#26469;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#25968;&#25454;&#38598;&#38480;&#21046;&#19979;&#20915;&#23450;PINN&#39044;&#27979;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#65292;&#20197;&#21450;&#36890;&#36807;&#21407;&#22987;&#24494;&#20998;&#26041;&#31243;&#20013;&#28304;&#39033;&#30340;&#35889;&#20998;&#35299;&#26469;&#37327;&#21270;&#32593;&#32476;&#24341;&#20837;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#35299;&#20915;&#24494;&#20998;&#26041;&#31243;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#65292;PINN&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#21327;&#35758;&#30340;&#36873;&#25321;&#38656;&#35201;&#31934;&#24515;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23545;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#36890;&#36807;&#21033;&#29992;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#22312;&#22823;&#25968;&#25454;&#38598;&#38480;&#21046;&#19979;&#20915;&#23450;PINN&#39044;&#27979;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#8212;&#8212;&#31070;&#32463;&#20449;&#24687;&#26041;&#31243;&#65288;NIE&#65289;&#12290;&#35813;&#26041;&#31243;&#36890;&#36807;&#21453;&#26144;&#26550;&#26500;&#36873;&#25321;&#30340;&#20869;&#26680;&#39033;&#26469;&#34917;&#20805;&#21407;&#22987;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#21407;&#22987;&#24494;&#20998;&#26041;&#31243;&#20013;&#28304;&#39033;&#30340;&#35889;&#20998;&#35299;&#26469;&#37327;&#21270;&#32593;&#32476;&#24341;&#20837;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physically informed neural networks (PINNs) are a promising emerging method for solving differential equations. As in many other deep learning approaches, the choice of PINN design and training protocol requires careful craftsmanship. Here, we suggest a comprehensive theoretical framework that sheds light on this important problem. Leveraging an equivalence between infinitely over-parameterized neural networks and Gaussian process regression (GPR), we derive an integro-differential equation that governs PINN prediction in the large data-set limit -- the Neurally-Informed Equation (NIE). This equation augments the original one by a kernel term reflecting architecture choices and allows quantifying implicit bias induced by the network via a spectral decomposition of the source term in the original differential equation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;X&#23556;&#32447;CT&#20013;&#20943;&#23569;&#25195;&#25551;&#35282;&#24230;&#30340;&#25968;&#37327;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.06343</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;X&#23556;&#32447;CT&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning. (arXiv:2307.06343v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;X&#23556;&#32447;CT&#20013;&#20943;&#23569;&#25195;&#25551;&#35282;&#24230;&#30340;&#25968;&#37327;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#20013;&#65292;&#38656;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#25237;&#24433;&#65292;&#24182;&#29992;&#20110;&#19977;&#32500;&#37325;&#24314;&#12290;&#20026;&#20102;&#20351;CT&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#65292;&#38656;&#35201;&#20943;&#23569;&#35282;&#24230;&#25968;&#30446;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;&#31232;&#30095;&#35282;&#24230;&#26029;&#23618;&#25195;&#25551;&#26159;&#20174;&#26377;&#38480;&#25968;&#25454;&#33719;&#21462;&#19977;&#32500;&#37325;&#24314;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#20026;&#20102;&#20248;&#21270;&#20854;&#24615;&#33021;&#65292;&#21487;&#20197;&#25353;&#24207;&#36866;&#24212;&#25195;&#25551;&#35282;&#24230;&#65292;&#36873;&#25321;&#27599;&#20010;&#25195;&#25551;&#23545;&#35937;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#35282;&#24230;&#12290;&#25968;&#23398;&#19978;&#65292;&#36825;&#23545;&#24212;&#20110;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;OED&#65289;&#38382;&#39064;&#12290;OED&#38382;&#39064;&#26159;&#39640;&#32500;&#12289;&#38750;&#20984;&#12289;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#27861;&#22312;&#32447;&#35299;&#20915;&#65292;&#21363;&#26080;&#27861;&#22312;&#25195;&#25551;&#36807;&#31243;&#20013;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;OED&#38382;&#39064;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#20013;&#24314;&#27169;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#27714;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#37327;&#31163;&#32447;&#35757;&#32451;&#23398;&#20064;&#39640;&#25928;&#30340;&#38750;&#36138;&#23146;&#31574;&#30053;&#26469;&#35299;&#20915;&#32473;&#23450;&#31867;&#21035;&#30340;OED&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In X-ray Computed Tomography (CT), projections from many angles are acquired and used for 3D reconstruction. To make CT suitable for in-line quality control, reducing the number of angles while maintaining reconstruction quality is necessary. Sparse-angle tomography is a popular approach for obtaining 3D reconstructions from limited data. To optimize its performance, one can adapt scan angles sequentially to select the most informative angles for each scanned object. Mathematically, this corresponds to solving and optimal experimental design (OED) problem. OED problems are high-dimensional, non-convex, bi-level optimization problems that cannot be solved online, i.e., during the scan. To address these challenges, we pose the OED problem as a partially observable Markov decision process in a Bayesian framework, and solve it through deep reinforcement learning. The approach learns efficient non-greedy policies to solve a given class of OED problems through extensive offline training rath
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#38477;&#35299;&#27169;&#22411;&#22312;&#25490;&#27745;&#31649;&#36947;CCTV&#26816;&#26597;&#35268;&#21010;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#38598;&#25104;&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26080;&#27861;&#25512;&#27979;&#38271;&#26399;&#38477;&#35299;&#65307;&#19982;&#27492;&#30456;&#21453;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20934;&#30830;&#24230;&#31245;&#20302;&#65292;&#20294;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#19968;&#33268;&#30340;&#38477;&#35299;&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2307.06341</link><description>&lt;p&gt;
&#35780;&#20272;&#38477;&#35299;&#27169;&#22411;&#22312;&#25490;&#27745;&#31649;&#36947;CCTV&#26816;&#26597;&#35268;&#21010;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessment of the suitability of degradation models for the planning of CCTV inspections of sewer pipes. (arXiv:2307.06341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#38477;&#35299;&#27169;&#22411;&#22312;&#25490;&#27745;&#31649;&#36947;CCTV&#26816;&#26597;&#35268;&#21010;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#38598;&#25104;&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26080;&#27861;&#25512;&#27979;&#38271;&#26399;&#38477;&#35299;&#65307;&#19982;&#27492;&#30456;&#21453;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20934;&#30830;&#24230;&#31245;&#20302;&#65292;&#20294;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#19968;&#33268;&#30340;&#38477;&#35299;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#27745;&#31649;&#36947;&#30340;&#38477;&#35299;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;&#32463;&#27982;&#12289;&#29615;&#22659;&#21644;&#20581;&#24247;&#38382;&#39064;&#12290;&#36825;&#20123;&#36164;&#20135;&#30340;&#32500;&#25252;&#38656;&#35201;&#26377;&#32467;&#26500;&#21270;&#30340;&#35745;&#21010;&#26469;&#36827;&#34892;&#26816;&#26597;&#65292;&#32771;&#34385;&#21040;&#32467;&#26500;&#21644;&#29615;&#22659;&#29305;&#24449;&#20197;&#21450;&#20197;&#21069;&#26816;&#26597;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#38477;&#35299;&#27169;&#22411;&#22312;&#35268;&#21010;&#26816;&#26597;&#26102;&#30340;&#36866;&#29992;&#24615;&#65292;&#32771;&#34385;&#20102;&#19977;&#20010;&#32500;&#24230;&#65306;&#20934;&#30830;&#24230;&#25351;&#26631;&#12289;&#33021;&#22815;&#20135;&#29983;&#38271;&#26399;&#38477;&#35299;&#26354;&#32447;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#38598;&#25104;&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#25512;&#27979;&#31649;&#36947;&#30340;&#38271;&#26399;&#38477;&#35299;&#65292;&#32780;&#36923;&#36753;&#22238;&#24402;&#21017;&#25552;&#20379;&#20102;&#30053;&#20302;&#20934;&#30830;&#24230;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#24456;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#33268;&#30340;&#38477;&#35299;&#26354;&#32447;&#12290;&#36890;&#36807;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#21450;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
The degradation of sewer pipes poses significant economical, environmental and health concerns. The maintenance of such assets requires structured plans to perform inspections, which are more efficient when structural and environmental features are considered along with the results of previous inspection reports. The development of such plans requires degradation models that can be based on statistical and machine learning methods. This work proposes a methodology to assess their suitability to plan inspections considering three dimensions: accuracy metrics, ability to produce long-term degradation curves and explainability. Results suggest that although ensemble models yield the highest accuracy, they are unable to infer the long-term degradation of the pipes, whereas the Logistic Regression offers a slightly less accurate model that is able to produce consistent degradation curves with a high explainability. A use case is presented to demonstrate this methodology and the efficiency o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#26631;&#27880;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#23884;&#20837;&#26469;&#24314;&#27169;&#35828;&#35805;&#32773;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24674;&#22797;&#24471;&#20998;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#65292;&#24182;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#32988;&#36807;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06337</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#20316;&#20026;&#39034;&#24207;&#36138;&#23146;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Incomplete Utterance Rewriting as Sequential Greedy Tagging. (arXiv:2307.06337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#26631;&#27880;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#23884;&#20837;&#26469;&#24314;&#27169;&#35828;&#35805;&#32773;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24674;&#22797;&#24471;&#20998;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#65292;&#24182;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#32988;&#36807;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#30340;&#20219;&#21153;&#26368;&#36817;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#20043;&#21069;&#30340;&#27169;&#22411;&#24456;&#38590;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36825;&#22312;&#24674;&#22797;&#24471;&#20998;&#20302;&#30340;&#24773;&#20917;&#19979;&#24471;&#20197;&#35777;&#26126;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#26631;&#27880;&#30340;&#27169;&#22411;&#65292;&#26356;&#25797;&#38271;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#23884;&#20837;&#26469;&#24314;&#27169;&#35828;&#35805;&#32773;&#30340;&#21464;&#21270;&#12290;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#20061;&#20010;&#24674;&#22797;&#24471;&#20998;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26524;&#65292;&#21516;&#26102;&#20854;&#20182;&#25351;&#26631;&#24471;&#20998;&#20063;&#19982;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#24471;&#30410;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at extracting information from context. Meanwhile, we introduce speaker-aware embedding to model speaker variation. Experiments on multiple public datasets show that our model achieves optimal results on all nine restoration scores while having other metric scores comparable to previous state-of-the-art models. Furthermore, benefitting from the model's simplicity, our approach outperforms most previous models on inference speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.06324</link><description>&lt;p&gt;
&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20801;&#35768;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#27425;&#36845;&#20195;&#30340;&#25972;&#20307;&#25928;&#26524;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#19968;&#27425;&#36845;&#20195;&#24402;&#32435;&#20351;&#29992;&#30340;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#30772;&#22351;&#19979;&#38477;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38271;&#36317;&#31163;&#27493;&#39588;&#65292;&#21487;&#33021;&#22312;&#30701;&#26399;&#20869;&#22686;&#21152;&#30446;&#26631;&#20540;&#65292;&#20294;&#22312;&#38271;&#26399;&#20869;&#24102;&#26469;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#29468;&#24819;&#65292;&#24182;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#32500;&#29615;&#22659;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#33021;&#28304;&#31995;&#32479;&#20934;&#30830;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25968;&#25454;&#22312;&#20915;&#31574;&#21644;&#30740;&#31350;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05926</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#25216;&#26415;&#22635;&#34917;&#26102;&#38388;&#24207;&#21015;&#38388;&#38553;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#22810;&#32500;&#29615;&#22659;&#33258;&#32534;&#30721;&#22120;&#30340;&#24314;&#31569;&#33021;&#28304;&#25968;&#25454;&#22635;&#34917;
&lt;/p&gt;
&lt;p&gt;
Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation. (arXiv:2307.05926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#32500;&#29615;&#22659;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#33021;&#28304;&#31995;&#32479;&#20934;&#30830;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25968;&#25454;&#22312;&#20915;&#31574;&#21644;&#30740;&#31350;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#33021;&#28304;&#39044;&#27979;&#21644;&#31649;&#29702;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21463;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#22686;&#38271;&#21644;&#26356;&#22810;&#33021;&#28304;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#30340;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#33021;&#28304;&#25968;&#25454;&#32463;&#24120;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#65292;&#21487;&#33021;&#19981;&#23436;&#25972;&#25110;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#38459;&#30861;&#33021;&#28304;&#31995;&#32479;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#31649;&#29702;&#65292;&#24182;&#38480;&#21046;&#25968;&#25454;&#22312;&#20915;&#31574;&#21644;&#30740;&#31350;&#20013;&#30340;&#29992;&#36884;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22635;&#34917;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#65292;&#21253;&#25324;&#38543;&#26426;&#38388;&#38553;&#21644;&#36830;&#32493;&#38388;&#38553;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#22312;&#21508;&#31181;&#24314;&#31569;&#21644;&#20202;&#34920;&#31867;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#20934;&#30830;&#35780;&#20272;&#19981;&#21516;&#22635;&#34917;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#23558;&#26368;&#20808;&#36827;&#30340;&#22635;&#34917;&#26041;&#27861;&#24212;&#29992;&#20110;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#12290;&#29616;&#20195;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;&#65292;&#22914;&#37096;&#20998;&#21367;&#31215;(PConv)&#65292;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Building energy prediction and management has become increasingly important in recent decades, driven by the growth of Internet of Things (IoT) devices and the availability of more energy data. However, energy data is often collected from multiple sources and can be incomplete or inconsistent, which can hinder accurate predictions and management of energy systems and limit the usefulness of the data for decision-making and research. To address this issue, past studies have focused on imputing missing gaps in energy data, including random and continuous gaps. One of the main challenges in this area is the lack of validation on a benchmark dataset with various building and meter types, making it difficult to accurately evaluate the performance of different imputation methods. Another challenge is the lack of application of state-of-the-art imputation methods for missing gaps in energy data. Contemporary image-inpainting methods, such as Partial Convolution (PConv), have been widely used 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#24322;&#26500;MEC/MCC&#29615;&#22659;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2307.05888</link><description>&lt;p&gt;
&#38024;&#23545;&#36793;&#32536;/&#20113;&#35745;&#31639;&#29615;&#22659;&#20013;&#25968;&#23383;&#23402;&#29983;&#30340;&#39640;&#25928;&#20219;&#21153;&#21368;&#36733;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment. (arXiv:2307.05888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#24322;&#26500;MEC/MCC&#29615;&#22659;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#25968;&#23383;&#23402;&#29983;&#34987;&#35270;&#20026;&#36830;&#25509;&#23454;&#29289;&#23545;&#35937;&#21644;&#25968;&#23383;&#19990;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36890;&#36807;&#34394;&#25311;&#21270;&#21644;&#27169;&#25311;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#23454;&#29616;&#22810;&#31181;&#21151;&#33021;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#31227;&#21160;&#20113;&#35745;&#31639;&#21644;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#24050;&#25104;&#20026;&#23454;&#29616;&#23454;&#26102;&#21453;&#39304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#21482;&#32771;&#34385;&#20102;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#20013;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#25110;&#20113;&#26381;&#21153;&#22120;&#65292;&#21516;&#26102;&#24573;&#30053;&#20102;&#20855;&#26377;&#22810;&#20010;&#25968;&#25454;&#36164;&#28304;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24322;&#26500;MEC/MCC&#29615;&#22659;&#30340;&#26032;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#65292;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#25968;&#23383;&#23402;&#29983;&#37117;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#37319;&#38598;&#35774;&#22791;&#22312;&#26381;&#21153;&#22120;&#20013;&#32500;&#25252;&#12290;&#36824;&#32771;&#34385;&#20102;&#21368;&#36733;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21368;&#36733;&#26041;&#26696;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to empower various areas as a bridge between physical objects and the digital world. Through virtualization and simulation techniques, multiple functions can be achieved by leveraging computing resources. In this process, Mobile Cloud Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key factors to achieve real-time feedback. However, current works only considered edge servers or cloud servers in the DT system models. Besides, The models ignore the DT with not only one data resource. In this paper, we propose a new DT system model considering a heterogeneous MEC/MCC environment. Each DT in the model is maintained in one of the servers via multiple data collection devices. The offloading decision-making problem is also considered and a new offloading scheme is proposed based on Distributed Deep Learning (DDL). Simulation results demonstrate that our proposed algorithm can effectively and efficie
&lt;/p&gt;</description></item><item><title>PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;</title><link>http://arxiv.org/abs/2307.05845</link><description>&lt;p&gt;
PIGEON: &#39044;&#27979;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05845
&lt;/p&gt;
&lt;p&gt;
PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;PIGEON&#65292;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#22312;&#22806;&#37096;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#26631;&#31614;&#24179;&#28369;&#65292;&#23545;&#20855;&#26377;&#22320;&#29702;&#20449;&#24687;&#30340;&#22270;&#20687;&#36827;&#34892;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;ProtoNets&#22312;&#20505;&#36873;&#22320;&#29702;&#21333;&#20803;&#38598;&#21512;&#20013;&#25913;&#36827;&#20301;&#32622;&#39044;&#27979;&#12290;PIGEON&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#30340;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#21019;&#24314;&#21644;&#20998;&#21106;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22320;&#29702;&#21333;&#20803;&#20869;&#37096;&#31934;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;StreetCLIP&#65292;&#20844;&#24320;&#25552;&#20379;&#65292;&#21487;&#29992;&#20110;&#19982;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#22478;&#24066;&#20065;&#26449;&#22330;&#26223;&#29702;&#35299;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03875</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20379;&#24212;&#38142;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20379;&#24212;&#38142;&#25805;&#20316;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20379;&#24212;&#38142;&#21463;&#30410;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20174;&#25163;&#21160;&#22788;&#29702;&#36807;&#28193;&#21040;&#33258;&#21160;&#21270;&#21644;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#36816;&#33829;&#32773;&#20173;&#28982;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#35299;&#37322;&#21644;&#35299;&#35835;&#20248;&#21270;&#32467;&#26524;&#32473;&#30456;&#20851;&#20154;&#22763;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#39072;&#35206;&#24615;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#24357;&#21512;&#20379;&#24212;&#38142;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#29702;&#35299;&#19982;&#20449;&#20219;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;\name{}&#30340;&#26694;&#26550;&#65292;&#23427;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24182;&#27809;&#26377;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#65292;&#32780;&#26159;&#21033;&#29992;&#23427;&#26469;&#23450;&#37327;&#22320;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#20379;&#24212;&#21830;B&#32780;&#19981;&#26159;&#20379;&#24212;&#21830;A&#65292;&#25104;&#26412;&#20250;&#22914;&#20309;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
&lt;/p&gt;</description></item><item><title>inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03854</link><description>&lt;p&gt;
inTformer: &#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#30340;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03854
&lt;/p&gt;
&lt;p&gt;
inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#26159;&#20027;&#21160;&#20132;&#36890;&#23433;&#20840;&#31649;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22810;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#12290;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#20107;&#25925;&#28508;&#22312;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22522;&#26412;&#21407;&#29702;&#26159;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;Transformer&#22312;&#21151;&#33021;&#19978;&#27604;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;LSTM&#65292;CNN&#31561;&#65289;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;Transformer&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;Transformer&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#12290;&#26368;&#21518;&#65292;Transformer&#19981;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#35748;&#35782;&#21040;Transformer&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#23454;&#36136;&#19978;&#26159;&#38024;&#23545;&#35813;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02719</link><description>&lt;p&gt;
&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#23454;&#36136;&#19978;&#26159;&#38024;&#23545;&#35813;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#39034;&#24207;&#22320;&#26597;&#35810;&#24403;&#21069;&#39044;&#27979;&#27169;&#22411;&#23545;&#25968;&#25454;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#20351;&#29992;&#24448;&#24448;&#26159;&#21551;&#21457;&#24335;&#30340;&#65306;&#65288;i&#65289;&#20851;&#20110;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#19979;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#20934;&#30830;&#23450;&#20041;&#27809;&#26377;&#20849;&#35782;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#33021;&#22815;&#32473;&#20986;&#19968;&#20010;&#26631;&#20934;&#21327;&#35758;&#26469;&#23454;&#26045;&#35813;&#31639;&#27861;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31561;&#20248;&#21270;&#31639;&#27861;&#26694;&#26550;&#19979;&#22914;&#20309;&#22788;&#29702;&#39034;&#24207;&#21040;&#36798;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#21644;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#30830;&#31435;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#26412;&#36136;&#19978;&#26159;&#38024;&#23545;&#36825;&#31181;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#19968;&#35266;&#28857;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#36866;&#24403;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01497</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#19968;&#33324;&#22122;&#22768;&#20551;&#35774;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31867;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#35266;&#27979;&#30340;&#22122;&#22768;&#30340;&#26041;&#24046;&#19982;&#31639;&#27861;&#20135;&#29983;&#30340;&#36817;&#20284;&#35299;&#30340;"&#20122;&#26368;&#20248;&#24615;" &#30456;&#20851;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#32479;&#35745;&#23398;&#20013;&#30340;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#37117;&#26410;&#36798;&#21040;&#26368;&#20248;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#8212;&#8212;&#38543;&#26426;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65288;SAGD&#65289;&#21644;&#38543;&#26426;&#26799;&#24230;&#22806;&#25512;&#65288;SGE&#65289;&#8212;&#8212;&#23427;&#20204;&#20855;&#26377;&#19968;&#31181;&#29305;&#27530;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.  We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#33041;&#21151;&#33021;&#36830;&#25509;&#30340;&#36712;&#36857;&#23884;&#20837;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#33041;&#36830;&#25509;&#22270;&#23884;&#20837;&#32763;&#35793;&#22120;&#65288;Brain TokenGT&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#33041;&#21151;&#33021;&#36830;&#25509;&#38543;&#30142;&#30149;&#36827;&#23637;&#30340;&#28436;&#21464;&#26041;&#24335;&#65292;&#23545;&#20110;&#21046;&#23450;&#26377;&#25928;&#30340;&#30142;&#30149;&#24178;&#39044;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.00858</link><description>&lt;p&gt;
&#36229;&#36234;&#24555;&#29031;&#65306;&#22522;&#20110;&#33041;&#36830;&#25509;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#33041;&#21151;&#33021;&#36830;&#25509;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding. (arXiv:2307.00858v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#33041;&#21151;&#33021;&#36830;&#25509;&#30340;&#36712;&#36857;&#23884;&#20837;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#33041;&#36830;&#25509;&#22270;&#23884;&#20837;&#32763;&#35793;&#22120;&#65288;Brain TokenGT&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#33041;&#21151;&#33021;&#36830;&#25509;&#38543;&#30142;&#30149;&#36827;&#23637;&#30340;&#28436;&#21464;&#26041;&#24335;&#65292;&#23545;&#20110;&#21046;&#23450;&#26377;&#25928;&#30340;&#30142;&#30149;&#24178;&#39044;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#32593;&#32476;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26694;&#26550;&#19979;&#65292;&#22522;&#20110;&#33041;&#21151;&#33021;&#36830;&#25509;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#35832;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31561;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#19987;&#38376;&#38024;&#23545;&#21333;&#20010;&#26102;&#38388;&#28857;&#30340;&#33041;&#21151;&#33021;&#36830;&#25509;&#32780;&#19981;&#26159;&#23545;&#33041;&#21151;&#33021;&#36830;&#25509;&#30340;&#36712;&#36857;&#36827;&#34892;&#34920;&#24449;&#12290;&#36776;&#26126;&#33041;&#21151;&#33021;&#36830;&#25509;&#38543;&#30142;&#30149;&#36827;&#23637;&#30340;&#28436;&#21464;&#26041;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;&#20110;&#35748;&#30693;&#27491;&#24120;&#30340;&#28096;&#31881;&#26679;&#27785;&#31215;&#25110;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#30340;&#21069;&#30196;&#21574;&#38454;&#27573;&#65292;&#23545;&#20110;&#25551;&#32472;&#30142;&#30149;&#20256;&#25773;&#27169;&#24335;&#24182;&#21046;&#23450;&#26377;&#25928;&#30340;&#20943;&#32531;&#29978;&#33267;&#38459;&#27490;&#30142;&#30149;&#36827;&#23637;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#33041;&#21151;&#33021;&#36830;&#25509;&#36712;&#36857;&#23884;&#20837;&#30340;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#21363;&#33041;&#36830;&#25509;&#22270;&#23884;&#20837;&#32763;&#35793;&#22120;&#65288;Brain TokenGT&#65289;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#22270;&#19981;&#21464;&#21644;&#21464;&#20307;&#23884;&#20837;&#65288;
&lt;/p&gt;
&lt;p&gt;
Under the framework of network-based neurodegeneration, brain functional connectome (FC)-based Graph Neural Networks (GNN) have emerged as a valuable tool for the diagnosis and prognosis of neurodegenerative diseases such as Alzheimer's disease (AD). However, these models are tailored for brain FC at a single time point instead of characterizing FC trajectory. Discerning how FC evolves with disease progression, particularly at the predementia stages such as cognitively normal individuals with amyloid deposition or individuals with mild cognitive impairment (MCI), is crucial for delineating disease spreading patterns and developing effective strategies to slow down or even halt disease advancement. In this work, we proposed the first interpretable framework for brain FC trajectory embedding with application to neurodegenerative disease diagnosis and prognosis, namely Brain Tokenized Graph Transformer (Brain TokenGT). It consists of two modules: 1) Graph Invariant and Variant Embedding (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#22122;&#38899;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65288;PeLPA&#65289;&#20197;&#32469;&#36807;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#38024;&#23545;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;CMARL&#65289;&#20013;&#30340;&#31169;&#26377;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#30340;&#20013;&#27602;&#23041;&#32961;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#65292;PeLPA&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24179;&#22343;&#27493;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.00268</link><description>&lt;p&gt;
&#26126;&#37324;&#26263;&#20013;&#65306;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#22122;&#38899;&#30340;&#25269;&#24481;&#24694;&#24847;&#25915;&#20987;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (arXiv:2307.00268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#22122;&#38899;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65288;PeLPA&#65289;&#20197;&#32469;&#36807;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#38024;&#23545;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;CMARL&#65289;&#20013;&#30340;&#31169;&#26377;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#30340;&#20013;&#27602;&#23041;&#32961;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#65292;PeLPA&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24179;&#22343;&#27493;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#34987;&#24341;&#20837;&#21040;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;CMARL&#65289;&#20013;&#65292;&#20197;&#20445;&#25252;&#26234;&#33021;&#20307;&#22312;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#20813;&#21463;&#23545;&#25163;&#30340;&#25512;&#26029;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#30001;DP&#26426;&#21046;&#24341;&#20837;&#30340;&#22122;&#38899;&#21487;&#33021;&#20250;&#22312;CMARL&#20013;&#30340;&#31169;&#26377;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#24847;&#22806;&#22320;&#20135;&#29983;&#19968;&#31181;&#26032;&#30340;&#20013;&#27602;&#23041;&#32961;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21033;&#29992;&#38544;&#31169;&#30340;&#12289;&#25269;&#24481;&#36867;&#36991;&#25915;&#20987;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65288;PeLPA&#65289;&#65292;&#21033;&#29992;&#20102;DP&#22122;&#38899;&#30340;&#29305;&#24615;&#65292;&#32469;&#36807;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#38459;&#30861;CMARL&#27169;&#22411;&#30340;&#26368;&#20248;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;PeLPA&#25915;&#20987;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38750;&#23545;&#25239;&#21644;&#22810;&#23545;&#25239;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20013;&#31561;&#35268;&#27169;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#27604;&#20363;&#20026;20%&#21644;40%&#30340;PeLPA&#25915;&#20987;&#21487;&#33021;&#23548;&#33268;&#24179;&#22343;&#27493;&#25968;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, differential privacy (DP) has been introduced in cooperative multiagent reinforcement learning (CMARL) to safeguard the agents' privacy against adversarial inference during knowledge sharing. Nevertheless, we argue that the noise introduced by DP mechanisms may inadvertently give rise to a novel poisoning threat, specifically in the context of private knowledge sharing during CMARL, which remains unexplored in the literature. To address this shortcoming, we present an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the inherent DP-noise to circumvent anomaly detection systems and hinder the optimal convergence of the CMARL model. We rigorously evaluate our proposed PeLPA attack in diverse environments, encompassing both non-adversarial and multiple-adversarial contexts. Our findings reveal that, in a medium-scale environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to an increase in average steps t
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24863;&#30693;&#27169;&#22359;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#25216;&#26415;&#12290;&#20854;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#20998;&#31867;&#31995;&#32479;&#65292;&#24635;&#32467;&#20102;&#38598;&#25104;&#25805;&#20316;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#38416;&#26126;&#20102;&#8220;&#29702;&#24819;&#8221;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#21487;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#20248;&#21270;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.11740</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#25968;&#25454;&#38598;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on deep learning approaches for data integration in autonomous driving system. (arXiv:2306.11740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24863;&#30693;&#27169;&#22359;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#25216;&#26415;&#12290;&#20854;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#20998;&#31867;&#31995;&#32479;&#65292;&#24635;&#32467;&#20102;&#38598;&#25104;&#25805;&#20316;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#38416;&#26126;&#20102;&#8220;&#29702;&#24819;&#8221;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#21487;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#20248;&#21270;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#30340;&#24863;&#30693;&#27169;&#22359;&#20381;&#36182;&#20110;&#22810;&#20256;&#24863;&#22120;&#31995;&#32479;&#26469;&#29702;&#35299;&#20854;&#29615;&#22659;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#24863;&#30693;&#27979;&#37327;&#30340;&#38598;&#25104;&#26041;&#27861;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#24863;&#30693;&#33021;&#21147;&#12290;&#26412;&#25991;&#23545;&#24212;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24863;&#30693;&#27169;&#22359;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#25216;&#26415;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#23558;&#38598;&#25104;&#26041;&#27861;&#20998;&#20026;&#8220;&#20309;&#26102;&#38598;&#25104;&#8221;&#65292;&#8220;&#22914;&#20309;&#38598;&#25104;&#8221;&#21644;&#8220;&#20309;&#26102;&#38598;&#25104;&#8221;&#19977;&#31867;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#20998;&#31867;&#31995;&#32479;&#65292;&#22522;&#20110;&#19977;&#20010;&#32500;&#24230;&#65306;&#22810;&#35270;&#35282;&#65292;&#22810;&#27169;&#24577;&#21644;&#22810;&#24103;&#12290;&#24635;&#32467;&#20102;&#38598;&#25104;&#25805;&#20316;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#38416;&#26126;&#20102;&#8220;&#29702;&#24819;&#8221;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#21487;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#32463;&#36807;&#23545;&#19978;&#30334;&#31687;&#30456;&#20851;&#35770;&#25991;&#30340;&#23457;&#26597;&#65292;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20248;&#21270;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The perception module of self-driving vehicles relies on a multi-sensor system to understand its environment. Recent advancements in deep learning have led to the rapid development of approaches that integrate multi-sensory measurements to enhance perception capabilities. This paper surveys the latest deep learning integration techniques applied to the perception module in autonomous driving systems, categorizing integration approaches based on "what, how, and when to integrate." A new taxonomy of integration is proposed, based on three dimensions: multi-view, multi-modality, and multi-frame. The integration operations and their pros and cons are summarized, providing new insights into the properties of an "ideal" data integration approach that can alleviate the limitations of existing methods. After reviewing hundreds of relevant papers, this survey concludes with a discussion of the key features of an optimal data integration approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06283</link><description>&lt;p&gt;
LLMs&#22914;&#20309;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#38750;&#24120;&#22797;&#26434;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#25110;&#35745;&#31639;&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#20013;&#26377;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#38656;&#35201;&#38750;&#24120;&#29305;&#23450;&#24418;&#24335;&#30340;&#32467;&#26500;&#20197;&#21450;&#24037;&#20855;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#25152;&#24102;&#26469;&#21487;&#29992;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#25361;&#25112;&#12290;&#21152;&#19978;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#22823;&#22810;&#25968;&#25968;&#25454;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#20107;&#23454;&#65292;&#20351;&#24471;&#36825;&#20123;&#24037;&#20855;&#30340;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35760;&#24405;&#20102;&#20851;&#20110;LLMs&#30340;&#40657;&#23458;&#26494;&#27963;&#21160;&#20013;&#26500;&#24314;&#30340;&#39033;&#30446;&#12290;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#29305;&#24615;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#21508;&#31181;&#21508;&#26679;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36825;&#20123;&#27169;&#22411;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemistry and materials science are complex. Recently, there have been great successes in addressing this complexity using data-driven or computational techniques. Yet, the necessity of input structured in very specific forms and the fact that there is an ever-growing number of tools creates usability and accessibility challenges. Coupled with the reality that much data in these disciplines is unstructured, the effectiveness of these tools is limited.  Motivated by recent works that indicated that large language models (LLMs) might help address some of these issues, we organized a hackathon event on the applications of LLMs in chemistry, materials science, and beyond. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications.  The diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#21452;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;FedDVA&#65289;&#26126;&#30830;&#20998;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#25429;&#25417;&#20849;&#20139;&#30693;&#35782;&#21644;&#23458;&#25143;&#29305;&#23450;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03570</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#20010;&#24615;&#21270;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Personalization Disentanglement for Federated Learning. (arXiv:2306.03570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#21452;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;FedDVA&#65289;&#26126;&#30830;&#20998;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#25429;&#25417;&#20849;&#20139;&#30693;&#35782;&#21644;&#23458;&#25143;&#29305;&#23450;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#24179;&#34913;&#30340;&#30693;&#35782;&#20849;&#20139;&#21644;&#27169;&#22411;&#20010;&#24615;&#21270;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#32852;&#21512;&#35757;&#32451;&#21508;&#31181;&#23616;&#37096;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#26126;&#30830;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#26469;&#35299;&#20915;PFL&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#20849;&#20139;&#30693;&#35782;&#21644;&#23458;&#25143;&#29305;&#23450;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;PFL&#12290;&#35813;&#20998;&#31163;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#21452;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;FedDVA&#65289;&#23454;&#29616;&#30340;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#32534;&#30721;&#22120;&#26469;&#25512;&#26029;&#20004;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#12290;FedDVA&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;PFL&#20013;&#20840;&#23616;&#30693;&#35782;&#20849;&#20139;&#21644;&#26412;&#22320;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#21464;&#20026;&#29992;&#20110;&#24322;&#26500;&#19979;&#28216;&#20219;&#21153;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#20998;&#31163;&#25152;&#24102;&#26469;&#30340;&#20248;&#21183;&#65292;&#24182;&#34920;&#26126;&#32463;&#36807;&#20998;&#31163;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#37027;&#20123;&#26222;&#36890;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (PFL) jointly trains a variety of local models through balancing between knowledge sharing across clients and model personalization per client. This paper addresses PFL via explicit disentangling latent representations into two parts to capture the shared knowledge and client-specific personalization, which leads to more reliable and effective PFL. The disentanglement is achieved by a novel Federated Dual Variational Autoencoder (FedDVA), which employs two encoders to infer the two types of representations. FedDVA can produce a better understanding of the trade-off between global knowledge sharing and local personalization in PFL. Moreover, it can be integrated with existing FL methods and turn them into personalized models for heterogeneous downstream tasks. Extensive experiments validate the advantages caused by disentanglement and show that models trained with disentangled representations substantially outperform those vanilla methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15639</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24191;&#20041;p-Laplacian&#27491;&#21017;&#21270;&#26694;&#26550;&#22270;&#21367;&#31215;&#32593;&#32476;: &#25910;&#25947;&#24615;&#12289;&#33021;&#37327;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#25193;&#25955;&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#33021;&#37327;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#24191;&#20041;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#22810;&#31181;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#65292;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#22270;p-Laplacian&#30340;Framelet&#32593;&#32476;&#65288;pL-UFG&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#24314;&#31435;&#23545;&#20854;&#24615;&#36136;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Framelet&#21367;&#31215;&#21518;&#38598;&#25104;p-Laplacian&#30340;&#38544;&#24335;&#23618;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;pL-UFG&#28176;&#36817;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#25506;&#32034;pL-UFG&#30340;&#24191;&#20041;Dirichlet&#33021;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Dirichlet&#33021;&#37327;&#20445;&#25345;&#38750;&#38646;&#65292;&#30830;&#20445;&#22312;pL-UFG&#25509;&#36817;&#25910;&#25947;&#26102;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#33021;&#37327;&#35270;&#35282;&#38416;&#26126;&#20102;pL-UFG&#20013;&#30340;&#38544;&#24335;&#23618;&#19982;&#22270;Framelets&#21327;&#21516;&#24037;&#20316;&#65292;&#22686;&#24378;&#20102;&#35813;&#27169;&#22411;&#23545;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38544;&#24335;&#23618;&#21487;&#20197;&#34987;&#35299;&#37322;&#25104;&#24191;&#20041;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#36825;&#20123;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#23548;&#33268;&#20102;&#32479;&#19968;&#30340;&#32467;&#35770;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive theoretical analysis of graph p-Laplacian based framelet network (pL-UFG) to establish a solid understanding of its properties. We begin by conducting a convergence analysis of the p-Laplacian based implicit layer integrated after the framelet convolution, providing insights into the asymptotic behavior of pL-UFG. By exploring the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it approaches convergence. Furthermore, we elucidate the dynamic energy perspective through which the implicit layer in pL-UFG synergizes with graph framelets, enhancing the model's adaptability to both homophilic and heterophilic data. Remarkably, we establish that the implicit layer can be interpreted as a generalized non-linear diffusion process, enabling training using diverse schemes. These multifaceted analyses lead to unified conclusions that provide novel insi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#21644;&#36820;&#22238;bootstrapping&#27493;&#39588;&#26469;&#23454;&#29616;&#28789;&#27963;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.12330</link><description>&lt;p&gt;
&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;&#36830;&#32493;&#27969;&#25511;&#21046;&#24212;&#29992;&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#21644;&#36820;&#22238;bootstrapping&#27493;&#39588;&#26469;&#23454;&#29616;&#28789;&#27963;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#25968;&#20540;&#27969;&#25511;&#38382;&#39064;&#30340;&#32806;&#21512;&#36817;&#26399;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#24182;&#20026;&#35813;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#27714;&#35299;&#22120;&#30340;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#24456;&#39640;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#26159;&#23454;&#29616;&#26377;&#25928;&#25511;&#21046;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#27969;&#25511;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20173;&#20381;&#36182;&#20110;on-policy&#31639;&#27861;&#65292;&#32780;&#36825;&#31181;&#31639;&#27861;&#30340;&#39640;&#24182;&#34892;&#36716;&#31227;&#25910;&#38598;&#21487;&#33021;&#20250;&#30772;&#22351;&#29702;&#35770;&#20551;&#35774;&#24182;&#23548;&#33268;&#27425;&#20248;&#30340;&#25511;&#21046;&#27169;&#22411;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#30340;&#24182;&#34892;&#27169;&#24335;&#65292;&#36890;&#36807;&#19968;&#20010;&#36820;&#22238;bootstrapping&#27493;&#39588;&#65292;&#20801;&#35768;&#28789;&#27963;&#22320;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#19968;&#20010;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#30340;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#31526;&#21512;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.11336</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders. (arXiv:2304.11336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#31526;&#21512;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#34987;&#35465;&#20026;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20998;&#26512;&#30340;&#38134;&#24377;&#12290;&#22914;&#26524;&#19968;&#26465;&#35760;&#24405;&#19981;&#26159;&#30495;&#23454;&#30340;&#65292;&#37027;&#20040;&#23427;&#24590;&#20040;&#20250;&#20405;&#29359;&#20010;&#20154;&#30340;&#38544;&#31169;&#21602;&#65311;&#27492;&#22806;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#34987;&#25104;&#21151;&#22320;&#29992;&#20110;&#36817;&#20284;&#34920;&#31034;&#25968;&#25454;&#30340;&#39640;&#32500;&#22797;&#26434;&#20998;&#24067;&#65292;&#24182;&#20174;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#20013;&#25277;&#21462;&#36924;&#30495;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24448;&#24448;&#24573;&#35270;&#29983;&#25104;&#27169;&#22411;&#23481;&#26131;&#35760;&#24518;&#20010;&#20154;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35768;&#22810;&#32454;&#33410;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#24448;&#24448;&#22826;&#31867;&#20284;&#20110;&#24213;&#23618;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#36829;&#21453;&#20102;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#25152;&#36935;&#21040;&#30340;&#24378;&#38544;&#31169;&#27861;&#35268;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#30830;&#20445;&#20445;&#25252;&#25935;&#24863;&#20010;&#20154;&#25968;&#25454;&#30340;&#22269;&#38469;&#20844;&#35748;&#30340;&#26368;&#20808;&#36827;&#26694;&#26550;&#65292;&#20351;&#24471;&#21487;&#20197;&#20844;&#24320;&#21457;&#24067;&#38598;&#21512;&#32479;&#35745;&#25968;&#25454;&#21644;&#29978;&#33267;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#20250;&#24433;&#21709;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#26426;&#21046;&#24448;&#24448;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#36807;&#22810;&#30340;&#22122;&#22768;&#65292;&#22240;&#27492;&#24456;&#38590;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#34987;&#27491;&#21017;&#21270;&#20197;&#30830;&#20445;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#30340;&#32534;&#30721;&#34920;&#31034;&#23613;&#21487;&#33021;&#22320;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#20139;&#26368;&#23569;&#30340;&#20449;&#24687;&#65292;&#32780;&#29983;&#25104;&#22120;&#21017;&#34987;&#35757;&#32451;&#20197;&#36817;&#20284;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#20005;&#26684;&#31526;&#21512;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data has been hailed as the silver bullet for privacy preserving data analysis. If a record is not real, then how could it violate a person's privacy? In addition, deep-learning based generative models are employed successfully to approximate complex high-dimensional distributions from data and draw realistic samples from this learned distribution. It is often overlooked though that generative models are prone to memorising many details of individual training records and often generate synthetic data that too closely resembles the underlying sensitive training data, hence violating strong privacy regulations as, e.g., encountered in health care. Differential privacy is the well-known state-of-the-art framework for guaranteeing protection of sensitive individuals' data, allowing aggregate statistics and even machine learning models to be released publicly without compromising privacy. The training mechanisms however often add too much noise during the training process, and thu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#23558;&#25910;&#25947;&#32467;&#35770;&#20174;&#21482;&#36866;&#29992;&#20110;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#32858;&#21512;&#20989;&#25968;&#25193;&#23637;&#21040;&#25152;&#26377;&#20256;&#32479;&#32858;&#21512;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.11140</link><description>&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#38543;&#26426;&#22270;&#19978;&#30340;&#36890;&#29992;&#32858;&#21512;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs. (arXiv:2304.11140v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#23558;&#25910;&#25947;&#32467;&#35770;&#20174;&#21482;&#36866;&#29992;&#20110;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#32858;&#21512;&#20989;&#25968;&#25193;&#23637;&#21040;&#25152;&#26377;&#20256;&#32479;&#32858;&#21512;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#24403;&#33410;&#28857;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#38480;&#26102;&#65292;&#35813;&#32593;&#32476;&#27169;&#22411;&#33021;&#25910;&#25947;&#20110;&#20854;&#36830;&#32493;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#35813;&#25910;&#25947;&#24615;&#32467;&#26524;&#21482;&#36866;&#29992;&#20110;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#20540;&#24418;&#24335;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;&#27492;&#32467;&#26524;&#25193;&#23637;&#21040;&#21253;&#21547;&#25152;&#26377;&#20256;&#32479;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#31867;&#32858;&#21512;&#20989;&#25968;&#19978;&#65292;&#20363;&#22914;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#26368;&#22823;&#21367;&#31215;&#30340;&#32593;&#32476;&#12290;&#22312;&#19968;&#23450;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#39640;&#27010;&#29575;&#30340;&#38750;&#28176;&#36827;&#19978;&#38480;&#26469;&#37327;&#21270;&#36825;&#31181;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22522;&#20110;McDiarmid&#19981;&#31561;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#29305;&#21035;&#22788;&#29702;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#38750;&#24120;&#19981;&#21516;&#30340;&#35777;&#26126;&#25216;&#24039;&#65292;&#24182;&#20135;&#29983;&#20102;&#23450;&#24615;&#19981;&#21516;&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence of message passing graph neural networks on random graph models to their continuous counterpart as the number of nodes tends to infinity. Until now, this convergence was only known for architectures with aggregation functions in the form of degree-normalized means. We extend such results to a very large class of aggregation functions, that encompasses all classically used message passing graph neural networks, such as attention-based mesage passing or max convolutional message passing on top of (degree-normalized) convolutional message passing. Under mild assumptions, we give non asymptotic bounds with high probability to quantify this convergence. Our main result is based on the McDiarmid inequality. Interestingly, we treat the case where the aggregation is a coordinate-wise maximum separately, at it necessitates a very different proof technique and yields a qualitatively different convergence rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#30284;&#30151;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08715</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#20998;&#31867;&#30340;EfficientNet&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
EfficientNet Algorithm for Classification of Different Types of Cancer. (arXiv:2304.08715v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#30284;&#30151;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#22320;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#23545;&#26089;&#26399;&#21457;&#29616;&#21644;&#26377;&#25928;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#33041;&#30244;&#12289;&#20083;&#33146;&#30284;&#20083;&#25151;X&#32447;&#25668;&#24433;&#12289;&#33016;&#37096;&#30284;&#21644;&#30382;&#32932;&#30284;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#24182;&#23545;&#22270;&#20687;&#36827;&#34892;&#39044;&#22788;&#29702;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#27604;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;EfficientNet&#31639;&#27861;&#22312;&#27599;&#20010;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#21644;&#39640;F1&#20998;&#25968;&#65292;&#20248;&#20110;&#25991;&#29486;&#20013;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;EfficientNet&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#21450;&#20854;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EfficientNet&#31639;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#30340;&#20998;&#31867;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient classification of different types of cancer is critical for early detection and effective treatment. In this paper, we present the results of our experiments using the EfficientNet algorithm for classification of brain tumor, breast cancer mammography, chest cancer, and skin cancer. We used publicly available datasets and preprocessed the images to ensure consistency and comparability. Our experiments show that the EfficientNet algorithm achieved high accuracy, precision, recall, and F1 scores on each of the cancer datasets, outperforming other state-of-the-art algorithms in the literature. We also discuss the strengths and weaknesses of the EfficientNet algorithm and its potential applications in clinical practice. Our results suggest that the EfficientNet algorithm is well-suited for classification of different types of cancer and can be used to improve the accuracy and efficiency of cancer diagnosis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36807;&#31243;&#35889;&#23494;&#24230;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#21644;&#36827;&#34892;&#20449;&#21495;&#25554;&#20540;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#26102;&#38388;-&#39030;&#28857;&#20449;&#21495;&#20272;&#35745;&#38382;&#39064;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06887</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;-&#39030;&#28857;&#35889;&#23398;&#20064;&#22270;ARMA&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Graph ARMA Processes from Time-Vertex Spectra. (arXiv:2302.06887v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36807;&#31243;&#35889;&#23494;&#24230;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#21644;&#36827;&#34892;&#20449;&#21495;&#25554;&#20540;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#26102;&#38388;-&#39030;&#28857;&#20449;&#21495;&#20272;&#35745;&#38382;&#39064;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26102;&#38388;&#21464;&#21270;&#30340;&#22270;&#20449;&#21495;&#24314;&#27169;&#20026;&#31283;&#24577;&#26102;&#38388;-&#39030;&#28857;&#38543;&#26426;&#36807;&#31243;&#65292;&#21487;&#20197;&#36890;&#36807;&#26377;&#25928;&#22320;&#21033;&#29992;&#36807;&#31243;&#22312;&#19981;&#21516;&#22270;&#33410;&#28857;&#21644;&#26102;&#38388;&#30636;&#38388;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#27169;&#24335;&#26469;&#25512;&#26029;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#23398;&#20064;&#36807;&#31243;&#30340;&#19981;&#23436;&#25972;&#23454;&#29616;&#30340;&#32852;&#21512;&#26102;&#38388;-&#39030;&#28857;&#21151;&#29575;&#35889;&#23494;&#24230;&#26469;&#35745;&#31639;&#22270;&#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343;&#65288;&#22270;ARMA&#65289;&#36807;&#31243;&#65292;&#20197;&#29992;&#20110;&#20449;&#21495;&#25554;&#20540;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#39318;&#20808;&#36890;&#36807;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#23454;&#29616;&#31895;&#30053;&#20272;&#35745;&#36807;&#31243;&#30340;&#32852;&#21512;&#35889;&#65292;&#28982;&#21518;&#36890;&#36807;&#20984;&#26494;&#24347;&#23558;&#20854;&#25237;&#24433;&#21040;&#22270;ARMA&#36807;&#31243;&#30340;&#35889;&#27969;&#24418;&#19978;&#26469;&#25913;&#36827;&#36825;&#20010;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#20272;&#35745;&#26368;&#21021;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;-&#39030;&#28857;&#20449;&#21495;&#20272;&#35745;&#38382;&#39064;&#20013;&#36798;&#21040;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modeling of time-varying graph signals as stationary time-vertex stochastic processes permits the inference of missing signal values by efficiently employing the correlation patterns of the process across different graph nodes and time instants. In this study, we propose an algorithm for computing graph autoregressive moving average (graph ARMA) processes based on learning the joint time-vertex power spectral density of the process from its incomplete realizations for the task of signal interpolation. Our solution relies on first roughly estimating the joint spectrum of the process from partially observed realizations and then refining this estimate by projecting it onto the spectrum manifold of the graph ARMA process through convex relaxations. The initially missing signal values are then estimated based on the learnt model. Experimental results show that the proposed approach achieves high accuracy in time-vertex signal estimation problems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#24341;&#20837;&#20102;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#65292;&#21152;&#36895;&#20102;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00482</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25209;&#37327;&#20248;&#21270;&#20256;&#36755;&#25913;&#36827;&#21644;&#27867;&#21270;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving and generalizing flow-based generative models with minibatch optimal transport. (arXiv:2302.00482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00482
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#24341;&#20837;&#20102;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#65292;&#21152;&#36895;&#20102;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#20854;&#22522;&#20110;&#27169;&#25311;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#23384;&#22312;&#23616;&#38480;&#24615;&#32780;&#21463;&#21040;&#32422;&#26463;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;CNFs&#30340;&#26080;&#27169;&#25311;&#35757;&#32451;&#30446;&#26631;&#30340;&#38598;&#21512;&#12290;CFM&#20855;&#26377;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#20013;&#29992;&#20110;&#35757;&#32451;&#38543;&#26426;&#27969;&#30340;&#31283;&#23450;&#22238;&#24402;&#30446;&#26631;&#65292;&#20294;&#21516;&#26102;&#20139;&#26377;&#30830;&#23450;&#24615;&#27969;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#26029;&#12290;&#19982;&#25193;&#25955;&#27169;&#22411;&#21644;&#20043;&#21069;&#30340;CNF&#35757;&#32451;&#31639;&#27861;&#30456;&#27604;&#65292;CFM&#19981;&#38656;&#35201;&#28304;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#20063;&#19981;&#38656;&#35201;&#23545;&#20854;&#23494;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#30340;&#19968;&#31181;&#21464;&#20307;&#26159;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#65292;&#23427;&#21019;&#24314;&#20102;&#26356;&#31616;&#21333;&#30340;&#27969;&#65292;&#26356;&#23481;&#26131;&#35757;&#32451;&#65292;&#24182;&#19988;&#23548;&#33268;&#26356;&#24555;&#30340;&#25512;&#26029;&#65292;&#22914;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#25152;&#31034;&#12290;&#27492;&#22806;&#65292;OT-CFM&#26159;&#31532;&#19968;&#31181;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;CFM&#35757;&#32451;CNFs&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;&#26465;&#20214;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, OT-CFM is the first method to compute dynamic OT in a simulation-free way. Training CNFs with CFM improves results on a variety of conditional and u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.00422</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26631;&#35760;&#30340;&#35266;&#27979;&#25968;&#25454;&#24182;&#19981;&#31616;&#21333;&#65292;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#24178;&#39044;&#25110;&#20351;&#29992;&#26114;&#36149;&#30340;&#27979;&#35797;&#35774;&#22791;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#25311;&#21512;&#27169;&#22411;&#26102;&#26368;&#20449;&#24687;&#25968;&#25454;&#28857;&#30340;&#24314;&#35758;&#12290;&#20943;&#23569;&#27169;&#22411;&#24320;&#21457;&#25152;&#38656;&#30340;&#35266;&#27979;&#25968;&#25454;&#25968;&#37327;&#21487;&#20197;&#20943;&#36731;&#35757;&#32451;&#25152;&#38656;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#26631;&#35760;&#30456;&#20851;&#30340;&#25805;&#20316;&#25903;&#20986;&#12290;&#29305;&#21035;&#26159;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#22312;&#38656;&#35201;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#20915;&#23450;&#26159;&#21542;&#33719;&#21462;&#25968;&#25454;&#28857;&#26631;&#35760;&#30340;&#39640;&#23481;&#37327;&#29983;&#20135;&#36807;&#31243;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#33268;&#21147;&#20110;&#24320;&#21457;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20294;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#32447;&#20027;&#21160;&#32447;&#24615;&#22238;&#24402;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#35777;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;StyleGAN-XL&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2301.12811</link><description>&lt;p&gt;
SAN: &#21033;&#29992;&#21028;&#21035;&#24335;&#24402;&#19968;&#21270;&#32447;&#24615;&#23618;&#35825;&#23548;GAN&#30340;&#21487;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer. (arXiv:2301.12811v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;StyleGAN-XL&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#26469;&#23398;&#20064;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#20248;&#21270;&#26159;&#21542;&#30495;&#27491;&#25552;&#20379;&#20102;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GAN&#30340;&#24418;&#24335;&#19982;&#20999;&#29255;&#26368;&#20248;&#36755;&#36816;&#30340;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#25512;&#23548;&#20102;&#21487;&#27979;&#24615;&#26465;&#20214;&#65292;&#21363;&#21028;&#21035;&#22120;&#20316;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#31216;&#20026;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#23558;&#24191;&#27867;&#31867;&#21035;&#30340;&#29616;&#26377;GAN&#36716;&#21270;&#20026;SAN&#12290;&#22312;&#21512;&#25104;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#21644;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;SAN&#24212;&#29992;&#20110;StyleGAN-XL&#65292;&#22312;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;GAN&#20013;&#26368;&#20808;&#36827;&#30340;FID&#65288;Frechet Inception Distance&#65289;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme, called slicing adversarial network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the SAN's effectiveness as compared to usual GANs. Furthermore, we also apply SAN to StyleGAN-XL, which leads to state-of-the-art FID score amongst GANs for class con
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#24615;&#33021;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23545;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.11873</link><description>&lt;p&gt;
&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#24615;&#33021;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23545;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#65288;BMC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31454;&#20105;&#35745;&#31639;&#27169;&#22411;&#30340;&#30456;&#23545;&#20248;&#21183;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;&#27169;&#22411;&#36873;&#25321;&#20915;&#31574;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#23884;&#22871;&#21442;&#25968;&#32467;&#26500;&#65292;BMC&#22312;&#24120;&#35265;&#30340;&#23618;&#27425;&#27169;&#22411;&#20013;&#24120;&#24120;&#38590;&#20197;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#21487;&#23454;&#20363;&#21270;&#20026;&#27010;&#29575;&#31243;&#24207;&#30340;&#23618;&#27425;&#27169;&#22411;&#38598;&#36827;&#34892;BMC&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#20309;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#20043;&#21069;&#65292;&#23545;&#21518;&#39564;&#27169;&#22411;&#27010;&#29575;&#36827;&#34892;&#39640;&#25928;&#30340;&#37325;&#26032;&#20272;&#35745;&#21644;&#24555;&#36895;&#24615;&#33021;&#39564;&#35777;&#12290;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#39564;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#27604;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26725;&#24335;&#25277;&#26679;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25152;&#26377;BMC&#35774;&#32622;&#20013;&#20986;&#33394;&#30340;&#20998;&#25674;&#25512;&#26029;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#20808;&#21069;&#34987;&#35748;&#20026;&#26159;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian model comparison (BMC) offers a principled approach for assessing the relative merits of competing computational models and propagating uncertainty into model selection decisions. However, BMC is often intractable for the popular class of hierarchical models due to their high-dimensional nested parameter structure. To address this intractability, we propose a deep learning method for performing BMC on any set of hierarchical models which can be instantiated as probabilistic programs. Since our method enables amortized inference, it allows efficient re-estimation of posterior model probabilities and fast performance validation prior to any real-data application. In a series of extensive validation studies, we benchmark the performance of our method against the state-of-the-art bridge sampling method and demonstrate excellent amortized inference across all BMC settings. We then showcase our method by comparing four hierarchical evidence accumulation models that have previously b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EPiC-GAN&#30340;&#31561;&#21464;&#28857;&#20113;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#29983;&#25104;&#19981;&#21516;&#22810;&#37325;&#24615;&#30340;&#28857;&#20113;&#65292;&#35813;&#32593;&#32476;&#22522;&#20110;&#28145;&#24230;&#38598;&#21512;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#27169;&#25311;&#31890;&#23376;&#21943;&#27969;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;EPiC-GAN&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08128</link><description>&lt;p&gt;
EPiC-GAN: &#31561;&#21464;&#28857;&#20113;&#29983;&#25104;&#29992;&#20110;&#31890;&#23376;&#21943;&#27880;
&lt;/p&gt;
&lt;p&gt;
EPiC-GAN: Equivariant Point Cloud Generation for Particle Jets. (arXiv:2301.08128v3 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EPiC-GAN&#30340;&#31561;&#21464;&#28857;&#20113;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#29983;&#25104;&#19981;&#21516;&#22810;&#37325;&#24615;&#30340;&#28857;&#20113;&#65292;&#35813;&#32593;&#32476;&#22522;&#20110;&#28145;&#24230;&#38598;&#21512;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#27169;&#25311;&#31890;&#23376;&#21943;&#27969;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;EPiC-GAN&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24403;&#21069;&#21644;&#26410;&#26469;&#39640;&#33021;&#23545;&#25758;&#26426;&#23454;&#39564;&#30340;&#24040;&#22823;&#25968;&#25454;&#37319;&#38598;&#33021;&#21147;&#65292;&#23545;&#20110;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27169;&#25311;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#20107;&#20214;&#29983;&#25104;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#25968;&#25454;&#32467;&#26500;&#21644;&#20005;&#26684;&#30340;&#25506;&#27979;&#22120;&#20960;&#20309;&#32422;&#26463;&#19979;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EPiC-GAN - &#31561;&#21464;&#28857;&#20113;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#21464;&#22810;&#37325;&#24615;&#30340;&#28857;&#20113;&#12290;&#36825;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#22522;&#20110;&#28145;&#24230;&#38598;&#21512;&#65292;&#24182;&#36866;&#29992;&#20110;&#27169;&#25311;&#31216;&#20026;&#21943;&#27880;&#30340;&#31890;&#23376;&#21943;&#27969;&#12290;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#21033;&#29992;&#22810;&#20010;EPiC&#23618;&#21644;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20840;&#23616;&#28508;&#22312;&#21521;&#37327;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;EPiC&#23618;&#19981;&#20381;&#36182;&#20110;&#31890;&#23376;&#20043;&#38388;&#30340;&#25104;&#23545;&#20449;&#24687;&#20849;&#20139;&#65292;&#36825;&#23548;&#33268;&#19982;&#22522;&#20110;&#22270;&#21644;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#22270;&#34920;&#30456;&#27604;&#30340;&#26174;&#33879;&#21152;&#36895;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;EPiC-GAN&#22312;&#22823;&#31890;&#23376;&#22810;&#37325;&#24615;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the vast data-collecting capabilities of current and future high-energy collider experiments, there is an increasing demand for computationally efficient simulations. Generative machine learning models enable fast event generation, yet so far these approaches are largely constrained to fixed data structures and rigid detector geometries. In this paper, we introduce EPiC-GAN - equivariant point cloud generative adversarial network - which can produce point clouds of variable multiplicity. This flexible framework is based on deep sets and is well suited for simulating sprays of particles called jets. The generator and discriminator utilize multiple EPiC layers with an interpretable global latent vector. Crucially, the EPiC layers do not rely on pairwise information sharing between particles, which leads to a significant speed-up over graph- and transformer-based approaches with more complex relation diagrams. We demonstrate that EPiC-GAN scales well to large particle multiplicities 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.03044</link><description>&lt;p&gt;
&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;Transformers&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#23548;&#31070;&#32463;&#26550;&#26500;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39046;&#22495;&#20013;&#20063;&#20986;&#29616;&#20102;&#31867;&#20284;&#30340;&#20351;&#29992;Transformers&#30340;&#28526;&#27969;&#65292;&#20294;&#38754;&#20020;&#30528;RL&#30340;&#29305;&#27530;&#35774;&#35745;&#36873;&#25321;&#21644;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;Transformers&#22312;RL&#20013;&#30340;&#21457;&#23637;&#23578;&#26410;&#34987;&#20805;&#20998;&#25581;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22312;RL&#20013;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#21644;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#35752;&#35770;&#20102;&#27599;&#20010;&#23376;&#39046;&#22495;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;Continual-Dreamer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;Minigrid&#21644;Minihack&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15944</link><description>&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of World Models for Continual Reinforcement Learning. (arXiv:2211.15944v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;Continual-Dreamer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;Minigrid&#21644;Minihack&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20123;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#65292;&#21363;&#26234;&#33021;&#20307;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#24773;&#20917;&#12290;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#36830;&#32493;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#23545;&#24615;&#33021;&#12289;&#36951;&#24536;&#21644;&#36801;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#30340;&#21508;&#31181;&#24314;&#27169;&#36873;&#39033;&#30340;&#24314;&#35758;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#31216;&#20026;Continual-Dreamer&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#25506;&#32034;&#12290;Continual-Dreamer&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;Minigrid&#21644;Minihack&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
World models power some of the most efficient reinforcement learning algorithms. In this work, we showcase that they can be harnessed for continual learning - a situation when the agent faces changing environments. World models typically employ a replay buffer for training, which can be naturally extended to continual learning. We systematically study how different selective experience replay methods affect performance, forgetting, and transfer. We also provide recommendations regarding various modeling options for using world models. The best set of choices is called Continual-Dreamer, it is task-agnostic and utilizes the world model for continual exploration. Continual-Dreamer is sample efficient and outperforms state-of-the-art task-agnostic continual reinforcement learning methods on Minigrid and Minihack benchmarks.
&lt;/p&gt;</description></item><item><title>Control Transformer&#26159;&#19968;&#31181;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#24341;&#23548;&#30340;&#20302;&#23618;&#31574;&#30053;&#24314;&#27169;&#36820;&#22238;&#26465;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.06407</link><description>&lt;p&gt;
&#25511;&#21046;&#21464;&#21387;&#22120;&#65306;&#36890;&#36807;PRM-Guided Return-Conditioned&#24207;&#21015;&#24314;&#27169;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling. (arXiv:2211.06407v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06407
&lt;/p&gt;
&lt;p&gt;
Control Transformer&#26159;&#19968;&#31181;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#24341;&#23548;&#30340;&#20302;&#23618;&#31574;&#30053;&#24314;&#27169;&#36820;&#22238;&#26465;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#20219;&#21153;&#65292;&#22914;&#23548;&#33322;&#65292;&#23545;&#20110;&#25104;&#21151;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#25552;&#20986;&#20102;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#22312;&#24050;&#30693;&#29615;&#22659;&#19979;&#65292;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#21487;&#20197;&#22312;&#19981;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#31283;&#20581;&#22320;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Control Transformer&#65292;&#36890;&#36807;&#30001;&#22522;&#20110;&#37319;&#26679;&#30340;&#27010;&#29575;&#22320;&#22270;&#65288;PRM&#65289;&#35268;&#21010;&#22120;&#24341;&#23548;&#30340;&#20302;&#23618;&#31574;&#30053;&#24314;&#27169;&#36820;&#22238;&#26465;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#20165;&#23616;&#37096;&#20449;&#24687;&#35299;&#20915;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#37096;&#20998;&#35266;&#23519;&#30340;&#36855;&#23467;&#23548;&#33322;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Ant&#65292;Point&#21644;Humanoid&#30340;MuJoCo&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Control Transformer&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#36855;&#23467;&#20013;&#23548;&#33322;&#24182;&#36716;&#31227;&#21040;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#24046;&#20998;&#39537;&#21160;&#26426;&#22120;&#20154;&#65288;Turtlebot3&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#35266;&#27979;&#19979;&#30340;&#38646;&#26679;&#26412;sim2real&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning long-horizon tasks such as navigation has presented difficult challenges for successfully applying reinforcement learning to robotics. From another perspective, under known environments, sampling-based planning can robustly find collision-free paths in environments without learning. In this work, we propose Control Transformer that models return-conditioned sequences from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM) planner. We demonstrate that our framework can solve long-horizon navigation tasks using only local information. We evaluate our approach on partially-observed maze navigation with MuJoCo robots, including Ant, Point, and Humanoid. We show that Control Transformer can successfully navigate through mazes and transfer to unknown environments. Additionally, we apply our method to a differential drive robot (Turtlebot3) and show zero-shot sim2real transfer under noisy observations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#25554;&#20540;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#21160;&#24577;&#27169;&#25311;&#20013;&#38477;&#20302;&#24314;&#27169;&#26102;&#38388;&#24182;&#20445;&#25345;&#39640;&#30340;&#29983;&#25104;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.01856</link><description>&lt;p&gt;
&#20154;&#31867;&#29983;&#29289;&#29289;&#29702;&#23398;&#20316;&#20026;&#32593;&#32476;&#26435;&#37325;&#65306;&#29992;&#20110;&#21160;&#24577;&#27169;&#25311;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation. (arXiv:2211.01856v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01856
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#25554;&#20540;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#21160;&#24577;&#27169;&#25311;&#20013;&#38477;&#20302;&#24314;&#27169;&#26102;&#38388;&#24182;&#20445;&#25345;&#39640;&#30340;&#29983;&#25104;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#30340;&#27169;&#25311;&#23545;&#20110;&#30740;&#31350;&#29983;&#29702;&#26426;&#21046;&#21644;&#24320;&#21457;&#20154;&#26426;&#30028;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#26377;&#38480;&#20803;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#24403;&#29983;&#25104;&#22823;&#37327;&#27169;&#25311;&#25110;&#27169;&#25311;&#20855;&#26377;&#36830;&#32493;&#21464;&#21270;&#32467;&#26500;&#21442;&#25968;&#30340;&#21160;&#24577;&#20107;&#20214;&#26102;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#20540;&#27169;&#22411;&#29366;&#24577;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#30340;&#26550;&#26500;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#24314;&#27169;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29983;&#25104;&#31934;&#24230;&#12290;&#20316;&#20026;&#36825;&#19968;&#27010;&#24565;&#30340;&#31034;&#33539;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioMime&#65292;&#19968;&#31181;&#28151;&#21512;&#32467;&#26500;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21160;&#24577;&#21464;&#21270;&#26399;&#38388;&#23545;&#29305;&#23450;&#29983;&#29289;&#29289;&#29702;&#31995;&#32479;&#36827;&#34892;&#20934;&#30830;&#12289;&#36229;&#24555;&#36895;&#21644;&#20219;&#24847;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#29983;&#29702;&#21644;&#20020;&#24202;&#30740;&#31350;&#20197;&#21450;&#25903;&#25345;&#20449;&#21495;&#20998;&#26512;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulations of biophysical systems are fundamental for studying physiological mechanisms and developing human machine interfaces. Whilst advanced numerical methods, such as finite element models, can excel in this task, they are extremely computationally expensive to use when generating a large number of simulations or simulating dynamic events with continuously changing structural parameters. We propose an architecture that uses a conditional generative model to interpolate between the numerical model states, dramatically lowering the modeling time while maintaining a high generation accuracy. As a demonstration of this concept, we present BioMime, a hybrid-structured generative model that enables an accurate, ultra-fast, and arbitrarily high temporal-resolution simulation of a specific biophysical system during dynamic changes. This methodology has wide applications in physiological and clinical research as well as in supporting data augmentation strategies for signal analysis, repre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#31574;&#30053;&#25915;&#20987;&#65292;&#25105;&#20204;&#25104;&#21151;&#25112;&#32988;&#20102;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI KataGo&#65292;&#25581;&#31034;&#20102;&#20854;&#26680;&#24515;&#24369;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#36229;&#32423;AI&#31995;&#32479;&#20063;&#21487;&#33021;&#23384;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2211.00241</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#31574;&#30053;&#25112;&#32988;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI
&lt;/p&gt;
&lt;p&gt;
Adversarial Policies Beat Superhuman Go AIs. (arXiv:2211.00241v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00241
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#31574;&#30053;&#25915;&#20987;&#65292;&#25105;&#20204;&#25104;&#21151;&#25112;&#32988;&#20102;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI KataGo&#65292;&#25581;&#31034;&#20102;&#20854;&#26680;&#24515;&#24369;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#36229;&#32423;AI&#31995;&#32479;&#20063;&#21487;&#33021;&#23384;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#23545;&#25239;&#24615;&#31574;&#30053;&#26469;&#25915;&#20987;&#26368;&#20808;&#36827;&#30340;&#22260;&#26827;AI&#31995;&#32479;KataGo&#65292;&#22312;&#36229;&#20154;&#31867;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#36229;&#36807;97%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23545;&#25163;&#24182;&#19981;&#26159;&#36890;&#36807;&#20986;&#33394;&#22320;&#19979;&#22260;&#26827;&#26469;&#33719;&#32988;&#65292;&#32780;&#26159;&#36890;&#36807;&#35825;&#20351;KataGo&#29359;&#19979;&#20005;&#37325;&#22833;&#35823;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#38646;&#25439;&#32791;&#22320;&#20256;&#36755;&#32473;&#20854;&#20182;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI&#65292;&#24182;&#19988;&#23545;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#26159;&#21487;&#20197;&#29702;&#35299;&#30340;&#65292;&#20182;&#20204;&#21487;&#20197;&#22312;&#27809;&#26377;&#31639;&#27861;&#36741;&#21161;&#30340;&#24773;&#20917;&#19979;&#23454;&#26045;&#36825;&#31181;&#25915;&#20987;&#26469;&#25345;&#32493;&#25112;&#32988;&#36229;&#32423;&#20154;&#31867;&#32423;AI&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#25581;&#31034;&#20102;KataGo&#30340;&#26680;&#24515;&#24369;&#28857;&#65292;&#21363;&#20351;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;KataGo&#20195;&#29702;&#20063;&#26080;&#27861;&#38450;&#24481;&#25105;&#20204;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#36229;&#32423;&#20154;&#31867;&#32423;&#30340;AI&#31995;&#32479;&#20063;&#21487;&#33021;&#23384;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We attack the state-of-the-art Go-playing AI system KataGo by training adversarial policies against it, achieving a &gt;97% win rate against KataGo running at superhuman settings. Our adversaries do not win by playing Go well. Instead, they trick KataGo into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing AIs, and is comprehensible to the extent that human experts can implement it without algorithmic assistance to consistently beat superhuman AIs. The core vulnerability uncovered by our attack persists even in KataGo agents adversarially trained to defend against our attack. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes. Example games are available https://goattack.far.ai/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;Laplacian&#27491;&#21017;&#21270;&#26694;&#26550;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21033;&#29992;&#20102;p-Laplacian&#30340;&#24615;&#36136;&#21644;&#22270;&#20449;&#21495;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#20449;&#21495;&#21435;&#22122;&#31561;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.15092</link><description>&lt;p&gt;
&#24191;&#20041;&#30340;Laplacian&#27491;&#21017;&#21270;&#26694;&#26550;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalized Laplacian Regularized Framelet Graph Neural Networks. (arXiv:2210.15092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;Laplacian&#27491;&#21017;&#21270;&#26694;&#26550;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21033;&#29992;&#20102;p-Laplacian&#30340;&#24615;&#36136;&#21644;&#22270;&#20449;&#21495;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#20449;&#21495;&#21435;&#22122;&#31561;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;p-Laplacian GNN&#30340;Framelet&#22270;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#20004;&#20010;&#27169;&#22411;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;p-Laplacian&#19981;&#19979;&#37319;&#26679;framelet&#22270;&#21367;&#31215;&#65288;pL-UFG&#65289;&#21644;&#24191;&#20041;p-Laplacian&#19981;&#19979;&#37319;&#26679;framelet&#22270;&#21367;&#31215;&#65288;pL-fUFG&#65289;&#65292;&#32487;&#25215;&#20102;p-Laplacian&#30340;&#24615;&#36136;&#65292;&#24182;&#32467;&#21512;&#20102;&#22270;&#20449;&#21495;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#35777;&#30740;&#31350;&#31361;&#20986;&#20102;pL-UFG&#21644;pL-fUFG&#22312;&#19981;&#21516;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#21644;&#20449;&#21495;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel Framelet Graph approach based on p-Laplacian GNN. The proposed two models, named p-Laplacian undecimated framelet graph convolution (pL-UFG) and generalized p-Laplacian undecimated framelet graph convolution (pL-fUFG) inherit the nature of p-Laplacian with the expressive power of multi-resolution decomposition of graph signals. The empirical study highlights the excellent performance of the pL-UFG and pL-fUFG in different graph learning tasks including node classification and signal denoising.
&lt;/p&gt;</description></item><item><title>RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.14905</link><description>&lt;p&gt;
RulE: &#20351;&#29992;&#35268;&#21017;&#23884;&#20837;&#30340;&#31070;&#32463;-&#31526;&#21495;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding. (arXiv:2210.14905v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14905
&lt;/p&gt;
&lt;p&gt;
RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25512;&#29702;&#23545;&#20110;&#30693;&#35782;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#26377;&#21407;&#21017;&#23450;&#20301;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;RulE&#65288;&#20195;&#34920;&#35268;&#21017;&#23884;&#20837;&#65289;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#22686;&#24378;KG&#25512;&#29702;&#12290;&#19982;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;RulE&#36890;&#36807;&#22312;&#32479;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#32852;&#21512;&#34920;&#31034;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#29616;&#26377;&#19977;&#20803;&#32452;&#21644;&#19968;&#38454;&#35268;&#21017;&#20013;&#23398;&#20064;&#35268;&#21017;&#23884;&#20837;&#12290;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#35268;&#21017;&#23884;&#20837;&#65292;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#35268;&#21017;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#21453;&#26144;&#20854;&#19982;&#35266;&#23519;&#21040;&#30340;&#19977;&#20803;&#32452;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20197;&#36719;&#26041;&#24335;&#36827;&#34892;&#36923;&#36753;&#35268;&#21017;&#25512;&#29702;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36923;&#36753;&#30340;&#33030;&#24369;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;RulE&#23558;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20016;&#23500;&#21644;&#35268;&#33539;&#21270;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#20063;&#20351;&#24471;&#20165;&#20351;&#29992;KGE&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;RulE&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#19988;&#22312;&#23454;&#39564;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26031;&#22374;&#26816;&#39564;&#30340;&#36866;&#37197;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#32500;&#24230;&#35266;&#27979;&#30340;&#27010;&#29575;&#23494;&#24230;&#27169;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#25991;&#26723;&#25110;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#26680;&#26031;&#22374;&#24046;&#24322;(KSD)&#21040;&#21487;&#21464;&#32500;&#24230;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;KSD&#36866;&#37197;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#26080;&#38656;&#23494;&#24230;&#24402;&#19968;&#21270;&#65292;&#24182;&#22312;&#31163;&#25955;&#39034;&#24207;&#25968;&#25454;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.10741</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#26031;&#22374;&#26816;&#39564;&#30340;&#39034;&#24207;&#27169;&#22411;&#36866;&#37197;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
A kernel Stein test of goodness of fit for sequential models. (arXiv:2210.10741v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10741
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26031;&#22374;&#26816;&#39564;&#30340;&#36866;&#37197;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#32500;&#24230;&#35266;&#27979;&#30340;&#27010;&#29575;&#23494;&#24230;&#27169;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#25991;&#26723;&#25110;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#26680;&#26031;&#22374;&#24046;&#24322;(KSD)&#21040;&#21487;&#21464;&#32500;&#24230;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;KSD&#36866;&#37197;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#26080;&#38656;&#23494;&#24230;&#24402;&#19968;&#21270;&#65292;&#24182;&#22312;&#31163;&#25955;&#39034;&#24207;&#25968;&#25454;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#32500;&#24230;&#35266;&#27979;&#30340;&#27010;&#29575;&#23494;&#24230;&#27169;&#22411;&#30340;&#36866;&#37197;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20363;&#22914;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#25110;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#30340;&#25991;&#26412;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#26159;&#26680;&#26031;&#22374;&#24046;&#24322;(Kernel Stein Discrepancy, KSD)&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;KSD&#24050;&#34987;&#29992;&#20110;&#26500;&#24314;&#38750;&#26631;&#20934;&#21270;&#23494;&#24230;&#30340;&#36866;&#37197;&#24615;&#26816;&#39564;&#12290;KSD&#36890;&#36807;&#26031;&#22374;&#31639;&#23376;&#26469;&#23450;&#20041;&#65292;&#30446;&#21069;&#29992;&#20110;&#26816;&#39564;&#30340;&#26031;&#22374;&#31639;&#23376;&#36866;&#29992;&#20110;&#22266;&#23450;&#32500;&#24230;&#31354;&#38388;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21512;&#36866;&#30340;&#26031;&#22374;&#31639;&#23376;&#65292;&#22312;&#21487;&#21464;&#32500;&#24230;&#35774;&#32622;&#19979;&#25193;&#23637;&#20102;KSD&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;KSD&#36866;&#37197;&#24615;&#26816;&#39564;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#21464;&#20307;&#19968;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;KSD&#19981;&#35201;&#27714;&#23494;&#24230;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#37327;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#22312;&#31163;&#25955;&#39034;&#24207;&#25968;&#25454;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a goodness-of-fit measure for probability densities modeling observations with varying dimensionality, such as text documents of differing lengths or variable-length sequences. The proposed measure is an instance of the kernel Stein discrepancy (KSD), which has been used to construct goodness-of-fit tests for unnormalized densities. The KSD is defined by its Stein operator: current operators used in testing apply to fixed-dimensional spaces. As our main contribution, we extend the KSD to the variable-dimension setting by identifying appropriate Stein operators, and propose a novel KSD goodness-of-fit test. As with the previous variants, the proposed KSD does not require the density to be normalized, allowing the evaluation of a large class of models. Our test is shown to perform well in practice on discrete sequential data benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.10081</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31163;&#25955;&#22411;Soft Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Discrete Soft Actor-Critic. (arXiv:2209.10081v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#65288;SAC&#65289;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;SAC&#26041;&#27861;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#20102;&#22312;&#31163;&#25955;&#35774;&#32622;&#19979;&#20854;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29109;&#24809;&#32602;&#21644;&#20855;&#26377;Q-clip&#30340;&#21452;&#24179;&#22343;Q-learning&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;Atari&#28216;&#25103;&#21644;&#19968;&#20010;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#22312;&#20869;&#30340;&#20856;&#22411;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;: https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;
&lt;p&gt;
We study the adaption of soft actor-critic (SAC) from continuous action space to discrete action space. We revisit vanilla SAC and provide an in-depth understanding of its Q value underestimation and performance instability issues when applied to discrete settings. We thereby propose entropy-penalty and double average Q-learning with Q-clip to address these issues. Extensive experiments on typical benchmarks with discrete action space, including Atari games and a large-scale MOBA game, show the efficacy of our proposed method. Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#21521;LSTM&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20813;&#25480;&#26435;NOMA&#20013;&#30340;&#22810;&#29992;&#25143;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#27963;&#21160;&#35774;&#22791;&#30340;&#20934;&#30830;&#35782;&#21035;&#21644;&#25968;&#25454;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2209.06392</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#21521;LSTM&#32593;&#32476;&#30340;&#20813;&#25480;&#26435;NOMA&#20013;&#36827;&#34892;&#32852;&#21512;&#29992;&#25143;&#21644;&#25968;&#25454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Joint User and Data Detection in Grant-Free NOMA with Attention-based BiLSTM Network. (arXiv:2209.06392v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#21521;LSTM&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20813;&#25480;&#26435;NOMA&#20013;&#30340;&#22810;&#29992;&#25143;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#27963;&#21160;&#35774;&#22791;&#30340;&#20934;&#30830;&#35782;&#21035;&#21644;&#25968;&#25454;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19978;&#34892;&#20813;&#25480;&#26435;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#65288;NOMA&#65289;&#20013;&#30340;&#22810;&#29992;&#25143;&#26816;&#27979;&#65288;MUD&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#25509;&#20837;&#28857;&#38656;&#35201;&#35782;&#21035;&#27963;&#21160;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#30340;&#24635;&#25968;&#21644;&#27491;&#30830;&#30340;&#36523;&#20221;&#65292;&#24182;&#35299;&#30721;&#23427;&#20204;&#20256;&#36755;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20551;&#35774;IoT&#35774;&#22791;&#20351;&#29992;&#22797;&#26434;&#30340;&#25193;&#39057;&#24207;&#21015;&#65292;&#24182;&#25353;&#29031;&#31361;&#21457;&#31232;&#30095;&#27169;&#22411;&#20197;&#38543;&#26426;&#25509;&#20837;&#26041;&#24335;&#20256;&#36755;&#20449;&#24687;&#65292;&#20854;&#20013;&#19968;&#20123;IoT&#35774;&#22791;&#22312;&#22810;&#20010;&#30456;&#37051;&#26102;&#38388;&#27133;&#20013;&#20197;&#39640;&#27010;&#29575;&#20256;&#36755;&#25968;&#25454;&#65292;&#32780;&#20854;&#20182;&#35774;&#22791;&#22312;&#19968;&#20010;&#24103;&#20869;&#21482;&#20256;&#36755;&#19968;&#27425;&#12290;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#32593;&#32476;&#26469;&#35299;&#20915;MUD&#38382;&#39064;&#12290;BiLSTM&#32593;&#32476;&#20351;&#29992;&#21069;&#21521;&#21644;&#21453;&#21521;LSTM&#21019;&#24314;&#35774;&#22791;&#28608;&#27963;&#21382;&#21490;&#30340;&#27169;&#24335;&#65292;&#32780;&#27880;&#24847;&#21147;&#26426;&#21046;&#20026;&#35774;&#22791;&#28608;&#27963;&#28857;&#25552;&#20379;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21487;&#20197;&#25353;&#29031;&#23618;&#27425;&#36335;&#24452;&#26816;&#27979;&#27963;&#21160;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the multi-user detection (MUD) problem in uplink grant-free non-orthogonal multiple access (NOMA), where the access point has to identify the total number and correct identity of the active Internet of Things (IoT) devices and decode their transmitted data. We assume that IoT devices use complex spreading sequences and transmit information in a random-access manner following the burst-sparsity model, where some IoT devices transmit their data in multiple adjacent time slots with a high probability, while others transmit only once during a frame. Exploiting the temporal correlation, we propose an attention-based bidirectional long short-term memory (BiLSTM) network to solve the MUD problem. The BiLSTM network creates a pattern of the device activation history using forward and reverse pass LSTMs, whereas the attention mechanism provides essential context to the device activation points. By doing so, a hierarchical pathway is followed for detecting active devices in a grant-f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19982;LID&#30456;&#20851;&#30340;&#24230;&#37327;&#26041;&#27861;NC-LID&#65292;&#29992;&#20110;&#37327;&#21270;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26412;&#22320;&#29305;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;NC-LID&#35774;&#35745;&#20102;LID&#24863;&#30693;&#30340;&#22270;&#23884;&#20837;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.11986</link><description>&lt;p&gt;
&#22270;&#30340;&#26412;&#22320;&#20869;&#22312;&#32500;&#24230;&#24230;&#37327;&#21450;&#20854;&#22312;&#22270;&#23884;&#20837;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Local Intrinsic Dimensionality Measures for Graphs, with Applications to Graph Embeddings. (arXiv:2208.11986v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19982;LID&#30456;&#20851;&#30340;&#24230;&#37327;&#26041;&#27861;NC-LID&#65292;&#29992;&#20110;&#37327;&#21270;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26412;&#22320;&#29305;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;NC-LID&#35774;&#35745;&#20102;LID&#24863;&#30693;&#30340;&#22270;&#23884;&#20837;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#20869;&#22312;&#32500;&#24230;&#24230;&#37327;&#65288;LID&#65289;&#30340;&#27010;&#24565;&#26159;&#25968;&#25454;&#32500;&#24230;&#20998;&#26512;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#65292;&#20855;&#26377;&#25968;&#25454;&#25366;&#25496;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#30456;&#20284;&#24615;&#25628;&#32034;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;LID&#20272;&#35745;&#22120;&#38024;&#23545;&#20197;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21521;&#37327;&#34920;&#31034;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35774;&#35745;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#32771;&#34385;&#22270;&#23884;&#20837;&#21644;&#22270;&#36317;&#31163;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19982;LID&#30456;&#20851;&#30340;&#24230;&#37327;&#26041;&#27861;NC-LID&#65292;&#29992;&#20110;&#37327;&#21270;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#19982;&#33410;&#28857;&#33258;&#28982;&#31038;&#21306;&#20043;&#38388;&#30340;&#26412;&#22320;&#29305;&#24615;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21046;&#23450;&#20004;&#31181;&#22522;&#20110;LID&#30340;&#33410;&#28857;2vec&#21464;&#31181;&#31639;&#27861;&#26469;&#35774;&#35745;LID&#24863;&#30693;&#30340;&#22270;&#23884;&#20837;&#31639;&#27861;&#65292;&#20854;&#20013;&#30340;&#36229;&#21442;&#25968;&#26681;&#25454;NC-LID&#30340;&#20540;&#36827;&#34892;&#35843;&#25972;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;NC-LID&#33021;&#22815;&#25351;&#31034;&#20855;&#26377;&#39640;&#36830;&#25509;&#24615;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notion of local intrinsic dimensionality (LID) is an important advancement in data dimensionality analysis, with applications in data mining, machine learning and similarity search problems. Existing distance-based LID estimators were designed for tabular datasets encompassing data points represented as vectors in a Euclidean space. After discussing their limitations for graph-structured data considering graph embeddings and graph distances, we propose NC-LID, a novel LID-related measure for quantifying the discriminatory power of the shortest-path distance with respect to natural communities of nodes as their intrinsic localities. It is shown how this measure can be used to design LID-aware graph embedding algorithms by formulating two LID-elastic variants of node2vec with personalized hyperparameters that are adjusted according to NC-LID values. Our empirical analysis of NC-LID on a large number of real-world graphs shows that this measure is able to point to nodes with high link
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07734</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#65306;&#31934;&#24515;&#31579;&#36873;&#30340;&#33258;&#30417;&#30563;&#23545;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25104;&#21151;&#20135;&#29983;&#20102;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07734
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#21019;&#24314;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#24040;&#22823;&#25104;&#26412;&#12290;&#23545;&#20110;&#26631;&#35760;&#24322;&#24120;&#31232;&#32570;&#25110;&#20960;&#20046;&#19981;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;SSL&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#36807;&#21435;&#24050;&#32463;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#36827;&#34892;&#22522;&#20110;SSL&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#30340;&#31867;&#22411;&#23545;&#20934;&#30830;&#24615;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#26816;&#27979;&#27169;&#22411;&#21644;420&#20010;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25968;&#23383;&#21644;&#21487;&#35270;&#35777;&#25454;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;SSAD&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#32780;&#22312;&#32570;&#20047;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;SSL&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20851;&#20110;&#22270;&#20687;&#22411;SSAD&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
&lt;/p&gt;</description></item><item><title>TRUST-LAPSE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#36830;&#32493;&#27169;&#22411;&#30417;&#25511;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28508;&#31354;&#38388;&#23884;&#20837;&#35780;&#20272;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#36317;&#31163;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#20197;&#21450;&#39034;&#24207;&#30456;&#20851;&#24615;&#20559;&#24046;&#26469;&#23454;&#29616;&#23545;&#27169;&#22411;&#30340;&#36830;&#32493;&#30417;&#25511;&#12290;</title><link>http://arxiv.org/abs/2207.11290</link><description>&lt;p&gt;
TRUST-LAPSE&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#27169;&#22411;&#30417;&#25511;&#19981;&#20449;&#20219;&#35780;&#20998;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring. (arXiv:2207.11290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11290
&lt;/p&gt;
&lt;p&gt;
TRUST-LAPSE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#36830;&#32493;&#27169;&#22411;&#30417;&#25511;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28508;&#31354;&#38388;&#23884;&#20837;&#35780;&#20272;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#36317;&#31163;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#20197;&#21450;&#39034;&#24207;&#30456;&#20851;&#24615;&#20559;&#24046;&#26469;&#23454;&#29616;&#23545;&#27169;&#22411;&#30340;&#36830;&#32493;&#30417;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30417;&#27979;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30830;&#23450;&#20309;&#26102;&#24212;&#35813;&#20449;&#20219;&#23427;&#20204;&#30340;&#39044;&#27979;&#21644;&#20309;&#26102;&#19981;&#24212;&#35813;&#20449;&#20219;&#23427;&#20204;&#30340;&#39044;&#27979;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#26159;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRUST-LAPSE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#27169;&#22411;&#30417;&#25511;&#30340;&#8220;&#19981;&#20449;&#20219;&#8221;&#35780;&#20998;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#28508;&#31354;&#38388;&#23884;&#20837;&#26469;&#35780;&#20272;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;a&#65289;&#25105;&#20204;&#30340;&#28508;&#31354;&#38388;&#19981;&#20449;&#20219;&#35780;&#20998;&#20351;&#29992;&#28508;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#65288;&#39532;&#27663;&#36317;&#31163;&#65289;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#26469;&#20272;&#35745;&#19981;&#20449;&#20219;&#24230;&#65292;&#65288;b&#65289;&#25105;&#20204;&#30340;&#39034;&#24207;&#19981;&#20449;&#20219;&#35780;&#20998;&#20351;&#29992;&#38750;&#21442;&#25968;&#12289;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#30830;&#23450;&#36807;&#21435;&#36755;&#20837;&#34920;&#31034;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#24615;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#25805;&#20316;&#30340;&#36830;&#32493;&#30417;&#25511;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#23545;TRUST-LAPSE&#36827;&#34892;&#35780;&#20272;&#65306;&#65288;1&#65289;&#20998;&#24067;&#20559;&#31227;&#36755;&#20837;&#26816;&#27979;&#65292;&#65288;2&#65289;&#25968;&#25454;&#28418;&#31227;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;-&#38899;&#39057;...
&lt;/p&gt;
&lt;p&gt;
Continuous monitoring of trained ML models to determine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post-hoc and actionable. We propose TRUST-LAPSE, a "mistrust" scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample's model prediction using a sequence of latent-space embeddings. Specifically, (a) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space and (b) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a non-parametric, sliding-window based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: (1) distributionally shifted input detection, and (2) data drift detection. We evaluate across diverse domains - audio 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.04827</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#31867;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multi-modality framework based on the associative memory model, which can store and retrieve a large amount of real-world data in a fault-tolerant manner, and can be used to infer missing modalities.
&lt;/p&gt;
&lt;p&gt;
&#22238;&#24518;&#36215;&#22810;&#24180;&#26410;&#35265;&#30340;&#26379;&#21451;&#30340;&#38754;&#23380;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20320;&#20204;&#20598;&#28982;&#30456;&#36935;&#65292;&#20320;&#20204;&#20250;&#36731;&#26131;&#22320;&#35748;&#20986;&#24444;&#27492;&#12290;&#29983;&#29289;&#35760;&#24518;&#37197;&#22791;&#20102;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#23384;&#20648;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#28982;&#21518;&#25512;&#26029;&#32454;&#33410;&#20197;&#21305;&#37197;&#24863;&#30693;&#12290;Willshaw Memory&#26159;&#19968;&#31181;&#29992;&#20110;&#30382;&#23618;&#35745;&#31639;&#30340;&#31616;&#21333;&#25277;&#35937;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29983;&#29289;&#35760;&#24518;&#30340;&#26426;&#21046;&#12290;&#20351;&#29992;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#35270;&#35273;&#27169;&#24335;&#30340;&#31232;&#30095;&#32534;&#30721;&#35268;&#21017;[34]&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#26694;&#26550;&#25193;&#23637;&#20102;&#22522;&#26412;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#35760;&#24518;&#21516;&#26102;&#23384;&#20648;&#27599;&#20010;&#27169;&#24335;&#30340;&#20960;&#31181;&#27169;&#24577;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#25110;&#25991;&#26412;&#65289;&#12290;&#35757;&#32451;&#21518;&#65292;&#24403;&#21482;&#24863;&#30693;&#21040;&#23376;&#38598;&#26102;&#65292;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#32534;&#30721;&#22120;-&#35760;&#24518;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawing from memory the face of a friend you have not seen in years is a difficult task. However, if you happen to cross paths, you would easily recognize each other. The biological memory is equipped with an impressive compression algorithm that can store the essential, and then infer the details to match perception. The Willshaw Memory is a simple abstract model for cortical computations which implements mechanisms of biological memories. Using our recently proposed sparse coding prescription for visual patterns [34], this model can store and retrieve an impressive amount of real-world data in a fault-tolerant manner. In this paper, we extend the capabilities of the basic Associative Memory Model by using a Multiple-Modality framework. In this setting, the memory stores several modalities (e.g., visual, or textual) of each pattern simultaneously. After training, the memory can be used to infer missing modalities when just a subset is perceived. Using a simple encoder-memory decoder a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#29992;&#20110;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20102;&#23450;&#20041;OOD&#27010;&#24565;&#21644;&#25552;&#20379;&#24378;&#26377;&#21147;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#19982;&#20043;&#21069;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#27979;&#35797;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;OOD&#23454;&#20363;&#20013;&#34920;&#29616;&#26356;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2206.09522</link><description>&lt;p&gt;
&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#29992;&#20110;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiple Testing Framework for Out-of-Distribution Detection. (arXiv:2206.09522v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#29992;&#20110;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20102;&#23450;&#20041;OOD&#27010;&#24565;&#21644;&#25552;&#20379;&#24378;&#26377;&#21147;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#19982;&#20043;&#21069;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#27979;&#35797;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;OOD&#23454;&#20363;&#20013;&#34920;&#29616;&#26356;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#25512;&#29702;&#26102;&#26816;&#27979;&#23398;&#20064;&#31639;&#27861;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#20449;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#20123;OOD&#26816;&#27979;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;OOD&#27010;&#24565;&#30340;&#23450;&#20041;&#65292;&#21253;&#25324;&#36755;&#20837;&#20998;&#24067;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#20026;&#26500;&#24314;&#24378;&#22823;&#30340;OOD&#26816;&#27979;&#27979;&#35797;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#21551;&#21457;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#31526;&#21512;&#24615;p&#20540;&#31995;&#32479;&#22320;&#32467;&#21512;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#19981;&#21516;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#23558;&#20837;&#32676;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#20026;OOD&#30340;&#27010;&#29575;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#24037;&#20316;&#20013;&#25552;&#20986;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#27979;&#35797;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;OOD&#23454;&#20363;&#20013;&#30340;&#34920;&#29616;&#24182;&#19981;&#19968;&#33268;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;m&#20010;&#19981;&#21516;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of Out-of-Distribution (OOD) detection, that is, detecting whether a learning algorithm's output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the learning algorithm, which provides insights for the construction of powerful tests for OOD detection. We propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the learning algorithm using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different types of OOD instances. In contrast, our proposed method that combines m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#21387;&#32553;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24494;&#25511;&#21046;&#22120;&#19978;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#20934;&#30830;&#24615;&#12289;&#20869;&#23384;&#28040;&#32791;&#12289;&#25191;&#34892;&#26102;&#38388;&#21644;&#21151;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2205.10369</link><description>&lt;p&gt;
&#22522;&#20110;Cortex-M&#24494;&#25511;&#21046;&#22120;&#30340;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#37096;&#32626;&#20351;&#29992;&#28145;&#24230;&#21387;&#32553;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient Deployment of Deep Learning Applications on Cortex-M based Microcontrollers using Deep Compression. (arXiv:2205.10369v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#21387;&#32553;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24494;&#25511;&#21046;&#22120;&#19978;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#20934;&#30830;&#24615;&#12289;&#20869;&#23384;&#28040;&#32791;&#12289;&#25191;&#34892;&#26102;&#38388;&#21644;&#21151;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26159;&#24403;&#20170;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30707;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#34987;&#35757;&#32451;&#22823;&#37327;&#25968;&#25454;&#38598;&#26102;&#33021;&#22815;&#20570;&#20986;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#38543;&#30528;&#29289;&#32852;&#32593;&#31561;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35299;&#37322;&#20256;&#24863;&#22120;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#19981;&#20165;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#32791;&#20063;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#32593;&#32476;&#21387;&#32553;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24494;&#25511;&#21046;&#22120;&#26550;&#26500;&#19978;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#19981;&#21516;&#30340;DNN&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#37096;&#32626;&#31574;&#30053;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#22522;&#20110;ARM Cortex-M&#30340;&#20302;&#21151;&#32791;&#31995;&#32479;&#12290;&#25506;&#32034;&#36807;&#31243;&#21487;&#20197;&#20998;&#26512;&#20934;&#30830;&#24615;&#12289;&#20869;&#23384;&#28040;&#32791;&#12289;&#25191;&#34892;&#26102;&#38388;&#21644;&#21151;&#32791;&#31561;&#20851;&#38190;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#19977;&#31181;&#19981;&#21516;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Deep Neural Networks (DNNs) are the backbone of today's artificial intelligence due to their ability to make accurate predictions when being trained on huge datasets. With advancing technologies, such as the Internet of Things, interpreting large quantities of data generated by sensors is becoming an increasingly important task. However, in many applications not only the predictive performance but also the energy consumption of deep learning models is of major interest. This paper investigates the efficient deployment of deep learning models on resource-constrained microcontroller architectures via network compression. We present a methodology for the systematic exploration of different DNN pruning, quantization, and deployment strategies, targeting different ARM Cortex-M based low-power systems. The exploration allows to analyze trade-offs between key metrics such as accuracy, memory consumption, execution time, and power consumption. We discuss experimental results on three dif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36125;&#21494;&#26031;&#31574;&#30053;&#22797;&#29992;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#31574;&#30053;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21363;&#26102;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#29366;&#24577;&#36716;&#25442;&#26679;&#26412;&#20316;&#20026;&#35266;&#27979;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35266;&#27979;&#27169;&#22411;&#26469;&#21152;&#36895;&#20219;&#21153;&#25512;&#26029;&#21644;&#31574;&#30053;&#22797;&#29992;&#12290;</title><link>http://arxiv.org/abs/2204.07729</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#35266;&#27979;&#27169;&#22411;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#31574;&#30053;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning. (arXiv:2204.07729v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36125;&#21494;&#26031;&#31574;&#30053;&#22797;&#29992;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#31574;&#30053;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21363;&#26102;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#29366;&#24577;&#36716;&#25442;&#26679;&#26412;&#20316;&#20026;&#35266;&#27979;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35266;&#27979;&#27169;&#22411;&#26469;&#21152;&#36895;&#20219;&#21153;&#25512;&#26029;&#21644;&#31574;&#30053;&#22797;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31574;&#30053;&#22797;&#29992;&#65288;BPR&#65289;&#26159;&#19968;&#31181;&#20174;&#31163;&#32447;&#24211;&#20013;&#36873;&#25321;&#28304;&#31574;&#30053;&#30340;&#36890;&#29992;&#31574;&#30053;&#36716;&#31227;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#25512;&#26029;&#22522;&#20110;&#19968;&#20123;&#35266;&#27979;&#20449;&#21495;&#21644;&#35757;&#32451;&#22909;&#30340;&#35266;&#27979;&#27169;&#22411;&#30340;&#20219;&#21153;&#20449;&#24565;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;BPR&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26356;&#39640;&#25928;&#30340;&#31574;&#30053;&#36716;&#31227;&#12290;&#39318;&#20808;&#65292;&#22823;&#22810;&#25968;BPR&#31639;&#27861;&#20351;&#29992;&#22238;&#21512;&#36820;&#22238;&#20316;&#20026;&#35266;&#27979;&#20449;&#21495;&#65292;&#20294;&#23427;&#21253;&#21547;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#24182;&#19988;&#30452;&#21040;&#22238;&#21512;&#32467;&#26463;&#25165;&#33021;&#33719;&#24471;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#20449;&#24687;&#37327;&#21644;&#21363;&#26102;&#24615;&#30340;&#29366;&#24577;&#36716;&#25442;&#26679;&#26412;&#20316;&#20026;&#35266;&#27979;&#20449;&#21495;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#21644;&#26356;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#12290;&#20854;&#27425;&#65292;BPR&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#26469;&#20272;&#35745;&#22522;&#20110;&#34920;&#26684;&#30340;&#35266;&#27979;&#27169;&#22411;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#29978;&#33267;&#19981;&#21487;&#34892;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#26679;&#26412;&#20316;&#20026;&#20449;&#21495;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25311;&#21512;&#30340;&#21487;&#25193;&#23637;&#35266;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27492;&#27169;&#22411;&#26469;&#21152;&#36895;&#20219;&#21153;&#25512;&#26029;&#21644;&#31574;&#30053;&#22797;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian policy reuse (BPR) is a general policy transfer framework for selecting a source policy from an offline library by inferring the task belief based on some observation signals and a trained observation model. In this paper, we propose an improved BPR method to achieve more efficient policy transfer in deep reinforcement learning (DRL). First, most BPR algorithms use the episodic return as the observation signal that contains limited information and cannot be obtained until the end of an episode. Instead, we employ the state transition sample, which is informative and instantaneous, as the observation signal for faster and more accurate task inference. Second, BPR algorithms usually require numerous samples to estimate the probability distribution of the tabular-based observation model, which may be expensive and even infeasible to learn and maintain, especially when using the state transition sample as the signal. Hence, we propose a scalable observation model based on fitting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#28151;&#21512;&#26102;&#38388;&#26377;&#20219;&#20309;&#20102;&#35299;&#65292;&#20294;&#22312;&#20984;&#38382;&#39064;&#20013;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#20197;&#21450;&#26102;&#24046;&#23398;&#20064;&#65292;&#24182;&#19988;&#23436;&#20840;&#26080;&#35270;&#28151;&#21512;&#26102;&#38388;&#12290;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26799;&#24230;&#20272;&#35745;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2202.04428</link><description>&lt;p&gt;
&#36866;&#24212;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#28151;&#21512;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Adapting to Mixing Time in Stochastic Optimization with Markovian Data. (arXiv:2202.04428v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#28151;&#21512;&#26102;&#38388;&#26377;&#20219;&#20309;&#20102;&#35299;&#65292;&#20294;&#22312;&#20984;&#38382;&#39064;&#20013;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#20197;&#21450;&#26102;&#24046;&#23398;&#20064;&#65292;&#24182;&#19988;&#23436;&#20840;&#26080;&#35270;&#28151;&#21512;&#26102;&#38388;&#12290;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26799;&#24230;&#20272;&#35745;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#25968;&#25454;&#20174;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#25552;&#21462;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#36825;&#31181;&#35774;&#32622;&#30340;&#26041;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#38142;&#30340;&#28151;&#21512;&#26102;&#38388;&#30340;&#20102;&#35299;&#65292;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20102;&#35299;&#28151;&#21512;&#26102;&#38388;&#30340;&#26368;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#20984;&#38382;&#39064;&#26102;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#65306;(i)&#23547;&#25214;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#31283;&#23450;&#28857;&#20197;&#21450;(ii)&#22312;&#26102;&#24046;&#23398;&#20064;&#20013;&#33719;&#24471;&#23545;&#28151;&#21512;&#26102;&#38388;&#26356;&#22909;&#30340;&#20381;&#36182;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#28151;&#21512;&#26102;&#38388;&#23436;&#20840;&#26080;&#35270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;(MLMC)&#26799;&#24230;&#20272;&#35745;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider stochastic optimization problems where data is drawn from a Markov chain. Existing methods for this setting crucially rely on knowing the mixing time of the chain, which in real-world applications is usually unknown. We propose the first optimization method that does not require the knowledge of the mixing time, yet obtains the optimal asymptotic convergence rate when applied to convex problems. We further show that our approach can be extended to: (i) finding stationary points in non-convex optimization with Markovian data, and (ii) obtaining better dependence on the mixing time in temporal difference (TD) learning; in both cases, our method is completely oblivious to the mixing time. Our method relies on a novel combination of multi-level Monte Carlo (MLMC) gradient estimation together with an adaptive learning method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#20110;&#21160;&#24577;&#26410;&#26469;&#30340;&#27010;&#24565;&#65292;&#24182;&#35748;&#20026;&#21069;&#30651;&#24615;&#23398;&#20064;&#26356;&#20934;&#30830;&#22320;&#25551;&#36848;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.07372</link><description>&lt;p&gt;
&#26410;&#26469;&#23398;&#20064;&#65306;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#21512;&#29702;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Prospective Learning: Principled Extrapolation to the Future. (arXiv:2201.07372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#20110;&#21160;&#24577;&#26410;&#26469;&#30340;&#27010;&#24565;&#65292;&#24182;&#35748;&#20026;&#21069;&#30651;&#24615;&#23398;&#20064;&#26356;&#20934;&#30830;&#22320;&#25551;&#36848;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26159;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#26356;&#26032;&#20915;&#31574;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#26410;&#26469;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#24120;&#24120;&#22312;&#20551;&#35774;&#26410;&#26469;&#19982;&#36807;&#21435;&#30340;&#20998;&#24067;&#30456;&#21516;&#25110;&#20250;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#25913;&#21464;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#20294;&#26159;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#35768;&#22810;&#38382;&#39064;&#26469;&#35828;&#65292;&#21487;&#33021;&#36807;&#20110;&#20048;&#35266;&#25110;&#24754;&#35266;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#22312;&#22810;&#20010;&#26102;&#31354;&#23610;&#24230;&#19978;&#28436;&#21464;&#65292;&#20855;&#26377;&#37096;&#20998;&#21487;&#39044;&#27979;&#30340;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#23398;&#20064;&#38382;&#39064;&#65292;&#23558;&#20854;&#32858;&#28966;&#20110;&#21160;&#24577;&#26410;&#26469;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#26410;&#26469;&#26159;&#37096;&#20998;&#21487;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#25512;&#27979;&#26576;&#20123;&#20219;&#21153;&#24207;&#21015;&#22312;&#22238;&#39038;&#24615;&#23398;&#20064;&#20013;&#19981;&#21487;&#23398;&#20064;&#65288;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#22266;&#23450;&#65289;&#65292;&#20294;&#22312;&#21069;&#30651;&#24615;&#23398;&#20064;&#20013;&#21487;&#23398;&#20064;&#65288;&#20854;&#20013;&#20998;&#24067;&#21487;&#33021;&#21160;&#24577;&#65289;&#65292;&#36825;&#34920;&#26126;&#21069;&#30651;&#24615;&#23398;&#20064;&#22312;&#26412;&#36136;&#19978;&#27604;&#22238;&#39038;&#24615;&#23398;&#20064;&#26356;&#22256;&#38590;&#12290;&#25105;&#20204;&#35748;&#20026;&#21069;&#30651;&#24615;&#23398;&#20064;&#26356;&#20934;&#30830;&#22320;&#25551;&#36848;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
Learning is a process which can update decision rules, based on past experience, such that future performance improves. Traditionally, machine learning is often evaluated under the assumption that the future will be identical to the past in distribution or change adversarially. But these assumptions can be either too optimistic or pessimistic for many problems in the real world. Real world scenarios evolve over multiple spatiotemporal scales with partially predictable dynamics. Here we reformulate the learning problem to one that centers around this idea of dynamic futures that are partially learnable. We conjecture that certain sequences of tasks are not retrospectively learnable (in which the data distribution is fixed), but are prospectively learnable (in which distributions may be dynamic), suggesting that prospective learning is more difficult in kind than retrospective learning. We argue that prospective learning more accurately characterizes many real world problems that (1) cur
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;Bregman&#24046;&#24322;&#21644;&#36229;&#39532;&#19969;&#26684;&#23572;&#28151;&#21512;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#36890;&#29992;&#36793;&#30028;&#65292;&#25511;&#21046;&#25351;&#25968;&#26063;&#21442;&#25968;&#19982;&#21442;&#25968;&#26377;&#38480;&#26679;&#26412;&#20272;&#35745;&#20043;&#38388;&#30340;Bregman&#24046;&#24322;&#65292;&#35813;&#36793;&#30028;&#26159;&#26102;&#38388;&#22343;&#21248;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;Bregman&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#23558;&#27492;&#36793;&#30028;&#24212;&#29992;&#20110;&#22810;&#20010;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#24182;&#24471;&#21040;&#20102;&#32622;&#20449;&#21306;&#38388;&#21644;Bregman&#20449;&#24687;&#22686;&#30410;&#30340;&#26126;&#30830;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2201.07306</link><description>&lt;p&gt;
&#19968;&#33324;&#25351;&#25968;&#26063;&#30340;Bregman&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Bregman Deviations of Generic Exponential Families. (arXiv:2201.07306v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07306
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;Bregman&#24046;&#24322;&#21644;&#36229;&#39532;&#19969;&#26684;&#23572;&#28151;&#21512;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#36890;&#29992;&#36793;&#30028;&#65292;&#25511;&#21046;&#25351;&#25968;&#26063;&#21442;&#25968;&#19982;&#21442;&#25968;&#26377;&#38480;&#26679;&#26412;&#20272;&#35745;&#20043;&#38388;&#30340;Bregman&#24046;&#24322;&#65292;&#35813;&#36793;&#30028;&#26159;&#26102;&#38388;&#22343;&#21248;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;Bregman&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#23558;&#27492;&#36793;&#30028;&#24212;&#29992;&#20110;&#22810;&#20010;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#24182;&#24471;&#21040;&#20102;&#32622;&#20449;&#21306;&#38388;&#21644;Bregman&#20449;&#24687;&#22686;&#30410;&#30340;&#26126;&#30830;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#28151;&#21512;&#25216;&#26415;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;&#19968;&#33324;&#25351;&#25968;&#26063;&#20013;&#30340;&#27987;&#24230;&#29616;&#35937;&#12290;&#23558;&#19982;&#23478;&#26063;&#30340;&#23545;&#25968;&#20998;&#21306;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;Bregman&#24046;&#24322;&#30340;&#29305;&#24615;&#19982;&#36229;&#39532;&#19969;&#26684;&#23572;&#28151;&#21512;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#30028;&#38480;&#65292;&#25511;&#21046;&#23478;&#26063;&#30340;&#21442;&#25968;&#19982;&#21442;&#25968;&#30340;&#26377;&#38480;&#26679;&#26412;&#20272;&#35745;&#20043;&#38388;&#30340;Bregman&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#26102;&#38388;&#22343;&#21248;&#30340;&#65292;&#24182;&#19988;&#20986;&#29616;&#20102;&#19968;&#31181;&#25193;&#23637;&#32463;&#20856;&#20449;&#24687;&#22686;&#30410;&#21040;&#25351;&#25968;&#26063;&#30340;&#37327;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Bregman&#20449;&#24687;&#22686;&#30410;&#12290;&#23545;&#20110;&#23454;&#36341;&#32773;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#39062;&#30340;&#30028;&#38480;&#23454;&#20363;&#21270;&#21040;&#20960;&#20010;&#32463;&#20856;&#23478;&#26063;&#20013;&#65292;&#20363;&#22914;&#65292;&#39640;&#26031;&#12289;&#20271;&#21162;&#21033;&#12289;&#25351;&#25968;&#12289;&#38886;&#20271;&#12289;&#24085;&#32047;&#25176;&#12289;&#27850;&#26494;&#21644;&#21345;&#26041;&#65292;&#24471;&#21040;&#20102;&#32622;&#20449;&#21306;&#38388;&#21644;Bregman&#20449;&#24687;&#22686;&#30410;&#30340;&#26126;&#30830;&#24418;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25968;&#20540;&#27604;&#36739;&#20102;&#19982;&#26102;&#38388;&#22343;&#21248;&#27987;&#24230;&#30340;&#26368;&#26032;&#26367;&#20195;&#26041;&#27861;&#24471;&#21040;&#30340;&#32622;&#20449;&#30028;&#38480;&#65292;&#24182;&#34920;&#26126;th&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the method of mixture technique, also known as the Laplace method, to study the concentration phenomenon in generic exponential families. Combining the properties of Bregman divergence associated with log-partition function of the family with the method of mixtures for super-martingales, we establish a generic bound controlling the Bregman divergence between the parameter of the family and a finite sample estimate of the parameter. Our bound is time-uniform and makes appear a quantity extending the classical information gain to exponential families, which we call the Bregman information gain. For the practitioner, we instantiate this novel bound to several classical families, e.g., Gaussian, Bernoulli, Exponential, Weibull, Pareto, Poisson and Chi-square yielding explicit forms of the confidence sets and the Bregman information gain. We further numerically compare the resulting confidence bounds to state-of-the-art alternatives for time-uniform concentration and show that th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.08440</link><description>&lt;p&gt;
&#27668;&#20505;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Climate-Invariant Machine Learning. (arXiv:2112.08440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#39044;&#27979;&#26159;&#19968;&#20010;&#27867;&#21270;&#38382;&#39064;&#65306;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#22312;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#30340;&#27668;&#20505;&#20013;&#23545;&#26368;&#36817;&#30340;&#36807;&#21435;&#36827;&#34892;&#22806;&#25512;&#12290;&#30446;&#21069;&#30340;&#27668;&#20505;&#27169;&#22411;&#38656;&#35201;&#23545;&#23567;&#20110;&#27169;&#22411;&#32593;&#26684;&#22823;&#23567;&#30340;&#23610;&#24230;&#19978;&#21457;&#29983;&#30340;&#36807;&#31243;&#36827;&#34892;&#34920;&#31034;&#65292;&#36825;&#20123;&#36807;&#31243;&#26159;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#26377;&#26395;&#25913;&#21892;&#36825;&#31181;&#36807;&#31243;&#34920;&#31034;&#65292;&#20294;&#24448;&#24448;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#27668;&#20505;&#29615;&#22659;&#20013;&#22806;&#25512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#29289;&#29702;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#8212;&#8212;&#31216;&#20026;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#8212;&#8212;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;ML&#31639;&#27861;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22823;&#27668;&#27169;&#22411;&#20013;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#29289;&#29702;&#30693;&#35782;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#22320;&#29699;&#31995;&#32479;&#36807;&#31243;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projecting climate change is a generalization problem: we extrapolate the recent past using physical models across past, present, and future climates. Current climate models require representations of processes that occur at scales smaller than model grid size, which have been the main source of model projection uncertainty. Recent machine learning (ML) algorithms hold promise to improve such process representations, but tend to extrapolate poorly to climate regimes they were not trained on. To get the best of the physical and statistical worlds, we propose a new framework -- termed "climate-invariant" ML -- incorporating knowledge of climate processes into ML algorithms, and show that it can maintain high accuracy across a wide range of climate and geographic conditions in three distinct atmospheric models. Our results suggest that explicitly incorporating physical knowledge into data-driven models of Earth system processes can improve their consistency, data efficiency, and generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36890;&#36807;&#20256;&#25773;&#26041;&#21521;&#19981;&#21464;&#24615;&#26469;&#27867;&#21270;&#21040;&#26032;&#39062;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#29087;&#24713;&#23545;&#35937;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#20294;&#20165;&#38480;&#20110;&#28041;&#21450;2D&#26059;&#36716;&#30340;&#29087;&#24713;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2109.13445</link><description>&lt;p&gt;
&#26032;&#39062;&#26041;&#21521;&#19978;&#23545;&#35937;&#27867;&#21270;&#30340;&#26032;&#20852;&#31070;&#32463;&#32593;&#32476;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations. (arXiv:2109.13445v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.13445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36890;&#36807;&#20256;&#25773;&#26041;&#21521;&#19981;&#21464;&#24615;&#26469;&#27867;&#21270;&#21040;&#26032;&#39062;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#29087;&#24713;&#23545;&#35937;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#20294;&#20165;&#38480;&#20110;&#28041;&#21450;2D&#26059;&#36716;&#30340;&#29087;&#24713;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35782;&#21035;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#30340;&#33021;&#21147;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;DNNs&#33021;&#22815;&#36890;&#36807;&#20256;&#25773;&#20174;&#22810;&#20010;&#35270;&#28857;&#35266;&#23519;&#21040;&#30340;&#29087;&#24713;&#23545;&#35937;&#33719;&#24471;&#30340;&#26041;&#21521;&#19981;&#21464;&#24615;&#26469;&#27867;&#21270;&#21040;&#26032;&#39062;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#35757;&#32451;DNN&#26102;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#30340;&#29087;&#24713;&#23545;&#35937;&#26102;&#20250;&#22686;&#24378;&#65292;&#20294;&#20165;&#38480;&#20110;&#28041;&#21450;&#21040;&#29087;&#24713;&#26041;&#21521;&#30340;2D&#26059;&#36716;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20256;&#25773;&#26159;&#36890;&#36807;&#35843;&#25972;&#21040;&#29087;&#24713;&#21644;&#19981;&#29087;&#24713;&#23545;&#35937;&#20043;&#38388;&#20849;&#21516;&#29305;&#24449;&#30340;&#31070;&#32463;&#20803;&#23454;&#29616;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#31867;&#33041;&#31070;&#32463;&#26426;&#21046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability of Deep Neural Networks (DNNs) to recognize objects in orientations outside the distribution of the training data is not well understood. We present evidence that DNNs are capable of generalizing to objects in novel orientations by disseminating orientation-invariance obtained from familiar objects seen from many viewpoints. This capability strengthens when training the DNN with an increasing number of familiar objects, but only in orientations that involve 2D rotations of familiar orientations. We show that this dissemination is achieved via neurons tuned to common features between familiar and unfamiliar objects. These results implicate brain-like neural mechanisms for generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#31895;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#23618;&#36229;&#22270;&#21010;&#20998;&#12290;&#36890;&#36807;Wasserstein&#24046;&#24322;&#24230;&#21327;&#35843;&#31895;&#21270;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#20256;&#36755;&#65292;&#24182;&#20351;&#29992;&#19977;&#28857;&#31639;&#27861;&#25214;&#21040;&#22312;&#24179;&#34913;&#32422;&#26463;&#19979;&#30340;&#26368;&#20339;&#21010;&#20998;&#12290;</title><link>http://arxiv.org/abs/2106.07501</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein&#24046;&#24322;&#24230;&#23454;&#29616;&#22810;&#23618;&#36229;&#22270;&#21010;&#20998;&#30340;&#24179;&#34913;&#31895;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Balanced Coarsening for Multilevel Hypergraph Partitioning via Wasserstein Discrepancy. (arXiv:2106.07501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#31895;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#23618;&#36229;&#22270;&#21010;&#20998;&#12290;&#36890;&#36807;Wasserstein&#24046;&#24322;&#24230;&#21327;&#35843;&#31895;&#21270;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#20256;&#36755;&#65292;&#24182;&#20351;&#29992;&#19977;&#28857;&#31639;&#27861;&#25214;&#21040;&#22312;&#24179;&#34913;&#32422;&#26463;&#19979;&#30340;&#26368;&#20339;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#23618;&#36229;&#22270;&#21010;&#20998;&#30340;&#24179;&#34913;&#31895;&#21270;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21021;&#22987;&#21010;&#20998;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;k-way&#36229;&#22270;&#21010;&#20998;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;LPT&#31639;&#27861;&#20998;&#37197;&#39030;&#28857;&#26435;&#37325;&#65292;&#25105;&#20204;&#22312;&#25918;&#26494;&#24179;&#34913;&#32422;&#26463;&#19979;&#29983;&#25104;&#20102;&#20808;&#39564;&#36229;&#22270;&#12290;&#21033;&#29992;&#20808;&#39564;&#36229;&#22270;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;Wasserstein&#24046;&#24322;&#24230;&#26469;&#21327;&#35843;&#31895;&#21270;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#20256;&#36755;&#12290;&#24182;&#20351;&#29992;Sinkhorn&#31639;&#27861;&#27714;&#35299;&#26368;&#20248;&#20256;&#36755;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#31895;&#21270;&#26041;&#26696;&#20805;&#20998;&#32771;&#34385;&#20102;&#36830;&#25509;&#24230;&#24230;&#37327;&#65288;&#30446;&#26631;&#20989;&#25968;&#65289;&#30340;&#26368;&#23567;&#21270;&#12290;&#23545;&#20110;&#21021;&#22987;&#21010;&#20998;&#38454;&#27573;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#30001;Fiedler&#21521;&#37327;&#24341;&#21457;&#30340;&#24402;&#19968;&#21270;&#20999;&#20989;&#25968;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#26159;&#19968;&#20010;&#20985;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#28857;&#31639;&#27861;&#20197;&#25214;&#21040;&#22312;&#24179;&#34913;&#32422;&#26463;&#19979;&#30340;&#26368;&#20339;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a balanced coarsening scheme for multilevel hypergraph partitioning. In addition, an initial partitioning algorithm is designed to improve the quality of k-way hypergraph partitioning. By assigning vertex weights through the LPT algorithm, we generate a prior hypergraph under a relaxed balance constraint. With the prior hypergraph, we have defined the Wasserstein discrepancy to coordinate the optimal transport of coarsening process. And the optimal transport matrix is solved by Sinkhorn algorithm. Our coarsening scheme fully takes into account the minimization of connectivity metric (objective function). For the initial partitioning stage, we define a normalized cut function induced by Fiedler vector, which is theoretically proved to be a concave function. Thereby, a three-point algorithm is designed to find the best cut under the balance constraint.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38646;&#30693;&#35782;&#21327;&#35843;&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24102;&#26377;&#25171;&#30772;&#24179;&#23616;&#30340;&#20854;&#20182;&#23545;&#23616;&#31639;&#27861;&#20316;&#20026;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.06613</link><description>&lt;p&gt;
&#19968;&#31181;&#38646;&#30693;&#35782;&#21327;&#35843;&#30340;&#26032;&#24418;&#24335;&#12289;&#26041;&#27861;&#21644;&#26410;&#35299;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
A New Formalism, Method and Open Issues for Zero-Shot Coordination. (arXiv:2106.06613v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38646;&#30693;&#35782;&#21327;&#35843;&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24102;&#26377;&#25171;&#30772;&#24179;&#23616;&#30340;&#20854;&#20182;&#23545;&#23616;&#31639;&#27861;&#20316;&#20026;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21327;&#35843;&#38382;&#39064;&#20013;&#65292;&#29420;&#31435;&#25512;&#29702;&#30340;&#20154;&#31867;&#33021;&#22815;&#21457;&#29616;&#20114;&#30456;&#20860;&#23481;&#30340;&#31574;&#30053;&#12290;&#30456;&#21453;&#65292;&#29420;&#31435;&#35757;&#32451;&#30340;&#33258;&#21338;&#24328;&#31574;&#30053;&#36890;&#24120;&#26159;&#20114;&#19981;&#20860;&#23481;&#30340;&#12290;&#38646;&#30693;&#35782;&#21327;&#35843;&#65288;ZSC&#65289;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26032;&#21069;&#27839;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#22522;&#26412;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20551;&#35774;&#29609;&#23478;&#21487;&#20197;&#22312;&#23398;&#20064;&#31639;&#27861;&#19978;&#36798;&#25104;&#19968;&#33268;&#65292;&#20294;&#22312;&#34892;&#21160;&#21644;&#35266;&#27979;&#26631;&#31614;&#19978;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#65292;&#25552;&#20986;&#20102;&#20854;&#20182;&#23545;&#23616;&#20316;&#20026;&#19968;&#31181;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#30452;&#21040;&#29616;&#22312;&#65292;&#36825;&#20010;&#8220;&#26080;&#26631;&#31614;&#8221;&#38382;&#39064;&#21482;&#26159;&#34987;&#38750;&#27491;&#24335;&#22320;&#23450;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#35774;&#32622;&#24418;&#24335;&#21270;&#20026;&#26080;&#26631;&#31614;&#21327;&#35843;&#65288;LFC&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#26080;&#26631;&#31614;&#21327;&#35843;&#21338;&#24328;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20182;&#23545;&#23616;&#19981;&#33021;&#25104;&#20026;LFC&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#20026;&#23427;&#26410;&#33021;&#22312;&#19981;&#20860;&#23481;&#30340;&#26368;&#22823;&#21270;&#32773;&#20043;&#38388;&#19968;&#33268;&#22320;&#25171;&#30772;&#24179;&#23616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#21363;&#24102;&#26377;&#25171;&#30772;&#24179;&#23616;&#30340;&#20854;&#20182;&#23545;&#23616;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this "label-free" problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32593;&#32476;&#20013;&#20302;&#31209;&#28508;&#22312;&#20013;&#23610;&#24230;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#32593;&#32476;&#27169;&#22411;&#21644;&#23454;&#38469;&#32593;&#32476;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#30340;&#8220;&#28508;&#22312;&#27169;&#24335;&#8221;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#36817;&#20284;&#32593;&#32476;&#30340;&#22823;&#22810;&#25968;&#23376;&#22270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2102.06984</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#23398;&#20064;&#20302;&#31209;&#28508;&#22312;&#20013;&#23610;&#24230;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning low-rank latent mesoscale structures in networks. (arXiv:2102.06984v5 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.06984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32593;&#32476;&#20013;&#20302;&#31209;&#28508;&#22312;&#20013;&#23610;&#24230;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#32593;&#32476;&#27169;&#22411;&#21644;&#23454;&#38469;&#32593;&#32476;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#30340;&#8220;&#28508;&#22312;&#27169;&#24335;&#8221;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#36817;&#20284;&#32593;&#32476;&#30340;&#22823;&#22810;&#25968;&#23376;&#22270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#32593;&#32476;&#26469;&#32534;&#30721;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#31038;&#20250;&#21644;&#20449;&#24687;&#31185;&#23398;&#12290;&#20026;&#20102;&#30740;&#31350;&#22797;&#26434;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#34892;&#20026;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#32593;&#32476;&#20013;&#30340;&#20013;&#23610;&#24230;&#32467;&#26500;&#20316;&#20026;&#24433;&#21709;&#36825;&#31181;&#34892;&#20026;&#30340;&#26500;&#24314;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#32593;&#32476;&#20013;&#20302;&#31209;&#28508;&#22312;&#20013;&#23610;&#24230;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#20960;&#20010;&#21512;&#25104;&#32593;&#32476;&#27169;&#22411;&#21644;&#23454;&#35777;&#21451;&#35850;&#12289;&#21512;&#20316;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPI&#65289;&#32593;&#32476;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#25968;&#37327;&#30340;&#8220;&#28508;&#22312;&#27169;&#24335;&#8221;&#65292;&#36825;&#20123;&#27169;&#24335;&#20849;&#21516;&#21487;&#20197;&#25104;&#21151;&#22320;&#36817;&#20284;&#32593;&#32476;&#30340;&#22823;&#22810;&#25968;&#23376;&#22270;&#22312;&#22266;&#23450;&#30340;&#20013;&#23610;&#24230;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#8220;&#32593;&#32476;&#23383;&#20856;&#23398;&#20064;&#8221;&#65288;NDL&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#32593;&#32476;&#37319;&#26679;&#26041;&#27861;&#21644;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65292;&#20197;&#23398;&#20064;&#32473;&#23450;&#32593;&#32476;&#30340;&#28508;&#22312;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to use networks to encode the architecture of interactions between entities in complex systems in the physical, biological, social, and information sciences. To study the large-scale behavior of complex systems, it is useful to examine mesoscale structures in networks as building blocks that influence such behavior. We present a new approach for describing low-rank mesoscale structures in networks, and we illustrate our approach using several synthetic network models and empirical friendship, collaboration, and protein--protein interaction (PPI) networks. We find that these networks possess a relatively small number of `latent motifs' that together can successfully approximate most subgraphs of a network at a fixed mesoscale. We use an algorithm for `network dictionary learning' (NDL), which combines a network-sampling method and nonnegative matrix factorization, to learn the latent motifs of a given network. The ability to encode a network using a set of latent motifs has
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#19968;&#35270;&#39057;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#21644;Siamese&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21040;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; RL &#31574;&#30053;&#30340;&#35757;&#32451;&#26368;&#23567;&#21270;&#36825;&#20010;&#36317;&#31163;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#25968;&#25454;&#21644;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#25439;&#22833;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#32500;&#24230;&#30340;&#20223;&#30495;&#26234;&#33021;&#20307;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/1901.07186</link><description>&lt;p&gt;
&#20174;&#19968;&#20010;&#21333;&#19968;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Learning to Imitate from a Single Video Demonstration. (arXiv:1901.07186v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.07186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#19968;&#35270;&#39057;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#21644;Siamese&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21040;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; RL &#31574;&#30053;&#30340;&#35757;&#32451;&#26368;&#23567;&#21270;&#36825;&#20010;&#36317;&#31163;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#25968;&#25454;&#21644;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#25439;&#22833;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#32500;&#24230;&#30340;&#20223;&#30495;&#26234;&#33021;&#20307;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#35270;&#39057;&#35266;&#23519;&#20013;&#23398;&#20064;&#27169;&#20223;&#30340;&#26234;&#33021;&#20307;&#8212;&#8212;\emph{&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#29366;&#24577;&#25110;&#21160;&#20316;&#20449;&#24687;}&#65292;&#23545;&#20110;&#22312;&#33258;&#28982;&#19990;&#30028;&#20013;&#30340;&#23398;&#20064;&#26356;&#20855;&#36866;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#21046;&#23450;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#23398;&#20064;&#19968;&#20010;&#23558;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#19982;&#21333;&#20010;&#28436;&#31034;&#36827;&#34892;&#27604;&#36739;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#20307;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#23398;&#20064;&#21160;&#20316;&#29255;&#27573;&#20043;&#38388;&#30340;&#22870;&#21169;&#65292;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;RL&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#36825;&#20010;&#36317;&#31163;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#20219;&#21153;&#25968;&#25454;&#21644;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#25439;&#22833;&#30340;&#24341;&#20837;&#25913;&#36827;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;2D&#20013;&#30340;&#27169;&#25311;&#20154;&#22411;&#12289;&#29399;&#21644;&#36805;&#29467;&#40857;&#26234;&#33021;&#20307;&#20197;&#21450;3D&#20013;&#30340;&#22235;&#36275;&#21160;&#29289;&#21644;&#20154;&#22411;&#26234;&#33021;&#20307;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents that can learn to imitate given video observation -- \emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-
&lt;/p&gt;</description></item></channel></rss>