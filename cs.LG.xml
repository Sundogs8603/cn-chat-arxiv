<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SCENES&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26497;&#32447;&#30417;&#30563;&#30340;&#20122;&#20687;&#32032;&#23545;&#24212;&#20851;&#31995;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#32780;&#19981;&#38656;&#35201;3D&#32467;&#26500;&#65292;&#20197;&#25918;&#23485;&#23545;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.10886</link><description>&lt;p&gt;
SCENES: &#20351;&#29992;&#26497;&#32447;&#30417;&#30563;&#30340;&#20122;&#20687;&#32032;&#23545;&#24212;&#20851;&#31995;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SCENES: Subpixel Correspondence Estimation With Epipolar Supervision. (arXiv:2401.10886v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10886
&lt;/p&gt;
&lt;p&gt;
SCENES&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26497;&#32447;&#30417;&#30563;&#30340;&#20122;&#20687;&#32032;&#23545;&#24212;&#20851;&#31995;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#32780;&#19981;&#38656;&#35201;3D&#32467;&#26500;&#65292;&#20197;&#25918;&#23485;&#23545;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22330;&#26223;&#30340;&#20004;&#20010;&#25110;&#26356;&#22810;&#35270;&#35282;&#20013;&#25552;&#21462;&#28857;&#23545;&#24212;&#20851;&#31995;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#21644;&#32467;&#26500;&#36816;&#21160;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23545;&#24212;&#20851;&#31995;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#39640;&#24230;&#20934;&#30830;&#30340;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#29305;&#24449;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24448;&#24448;&#26080;&#27861;&#24456;&#22909;&#22320;&#27867;&#21270;&#65292;&#36825;&#19982;&#32463;&#20856;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19981;&#21516;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#38656;&#35201;&#24494;&#35843;&#65292;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#22320;&#38754;&#23454;&#20917;&#23545;&#24212;&#20851;&#31995;&#25110;&#22320;&#38754;&#23454;&#20917;&#30456;&#26426;&#23039;&#24577;&#21644;3D&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#21462;&#28040;&#23545;3D&#32467;&#26500;&#65288;&#22914;&#28145;&#24230;&#22270;&#25110;&#28857;&#20113;&#65289;&#30340;&#35201;&#27714;&#65292;&#21482;&#38656;&#35201;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#65288;&#21487;&#20197;&#20174;&#37324;&#31243;&#35745;&#33719;&#24471;&#65289;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23545;&#24212;&#20851;&#31995;&#25439;&#22833;&#26367;&#25442;&#20026;&#26497;&#32447;&#25439;&#22833;&#26469;&#23454;&#29616;&#65292;&#26497;&#32447;&#25439;&#22833;&#40723;&#21169;&#20551;&#35774;&#21305;&#37197;&#28857;&#20301;&#20110;&#30456;&#20851;&#30340;&#26497;&#32447;&#19978;&#12290;&#34429;&#28982;&#23427;&#30456;&#23545;&#26469;&#35828;&#27604;&#23545;&#24212;&#20851;&#31995;&#36739;&#24369;&#65292;&#20294;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#22312;&#26080;&#38656;3D&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20986;&#30456;&#26426;&#30340;&#30456;&#23545;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than corresponde
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#24402;&#19968;&#21270;&#27969;&#24212;&#29992;&#20110;&#26684;&#37327;&#23376;&#22330;&#35770;&#20013;&#65292;&#29983;&#25104;&#30456;&#20851;&#30340;&#26684;&#35268;&#22330;&#24577;&#38598;&#21512;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#21033;&#29992;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#35266;&#27979;&#37327;&#26102;&#30340;&#26041;&#24046;&#12290;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#20855;&#20307;&#24212;&#29992;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#26126;&#26174;&#38477;&#20302;&#20102;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10874</link><description>&lt;p&gt;
&#23558;&#27969;&#27169;&#22411;&#24212;&#29992;&#20110;&#30456;&#20851;&#26684;QCD&#24577;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Applications of flow models to the generation of correlated lattice QCD ensembles. (arXiv:2401.10874v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#24402;&#19968;&#21270;&#27969;&#24212;&#29992;&#20110;&#26684;&#37327;&#23376;&#22330;&#35770;&#20013;&#65292;&#29983;&#25104;&#30456;&#20851;&#30340;&#26684;&#35268;&#22330;&#24577;&#38598;&#21512;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#21033;&#29992;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#35266;&#27979;&#37327;&#26102;&#30340;&#26041;&#24046;&#12290;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#20855;&#20307;&#24212;&#29992;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#26126;&#26174;&#38477;&#20302;&#20102;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#29992;&#20110;&#26684;&#37327;&#23376;&#22330;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#22312;&#19981;&#21516;&#20316;&#29992;&#21442;&#25968;&#19979;&#29983;&#25104;&#32479;&#35745;&#30456;&#20851;&#30340;&#26684;&#35268;&#22330;&#24577;&#38598;&#21512;&#12290;&#26412;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#30456;&#20851;&#24615;&#26469;&#20943;&#23569;&#35745;&#31639;&#35266;&#27979;&#37327;&#26102;&#30340;&#26041;&#24046;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27531;&#24046;&#27969;&#32467;&#26500;&#65292;&#22312;&#36830;&#32493;&#26497;&#38480;&#30340;&#35268;&#33539;&#29702;&#35770;&#12289;QCD&#35266;&#27979;&#37327;&#30340;&#36136;&#37327;&#20381;&#36182;&#24615;&#21644;&#22522;&#20110;&#36153;&#26364;-&#36203;&#23572;&#26364;&#26041;&#27861;&#30340;&#24378;&#23376;&#30697;&#38453;&#20803;&#31561;&#19977;&#20010;&#27010;&#24565;&#35777;&#26126;&#24212;&#29992;&#20013;&#12290;&#22312;&#25152;&#26377;&#19977;&#31181;&#24773;&#20917;&#19979;&#65292;&#19982;&#20351;&#29992;&#19981;&#30456;&#20851;&#38598;&#21512;&#25110;&#30452;&#25509;&#37325;&#21152;&#26435;&#36827;&#34892;&#30340;&#30456;&#21516;&#35745;&#31639;&#30456;&#27604;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27969;&#30340;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learned normalizing flows can be used in the context of lattice quantum field theory to generate statistically correlated ensembles of lattice gauge fields at different action parameters. This work demonstrates how these correlations can be exploited for variance reduction in the computation of observables. Three different proof-of-concept applications are demonstrated using a novel residual flow architecture: continuum limits of gauge theories, the mass dependence of QCD observables, and hadronic matrix elements based on the Feynman-Hellmann approach. In all three cases, it is shown that statistical uncertainties are significantly reduced when machine-learned flows are incorporated as compared with the same calculations performed with uncorrelated ensembles or direct reweighting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>Ensembler&#26159;&#19968;&#20010;&#38450;&#27490;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#38598;&#25104;&#21644;&#24341;&#20837;&#25200;&#21160;&#30340;&#26041;&#24335;&#65292;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.10859</link><description>&lt;p&gt;
Ensembler:&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#38450;&#27490;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Ensembler: Combating model inversion attacks using model ensemble during collaborative inference. (arXiv:2401.10859v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10859
&lt;/p&gt;
&lt;p&gt;
Ensembler&#26159;&#19968;&#20010;&#38450;&#27490;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#38598;&#25104;&#21644;&#24341;&#20837;&#25200;&#21160;&#30340;&#26041;&#24335;&#65292;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#20419;&#20351;&#36793;&#32536;&#35774;&#22791;&#23558;&#25512;&#29702;&#36807;&#31243;&#30340;&#22823;&#37096;&#20998;&#36716;&#31227;&#21040;&#20113;&#31471;&#12290;&#34429;&#28982;&#36825;&#31181;&#20570;&#27861;&#24102;&#26469;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#20113;&#26381;&#21153;&#22120;&#30340;&#21487;&#20449;&#24230;&#21463;&#21040;&#36136;&#30097;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#23454;&#29992;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Ensembler&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22823;&#22823;&#22686;&#21152;&#23545;&#25239;&#26041;&#36827;&#34892;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#38590;&#24230;&#12290;Ensembler&#21033;&#29992;&#22312;&#23545;&#25239;&#26381;&#21153;&#22120;&#19978;&#36816;&#34892;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#30340;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#25200;&#21160;&#21040;&#25935;&#24863;&#25968;&#25454;&#30340;&#26041;&#27861;&#24182;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;&#22522;&#26412;&#30340;&#39640;&#26031;&#22122;&#22768;&#30456;&#32467;&#21512;&#65292;Ensembler&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#37325;&#24314;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have exhibited remarkable performance across various domains. Nevertheless, the burgeoning model sizes compel edge devices to offload a significant portion of the inference process to the cloud. While this practice offers numerous advantages, it also raises critical concerns regarding user data privacy. In scenarios where the cloud server's trustworthiness is in question, the need for a practical and adaptable method to safeguard data privacy becomes imperative. In this paper, we introduce Ensembler, an extensible framework designed to substantially increase the difficulty of conducting model inversion attacks for adversarial parties. Ensembler leverages model ensembling on the adversarial server, running in parallel with existing approaches that introduce perturbations to sensitive data during colloborative inference. Our experiments demonstrate that when combined with even basic Gaussian noise, Ensembler can effectively shield images from reconstruction attacks, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32676;&#20307;&#35299;&#30721;&#23545;&#20110;&#30284;&#30151;&#20122;&#22411;&#35786;&#26029;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37319;&#29992;&#21462;&#32988;&#24102;&#36208;(WTA)&#32593;&#32476;&#21644;&#22522;&#22240;&#30456;&#20284;&#24615;&#32593;&#32476;&#30340;&#26041;&#27861;&#23545;&#22810;&#32452;&#23398;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23637;&#29616;&#20102;WTA&#32593;&#32476;&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.10844</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#35299;&#30721;&#21644;&#19981;&#24179;&#34913;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#29992;&#20110;&#30284;&#30151;&#20122;&#22411;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer Subtype Diagnosis. (arXiv:2401.10844v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32676;&#20307;&#35299;&#30721;&#23545;&#20110;&#30284;&#30151;&#20122;&#22411;&#35786;&#26029;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37319;&#29992;&#21462;&#32988;&#24102;&#36208;(WTA)&#32593;&#32476;&#21644;&#22522;&#22240;&#30456;&#20284;&#24615;&#32593;&#32476;&#30340;&#26041;&#27861;&#23545;&#22810;&#32452;&#23398;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23637;&#29616;&#20102;WTA&#32593;&#32476;&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#31070;&#32463;&#35745;&#31639;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#37319;&#29992;&#20102;&#21462;&#32988;&#24102;&#36208;(WTA)&#30005;&#36335;&#26469;&#20419;&#36827;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#65292;&#20316;&#20026;&#20449;&#24687;&#22788;&#29702;&#30340;&#31070;&#32463;&#29983;&#29289;&#23398;&#21512;&#29702;&#27169;&#22411;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#20998;&#31867;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;MNIST&#25968;&#25454;&#38598;&#65292;&#26469;&#39564;&#35777;&#36825;&#20123;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#23578;&#26410;&#23601;&#22914;&#20309;&#26368;&#20339;&#22320;&#23558;&#36825;&#20123;&#32593;&#32476;&#30340;&#38543;&#26426;&#21709;&#24212;&#36716;&#21270;&#20026;&#31163;&#25955;&#20915;&#31574;&#36798;&#25104;&#19968;&#33268;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#32676;&#20307;&#35299;&#30721;&#12290;&#23613;&#31649;&#32676;&#20307;&#35299;&#30721;&#26159;SNNs&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#37096;&#20998;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26174;&#31034;&#32676;&#20307;&#35299;&#30721;&#23545;WTA&#32593;&#32476;&#30340;&#20998;&#31867;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;WTA&#32593;&#32476;&#24212;&#29992;&#20110;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#30284;&#30151;&#20122;&#22411;&#35786;&#26029;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#30284;&#30151;&#22522;&#22240;&#22270;&#35889;&#25968;&#25454;&#24211;(TCGA)&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#22240;&#30456;&#20284;&#24615;&#32593;&#32476;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent strides in the field of neural computation has seen the adoption of Winner Take All (WTA) circuits to facilitate the unification of hierarchical Bayesian inference and spiking neural networks as a neurobiologically plausible model of information processing. Current research commonly validates the performance of these networks via classification tasks, particularly of the MNIST dataset. However, researchers have not yet reached consensus about how best to translate the stochastic responses from these networks into discrete decisions, a process known as population decoding. Despite being an often underexamined part of SNNs, in this work we show that population decoding has a significanct impact on the classification performance of WTA networks. For this purpose, we apply a WTA network to the problem of cancer subtype diagnosis from multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing so we utilise a novel implementation of gene similarity networks, a featu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#27493;&#20869;&#22686;&#24378;&#29305;&#24449;&#23398;&#20064;&#21644;&#28608;&#27963;&#25928;&#29575;&#65292;&#20026;&#26356;&#21152;&#33410;&#33021;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10843</link><description>&lt;p&gt;
&#20197;&#25913;&#36827;&#25928;&#29575;&#21644;&#26368;&#23567;&#24310;&#36831;&#35757;&#32451;&#36890;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency. (arXiv:2401.10843v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#27493;&#20869;&#22686;&#24378;&#29305;&#24449;&#23398;&#20064;&#21644;&#28608;&#27963;&#25928;&#29575;&#65292;&#20026;&#26356;&#21152;&#33410;&#33021;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#20107;&#20214;&#39537;&#21160;&#26041;&#24335;&#36816;&#34892;&#24182;&#37319;&#29992;&#20108;&#36827;&#21046;&#33033;&#20914;&#34920;&#31034;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#34987;&#35748;&#20026;&#26159;&#33410;&#33021;&#35745;&#31639;&#30340;&#26377;&#24076;&#26395;&#20505;&#36873;&#32773;&#12290;&#28982;&#32780;&#65292;&#39640;&#24615;&#33021;SNNs&#30340;&#35757;&#32451;&#23384;&#22312;&#25104;&#26412;&#29942;&#39048;&#65306;&#35757;&#32451;SNN&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#27493;&#25968;&#20197;&#21450;&#36890;&#24120;&#30340;&#23398;&#20064;&#36845;&#20195;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#27493;&#20869;&#22686;&#24378;&#29305;&#24449;&#23398;&#20064;&#21644;&#28608;&#27963;&#25928;&#29575;&#65292;&#20026;&#26356;&#21152;&#33410;&#33021;&#30340;SNNs&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;SNN&#20013;&#30340;&#31070;&#32463;&#20803;&#20174;&#19981;&#21516;&#30340;&#24863;&#21463;&#37326;&#23398;&#20064;&#24378;&#22823;&#30340;&#33033;&#20914;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#21050;&#28608;&#21644;&#26469;&#33258;&#20854;&#20182;&#31070;&#32463;&#20803;&#30340;&#24490;&#29615;&#20449;&#24687;&#26469;&#26356;&#26032;&#31070;&#32463;&#20803;&#29366;&#24577;&#12290;&#35813;&#35774;&#32622;&#22312;&#21333;&#20010;&#26102;&#38388;&#27493;&#20869;&#36830;&#32493;&#34917;&#20805;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25237;&#24433;&#20989;&#25968;&#26469;&#21512;&#24182;&#36825;&#20004;&#20010;&#21050;&#28608;&#65292;&#20197;&#24179;&#28369;&#20248;&#21270;&#31070;&#32463;&#20803;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs. Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10841</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21457;&#29616;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#32534;&#30721;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#34067;&#24310;&#32473;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24102;&#26469;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#19968;&#20010;&#29305;&#27530;&#30340;&#25361;&#25112;&#19982;&#20351;&#29992;&#32534;&#30721;&#35821;&#35328;&#30340;&#32676;&#20307;&#26377;&#20851;&#65292;&#36825;&#20123;&#32676;&#20307;&#26082;&#24819;&#20026;&#20854;&#29992;&#25143;&#21019;&#36896;&#24402;&#23646;&#24863;&#65292;&#21448;&#24819;&#22238;&#36991;&#26816;&#27979;&#12290;&#32534;&#30721;&#35821;&#35328;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20351;&#29992;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#22312;&#32447;&#21453;&#29369;&#22826;&#35328;&#35770;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25235;&#21462;&#30340;&#24086;&#23376;&#65292;&#36890;&#24120;&#26159;&#26497;&#31471;&#20027;&#20041;&#29992;&#25143;&#20351;&#29992;&#30340;&#12290;&#24086;&#23376;&#26159;&#20351;&#29992;&#19982;&#20197;&#21069;&#24050;&#30693;&#30340;&#38024;&#23545;&#29369;&#22826;&#20154;&#30340;&#20167;&#24680;&#35328;&#35770;&#30456;&#20851;&#30340;&#31181;&#23376;&#34920;&#36798;&#24335;&#36827;&#34892;&#25235;&#21462;&#30340;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#24086;&#23376;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#12290;&#36807;&#28388;&#25481;&#35821;&#27861;&#19981;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#21644;&#20043;&#21069;&#36935;&#21040;&#36807;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#35821;&#20041;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#65288;SCD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#26641;&#26126;&#30830;&#34920;&#31034;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#65292;&#20197;&#21450;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#23398;&#20064;&#21442;&#25968;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10840</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#23454;&#29616;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#30340;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems. (arXiv:2401.10840v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#65288;SCD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#26641;&#26126;&#30830;&#34920;&#31034;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#65292;&#20197;&#21450;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#23398;&#20064;&#21442;&#25968;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#35780;&#20272;&#26159;&#23398;&#29983;&#23398;&#20064;&#30340;&#22522;&#26412;&#19988;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#23427;&#24314;&#27169;&#20102;&#23398;&#29983;&#19982;&#32451;&#20064;&#30340;&#20114;&#21160;&#65292;&#24182;&#21457;&#29616;&#20102;&#23398;&#29983;&#23545;&#27599;&#20010;&#30693;&#35782;&#23646;&#24615;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#23454;&#38469;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#65292;&#35748;&#30693;&#35786;&#26029;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#21516;&#26679;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#20108;&#32773;&#20860;&#39038;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#35748;&#30693;&#35786;&#26029;&#65288;SCD&#65289;&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;SCD&#26694;&#26550;&#21033;&#29992;&#31526;&#21495;&#26641;&#26126;&#30830;&#22320;&#34920;&#31034;&#22797;&#26434;&#30340;&#23398;&#29983;&#19982;&#32451;&#20064;&#20114;&#21160;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#23398;&#20064;&#23398;&#29983;&#21644;&#32451;&#20064;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25361;&#25112;&#22312;&#20110;&#25105;&#20204;&#38656;&#35201;&#20256;&#36882;&#31163;&#25955;&#30340;&#31526;&#21495;&#34920;&#31034;&#21644;&#36830;&#32493;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis assessment is a fundamental and crucial task for student learning. It models the student-exercise interaction, and discovers the students' proficiency levels on each knowledge attribute. In real-world intelligent education systems, generalization and interpretability of cognitive diagnosis methods are of equal importance. However, most existing methods can hardly make the best of both worlds due to the complicated student-exercise interaction. To this end, this paper proposes a symbolic cognitive diagnosis~(SCD) framework to simultaneously enhance generalization and interpretability. The SCD framework incorporates the symbolic tree to explicably represent the complicated student-exercise interaction function, and utilizes gradient-based optimization methods to effectively learn the student and exercise parameters. Meanwhile, the accompanying challenge is that we need to tunnel the discrete symbolic representation and continuous parameter optimization. To address thi
&lt;/p&gt;</description></item><item><title>Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials.</title><link>http://arxiv.org/abs/2401.10839</link><description>&lt;p&gt;
&#24377;&#24615;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65306;Holonic Learning
&lt;/p&gt;
&lt;p&gt;
Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework. (arXiv:2401.10839v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10839
&lt;/p&gt;
&lt;p&gt;
Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#26222;&#21450;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#25512;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#21521;&#26356;&#20998;&#24067;&#24335;&#30340;&#26041;&#27861;&#36716;&#21464;&#12290;&#36825;&#31181;&#36716;&#21464;&#19981;&#20165;&#26088;&#22312;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#21644;&#36164;&#28304;&#20998;&#24067;&#25361;&#25112;&#65292;&#36824;&#35201;&#35299;&#20915;&#32039;&#36843;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#23545;&#36825;&#19968;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Holonic Learning (HoL)&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#21327;&#20316;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Holonic&#27010;&#24565;&#65292;HoL&#26694;&#26550;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#33258;&#30456;&#20284;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27599;&#20010;holon&#30340;&#20010;&#20307;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20869;&#37096;holon&#25215;&#35834;&#21644;&#36890;&#20449;&#27169;&#24335;&#65292;&#23454;&#29616;&#23545;&#21327;&#20316;&#30340;&#26356;&#32454;&#33268;&#25511;&#21046;&#12290;Holonic Learning&#22312;&#20854;&#36890;&#29992;&#24418;&#24335;&#20013;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#35774;&#35745;&#21644;&#28789;&#27963;&#24615;&#28508;&#21147;&#12290;&#20026;&#20102;&#32463;&#39564;&#20998;&#26512;&#21644;&#23637;&#31034;&#20854;&#25928;&#26524;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;Holo
&lt;/p&gt;
&lt;p&gt;
Ever-increasing ubiquity of data and computational resources in the last decade have propelled a notable transition in the machine learning paradigm towards more distributed approaches. Such a transition seeks to not only tackle the scalability and resource distribution challenges but also to address pressing privacy and security concerns. To contribute to the ongoing discourse, this paper introduces Holonic Learning (HoL), a collaborative and privacy-focused learning framework designed for training deep learning models. By leveraging holonic concepts, the HoL framework establishes a structured self-similar hierarchy in the learning process, enabling more nuanced control over collaborations through the individual model aggregation approach of each holon, along with their intra-holon commitment and communication patterns. HoL, in its general form, provides extensive design and flexibility potentials. For empirical analysis and to demonstrate its effectiveness, this paper implements Holo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10831</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#27010;&#24565;&#21457;&#29616;&#29702;&#35299;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#22522;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#39640;&#23618;&#26102;&#31354;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20197;&#24448;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35270;&#39057;&#27169;&#22411;&#22788;&#29702;&#20102;&#39069;&#22806;&#30340;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#21160;&#24577;&#27010;&#24565;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;(VTCD)&#31639;&#27861;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21333;&#20803;&#65288;&#27010;&#24565;&#65289;&#24182;&#23545;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#24471;&#21040;&#30340;&#27010;&#24565;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.10825</link><description>&lt;p&gt;
&#26368;&#26032;&#36827;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on recent advances in named entity recognition. (arXiv:2401.10825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10825
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#21629;&#21517;&#30495;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#23376;&#23383;&#31526;&#20018;&#65292;&#24182;&#30830;&#23450;&#20854;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#26159;&#21542;&#25351;&#20154;&#29289;&#25110;&#32452;&#32455;&#65289;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26368;&#36817;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#20851;&#27880;&#20102;&#22522;&#20110;&#22270;&#21644;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24456;&#23569;&#22312;&#20854;&#20182;&#32508;&#36848;&#20013;&#28041;&#21450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#31232;&#32570;&#27880;&#37322;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20027;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#23454;&#29616;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#65288;&#39046;&#22495;&#12289;&#35268;&#27169;&#21644;&#31867;&#21035;&#25968;&#65289;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#26410;&#21516;&#26102;&#32771;&#34385;&#30340;&#31639;&#27861;&#30340;&#28145;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#27604;&#36739;&#30340;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, but we also look at graph- and transformer- based methods including Large Language Models (LLMs) that have not had much coverage in other surveys. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that are never considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods that we compare.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21270;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#31946;&#25512;&#29702;&#21644;&#27010;&#29575;&#25512;&#29702;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24471;&#21040;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#22914;&#19982;&#20044;&#40486;&#24726;&#35770;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.10819</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimisation in Neurosymbolic Learning Systems. (arXiv:2401.10819v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10819
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21270;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#31946;&#25512;&#29702;&#21644;&#27010;&#29575;&#25512;&#29702;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24471;&#21040;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#22914;&#19982;&#20044;&#40486;&#24726;&#35770;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#38598;&#25104;&#26377;&#35768;&#22810;&#20248;&#21183;&#65292;&#20363;&#22914;&#20943;&#23569;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#25552;&#39640;&#27169;&#22411;&#31572;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#20197;&#21450;&#39564;&#35777;&#35757;&#32451;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#31526;&#21495;&#35821;&#35328;&#34920;&#31034;&#25968;&#25454;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;&#25105;&#20204;&#22914;&#20309;&#36830;&#25509;&#31526;&#21495;&#21644;&#31070;&#32463;&#32452;&#20214;&#20197;&#20256;&#36798;&#36825;&#20123;&#30693;&#35782;&#21602;&#65311;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#27169;&#31946;&#25512;&#29702;&#65292;&#23427;&#30740;&#31350;&#30340;&#26159;&#30495;&#23454;&#31243;&#24230;&#12290;&#20363;&#22914;&#65292;&#36523;&#39640;&#24182;&#19981;&#26159;&#20108;&#20803;&#27010;&#24565;&#12290;&#30456;&#21453;&#65292;&#27010;&#29575;&#25512;&#29702;&#30740;&#31350;&#30340;&#26159;&#26576;&#20214;&#20107;&#24773;&#25104;&#20026;&#30495;&#23454;&#25110;&#23558;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#30740;&#31350;&#38382;&#39064;&#25506;&#35752;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#27169;&#31946;&#25512;&#29702;&#19982;&#23398;&#20064;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#19982;&#20044;&#40486;&#24726;&#35770;&#30340;&#32852;&#31995;&#65292;&#21363;&#24403;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#32511;&#33394;&#30340;&#33529;&#26524;&#26102;&#65292;&#25105;&#20204;&#30830;&#35748;&#8220;&#20044;&#40486;&#26159;&#40657;&#33394;&#30340;&#8221;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#20351;&#29992;&#32972;&#26223;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI aims to integrate deep learning with symbolic AI. This integration has many promises, such as decreasing the amount of data required to train a neural network, improving the explainability and interpretability of answers given by models and verifying the correctness of trained systems. We study neurosymbolic learning, where we have both data and background knowledge expressed using symbolic languages. How do we connect the symbolic and neural components to communicate this knowledge? One option is fuzzy reasoning, which studies degrees of truth. For example, being tall is not a binary concept. Instead, probabilistic reasoning studies the probability that something is true or will happen. Our first research question studies how different forms of fuzzy reasoning combine with learning. We find surprising results like a connection to the Raven paradox stating we confirm "ravens are black" when we observe a green apple. In this study, we did not use the background knowledg
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;</title><link>http://arxiv.org/abs/2401.10816</link><description>&lt;p&gt;
Co-Pilot for Health: &#20010;&#24615;&#21270;&#31639;&#27861;AI&#24341;&#23548;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#33258;&#21160;&#22609;&#36896;&#22823;&#22411;&#20154;&#32676;&#30340;&#20581;&#24247;&#34892;&#20026;&#65292;&#36328;&#21487;&#31359;&#25140;&#35774;&#22791;&#31867;&#22411;&#21644;&#30142;&#30149;&#29366;&#20917;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#20840;&#29699;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#20581;&#36523;&#35774;&#22791;&#30340;&#31934;&#32454;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#29992;&#20110;&#25968;&#23383;&#31639;&#27861;&#24341;&#23548;&#12290;&#22312;&#27492;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#24179;&#21488;&#22312;&#26032;&#21152;&#22369;&#38024;&#23545;$n=84,764$&#20010;&#20010;&#20307;&#30340;12&#21608;&#26399;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#30340;&#26377;&#25928;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#32479;&#35745;&#39564;&#35777;&#20102;&#30446;&#26631;&#32452;&#20013;&#25509;&#21463;&#27492;&#31867;AI&#20248;&#21270;&#26085;&#24120;&#24341;&#23548;&#30340;&#21442;&#19982;&#32773;&#30456;&#36739;&#20110;&#25511;&#21046;&#32452;&#20013;&#26410;&#25509;&#21463;&#20219;&#20309;&#24341;&#23548;&#30340;&#21305;&#37197;&#21442;&#19982;&#32773;&#65292;&#20854;&#27599;&#22825;&#30340;&#27493;&#25968;&#22686;&#21152;&#20102;6.17%&#65288;$p = 3.09\times10^{-4}$&#65289;&#65292;&#27599;&#21608;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#65288;MVPA&#65289;&#20998;&#38047;&#22686;&#21152;&#20102;7.61%&#65288;$p = 1.16\times10^{-2}$&#65289;&#12290;&#27492;&#22806;&#65292;&#27492;&#31867;&#24341;&#23548;&#38750;&#24120;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SBBO&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20165;&#38656;&#22522;&#20110;&#37319;&#26679;&#30340;&#35775;&#38382;&#26469;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.10811</link><description>&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simulation Based Bayesian Optimization. (arXiv:2401.10811v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SBBO&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20165;&#38656;&#22522;&#20110;&#37319;&#26679;&#30340;&#35775;&#38382;&#26469;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#19982;&#25345;&#32493;&#20989;&#25968;&#35780;&#20272;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#26500;&#24314;&#19982;&#21327;&#21464;&#37327;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#26469;&#25351;&#23548;&#26410;&#26469;&#35780;&#20272;&#28857;&#30340;&#36873;&#25321;&#12290;&#23545;&#20110;&#24179;&#28369;&#36830;&#32493;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#39640;&#26031;&#36807;&#31243;&#32463;&#24120;&#34987;&#29992;&#20316;&#20195;&#29702;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#23545;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#35299;&#26512;&#35775;&#38382;&#65292;&#20174;&#32780;&#20415;&#20110;&#35745;&#31639;&#21644;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#23545;&#20998;&#31867;&#25110;&#28151;&#21512;&#21327;&#21464;&#37327;&#31354;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#65292;&#39640;&#26031;&#36807;&#31243;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SBBO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#23545;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#22522;&#20110;&#37319;&#26679;&#30340;&#35775;&#38382;&#65292;&#20197;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimizations over categorical or mixed covariate spaces, GPs may not be ideal.  This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires \emph{sampling-based} access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#24809;&#32602;&#20108;&#38454;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#21017;&#24456;&#23569;&#33021;&#24102;&#26469;&#36825;&#26679;&#30340;&#22909;&#22788;&#12290;&#20316;&#32773;&#36890;&#36807;&#23545;&#25439;&#22833;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#32467;&#26500;&#36827;&#34892;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#30340;&#24320;&#21457;&#21644;&#29305;&#24449;&#30340;&#25506;&#32034;&#20043;&#38388;&#30340;&#37327;&#21270;&#20998;&#31163;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#21457;&#29616;&#24573;&#35270;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#35823;&#24046;&#30697;&#38453; (NME) &#23454;&#38469;&#19978;&#24456;&#37325;&#35201;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26799;&#24230;&#24809;&#32602;&#23545;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36890;&#36807;&#35774;&#35745;&#24178;&#39044;&#25514;&#26045;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#25361;&#25112;&#20102;&#20197;&#24448;&#30340;&#35266;&#28857;&#65292;&#35748;&#20026;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#26159;&#31561;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.10809</link><description>&lt;p&gt;
&#34987;&#24573;&#35270;&#30340;&#40657;&#22622; (Hessian) &#32452;&#20214;&#35299;&#37322;&#20102;&#38160;&#21270;&#27491;&#21017;&#21270;&#20013;&#30340;&#35868;&#22242;
&lt;/p&gt;
&lt;p&gt;
Neglected Hessian component explains mysteries in Sharpness regularization. (arXiv:2401.10809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10809
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#24809;&#32602;&#20108;&#38454;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#21017;&#24456;&#23569;&#33021;&#24102;&#26469;&#36825;&#26679;&#30340;&#22909;&#22788;&#12290;&#20316;&#32773;&#36890;&#36807;&#23545;&#25439;&#22833;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#32467;&#26500;&#36827;&#34892;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#30340;&#24320;&#21457;&#21644;&#29305;&#24449;&#30340;&#25506;&#32034;&#20043;&#38388;&#30340;&#37327;&#21270;&#20998;&#31163;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#21457;&#29616;&#24573;&#35270;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#35823;&#24046;&#30697;&#38453; (NME) &#23454;&#38469;&#19978;&#24456;&#37325;&#35201;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26799;&#24230;&#24809;&#32602;&#23545;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36890;&#36807;&#35774;&#35745;&#24178;&#39044;&#25514;&#26045;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#25361;&#25112;&#20102;&#20197;&#24448;&#30340;&#35266;&#28857;&#65292;&#35748;&#20026;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#26159;&#31561;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;SAM&#36825;&#26679;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#24809;&#32602;&#20108;&#38454;&#20449;&#24687;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#22914;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#65292;&#32463;&#24120;&#26080;&#27861;&#25552;&#20379;&#36825;&#26679;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#24046;&#24322;&#21487;&#20197;&#36890;&#36807;&#25439;&#22833;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#32467;&#26500;&#26469;&#35299;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#20998;&#35299;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;&#20026;&#23558;&#29305;&#24449;&#30340;&#24320;&#21457;&#21644;&#29305;&#24449;&#30340;&#25506;&#32034;&#20998;&#24320;&#12290;&#29305;&#24449;&#30340;&#25506;&#32034;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#24314;&#27169;&#35823;&#24046;&#30697;&#38453;(NME)&#26469;&#25551;&#36848;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#36890;&#24120;&#34987;&#24573;&#35270;&#65292;&#22240;&#20026;&#23427;&#22312;&#25554;&#20540;&#20013;&#28040;&#22833;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;NME&#20107;&#23454;&#19978;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26799;&#24230;&#24809;&#32602;&#23545;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#35777;&#25454;&#26469;&#25361;&#25112;&#20102;&#38271;&#26399;&#20197;&#26469;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#31561;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;TAMS&#21644;Next-Generation Reservoir Computing&#25216;&#26415;&#65292;&#21033;&#29992;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;&#26469;&#28304;&#20110;&#25968;&#25454;&#30340;&#30830;&#23450;&#20989;&#25968;&#65292;&#26469;&#35745;&#31639;&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10800</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;AMOC&#36716;&#25442;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm. (arXiv:2401.10800v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;TAMS&#21644;Next-Generation Reservoir Computing&#25216;&#26415;&#65292;&#21033;&#29992;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;&#26469;&#28304;&#20110;&#25968;&#25454;&#30340;&#30830;&#23450;&#20989;&#25968;&#65292;&#26469;&#35745;&#31639;&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#26159;&#20840;&#29699;&#27668;&#20505;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20020;&#30028;&#22240;&#32032;&#65292;&#21487;&#20197;&#22312;&#20840;&#29699;&#21464;&#26262;&#19979;&#23849;&#28291;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#19968;&#31181;&#31232;&#20107;&#20214;&#31639;&#27861;&#65288;Trajectory-Adaptive Multilevel Splitting&#65292;TAMS&#65289;&#35745;&#31639;AMOC&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;TAMS&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#24471;&#20998;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#34429;&#28982;&#24050;&#30693;&#26368;&#20339;&#24471;&#20998;&#20989;&#25968;&#30340;&#23450;&#20041;&#65292;&#31216;&#20026;&#8220;&#30830;&#23450;&#20989;&#25968;&#8221;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#20808;&#39564;&#22320;&#35745;&#31639;&#23427;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;TAMS&#19982;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20174;&#31232;&#20107;&#20214;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#30830;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;AMOC&#30340;&#38543;&#26426;&#30418;&#27169;&#22411;&#20013;&#27979;&#35797;&#20102;&#36825;&#31181;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#23384;&#22312;&#20004;&#31181;&#36716;&#21464;&#31867;&#22411;&#65292;&#31216;&#20026;F&#65288;&#24555;&#36895;&#65289;&#36716;&#21464;&#21644;S&#65288;&#32531;&#24930;&#65289;&#36716;&#21464;&#12290;F&#36716;&#21464;&#30340;&#32467;&#26524;&#19982;&#37027;&#20123;&#36827;&#34892;&#20102;&#26377;&#21033;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Atlantic Meridional Overturning Circulation (AMOC) is an important component of the global climate, known to be a tipping element, as it could collapse under global warming. The main objective of this study is to compute the probability that the AMOC collapses within a specified time window, using a rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS). However, the efficiency and accuracy of TAMS depend on the choice of the score function. Although the definition of the optimal score function, called ``committor function" is known, it is impossible in general to compute it a priori. Here, we combine TAMS with a Next-Generation Reservoir Computing technique that estimates the committor function from the data generated by the rare-event algorithm. We test this technique in a stochastic box model of the AMOC for which two types of transition exist, the so-called F(ast)-transitions and S(low)-transitions. Results for the F-transtions compare favorably with those 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#26469;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#30340;&#24615;&#33021;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#34920;&#26684;&#24615;&#33021;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#26469;&#25429;&#25417;&#26679;&#26412;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#24037;&#31243;&#21644;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#24418;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Novel Representation Learning Technique using Graphs for Performance Analytics. (arXiv:2401.10799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#26469;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#30340;&#24615;&#33021;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#34920;&#26684;&#24615;&#33021;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#26469;&#25429;&#25417;&#26679;&#26412;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#24037;&#31243;&#21644;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#20013;&#30340;&#24615;&#33021;&#20998;&#26512;&#39046;&#22495;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#65292;&#20363;&#22914;&#39044;&#27979;&#25191;&#34892;&#26102;&#38388;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#21033;&#29992;&#32473;&#23450;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20174;&#21407;&#22987;&#29305;&#24449;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#32791;&#36153;&#26102;&#38388;&#21644;&#20154;&#24037;&#21162;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20004;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#23558;&#34920;&#26684;&#24615;&#33021;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#24418;&#65292;&#20197;&#21033;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25216;&#26415;&#22312;&#25429;&#25417;&#29305;&#24449;&#21644;&#26679;&#26412;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#19982;&#20854;&#20182;&#24212;&#29992;&#39046;&#22495;&#65288;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#65289;&#19981;&#21516;&#65292;&#22270;&#24418;&#26159;&#19981;&#23384;&#22312;&#30340;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#38656;&#35201;&#26500;&#24314;&#23427;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26500;&#24314;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#33410;&#28857;&#20195;&#34920;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance analytics domain in High Performance Computing (HPC) uses tabular data to solve regression problems, such as predicting the execution time. Existing Machine Learning (ML) techniques leverage the correlations among features given tabular datasets, not leveraging the relationships between samples directly. Moreover, since high-quality embeddings from raw features improve the fidelity of the downstream predictive models, existing methods rely on extensive feature engineering and pre-processing steps, costing time and manual effort. To fill these two gaps, we propose a novel idea of transforming tabular performance data into graphs to leverage the advancement of Graph Neural Network-based (GNN) techniques in capturing complex relationships between features and samples. In contrast to other ML application domains, such as social networks, the graph is not given; instead, we need to build it. To address this gap, we propose graph-building methods where nodes represent samples
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#27963;&#21160;&#24863;&#30693;&#21160;&#24577;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;SlowFast&#27169;&#22411;&#23454;&#29616;&#31934;&#30830;&#30417;&#27979;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.10794</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#27963;&#21160;&#24863;&#30693;&#21160;&#24577;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Empowered Activity-Aware Dynamic Health Monitoring Systems. (arXiv:2401.10794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#27963;&#21160;&#24863;&#30693;&#21160;&#24577;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;SlowFast&#27169;&#22411;&#23454;&#29616;&#31934;&#30830;&#30417;&#27979;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#21307;&#30103;&#20013;&#65292;&#20581;&#24247;&#30417;&#27979;&#21033;&#29992;&#21508;&#31181;&#24037;&#20855;&#21644;&#25216;&#26415;&#20998;&#26512;&#24739;&#32773;&#30340;&#23454;&#26102;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#65292;&#23454;&#29616;&#21363;&#26102;&#30340;&#25805;&#20316;&#21644;&#24178;&#39044;&#12290;&#29616;&#26377;&#30340;&#30417;&#27979;&#26041;&#27861;&#26159;&#22312;&#21307;&#30103;&#35774;&#22791;&#21516;&#26102;&#36319;&#36394;&#22810;&#20010;&#20581;&#24247;&#25351;&#26631;&#30340;&#21069;&#25552;&#19979;&#35774;&#35745;&#30340;&#65292;&#20197;&#36866;&#24212;&#20854;&#25351;&#23450;&#30340;&#21151;&#33021;&#33539;&#22260;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#25253;&#21578;&#35813;&#33539;&#22260;&#20869;&#30340;&#25152;&#26377;&#30456;&#20851;&#20581;&#24247;&#25968;&#20540;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#22810;&#30340;&#36164;&#28304;&#20351;&#29992;&#21644;&#25910;&#38598;&#19981;&#30456;&#20851;&#30340;&#20581;&#24247;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;SlowFast&#27169;&#22411;&#30340;&#21160;&#24577;&#27963;&#21160;&#24863;&#30693;&#20581;&#24247;&#30417;&#27979;&#31574;&#30053;&#65288;DActAHM&#65289;&#65292;&#20197;&#22312;&#29992;&#25143;&#27963;&#21160;&#22522;&#30784;&#19978;&#30830;&#20445;&#31934;&#30830;&#30417;&#27979;&#65292;&#23454;&#29616;&#30417;&#27979;&#24615;&#33021;&#21644;&#25104;&#26412;&#25928;&#30410;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DActAHM&#20351;&#29992;SlowFast&#27169;&#22411;&#39640;&#25928;&#22320;&#35782;&#21035;&#20010;&#20307;&#27963;&#21160;&#65292;&#24182;&#25429;&#25417;&#36825;&#20123;&#32467;&#26524;&#20197;&#36827;&#34892;&#22686;&#24378;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In smart healthcare, health monitoring utilizes diverse tools and technologies to analyze patients' real-time biosignal data, enabling immediate actions and interventions. Existing monitoring approaches were designed on the premise that medical devices track several health metrics concurrently, tailored to their designated functional scope. This means that they report all relevant health values within that scope, which can result in excess resource use and the gathering of extraneous data due to monitoring irrelevant health metrics. In this context, we propose Dynamic Activity-Aware Health Monitoring strategy (DActAHM) for striking a balance between optimal monitoring performance and cost efficiency, a novel framework based on Deep Reinforcement Learning (DRL) and SlowFast Model to ensure precise monitoring based on users' activities. Specifically, with the SlowFast Model, DActAHM efficiently identifies individual activities and captures these results for enhanced processing. Subsequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.10791</link><description>&lt;p&gt;
&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
Early alignment in two-layer networks training is a two-edged sword. (arXiv:2401.10791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#26680;&#24515;&#12290;&#21021;&#22987;&#21270;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#20026;&#23567;&#30340;&#21021;&#22987;&#21270;&#36890;&#24120;&#19982;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30456;&#20851;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#23545;&#31616;&#21333;&#35299;&#38544;&#21547;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26089;&#26399;&#23545;&#40784;&#38454;&#27573;&#30340;&#26222;&#36941;&#21644;&#37327;&#21270;&#25551;&#36848;&#65292;&#26368;&#21021;&#30001;Maennel&#31561;&#20154;&#25552;&#20986;&#12290;&#23545;&#20110;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#65292;&#35757;&#32451;&#21160;&#24577;&#30340;&#26089;&#26399;&#38454;&#27573;&#23548;&#33268;&#31070;&#32463;&#20803;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#24341;&#21457;&#20102;&#32593;&#32476;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#36825;&#19982;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#26159;&#20197;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#30446;&#26631;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#20026;&#20195;&#20215;&#30340;&#65306;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#31034;&#20363;&#65292;&#20854;&#20013;&#36229;&#21442;&#25968;&#32593;&#32476;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018) . For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22330;&#26223;&#32423;&#23545;&#35937;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#24433;&#21709;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#27169;&#22411;&#22312;&#26377;&#21644;&#27809;&#26377;&#29305;&#23450;&#22330;&#26223;&#32423;&#23545;&#35937;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#23545;&#35937;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.10790</link><description>&lt;p&gt;
&#34913;&#37327;&#22330;&#26223;&#32423;&#23545;&#35937;&#23545;&#30446;&#26631;&#26816;&#27979;&#30340;&#24433;&#21709;&#65306;&#36208;&#21521;&#23545;&#26816;&#27979;&#20915;&#31574;&#30340;&#23450;&#37327;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Measuring the Impact of Scene Level Objects on Object Detection: Towards Quantitative Explanations of Detection Decisions. (arXiv:2401.10790v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22330;&#26223;&#32423;&#23545;&#35937;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#24433;&#21709;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#27169;&#22411;&#22312;&#26377;&#21644;&#27809;&#26377;&#29305;&#23450;&#22330;&#26223;&#32423;&#23545;&#35937;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#23545;&#35937;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20934;&#30830;&#24230;&#21644;&#20854;&#20182;&#24120;&#35265;&#25351;&#26631;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#29992;&#31383;&#21475;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#28145;&#20837;&#35270;&#35282;&#12290;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#21644;&#36807;&#31243;&#30340;&#36136;&#37327;&#22914;&#20309;&#65292;&#26080;&#27861;&#20445;&#35777;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;&#27169;&#22411;&#21487;&#33021;&#20250;&#23398;&#20064;&#21040;&#26576;&#20123;&#32972;&#26223;&#19978;&#19979;&#25991;&#65288;&#21363;&#22330;&#26223;&#32423;&#23545;&#35937;&#65289;&#19982;&#26631;&#35760;&#31867;&#21035;&#30340;&#23384;&#22312;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26631;&#20934;&#30340;&#24615;&#33021;&#39564;&#35777;&#21644;&#25351;&#26631;&#26080;&#27861;&#35782;&#21035;&#36825;&#19968;&#29616;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#39564;&#35777;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#25214;&#20986;&#22330;&#26223;&#32423;&#23545;&#35937;&#23545;&#22270;&#20687;&#20013;&#29289;&#20307;&#35782;&#21035;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27604;&#36739;&#27169;&#22411;&#22312;&#26377;&#21644;&#27809;&#26377;&#29305;&#23450;&#22330;&#26223;&#32423;&#23545;&#35937;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#36825;&#20123;&#23545;&#35937;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#12290;&#26412;&#23454;&#39564;&#23558;&#35780;&#20272;&#24314;&#31569;&#29289;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although accuracy and other common metrics can provide a useful window into the performance of an object detection model, they lack a deeper view of the model's decision process. Regardless of the quality of the training data and process, the features that an object detection model learns cannot be guaranteed. A model may learn a relationship between certain background context, i.e., scene level objects, and the presence of the labeled classes. Furthermore, standard performance verification and metrics would not identify this phenomenon. This paper presents a new black box explainability method for additional verification of object detection models by finding the impact of scene level objects on the identification of the objects within the image. By comparing the accuracies of a model on test data with and without certain scene level objects, the contributions of these objects to the model's performance becomes clearer. The experiment presented here will assess the impact of buildings 
&lt;/p&gt;</description></item><item><title>Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2401.10774</link><description>&lt;p&gt;
Medusa: &#22810;&#35299;&#30721;&#22836;&#30340;&#31616;&#27905;LLM&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10774
&lt;/p&gt;
&lt;p&gt;
Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#21463;&#38480;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#24182;&#34892;&#24615;&#32570;&#22833;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#25805;&#20316;&#21463;&#38480;&#20110;&#21152;&#36895;&#22120;&#30340;&#20869;&#23384;&#24102;&#23485;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#33719;&#24471;&#21644;&#32500;&#25252;&#29420;&#31435;&#30340;&#33609;&#31295;&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#65292;&#20197;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#12290;Medusa&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#21516;&#26102;&#26500;&#36896;&#22810;&#20010;&#20505;&#36873;&#24310;&#32493;&#24182;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#65292;Medusa&#22312;&#21333;&#27493;&#24310;&#36831;&#26041;&#38754;&#20165;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#35299;&#30721;&#27493;&#39588;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
&lt;/p&gt;</description></item><item><title>Starlit&#26159;&#19968;&#20010;&#26032;&#30340;&#21487;&#25193;&#23637;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#27491;&#24335;&#30340;&#23433;&#20840;&#23450;&#20041;&#21644;&#35777;&#26126;&#12289;&#20551;&#23450;&#20923;&#32467;&#36134;&#25143;&#12289;&#35268;&#27169;&#25193;&#22823;&#12289;&#36523;&#20221;&#23545;&#40784;&#38454;&#27573;&#21644;&#38590;&#20197;&#25269;&#25239;&#23458;&#25143;&#31471;&#36864;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.10765</link><description>&lt;p&gt;
Starlit: &#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#20197;&#22686;&#24378;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection. (arXiv:2401.10765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10765
&lt;/p&gt;
&lt;p&gt;
Starlit&#26159;&#19968;&#20010;&#26032;&#30340;&#21487;&#25193;&#23637;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#27491;&#24335;&#30340;&#23433;&#20840;&#23450;&#20041;&#21644;&#35777;&#26126;&#12289;&#20551;&#23450;&#20923;&#32467;&#36134;&#25143;&#12289;&#35268;&#27169;&#25193;&#22823;&#12289;&#36523;&#20221;&#23545;&#40784;&#38454;&#27573;&#21644;&#38590;&#20197;&#25269;&#25239;&#23458;&#25143;&#31471;&#36864;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#25968;&#25454;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#26412;&#22320;&#25968;&#25454;&#30340;&#21508;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#65292;&#36991;&#20813;&#30452;&#25509;&#25968;&#25454;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35782;&#21035;&#27450;&#35784;&#37329;&#34701;&#20132;&#26131;&#30340;&#26368;&#26032;FL&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20197;&#19979;&#19968;&#20123;&#38480;&#21046;&#65306;&#32570;&#20047;&#27491;&#24335;&#30340;&#23433;&#20840;&#23450;&#20041;&#21644;&#35777;&#26126;&#65292;&#20551;&#23450;&#37329;&#34701;&#26426;&#26500;&#20107;&#20808;&#20923;&#32467;&#21487;&#30097;&#23458;&#25143;&#30340;&#36134;&#25143;&#65288;&#38480;&#21046;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#37319;&#29992;&#65289;&#65292;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#28041;&#21450;$O(n^2)$&#30340;&#35745;&#31639;&#26114;&#36149;&#30340;&#27169;&#22359;&#25351;&#25968;&#36816;&#31639;&#65288;&#20854;&#20013;$n$&#26159;&#37329;&#34701;&#26426;&#26500;&#30340;&#24635;&#25968;&#65289;&#25110;&#32773;&#39640;&#25928;&#29575;&#20302;&#30340;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#65292;&#20551;&#35774;&#21508;&#26041;&#24050;&#32463;&#23436;&#25104;&#20102;&#36523;&#20221;&#23545;&#40784;&#38454;&#27573;&#65292;&#22240;&#27492;&#23558;&#20854;&#25490;&#38500;&#22312;&#23454;&#26045;&#12289;&#24615;&#33021;&#35780;&#20272;&#21644;&#23433;&#20840;&#20998;&#26512;&#20043;&#22806;&#65292;&#24182;&#19988;&#38590;&#20197;&#25269;&#25239;&#23458;&#25143;&#31471;&#30340;&#36864;&#20986;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25193;&#23637;&#38544;&#31169;&#20445;&#25252;FL&#26426;&#21046;&#8212;&#8212;Starlit&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a data-minimization approach enabling collaborative model training across diverse clients with local data, avoiding direct data exchange. However, state-of-the-art FL solutions to identify fraudulent financial transactions exhibit a subset of the following limitations. They (1) lack a formal security definition and proof, (2) assume prior freezing of suspicious customers' accounts by financial institutions (limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$ computationally expensive modular exponentiation (where $n$ is the total number of financial institutions) or highly inefficient fully homomorphic encryption, (4) assume the parties have already completed the identity alignment phase, hence excluding it from the implementation, performance evaluation, and security analysis, and (5) struggle to resist clients' dropouts. This work introduces Starlit, a novel scalable privacy-preserving FL mechanism that overcomes these limitations
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#20132;&#36890;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#21644;&#25513;&#30721;&#30340;&#22686;&#24378;&#22312;&#20132;&#36890;&#20998;&#31867;&#20013;&#26356;&#36866;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#35299;&#37322;&#22686;&#24378;&#25928;&#26524;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.10754</link><description>&lt;p&gt;
&#20132;&#36890;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Traffic Classification. (arXiv:2401.10754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#20132;&#36890;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#21644;&#25513;&#30721;&#30340;&#22686;&#24378;&#22312;&#20132;&#36890;&#20998;&#31867;&#20013;&#26356;&#36866;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#35299;&#37322;&#22686;&#24378;&#25928;&#26524;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#26679;&#26412;&#26469;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#36890;&#20998;&#31867;&#65288;TC&#65289;&#20219;&#21153;&#20013;&#65292;DA&#24456;&#38590;&#33719;&#24471;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;18&#31181;&#22686;&#24378;&#20989;&#25968;&#24212;&#29992;&#20110;3&#20010;TC&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21253;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#21508;&#31181;&#35757;&#32451;&#26465;&#20214;&#65292;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;i&#65289;DA&#21487;&#20197;&#33719;&#24471;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#22909;&#22788;&#65292;&#65288;ii&#65289;&#20316;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#21644;&#25513;&#30721;&#30340;&#22686;&#24378;&#22312;TC&#20013;&#26356;&#21512;&#36866;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#31616;&#21333;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20026;&#20160;&#20040;&#22686;&#24378;&#20250;&#20135;&#29983;&#31215;&#26497;&#25110;&#28040;&#26497;&#24433;&#21709;&#30340;&#19968;&#20123;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) -- enriching training data by adding synthetic samples -- is a technique widely adopted in Computer Vision (CV) and Natural Language Processing (NLP) tasks to improve models performance. Yet, DA has struggled to gain traction in networking contexts, particularly in Traffic Classification (TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation functions applied to 3 TC datasets using packet time series as input representation and considering a variety of training conditions. Our results show that (i) DA can reap benefits previously unexplored with (ii) augmentations acting on time series sequence order and masking being a better suit for TC and (iii) simple latent space analysis can provide hints about why augmentations have positive or negative effects.
&lt;/p&gt;</description></item><item><title>BoolGebra&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#23646;&#24615;&#22270;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#25913;&#36827;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#36923;&#36753;&#32508;&#21512;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#24182;&#39640;&#25928;&#23450;&#20301;&#20248;&#21270;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#36328;&#35774;&#35745;&#25512;&#35770;&#30340;&#36890;&#29992;&#24615;&#21644;&#35268;&#27169;&#21270;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10753</link><description>&lt;p&gt;
BoolGebra: &#29992;&#20110;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#23646;&#24615;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation. (arXiv:2401.10753v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10753
&lt;/p&gt;
&lt;p&gt;
BoolGebra&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#23646;&#24615;&#22270;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#25913;&#36827;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#36923;&#36753;&#32508;&#21512;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#24182;&#39640;&#25928;&#23450;&#20301;&#20248;&#21270;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#36328;&#35774;&#35745;&#25512;&#35770;&#30340;&#36890;&#29992;&#24615;&#21644;&#35268;&#27169;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#26159;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#20805;&#20998;&#21033;&#29992;&#20248;&#21270;&#26426;&#20250;&#65292;&#24182;&#19988;&#24448;&#24448;&#21463;&#21040;&#25628;&#32034;&#31354;&#38388;&#29190;&#28856;&#21644;&#21487;&#25193;&#23637;&#24615;&#25928;&#29575;&#26377;&#38480;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BoolGebra&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#24067;&#23572;&#20195;&#25968;&#25805;&#20316;&#30340;&#23646;&#24615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#22522;&#30784;&#36923;&#36753;&#32508;&#21512;&#12290;BoolGebra&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24182;&#23558;&#32467;&#26500;&#21644;&#21151;&#33021;&#20449;&#24687;&#30340;&#21021;&#22987;&#29305;&#24449;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#12290;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20316;&#30452;&#25509;&#20248;&#21270;&#32467;&#26524;&#39044;&#27979;&#30340;&#39044;&#27979;&#22120;&#65292;&#26174;&#33879;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#39640;&#25928;&#23450;&#20301;&#20248;&#21270;&#31354;&#38388;&#12290;&#23454;&#39564;&#28041;&#21450;&#22521;&#35757;BoolGebra&#27169;&#22411;&#20851;&#20110;&#35774;&#35745;&#29305;&#23450;&#21644;&#36328;&#35774;&#35745;&#25512;&#35770;&#65292;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;BoolGebra&#23637;&#31034;&#20102;&#36328;&#35774;&#35745;&#25512;&#35770;&#30340;&#36890;&#29992;&#24615;&#21644;&#35268;&#27169;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean algebraic manipulation is at the core of logic synthesis in Electronic Design Automation (EDA) design flow. Existing methods struggle to fully exploit optimization opportunities, and often suffer from an explosive search space and limited scalability efficiency. This work presents BoolGebra, a novel attributed graph-learning approach for Boolean algebraic manipulation that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph Neural Networks (GNNs) and takes initial feature embeddings from both structural and functional information as inputs. A fully connected neural network is employed as the predictor for direct optimization result predictions, significantly reducing the search space and efficiently locating the optimization space. The experiments involve training the BoolGebra model w.r.t design-specific and cross-design inferences using the trained model, where BoolGebra demonstrates generalizability for cross-design inference and its potential to scale 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;ReliCD&#65292;&#33021;&#22815;&#37327;&#21270;&#35786;&#26029;&#21453;&#39304;&#30340;&#20449;&#24515;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35748;&#30693;&#35786;&#26029;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10749</link><description>&lt;p&gt;
&#21487;&#38752;&#24615;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;ReliCD:&#20855;&#26377;&#20449;&#24515;&#35748;&#30693;&#30340;&#21487;&#38752;&#24615;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ReliCD: A Reliable Cognitive Diagnosis Framework with Confidence Awareness. (arXiv:2401.10749v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;ReliCD&#65292;&#33021;&#22815;&#37327;&#21270;&#35786;&#26029;&#21453;&#39304;&#30340;&#20449;&#24515;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35748;&#30693;&#35786;&#26029;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#35748;&#30693;&#35786;&#26029;&#24314;&#27169;&#21560;&#24341;&#20102;&#35745;&#31639;&#25945;&#32946;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#21487;&#20197;&#37327;&#21270;&#23398;&#29983;&#30340;&#23398;&#20064;&#29366;&#20917;&#21644;&#30693;&#35782;&#25484;&#25569;&#27700;&#24179;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#23398;&#20064;&#23398;&#29983;&#21644;&#32451;&#20064;&#30340;&#28145;&#23618;&#34920;&#31034;&#22823;&#22823;&#25552;&#39640;&#20102;&#20256;&#32479;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#22312;&#39044;&#27979;&#23398;&#29983;&#25484;&#25569;&#27700;&#24179;&#26102;&#23384;&#22312;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#29616;&#23454;&#20013;&#23398;&#29983;-&#32451;&#20064;&#20132;&#20114;&#25968;&#25454;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;&#24341;&#36215;&#30340;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#35748;&#30693;&#21453;&#39304;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#38752;&#24615;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;ReliCD&#65292;&#21487;&#20197;&#37327;&#21270;&#35786;&#26029;&#21453;&#39304;&#30340;&#20449;&#24515;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#35748;&#30693;&#35786;&#26029;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the past few decades, cognitive diagnostics modeling has attracted increasing attention in computational education communities, which is capable of quantifying the learning status and knowledge mastery levels of students. Indeed, the recent advances in neural networks have greatly enhanced the performance of traditional cognitive diagnosis models through learning the deep representations of students and exercises. Nevertheless, existing approaches often suffer from the issue of overconfidence in predicting students' mastery levels, which is primarily caused by the unavoidable noise and sparsity in realistic student-exercise interaction data, severely hindering the educational application of diagnostic feedback. To address this, in this paper, we propose a novel Reliable Cognitive Diagnosis(ReliCD) framework, which can quantify the confidence of the diagnosis feedback and is flexible for different cognitive diagnostic functions. Specifically, we first propose a Bayesian method to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#29305;&#21270;&#12290;&#22312;&#19968;&#20010;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19978;&#25104;&#21151;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.10748</link><description>&lt;p&gt;
&#38024;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast gradient-free activation maximization for neurons in spiking neural networks. (arXiv:2401.10748v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#29305;&#21270;&#12290;&#22312;&#19968;&#20010;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19978;&#25104;&#21151;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#65292;&#37117;&#26159;&#30001;&#31070;&#32463;&#20803;&#26500;&#25104;&#30340;&#22797;&#26434;&#31995;&#32479;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#37117;&#26377;&#33258;&#24049;&#30340;&#19987;&#19994;&#21270;&#12290;&#25581;&#31034;&#36825;&#20123;&#19987;&#19994;&#21270;&#23545;&#20110;&#29702;&#35299;NNs&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#19968;&#20010;&#29983;&#29289;&#31995;&#32479;&#65292;&#20854;&#23545;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#19981;&#26159;&#24050;&#30693;&#30340;&#65288;&#26356;&#19981;&#29992;&#35828;&#26159;&#21487;&#24494;&#20998;&#30340;&#20989;&#25968;&#65289;&#65292;&#21807;&#19968;&#30340;&#26041;&#24335;&#26159;&#24314;&#31435;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#23558;&#20854;&#26292;&#38706;&#20110;&#21050;&#28608;&#20043;&#20013;&#65292;&#20854;&#24615;&#36136;&#21487;&#20197;&#36845;&#20195;&#22320;&#21464;&#21270;&#65292;&#20197;&#23547;&#27714;&#26368;&#22823;&#21709;&#24212;&#30340;&#26041;&#21521;&#12290;&#20026;&#20102;&#22312;&#19968;&#20010;&#29983;&#29289;&#32593;&#32476;&#19978;&#27979;&#35797;&#36825;&#26679;&#30340;&#24490;&#29615;&#65292;&#39318;&#20808;&#38656;&#35201;&#23398;&#20250;&#24555;&#36895;&#21644;&#39640;&#25928;&#22320;&#36816;&#34892;&#23427;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#26368;&#26377;&#25928;&#30340;&#21050;&#28608;&#65288;&#26368;&#22823;&#21270;&#26576;&#20123;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#26377;&#25928;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#25104;&#21151;&#22320;&#22312;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65292;&#27169;&#25311;&#29983;&#29289;&#22823;&#33041;NNs&#34892;&#20026;&#30340;&#27169;&#22411;&#65289;&#19978;&#27979;&#35797;&#20102;&#23427;&#12290;&#25105;&#20204;&#29992;&#20110;&#28608;&#27963;&#26368;&#22823;&#21270;&#65288;AM&#65289;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#22522;&#20110;&#24555;&#26799;&#24230;&#26041;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs), both living and artificial, work due to being complex systems of neurons, each having its own specialization. Revealing these specializations is important for understanding NNs inner working mechanisms. The only way to do this for a living system, the neural response of which to a stimulus is not a known (let alone differentiable) function is to build a feedback loop of exposing it to stimuli, the properties of which can be iteratively varied aiming in the direction of maximal response. To test such a loop on a living network, one should first learn how to run it quickly and efficiently, reaching most effective stimuli (ones that maximize certain neurons activation) in least possible number of iterations. We present a framework with an effective design of such a loop, successfully testing it on an artificial spiking neural network (SNN, a model that mimics the behaviour of NNs in living brains). Our optimization method used for activation maximization (AM) was ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10745</link><description>&lt;p&gt;
&#23545;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#30340;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models. (arXiv:2401.10745v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;ChatGPT&#12289;LaMDA&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#25216;&#26415;&#34892;&#19994;&#21644;&#20854;&#20182;&#34892;&#19994;&#23545;LLMs&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#26377;&#25152;&#22686;&#21152;&#12290;&#34429;&#28982;LLMs&#30340;&#27700;&#24179;&#23578;&#26410;&#36229;&#36807;&#20154;&#31867;&#26234;&#33021;&#65292;&#20294;&#24635;&#26377;&#19968;&#22825;&#20250;&#36798;&#21040;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;LLMs&#21487;&#20197;&#31216;&#20026;&#39640;&#32423;LLMs&#12290;&#30446;&#21069;&#65292;&#30001;&#20110;&#23578;&#26410;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#20351;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#19968;&#26086;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#26080;&#27861;&#20805;&#20998;&#20934;&#22791;&#22909;&#20197;&#36947;&#24503;&#21644;&#26368;&#20339;&#26041;&#24335;&#22788;&#29702;&#20854;&#20135;&#29983;&#30340;&#21518;&#26524;&#65292;&#36825;&#23558;&#23548;&#33268;&#19981;&#21487;&#39044;&#26399;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10726</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23454;&#29992;&#24037;&#20855;&#22686;&#24378;&#32858;&#21512;&#22120;&#33021;&#21147;&#65306;&#21033;&#29992;&#32858;&#21512;&#19982;&#20998;&#25955;&#30340;&#28789;&#27963;&#24615;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response. (arXiv:2401.10726v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#24102;&#26469;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32858;&#21512;&#22120;&#21644;&#24314;&#31569;&#29289;&#23621;&#20303;&#32773;&#36890;&#36807;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#26041;&#26696;&#28608;&#27963;&#28789;&#27963;&#24615;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#30528;&#37325;&#20110;&#23454;&#29616;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#21644;&#32858;&#31867;&#25216;&#26415;&#35782;&#21035;&#24314;&#31569;&#29289;&#23621;&#27665;&#30340;&#27963;&#21160;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;DR&#20107;&#20214;&#26399;&#38388;&#20379;&#28909;&#36890;&#39118;&#31354;&#35843;&#65288;HVAC&#65289;&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#31934;&#30830;&#30340;&#35774;&#22791;&#32423;&#20998;&#26512;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20026;&#32858;&#21512;&#22120;&#22312;&#25972;&#20010;&#24314;&#31569;&#29289;&#28040;&#36153;&#20165;&#26377;&#19968;&#20010;&#26234;&#33021;&#30005;&#34920;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#28789;&#27963;&#24615;&#26381;&#21153;&#25552;&#20379;&#20102;&#19968;&#26465;&#38750;&#20405;&#20837;&#24615;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES). Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns. Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis. The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#38646;&#26085;&#25915;&#20987;&#12290;&#23427;&#35299;&#20915;&#20102;&#27773;&#36710;&#25511;&#21046;&#21306;&#22495;&#32593;&#32476;&#20013;&#38646;&#26085;&#25915;&#20987;&#30340;&#26816;&#27979;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#38543;&#30528;&#20027;&#21160;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#21450;&#26102;&#26816;&#27979;&#24182;&#38450;&#27490;&#20256;&#25773;&#25104;&#20026;&#20102;&#19968;&#39033;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10724</link><description>&lt;p&gt;
&#38754;&#21521;FPGA&#30340;&#27773;&#36710;&#25511;&#21046;&#21306;&#22495;&#32593;&#32476;&#30340;&#23454;&#26102;&#38646;&#26085;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Real-Time Zero-Day Intrusion Detection System for Automotive Controller Area Network on FPGAs. (arXiv:2401.10724v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#38646;&#26085;&#25915;&#20987;&#12290;&#23427;&#35299;&#20915;&#20102;&#27773;&#36710;&#25511;&#21046;&#21306;&#22495;&#32593;&#32476;&#20013;&#38646;&#26085;&#25915;&#20987;&#30340;&#26816;&#27979;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#38543;&#30528;&#20027;&#21160;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#21450;&#26102;&#26816;&#27979;&#24182;&#38450;&#27490;&#20256;&#25773;&#25104;&#20026;&#20102;&#19968;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27773;&#36710;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#36830;&#25509;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#36710;&#36742;&#33258;&#21160;&#21270;&#31243;&#24230;&#65292;&#26292;&#38706;&#20102;&#20197;&#21069;&#29420;&#31435;&#23384;&#22312;&#30340;&#27773;&#36710;&#32593;&#32476;&#65292;&#22914;&#25511;&#21046;&#21306;&#22495;&#32593;&#32476;&#65288;CAN&#65289;&#30340;&#28431;&#27934;&#12290;CAN&#30340;&#29305;&#24615;&#65292;&#22914;&#22522;&#20110;&#24191;&#25773;&#30340;&#30005;&#23376;&#25511;&#21046;&#21333;&#20803;&#65288;ECU&#65289;&#20043;&#38388;&#30340;&#36890;&#20449;&#38477;&#20302;&#20102;&#37096;&#32626;&#25104;&#26412;&#65292;&#29616;&#22312;&#34987;&#21033;&#29992;&#26469;&#36827;&#34892;&#20027;&#21160;&#27880;&#20837;&#25915;&#20987;&#65292;&#22914;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#65292;&#27169;&#31946;&#27979;&#35797;&#21644;&#27450;&#39575;&#25915;&#20987;&#12290;&#30740;&#31350;&#25991;&#29486;&#25552;&#20986;&#20102;&#22810;&#20010;&#20197;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26469;&#26816;&#27979;&#36825;&#31181;&#24694;&#24847;&#27963;&#21160;; &#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#24456;&#22823;&#31243;&#24230;&#19978;&#23616;&#38480;&#20110;&#35782;&#21035;&#20808;&#21069;&#24050;&#30693;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#38543;&#30528;&#20027;&#21160;&#27880;&#20837;&#25915;&#20987;&#22797;&#26434;&#24615;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#23454;&#26102;&#26816;&#27979;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#38646;&#26085;&#65288;&#26032;&#39062;&#65289;&#25915;&#20987;&#65288;&#20197;&#38450;&#27490;&#20256;&#25773;&#65289;&#25104;&#20026;&#19968;&#20010;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#38646;&#26085;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasing automation in vehicles enabled by increased connectivity to the outside world has exposed vulnerabilities in previously siloed automotive networks like controller area networks (CAN). Attributes of CAN such as broadcast-based communication among electronic control units (ECUs) that lowered deployment costs are now being exploited to carry out active injection attacks like denial of service (DoS), fuzzing, and spoofing attacks. Research literature has proposed multiple supervised machine learning models deployed as Intrusion detection systems (IDSs) to detect such malicious activity; however, these are largely limited to identifying previously known attack vectors. With the ever-increasing complexity of active injection attacks, detecting zero-day (novel) attacks in these networks in real-time (to prevent propagation) becomes a problem of particular interest. This paper presents an unsupervised-learning-based convolutional autoencoder architecture for detecting zero-day attac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21453;&#24212;&#36335;&#24452;&#21021;&#22987;&#29468;&#27979;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#26426;&#21453;&#24212;&#30340;&#22797;&#26434;&#21453;&#24212;&#36335;&#24452;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10721</link><description>&lt;p&gt;
&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#26368;&#32456;&#29366;&#24577;&#30340;&#21453;&#24212;&#36335;&#24452;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Model for Constructing Reaction Path from Initial to Final States. (arXiv:2401.10721v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21453;&#24212;&#36335;&#24452;&#21021;&#22987;&#29468;&#27979;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#26426;&#21453;&#24212;&#30340;&#22797;&#26434;&#21453;&#24212;&#36335;&#24452;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32472;&#21046;&#21453;&#24212;&#36335;&#24452;&#21450;&#20854;&#30456;&#24212;&#30340;&#27963;&#21270;&#33021;&#22418;&#26159;&#20998;&#23376;&#27169;&#25311;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#38750;&#32447;&#24615;&#65292;&#29978;&#33267;&#29983;&#25104;&#36825;&#20123;&#36335;&#24452;&#30340;&#21021;&#22987;&#29468;&#27979;&#37117;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36825;&#20123;&#21453;&#24212;&#36335;&#24452;&#30340;&#21021;&#22987;&#29468;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36755;&#20837;&#21021;&#22987;&#29366;&#24577;&#30340;&#22352;&#26631;&#65292;&#38543;&#21518;&#36880;&#27493;&#23545;&#20854;&#32467;&#26500;&#36827;&#34892;&#25913;&#21464;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#26368;&#32456;&#29983;&#25104;&#20102;&#23545;&#21453;&#24212;&#36335;&#24452;&#30340;&#36817;&#20284;&#34920;&#31034;&#20197;&#21450;&#26368;&#32456;&#29366;&#24577;&#30340;&#22352;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#26377;&#26426;&#21453;&#24212;&#25152;&#31034;&#30340;&#22797;&#26434;&#21453;&#24212;&#36335;&#24452;&#12290;&#35757;&#32451;&#26159;&#22312;Transition1x&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26377;&#26426;&#21453;&#24212;&#36335;&#24452;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#21453;&#24212;&#19982;&#30456;&#24212;&#30340;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#24403;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping out reaction pathways and their corresponding activation barriers is a significant aspect of molecular simulation. Given their inherent complexity and nonlinearity, even generating a initial guess of these paths remains a challenging problem. Presented in this paper is an innovative approach that utilizes neural networks to generate initial guess for these reaction pathways. The proposed method is initiated by inputting the coordinates of the initial state, followed by progressive alterations to its structure. This iterative process culminates in the generation of the approximate representation of the reaction path and the coordinates of the final state. The application of this method extends to complex reaction pathways illustrated by organic reactions. Training was executed on the Transition1x dataset, an organic reaction pathway dataset. The results revealed generation of reactions that bore substantial similarities with the corresponding test data. The method's flexibility 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10710</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification with neural networks with quadratic decision functions. (arXiv:2401.10710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20351;&#29992;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#20316;&#20026;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#21697;&#26469;&#23454;&#29616;&#19968;&#31181;&#20248;&#21183;&#65292;&#24403;&#38656;&#35201;&#35782;&#21035;&#30340;&#23545;&#35937;&#20855;&#26377;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#65288;&#22914;&#22278;&#24418;&#12289;&#26925;&#22278;&#24418;&#31561;&#65289;&#26102;&#65292;&#36825;&#31181;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#31867;&#38382;&#39064;&#19978;&#20351;&#29992;&#36825;&#31181;&#20551;&#35774;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#20102;&#35813;&#31639;&#27861;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;Tensorflow&#21644;Keras&#36719;&#20214;&#20013;&#21487;&#20197;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network with quadratic decision functions have been introduced as alternatives to standard neural networks with affine linear one. They are advantageous when the objects to be identified are of compact basic geometries like circles, ellipsis etc. In this paper we investigate the use of such ansatz functions for classification. In particular we test and compare the algorithm on the MNIST dataset for classification of handwritten digits and for classification of subspecies. We also show, that the implementation can be based on the neural network structure in the software Tensorflow and Keras, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#34892;&#24615;&#23548;&#21521;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24179;&#34913;&#23433;&#20840;&#32422;&#26463;&#28385;&#36275;&#12289;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#35268;&#33539;&#21270;&#12290;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#23558;&#30828;&#23433;&#20840;&#32422;&#26463;&#36716;&#21270;&#20026;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#26368;&#22823;&#21487;&#34892;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.10700</link><description>&lt;p&gt;
&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#21487;&#34892;&#24615;&#23548;&#21521;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#34892;&#24615;&#23548;&#21521;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24179;&#34913;&#23433;&#20840;&#32422;&#26463;&#28385;&#36275;&#12289;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#35268;&#33539;&#21270;&#12290;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#23558;&#30828;&#23433;&#20840;&#32422;&#26463;&#36716;&#21270;&#20026;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#26368;&#22823;&#21487;&#34892;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#36991;&#20813;&#39118;&#38505;&#30340;&#22312;&#32447;&#20132;&#20114;&#20197;&#23454;&#29616;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#24378;&#21046;&#36719;&#32422;&#26463;&#65292;&#21363;&#23558;&#26399;&#26395;&#30340;&#23433;&#20840;&#36829;&#35268;&#32422;&#26463;&#22312;&#39044;&#23450;&#30340;&#38408;&#20540;&#20197;&#19979;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#32467;&#26524;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#21478;&#19968;&#31181;&#36873;&#25321;&#26159;&#24378;&#21046;&#38646;&#36829;&#35268;&#30340;&#30828;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#22312;&#23433;&#20840;&#32422;&#26463;&#28385;&#36275;&#12289;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#25152;&#26045;&#21152;&#30340;&#34892;&#20026;&#27491;&#21017;&#21270;&#20043;&#38388;&#21462;&#24471;&#36866;&#24403;&#30340;&#24179;&#34913;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#23433;&#20840;&#25511;&#21046;&#29702;&#35770;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#30828;&#23433;&#20840;&#32422;&#26463;&#21487;&#20197;&#31561;&#25928;&#22320;&#36716;&#21270;&#20026;&#35782;&#21035;&#32473;&#23450;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#26368;&#22823;&#21487;&#34892;&#21306;&#22495;&#12290;&#36825;&#26080;&#32541;&#22320;&#23558;&#21407;&#22987;&#30340;&#19977;&#37325;&#38382;&#39064;&#36716;&#21270;&#20026;&#20381;&#36182;&#20110;&#21487;&#34892;&#24615;&#30340;&#30446;&#26631;&#65292;&#21363;&#22312;&#21487;&#34892;&#24615;&#33539;&#22260;&#20869;&#26368;&#22823;&#21270;&#22870;&#21169;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;FPGAs&#19978;&#37096;&#32626;&#30340;&#36731;&#37327;&#32423;&#22810;&#25915;&#20987;CAN&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#24182;&#32531;&#35299;CAN&#20013;&#30340;&#22810;&#20010;&#25915;&#20987;&#21521;&#37327;&#12290;&#35813;&#31995;&#32479;&#28040;&#32791;&#36739;&#20302;&#30340;&#21151;&#32791;&#65292;&#36991;&#20813;&#20102;&#23545;GPU&#31561;&#19987;&#29992;&#35745;&#31639;&#21333;&#20803;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2401.10689</link><description>&lt;p&gt;
&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#25915;&#20987;CAN&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#22312;&#28151;&#21512;FPGAs&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid FPGAs. (arXiv:2401.10689v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;FPGAs&#19978;&#37096;&#32626;&#30340;&#36731;&#37327;&#32423;&#22810;&#25915;&#20987;CAN&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#24182;&#32531;&#35299;CAN&#20013;&#30340;&#22810;&#20010;&#25915;&#20987;&#21521;&#37327;&#12290;&#35813;&#31995;&#32479;&#28040;&#32791;&#36739;&#20302;&#30340;&#21151;&#32791;&#65292;&#36991;&#20813;&#20102;&#23545;GPU&#31561;&#19987;&#29992;&#35745;&#31639;&#21333;&#20803;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#20013;&#30340;&#26085;&#30410;&#36830;&#25509;&#24615;&#20351;&#24471;&#26032;&#30340;&#33021;&#21147;&#25104;&#20026;&#21487;&#33021;&#65292;&#20363;&#22914;&#36830;&#25509;&#30340;&#33258;&#20027;&#39550;&#39542;&#21644;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#65292;&#20197;&#25552;&#39640;&#19979;&#19968;&#20195;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#36710;&#20869;&#21151;&#33021;&#30340;&#22686;&#21152;&#35753;&#20351;&#29992;&#36951;&#30041;&#36710;&#20869;&#32593;&#32476;&#65288;&#22914;CAN&#65289;&#30340;&#20851;&#38190;&#33021;&#21147;&#21463;&#21040;&#20102;&#23041;&#32961;&#65292;&#22240;&#20026;CAN&#27809;&#26377;&#22266;&#26377;&#30340;&#23433;&#20840;&#24615;&#25110;&#35748;&#35777;&#26426;&#21046;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;CAN&#20013;&#30340;&#22810;&#20010;&#25915;&#20987;&#21521;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37096;&#32626;&#38656;&#35201;&#20687;GPU&#36825;&#26679;&#30340;&#19987;&#29992;&#35745;&#31639;&#21333;&#20803;&#25191;&#34892;&#32447;&#36895;&#26816;&#27979;&#65292;&#28040;&#32791;&#26356;&#39640;&#30340;&#21151;&#32791;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22810;&#25915;&#20987;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#36187;&#28789;&#24605;&#30340;&#28145;&#24230;&#23398;&#20064;&#22788;&#29702;&#21333;&#20803;IP&#22312;Zynq Ultrascale+&#65288;XCZU3EG&#65289;FPGA&#19978;&#37096;&#32626;&#65292;&#24182;&#20351;&#29992;p&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rising connectivity in vehicles is enabling new capabilities like connected autonomous driving and advanced driver assistance systems (ADAS) for improving the safety and reliability of next-generation vehicles. This increased access to in-vehicle functions compromises critical capabilities that use legacy invehicle networks like Controller Area Network (CAN), which has no inherent security or authentication mechanism. Intrusion detection and mitigation approaches, particularly using machine learning models, have shown promising results in detecting multiple attack vectors in CAN through their ability to generalise to new vectors. However, most deployments require dedicated computing units like GPUs to perform line-rate detection, consuming much higher power. In this paper, we present a lightweight multi-attack quantised machine learning model that is deployed using Xilinx's Deep Learning Processing Unit IP on a Zynq Ultrascale+ (XCZU3EG) FPGA, which is trained and validated using the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#25506;&#35752;&#20102;L1&#27491;&#21017;&#21270;&#21644;&#34920;&#31034;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#23427;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.10686</link><description>&lt;p&gt;
&#25805;&#32437;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Manipulating Sparse Double Descent. (arXiv:2401.10686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#25506;&#35752;&#20102;L1&#27491;&#21017;&#21270;&#21644;&#34920;&#31034;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#23427;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#37325;&#28857;&#20851;&#27880;L1&#27491;&#21017;&#21270;&#21644;&#34920;&#31034;&#32500;&#24230;&#30340;&#20316;&#29992;&#12290;&#23427;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#31232;&#30095;&#21452;&#19979;&#38477;&#30340;&#26367;&#20195;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#26356;&#22810;&#26679;&#21270;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#24314;&#35758;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the double descent phenomenon in two-layer neural networks, focusing on the role of L1 regularization and representation dimensions. It explores an alternative double descent phenomenon, named sparse double descent. The study emphasizes the complex relationship between model complexity, sparsity, and generalization, and suggests further research into more diverse models and datasets. The findings contribute to a deeper understanding of neural network training and optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10685</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#20551;&#20266;&#36317;&#20462;&#27491;&#23454;&#29616;&#31471;&#21040;&#31471;GPS&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End GPS Localization with Neural Pseudorange Correction. (arXiv:2401.10685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#36317;&#35823;&#24046;&#26159;GPS&#23450;&#20301;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20266;&#36317;&#35823;&#24046;&#22238;&#24402;&#21644;&#28040;&#38500;&#12290;&#19982;&#20043;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#20351;&#29992;GPS&#25509;&#25910;&#26426;&#29366;&#24577;&#30340;&#30495;&#23454;&#20540;&#35745;&#31639;&#26368;&#32456;&#20219;&#21153;&#25439;&#22833;&#65292;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#20266;&#36317;&#20462;&#27491;&#30340;&#31070;&#32463;&#32593;&#32476;PrNet&#12290;&#25439;&#22833;&#23545;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26799;&#24230;&#36890;&#36807;&#21487;&#24494;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#22120;&#21453;&#21521;&#20256;&#25773;&#21040;PrNet&#12290;&#36890;&#36807;&#20351;&#29992;Android&#25163;&#26426;&#25910;&#38598;&#30340;GPS&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;E2E-PrNet&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudorange errors are the root cause of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet. The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23884;&#20837;&#24335;&#27773;&#36710;CAN&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#19987;&#29992;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#23454;&#29616;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#30340;&#36879;&#26126;&#38598;&#25104;&#65292;&#24179;&#22343;&#26816;&#27979;&#20934;&#30830;&#29575;&#36229;&#36807;99%&#12290;</title><link>http://arxiv.org/abs/2401.10674</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23884;&#20837;&#24335;&#27773;&#36710;CAN&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Embedded Intrusion Detection System for Automotive CAN. (arXiv:2401.10674v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23884;&#20837;&#24335;&#27773;&#36710;CAN&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#19987;&#29992;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#23454;&#29616;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#30340;&#36879;&#26126;&#38598;&#25104;&#65292;&#24179;&#22343;&#26816;&#27979;&#20934;&#30830;&#29575;&#36229;&#36807;99%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36733;&#30005;&#23376;&#35774;&#22791;&#30340;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#20351;&#24471;&#20687;&#33258;&#21160;&#39550;&#39542;&#21644;&#20027;&#21160;&#23433;&#20840;&#36825;&#26679;&#30340;&#26032;&#33021;&#21147;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#22686;&#21152;&#30340;&#33258;&#21160;&#21270;&#20063;&#22686;&#21152;&#20102;&#23433;&#20840;&#23041;&#32961;&#30340;&#39118;&#38505;&#65292;&#36825;&#22312;CAN&#31561;&#20256;&#32479;&#32593;&#32476;&#20013;&#32570;&#20047;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#20005;&#37325;&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#35266;&#23519;&#12289;&#31713;&#25913;&#21644;&#20462;&#25913;&#22312;&#36825;&#20123;&#24191;&#25773;&#32593;&#32476;&#19978;&#20849;&#20139;&#30340;&#20449;&#24687;&#12290;&#21508;&#31181;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#21644;&#35299;&#20915;&#36825;&#20123;&#23041;&#32961;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#38656;&#35201;&#36890;&#36807;&#39640;&#31471;&#22788;&#29702;&#22120;&#25110;GPU&#25552;&#20379;&#39640;&#22788;&#29702;&#33021;&#21147;&#20197;&#36924;&#36817;&#32447;&#36895;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;FPGA-based ECU&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#19987;&#29992;&#30340;&#29616;&#25104;&#30828;&#20214;&#21152;&#36895;&#22120;&#26469;&#36879;&#26126;&#22320;&#38598;&#25104;IDS&#21151;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25915;&#20987;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#24179;&#22343;&#31934;&#24230;&#36229;&#36807;99%&#12290;
&lt;/p&gt;
&lt;p&gt;
Rising complexity of in-vehicle electronics is enabling new capabilities like autonomous driving and active safety. However, rising automation also increases risk of security threats which is compounded by lack of in-built security measures in legacy networks like CAN, allowing attackers to observe, tamper and modify information shared over such broadcast networks. Various intrusion detection approaches have been proposed to detect and tackle such threats, with machine learning models proving highly effective. However, deploying machine learning models will require high processing power through high-end processors or GPUs to perform them close to line rate. In this paper, we propose a hybrid FPGA-based ECU approach that can transparently integrate IDS functionality through a dedicated off-the-shelf hardware accelerator that implements a deep-CNN intrusion detection model. Our results show that the proposed approach provides an average accuracy of over 99% across multiple attack dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23637;&#31034;&#20102;&#22522;&#22240;&#32452;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#36716;&#25442;&#30340;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#29983;&#25104;&#26377;&#27602;&#25968;&#25454;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#65292;&#20934;&#30830;&#29575;&#38477;&#20302;&#65292;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2401.10657</link><description>&lt;p&gt;
FIMBA:&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#23545;&#25239;&#25915;&#20987;&#35780;&#20272;&#22522;&#22240;&#32452;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks. (arXiv:2401.10657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23637;&#31034;&#20102;&#22522;&#22240;&#32452;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#36716;&#25442;&#30340;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#29983;&#25104;&#26377;&#27602;&#25968;&#25454;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#65292;&#20934;&#30830;&#29575;&#38477;&#20302;&#65292;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#29983;&#29289;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#22522;&#22240;&#32452;&#27979;&#24207;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31639;&#27861;&#21644;&#24037;&#20855;&#36827;&#20837;&#30740;&#31350;&#21644;&#29983;&#20135;&#38454;&#27573;&#65292;&#24433;&#21709;&#30528;&#33647;&#29289;&#21457;&#29616;&#21644;&#20020;&#24202;&#32467;&#26524;&#31561;&#37325;&#35201;&#20915;&#31574;&#27969;&#31243;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20844;&#35748;&#30340;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#65292;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#27169;&#25311;&#30495;&#23454;&#25968;&#25454;&#24182;&#28151;&#28102;&#27169;&#22411;&#20915;&#31574;&#30340;&#36755;&#20837;&#36716;&#25442;&#25915;&#20987;&#65292;&#20174;&#32780;&#22823;&#24133;&#24230;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#26377;&#27602;&#25968;&#25454;&#26469;&#22686;&#24378;&#25105;&#20204;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26126;&#30830;&#34920;&#26126;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#65292;&#20934;&#30830;&#29575;&#38477;&#20302;&#65292;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the steady rise of the use of AI in bio-technical applications and the widespread adoption of genomics sequencing, an increasing amount of AI-based algorithms and tools is entering the research and production stage affecting critical decision-making streams like drug discovery and clinical outcomes. This paper demonstrates the vulnerability of AI models often utilized downstream tasks on recognized public genomics datasets. We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance. Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model. Our empirical findings unequivocally demonstrate a decline in model performance, underscored by diminished accuracy and an upswing in false positives and false negatives. Furthermore, we analyze the resulting adversarial samples vi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;Transformer&#26694;&#26550;&#21644;"&#20851;&#27880;&#34701;&#21512;"&#23618;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#26512;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10653</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65306;&#20851;&#27880;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech Detection. (arXiv:2401.10653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;Transformer&#26694;&#26550;&#21644;"&#20851;&#27880;&#34701;&#21512;"&#23618;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#26512;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#37327;&#30340;&#28608;&#22686;&#21644;&#25351;&#25968;&#22686;&#38271;&#65292;&#23457;&#26597;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#20167;&#24680;&#20869;&#23481;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#30740;&#31350;&#32773;&#20204;&#33258;&#36807;&#21435;&#21313;&#24180;&#20197;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#21306;&#20998;&#23459;&#20256;&#20167;&#24680;&#21644;&#38750;&#23459;&#20256;&#20167;&#24680;&#30340;&#20869;&#23481;&#12290;&#20256;&#32479;&#19978;&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23545;&#25991;&#26412;&#20869;&#23481;&#30340;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20063;&#24320;&#22987;&#28041;&#21450;&#23545;&#22522;&#20110;&#38899;&#39057;&#30340;&#20869;&#23481;&#30340;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#20165;&#20165;&#20381;&#36182;&#38899;&#39057;&#25110;&#22522;&#20110;&#25991;&#26412;&#30340;&#20869;&#23481;&#21487;&#33021;&#26159;&#26080;&#25928;&#30340;&#65292;&#22240;&#20026;&#36817;&#26399;&#30340;&#28608;&#22686;&#34920;&#26126;&#20010;&#20307;&#22312;&#35328;&#36766;&#21644;&#20889;&#20316;&#20013;&#32463;&#24120;&#20351;&#29992;&#35773;&#21050;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#26469;&#21028;&#26029;&#19968;&#27573;&#35328;&#36766;&#26159;&#21542;&#23459;&#20256;&#20102;&#20167;&#24680;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#23618;&#31216;&#20026;"&#20851;&#27880;&#34701;&#21512;"&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent surge and exponential growth of social media usage, scrutinizing social media content for the presence of any hateful content is of utmost importance. Researchers have been diligently working since the past decade on distinguishing between content that promotes hatred and content that does not. Traditionally, the main focus has been on analyzing textual content. However, recent research attempts have also commenced into the identification of audio-based content. Nevertheless, studies have shown that relying solely on audio or text-based content may be ineffective, as recent upsurge indicates that individuals often employ sarcasm in their speech and writing. To overcome these challenges, we present an approach to identify whether a speech promotes hate or not utilizing both audio and textual representations. Our methodology is based on the Transformer framework that incorporates both audio and text sampling, accompanied by our very own layer called "Attentive Fusion". Th
&lt;/p&gt;</description></item><item><title>AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2401.10652</link><description>&lt;p&gt;
AutoChunk: &#33258;&#21160;&#28608;&#27963;&#22359;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference. (arXiv:2401.10652v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10652
&lt;/p&gt;
&lt;p&gt;
AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20869;&#23384;&#30340;&#22823;&#37327;&#38656;&#27714;&#65292;&#21253;&#25324;&#21442;&#25968;&#20869;&#23384;&#21644;&#28608;&#27963;&#20869;&#23384;&#65292;&#24050;&#32463;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#21442;&#25968;&#20869;&#23384;&#65292;&#23545;&#28608;&#27963;&#20869;&#23384;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#28608;&#27963;&#20869;&#23384;&#39044;&#35745;&#20250;&#32463;&#21382;&#26174;&#33879;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoChunk&#65292;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#30340;&#20248;&#21270;&#29983;&#25104;&#22359;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#22359;&#25628;&#32034;&#36890;&#36807;&#25506;&#32034;&#25152;&#26377;&#21487;&#33021;&#30340;&#22359;&#20505;&#36873;&#39033;&#65292;&#22359;&#36873;&#25321;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#22359;&#36827;&#34892;&#12290;&#36816;&#34892;&#26102;&#65292;AutoChunk&#37319;&#29992;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#24212;&#29992;&#22359;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Area2Vec&#30340;&#26032;&#39062;&#21306;&#22495;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#29992;&#25143;&#30340;&#20301;&#32622;&#25968;&#25454;&#23545;&#21306;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#21306;&#22495;&#20351;&#29992;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.10648</link><description>&lt;p&gt;
&#20351;&#29992;&#20572;&#30041;&#20449;&#24687;&#36827;&#34892;&#22823;&#35268;&#27169;&#29992;&#25143;&#21306;&#22495;&#24314;&#27169;&#21450;COVID-19&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Area Modeling using Stay Information for Large-Scale Users and Analysis for Influence of COVID-19. (arXiv:2401.10648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Area2Vec&#30340;&#26032;&#39062;&#21306;&#22495;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#29992;&#25143;&#30340;&#20301;&#32622;&#25968;&#25454;&#23545;&#21306;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#21306;&#22495;&#20351;&#29992;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20154;&#20204;&#22312;&#22478;&#24066;&#20013;&#22914;&#20309;&#20351;&#29992;&#21306;&#22495;&#26159;&#19968;&#31181;&#23453;&#36149;&#30340;&#20449;&#24687;&#65292;&#22312;&#20174;&#24066;&#22330;&#33829;&#38144;&#21040;&#22478;&#24066;&#35268;&#21010;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#21306;&#22495;&#30340;&#20351;&#29992;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#21463;&#21040;&#23395;&#33410;&#21464;&#21270;&#21644;&#22823;&#27969;&#34892;&#31561;&#21508;&#31181;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;&#22312;&#26234;&#33021;&#25163;&#26426;&#26222;&#21450;&#20043;&#21069;&#65292;&#36825;&#20123;&#25968;&#25454;&#26159;&#36890;&#36807;&#38382;&#21367;&#35843;&#26597;&#26469;&#25910;&#38598;&#30340;&#12290;&#28982;&#32780;&#65292;&#20174;&#26102;&#38388;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#21487;&#25345;&#32493;&#12290;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#20851;&#20110;&#21306;&#22495;&#24314;&#27169;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#20351;&#29992;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25110;&#21306;&#22495;&#38388;&#31227;&#21160;&#25968;&#25454;&#31561;&#26576;&#31181;&#20449;&#24687;&#26469;&#25551;&#36848;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;POI&#25968;&#25454;&#19982;&#31354;&#38388;&#38745;&#24577;&#32465;&#23450;&#65292;&#21306;&#22495;&#38388;&#31227;&#21160;&#25968;&#25454;&#24573;&#30053;&#20102;&#21306;&#22495;&#20869;&#20154;&#20204;&#30340;&#34892;&#20026;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25429;&#25417;&#21306;&#22495;&#20351;&#29992;&#21464;&#21270;&#26041;&#38754;&#24182;&#19981;&#36275;&#22815;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#22495;&#24314;&#27169;&#26041;&#27861;Area2Vec&#65292;&#21463;Word2Vec&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20154;&#20204;&#30340;&#20301;&#32622;&#25968;&#25454;&#23545;&#21306;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how people use area in a city can be a valuable information in a wide range of fields, from marketing to urban planning. Area usage is subject to change over time due to various events including seasonal shifts and pandemics. Before the spread of smartphones, this data had been collected through questionnaire survey. However, this is not a sustainable approach in terms of time to results and cost. There are many existing studies on area modeling, which characterize an area with some kind of information, using Point of Interest (POI) or inter-area movement data. However, since POI is data that is statically tied to space, and inter-area movement data ignores the behavior of people within an area, existing methods are not sufficient in terms of capturing area usage changes. In this paper, we propose a novel area modeling method named Area2Vec, inspired by Word2Vec, which models areas based on people's location data. This method is based on the discovery that it is possible 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#31867;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#30828;&#20214;&#32593;&#32476;&#22330;&#26223;&#65292;&#20811;&#26381;&#20102;&#35774;&#22791;&#32570;&#20047;&#20934;&#30830;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#22788;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10646</link><description>&lt;p&gt;
&#20197;&#39640;&#25928;&#25968;&#25454;&#26631;&#27880;&#36171;&#33021;&#30828;&#20214;&#32593;&#32476;&#65306;&#19968;&#31181;&#32858;&#31867;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Empowering HWNs with Efficient Data Labeling: A Clustered Federated Semi-Supervised Learning Approach. (arXiv:2401.10646v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10646
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#31867;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#30828;&#20214;&#32593;&#32476;&#22330;&#26223;&#65292;&#20811;&#26381;&#20102;&#35774;&#22791;&#32570;&#20047;&#20934;&#30830;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;CFL&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20811;&#26381;&#32479;&#35745;&#25361;&#25112;&#30340;&#31574;&#30053;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22810;&#29992;&#25143;&#20043;&#38388;&#23384;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non IID&#65289;&#25968;&#25454;&#26102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CFL&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#35774;&#22791;&#21487;&#20197;&#35775;&#38382;&#20934;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#36825;&#20010;&#20551;&#35774;&#22312;&#20998;&#23618;&#26080;&#32447;&#32593;&#32476;&#65288;HWNs&#65289;&#20013;&#23588;&#20026;&#26377;&#38382;&#39064;&#65292;&#20854;&#20013;&#36793;&#32536;&#32593;&#32476;&#21253;&#21547;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#22788;&#29702;&#26102;&#38388;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#22312;&#20004;&#23618;&#27169;&#22411;&#32858;&#21512;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#32858;&#31867;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;CFSL&#65289;&#65292;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;HWN&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#31181;&#34920;&#29616;&#26368;&#20339;&#30340;&#19987;&#38376;&#27169;&#22411;&#31639;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#34987;&#20998;&#37197;&#19968;&#20010;&#19987;&#38376;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#26041;&#38754;&#38750;&#24120;&#29087;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustered Federated Multitask Learning (CFL) has gained considerable attention as an effective strategy for overcoming statistical challenges, particularly when dealing with non independent and identically distributed (non IID) data across multiple users. However, much of the existing research on CFL operates under the unrealistic premise that devices have access to accurate ground truth labels. This assumption becomes especially problematic in hierarchical wireless networks (HWNs), where edge networks contain a large amount of unlabeled data, resulting in slower convergence rates and increased processing times, particularly when dealing with two layers of model aggregation. To address these issues, we introduce a novel framework, Clustered Federated Semi-Supervised Learning (CFSL), designed for more realistic HWN scenarios. Our approach leverages a best-performing specialized model algorithm, wherein each device is assigned a specialized model that is highly adept at generating accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26631;&#20934;&#20197;&#21450;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;</title><link>http://arxiv.org/abs/2401.10643</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#32508;&#21512;&#35843;&#26597;&#65306;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges. (arXiv:2401.10643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26631;&#20934;&#20197;&#21450;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#26088;&#22312;&#23558;&#26469;&#33258;&#20998;&#24067;&#22312;&#19981;&#21516;&#20132;&#36890;&#29615;&#22659;&#30340;&#25668;&#20687;&#22836;&#32593;&#32476;&#20013;&#30340;&#36710;&#36742;&#22270;&#20687;&#36827;&#34892;&#20851;&#32852;&#12290;&#22312;&#36710;&#36742;&#20013;&#24515;&#25216;&#26415;&#33539;&#30068;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22312;&#37096;&#32626;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#21644;&#25512;&#36827;&#26234;&#24935;&#22478;&#24066;&#20513;&#35758;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#36710;&#36742; ReID &#25216;&#26415;&#30340;&#28436;&#36827;&#12290;&#22240;&#27492;&#65292;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#24050;&#21464;&#24471;&#36843;&#20999;&#19988;&#19981;&#21487;&#36991;&#20813;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#36710;&#36742; ReID &#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20123;&#20998;&#31867;&#20013;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#24182;&#21246;&#21202;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Vehicle re-identification (ReID) endeavors to associate vehicle images collected from a distributed network of cameras spanning diverse traffic environments. This task assumes paramount importance within the spectrum of vehicle-centric technologies, playing a pivotal role in deploying Intelligent Transportation Systems (ITS) and advancing smart city initiatives. Rapid advancements in deep learning have significantly propelled the evolution of vehicle ReID technologies in recent years. Consequently, undertaking a comprehensive survey of methodologies centered on deep learning for vehicle re-identification has become imperative and inescapable. This paper extensively explores deep learning techniques applied to vehicle ReID. It outlines the categorization of these methods, encompassing supervised and unsupervised approaches, delves into existing research within these categories, introduces datasets and evaluation criteria, and delineates forthcoming challenges and potential research dire
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20266;&#20581;&#24247;&#37325;&#24314;&#23454;&#29616;&#23545;&#26356;&#24191;&#27867;&#30149;&#21464;&#33539;&#22260;&#30340;&#26816;&#27979;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#25104;&#20687;&#27169;&#24577;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10637</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#23454;&#29616;&#26222;&#36866;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Universal Unsupervised Anomaly Detection in Medical Imaging. (arXiv:2401.10637v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20266;&#20581;&#24247;&#37325;&#24314;&#23454;&#29616;&#23545;&#26356;&#24191;&#27867;&#30149;&#21464;&#33539;&#22260;&#30340;&#26816;&#27979;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#25104;&#20687;&#27169;&#24577;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#21508;&#31181;&#30149;&#29702;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#25429;&#25417;&#24191;&#27867;&#24322;&#24120;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#36890;&#24120;&#38480;&#21046;&#20854;&#22312;&#33041;&#37096;&#25195;&#25551;&#20013;&#29305;&#23450;&#30149;&#21464;&#31867;&#22411;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#21453;&#21521;&#33258;&#32534;&#30721;&#22120;&#65288;RA&#65289;&#8221;&#65292;&#26088;&#22312;&#21019;&#24314;&#36924;&#30495;&#30340;&#20266;&#20581;&#24247;&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26356;&#24191;&#27867;&#30149;&#21464;&#33539;&#22260;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21253;&#25324;&#33041;&#37096;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12289;&#20799;&#31461;&#33109;&#20851;&#33410;X&#23556;&#32447;&#21644;&#33016;&#37096;X&#23556;&#32447;&#22312;&#20869;&#30340;&#21508;&#31181;&#25104;&#20687;&#27169;&#24577;&#20013;&#65292;&#24182;&#19988;&#35777;&#26126;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#26410;&#30693;&#30149;&#29702;&#26469;&#25552;&#39640;&#21307;&#23398;&#25104;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;: \url{htt
&lt;/p&gt;
&lt;p&gt;
The increasing complexity of medical imaging data underscores the need for advanced anomaly detection methods to automatically identify diverse pathologies. Current methods face challenges in capturing the broad spectrum of anomalies, often limiting their use to specific lesion types in brain scans. To address this challenge, we introduce a novel unsupervised approach, termed \textit{Reversed Auto-Encoders (RA)}, designed to create realistic pseudo-healthy reconstructions that enable the detection of a wider range of pathologies. We evaluate the proposed method across various imaging modalities, including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray, and chest X-ray, and demonstrate superior performance in detecting anomalies compared to existing state-of-the-art methods. Our unsupervised anomaly detection approach may enhance diagnostic accuracy in medical imaging by identifying a broader range of unknown pathologies. Our code is publicly available at: \url{htt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#24050;&#30693;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#24773;&#20917;&#19979;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;PDAG&#65289;&#24314;&#27169;&#21644;&#34913;&#37327;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10632</link><description>&lt;p&gt;
&#37096;&#20998;&#24050;&#30693;&#22240;&#26524;&#22270;&#19978;&#30340;&#24178;&#39044;&#20844;&#24179;&#24615;: &#19968;&#31181;&#21463;&#38480;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach. (arXiv:2401.10632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#24050;&#30693;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#24773;&#20917;&#19979;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;PDAG&#65289;&#24314;&#27169;&#21644;&#34913;&#37327;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26088;&#22312;&#38450;&#27490;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65289;&#23545;&#20010;&#20307;&#25110;&#23376;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#12290;&#36817;&#24180;&#26469;&#65292;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#29992;&#20110;&#36890;&#36807;&#22240;&#26524;&#25928;&#24212;&#26469;&#34913;&#37327;&#19981;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#20551;&#35774;&#24050;&#30693;&#30495;&#23454;&#30340;&#22240;&#26524;&#22270;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#24050;&#30693;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#24773;&#20917;&#19979;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;PDAG&#65289;&#26469;&#24314;&#27169;&#20844;&#24179;&#39044;&#27979;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#20013;&#23398;&#20064;&#30340;&#19968;&#31867;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;PDAG&#29992;&#20110;&#34913;&#37327;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#26469;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race. In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects. However, current methods assume that the true causal graph is given, which is often not true in real-world applications. To address this limitation, this paper proposes a framework for achieving causal fairness based on the notion of interventions when the true causal graph is partially known. The proposed approach involves modeling fair prediction using a Partially Directed Acyclic Graph (PDAG), specifically, a class of causal DAGs that can be learned from observational data combined with domain knowledge. The PDAG is used to measure causal fairness, and a constrained optimization problem is formulated to balance between fairness and accuracy. Results on both simulated and real-world datasets demonstrate th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24418;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21253;&#25324;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#38750;&#32447;&#24615;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#20984;&#32452;&#21512;&#35299;&#30721;&#22120;&#21644;&#19968;&#20010;&#24179;&#28369;&#32858;&#31867;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#30830;&#20445;&#25152;&#26377;&#37325;&#24314;&#29366;&#24577;&#37117;&#20301;&#20110;&#19968;&#20010;&#22810;&#38754;&#20307;&#20869;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#22810;&#38754;&#20307;&#36136;&#37327;&#30340;&#25351;&#26631;&#12290;&#19982;&#20256;&#32479;&#30340;POD&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#35777;&#37325;&#24314;&#35823;&#24046;&#21487;&#25509;&#21463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26368;&#23569;&#30340;&#20984;&#22352;&#26631;&#36827;&#34892;&#22810;&#38754;&#20307;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10620</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#28369;&#32858;&#31867;&#30340;&#22810;&#35282;&#24418;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#27969;&#20307;&#31616;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Polytopic Autoencoders with Smooth Clustering for Reduced-order Modelling of Flows. (arXiv:2401.10620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10620
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24418;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21253;&#25324;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#38750;&#32447;&#24615;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#20984;&#32452;&#21512;&#35299;&#30721;&#22120;&#21644;&#19968;&#20010;&#24179;&#28369;&#32858;&#31867;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#30830;&#20445;&#25152;&#26377;&#37325;&#24314;&#29366;&#24577;&#37117;&#20301;&#20110;&#19968;&#20010;&#22810;&#38754;&#20307;&#20869;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#22810;&#38754;&#20307;&#36136;&#37327;&#30340;&#25351;&#26631;&#12290;&#19982;&#20256;&#32479;&#30340;POD&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#35777;&#37325;&#24314;&#35823;&#24046;&#21487;&#25509;&#21463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26368;&#23569;&#30340;&#20984;&#22352;&#26631;&#36827;&#34892;&#22810;&#38754;&#20307;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#27493;&#65292;&#24212;&#29992;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#31616;&#21270;&#27169;&#22411;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#22312;&#25968;&#37327;&#21644;&#31181;&#31867;&#26041;&#38754;&#26174;&#33879;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24418;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21253;&#25324;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#38750;&#32447;&#24615;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#20984;&#32452;&#21512;&#35299;&#30721;&#22120;&#21644;&#19968;&#20010;&#24179;&#28369;&#32858;&#31867;&#32593;&#32476;&#12290;&#36890;&#36807;&#22810;&#39033;&#35777;&#26126;&#25903;&#25345;&#30340;&#27169;&#22411;&#26550;&#26500;&#30830;&#20445;&#25152;&#26377;&#37325;&#24314;&#29366;&#24577;&#37117;&#20301;&#20110;&#22810;&#38754;&#20307;&#20869;&#65292;&#24182;&#38468;&#24102;&#19968;&#20010;&#25351;&#31034;&#26500;&#24314;&#30340;&#22810;&#38754;&#20307;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#22810;&#38754;&#20307;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20026;&#22810;&#38754;&#20307;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;&#31995;&#32479;&#25552;&#20379;&#20102;&#26368;&#23567;&#25968;&#37327;&#30340;&#20984;&#22352;&#26631;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;POD&#30456;&#27604;&#21487;&#25509;&#21463;&#30340;&#37325;&#24314;&#35823;&#24046;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28041;&#21450;&#19981;&#21487;&#21387;&#32553;Navier-Stokes&#26041;&#31243;&#30340;&#20004;&#31181;&#27969;&#21160;&#22330;&#26223;&#30340;&#20223;&#30495;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#20445;&#35777;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of neural networks, there has been a notable increase, both in terms of quantity and variety, in research publications concerning the application of autoencoders to reduced-order models. We propose a polytopic autoencoder architecture that includes a lightweight nonlinear encoder, a convex combination decoder, and a smooth clustering network. Supported by several proofs, the model architecture ensures that all reconstructed states lie within a polytope, accompanied by a metric indicating the quality of the constructed polytopes, referred to as polytope error. Additionally, it offers a minimal number of convex coordinates for polytopic linear-parameter varying systems while achieving acceptable reconstruction errors compared to proper orthogonal decomposition (POD). To validate our proposed model, we conduct simulations involving two flow scenarios with the incompressible Navier-Stokes equation. Numerical results demonstrate the guaranteed properties of the model, l
&lt;/p&gt;</description></item><item><title>ZnTrack&#26159;&#19968;&#20010;Python&#39537;&#21160;&#30340;&#25968;&#25454;&#29256;&#26412;&#25511;&#21046;&#24037;&#20855;&#65292;&#36890;&#36807;&#31616;&#21270;&#22823;&#22411;&#25968;&#25454;&#38598;&#20026;Python&#33050;&#26412;&#30340;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20195;&#30721;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#21442;&#25968;&#36319;&#36394;&#12289;&#24037;&#20316;&#27969;&#35774;&#35745;&#20197;&#21450;&#25968;&#25454;&#23384;&#20648;&#21644;&#20849;&#20139;&#30340;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.10603</link><description>&lt;p&gt;
ZnTrack -- &#25968;&#25454;&#21363;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
ZnTrack -- Data as Code. (arXiv:2401.10603v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10603
&lt;/p&gt;
&lt;p&gt;
ZnTrack&#26159;&#19968;&#20010;Python&#39537;&#21160;&#30340;&#25968;&#25454;&#29256;&#26412;&#25511;&#21046;&#24037;&#20855;&#65292;&#36890;&#36807;&#31616;&#21270;&#22823;&#22411;&#25968;&#25454;&#38598;&#20026;Python&#33050;&#26412;&#30340;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20195;&#30721;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#21442;&#25968;&#36319;&#36394;&#12289;&#24037;&#20316;&#27969;&#35774;&#35745;&#20197;&#21450;&#25968;&#25454;&#23384;&#20648;&#21644;&#20849;&#20139;&#30340;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#65292;&#32780;&#19988;&#27809;&#26377;&#36857;&#35937;&#34920;&#26126;&#36825;&#31181;&#36235;&#21183;&#20250;&#22312;&#30701;&#26399;&#20869;&#20943;&#24930;&#12290;&#26426;&#22120;&#23398;&#20064;&#12289;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#21644;&#22686;&#21152;&#30340;&#34892;&#19994;&#20851;&#27880;&#23548;&#33268;&#20102;&#23545;&#25968;&#25454;&#31649;&#29702;&#12289;&#27169;&#25311;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#35745;&#31639;&#26426;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#30340;&#25237;&#36164;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#25968;&#25454;&#30340;&#25193;&#22823;&#20063;&#24102;&#26469;&#20102;&#25968;&#25454;&#23384;&#20648;&#12289;&#20849;&#20139;&#21644;&#36861;&#36394;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ZnTrack&#30340;Python&#39537;&#21160;&#25968;&#25454;&#29256;&#26412;&#25511;&#21046;&#24037;&#20855;&#12290;ZnTrack&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;&#29256;&#26412;&#25511;&#21046;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#36319;&#36394;&#23454;&#39564;&#20013;&#30340;&#21442;&#25968;&#12289;&#35774;&#35745;&#24037;&#20316;&#27969;&#12289;&#20197;&#21450;&#23384;&#20648;&#21644;&#20849;&#20139;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31616;&#21270;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;Python&#33050;&#26412;&#65292;&#20135;&#29983;&#20102;&#8220;&#25968;&#25454;&#21363;&#20195;&#30721;&#8221;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#26412;&#25991;&#25152;&#20171;&#32461;&#30340;&#24037;&#20316;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#20063;&#26159;&#35745;&#31639;&#26102;&#20195;&#32487;&#32493;&#28436;&#21464;&#30340;&#19968;&#20010;&#26080;&#30097;&#37325;&#35201;&#30340;&#27010;&#24565;&#12290;ZnTrack&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25968;&#25454;&#24403;&#20316;&#20195;&#30721;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has seen tremendous breakthroughs in computation and there is no indication that this will slow any time soon. Machine learning, large-scale computing resources, and increased industry focus have resulted in rising investments in computer-driven solutions for data management, simulations, and model generation. However, with this growth in computation has come an even larger expansion of data and with it, complexity in data storage, sharing, and tracking. In this work, we introduce ZnTrack, a Python-driven data versioning tool. ZnTrack builds upon established version control systems to provide a user-friendly and easy-to-use interface for tracking parameters in experiments, designing workflows, and storing and sharing data. From this ability to reduce large datasets to a simple Python script emerges the concept of Data as Code, a core component of the work presented here and an undoubtedly important concept as the age of computation continues to evolve. ZnTrack offers an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BA-SGCL&#30340;&#40065;&#26834;SGNN&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#23545;&#27604;&#23398;&#20064;&#21407;&#21017;&#21644;&#24179;&#34913;&#22686;&#24378;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#24102;&#31526;&#21495;&#22270;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#24179;&#34913;&#30456;&#20851;&#20449;&#24687;&#19981;&#21487;&#36870;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10590</link><description>&lt;p&gt;
Adversarially Robust Signed Graph Contrastive Learning from Balance Augmentation&#65288;&#20174;&#24179;&#34913;&#22686;&#24378;&#20013;&#25552;&#21462;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#24102;&#31526;&#21495;&#22270;&#23545;&#27604;&#23398;&#20064;&#65289;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Signed Graph Contrastive Learning from Balance Augmentation. (arXiv:2401.10590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BA-SGCL&#30340;&#40065;&#26834;SGNN&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#23545;&#27604;&#23398;&#20064;&#21407;&#21017;&#21644;&#24179;&#34913;&#22686;&#24378;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#24102;&#31526;&#21495;&#22270;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#24179;&#34913;&#30456;&#20851;&#20449;&#24687;&#19981;&#21487;&#36870;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#31526;&#21495;&#22270;&#30001;&#36793;&#21644;&#31526;&#21495;&#32452;&#25104;&#65292;&#21487;&#20197;&#20998;&#20026;&#32467;&#26500;&#20449;&#24687;&#21644;&#24179;&#34913;&#30456;&#20851;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#24102;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#24179;&#34913;&#30456;&#20851;&#20449;&#24687;&#26469;&#29983;&#25104;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#24179;&#34913;&#30456;&#20851;&#20449;&#24687;&#20135;&#29983;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#31867;&#20284;&#20110;&#32467;&#26500;&#23398;&#20064;&#21487;&#20197;&#24674;&#22797;&#26080;&#31526;&#21495;&#22270;&#65292;&#36890;&#36807;&#25913;&#36827;&#34987;&#27745;&#26579;&#22270;&#30340;&#24179;&#34913;&#24230;&#65292;&#21487;&#20197;&#23558;&#24179;&#34913;&#23398;&#20064;&#24212;&#29992;&#20110;&#24102;&#31526;&#21495;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#30528;&#8220;&#24179;&#34913;&#30456;&#20851;&#20449;&#24687;&#30340;&#19981;&#21487;&#36870;&#24615;&#8221;&#25361;&#25112;-&#23613;&#31649;&#24179;&#34913;&#24230;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#24674;&#22797;&#30340;&#36793;&#21487;&#33021;&#19981;&#26159;&#26368;&#21021;&#21463;&#21040;&#25915;&#20987;&#24433;&#21709;&#30340;&#36793;&#65292;&#23548;&#33268;&#38450;&#24481;&#25928;&#26524;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;SGNN&#26694;&#26550;&#65292;&#31216;&#20026;&#24179;&#34913;&#22686;&#24378;&#24102;&#31526;&#21495;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;BA-SGCL&#65289;&#65292;&#23427;&#23558;&#22270;&#23545;&#27604;&#23398;&#20064;&#21407;&#21017;&#19982;&#24179;&#34913;&#22686;&#24378;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signed graphs consist of edges and signs, which can be separated into structural information and balance-related information, respectively. Existing signed graph neural networks (SGNNs) typically rely on balance-related information to generate embeddings. Nevertheless, the emergence of recent adversarial attacks has had a detrimental impact on the balance-related information. Similar to how structure learning can restore unsigned graphs, balance learning can be applied to signed graphs by improving the balance degree of the poisoned graph. However, this approach encounters the challenge "Irreversibility of Balance-related Information" - while the balance degree improves, the restored edges may not be the ones originally affected by attacks, resulting in poor defense effectiveness. To address this challenge, we propose a robust SGNN framework called Balance Augmented-Signed Graph Contrastive Learning (BA-SGCL), which combines Graph Contrastive Learning principles with balance augmentati
&lt;/p&gt;</description></item><item><title>PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.10586</link><description>&lt;p&gt;
PuriDefense&#65306;&#29992;&#20110;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38543;&#26426;&#23616;&#37096;&#38544;&#24335;&#23545;&#25239;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10586
&lt;/p&gt;
&lt;p&gt;
PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;&#23545;&#25239;&#35757;&#32451;&#12289;&#26799;&#24230;&#25513;&#30422;&#21644;&#36755;&#20837;&#36716;&#25442;&#65292;&#35201;&#20040;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#35201;&#20040;&#25439;&#23475;&#38750;&#23545;&#25239;&#36755;&#20837;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;PuriDefense&#65292;&#22312;&#20302;&#25512;&#29702;&#25104;&#26412;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#30340;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#23616;&#37096;&#38544;&#24335;&#20989;&#25968;&#24182;&#37325;&#24314;&#33258;&#28982;&#22270;&#20687;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#32435;&#20837;&#20928;&#21270;&#36807;&#31243;&#26469;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23545;CIFAR-10&#21644;ImageNet&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20928;&#21270;&#22120;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#20272;&#35745;&#25972;&#20307;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10566</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Modal Density Estimation. (arXiv:2401.10566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#20272;&#35745;&#25972;&#20307;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#30340;&#38656;&#27714;&#12290;&#34429;&#28982;&#26377;&#20960;&#20010;&#25351;&#26631;&#21487;&#20197;&#34920;&#24449;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#36127;&#23545;&#25968;&#20284;&#28982;&#12289;Jensen-Shannon&#25955;&#24230;&#65289;&#65292;&#20294;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#20316;&#29992;&#20110;&#27010;&#29575;&#23494;&#24230;&#19978;&#12290;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#32431;&#31929;&#22522;&#20110;&#26679;&#26412;&#30340;&#39044;&#27979;&#27169;&#22411;&#38656;&#35201;&#20272;&#35745;&#24213;&#23618;&#23494;&#24230;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#22914;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20272;&#35745;&#38382;&#39064;&#20013;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;ROME&#65288;RObust Multi-modal density Estimator&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#20272;&#35745;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#30340;&#25361;&#25112;&#12290;ROME&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#28982;&#21518;&#32467;&#21512;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#24471;&#21040;&#24635;&#20307;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of multi-modal, probabilistic prediction models has lead to a need for comprehensive evaluation metrics. While several metrics can characterize the accuracy of machine-learned models (e.g., negative log-likelihood, Jensen-Shannon divergence), these metrics typically operate on probability densities. Applying them to purely sample-based prediction models thus requires that the underlying density function is estimated. However, common methods such as kernel density estimation (KDE) have been demonstrated to lack robustness, while more complex methods have not been evaluated in multi-modal estimation problems. In this paper, we present ROME (RObust Multi-modal density Estimator), a non-parametric approach for density estimation which addresses the challenge of estimating multi-modal, non-normal, and highly correlated distributions. ROME utilizes clustering to segment a multi-modal set of samples into multiple uni-modal ones and then combines simple KDE estimates obtained for i
&lt;/p&gt;</description></item><item><title>OrchMoE&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#21644;&#33258;&#21160;&#20219;&#21153;&#35782;&#21035;&#65292;&#25552;&#21319;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10559</link><description>&lt;p&gt;
OrchMoE&#65306;&#20855;&#26377;&#20219;&#21153;-&#25216;&#33021;&#21327;&#21516;&#25928;&#24212;&#30340;&#39640;&#25928;&#22810;&#36866;&#37197;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10559
&lt;/p&gt;
&lt;p&gt;
OrchMoE&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#21644;&#33258;&#21160;&#20219;&#21153;&#35782;&#21035;&#65292;&#25552;&#21319;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#21019;&#26032;&#30340;&#22810;&#36866;&#37197;&#22120;&#26041;&#27861;OrchMoE&#25512;&#36827;&#20102;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#65288;PEFT&#65289;&#39046;&#22495;&#65292;&#21033;&#29992;&#27169;&#22359;&#21270;&#25216;&#33021;&#26550;&#26500;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#20256;&#36882;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20219;&#21153;&#35782;&#21035;&#36755;&#20837;&#30340;&#20808;&#21069;&#27169;&#22411;&#19981;&#21516;&#65292;OrchMoE&#33258;&#21160;&#35782;&#21035;&#20219;&#21153;&#31867;&#21035;&#65292;&#31616;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#25972;&#21512;&#26426;&#21046;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#20219;&#21153;&#20998;&#31867;&#27169;&#22359;&#21644;&#20219;&#21153;-&#25216;&#33021;&#20998;&#37197;&#27169;&#22359;&#65292;&#20849;&#21516;&#25512;&#26029;&#20219;&#21153;&#29305;&#23450;&#30340;&#20998;&#31867;&#24182;&#35843;&#25972;&#25216;&#33021;&#20998;&#37197;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;&#8220;&#36229;&#33258;&#28982;&#25351;&#20196;&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1,600&#20010;&#22810;&#26679;&#30340;&#25351;&#20196;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;OrchMoE&#22312;&#24615;&#33021;&#21644;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#21487;&#27604;&#30340;&#22810;&#36866;&#37197;&#22120;&#22522;&#32447;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#21442;&#25968;&#38480;&#21046;&#19979;&#36816;&#34892;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;OrchMoE&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel multi-adapter method, OrchMoE, which capitalizes on modular skill architecture for enhanced forward transfer in neural networks. Unlike prior models that depend on explicit task identification inputs, OrchMoE automatically discerns task categories, streamlining the learning process. This is achieved through an integrated mechanism comprising an Automatic Task Classification module and a Task-Skill Allocation module, which collectively deduce task-specific classifications and tailor skill allocation matrices. Our extensive evaluations on the 'Super Natural Instructions' dataset, featuring 1,600 diverse instructional tasks, indicate that OrchMoE substantially outperforms comparable multi-adapter baselines in terms of both performance and sample utilization efficiency, all while operating within the same parameter constraints. These findings suggest that OrchMoE offers a significant leap forward in multi-task le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;UNIFIER&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#23384;&#22312;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#33719;&#21462;&#30340;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#24341;&#23548;&#34917;&#20840;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#29305;&#24449;&#36873;&#25321;&#24615;&#33021;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#23616;&#37096;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2401.10549</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#32479;&#19968;&#34917;&#20840;&#21644;&#29305;&#24449;&#36873;&#25321;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unified View Imputation and Feature Selection Learning for Incomplete Multi-view Data. (arXiv:2401.10549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10549
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;UNIFIER&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#23384;&#22312;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#33719;&#21462;&#30340;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#24341;&#23548;&#34917;&#20840;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#29305;&#24449;&#36873;&#25321;&#24615;&#33021;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#23616;&#37096;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65288;MUFS&#65289;&#26159;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38477;&#32500;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#23384;&#22312;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#26576;&#20123;&#35270;&#22270;&#20013;&#20250;&#26377;&#19968;&#20123;&#26679;&#26412;&#20002;&#22833;&#12290;&#36825;&#20123;&#26041;&#27861;&#24212;&#39318;&#20808;&#23545;&#32570;&#22833;&#30340;&#25968;&#25454;&#24212;&#29992;&#39044;&#23450;&#20540;&#36827;&#34892;&#22635;&#34917;&#65292;&#28982;&#21518;&#22312;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;&#23558;&#34917;&#20840;&#21644;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20998;&#31163;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#28508;&#22312;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#21363;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#33719;&#24471;&#30340;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#24341;&#23548;&#34917;&#20840;&#36807;&#31243;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25913;&#21892;&#29305;&#24449;&#36873;&#25321;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#20851;&#27880;&#20110;&#21033;&#29992;&#26679;&#26412;&#30340;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#22266;&#26377;&#23616;&#37096;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MUFS&#26041;&#27861;&#65292;&#31216;&#20026;&#32479;&#19968;&#35270;&#22270;&#34917;&#20840;&#21644;&#29305;&#24449;&#36873;&#25321;&#23398;&#20064;&#65288;UNIFIER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although multi-view unsupervised feature selection (MUFS) is an effective technology for reducing dimensionality in machine learning, existing methods cannot directly deal with incomplete multi-view data where some samples are missing in certain views. These methods should first apply predetermined values to impute missing data, then perform feature selection on the complete dataset. Separating imputation and feature selection processes fails to capitalize on the potential synergy where local structural information gleaned from feature selection could guide the imputation, thereby improving the feature selection performance in turn. Additionally, previous methods only focus on leveraging samples' local structure information, while ignoring the intrinsic locality of the feature space. To tackle these problems, a novel MUFS method, called UNified view Imputation and Feature selectIon lEaRning (UNIFIER), is proposed. UNIFIER explores the local structure of multi-view data by adaptively le
&lt;/p&gt;</description></item><item><title>PhoGAD&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#20248;&#21270;&#26469;&#28548;&#28165;&#34892;&#20026;&#36793;&#30028;&#65292;&#35774;&#35745;&#30456;&#37051;&#36793;&#26435;&#37325;&#20197;&#20943;&#36731;&#23616;&#37096;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#35299;&#20915;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10547</link><description>&lt;p&gt;
PhoGAD: &#22522;&#20110;&#22270;&#24418;&#30340;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#19982;&#25345;&#20037;&#21516;&#35843;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology Optimization. (arXiv:2401.10547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10547
&lt;/p&gt;
&lt;p&gt;
PhoGAD&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#20248;&#21270;&#26469;&#28548;&#28165;&#34892;&#20026;&#36793;&#30028;&#65292;&#35774;&#35745;&#30456;&#37051;&#36793;&#26435;&#37325;&#20197;&#20943;&#36731;&#23616;&#37096;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#35299;&#20915;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#26377;&#23475;&#30340;&#22312;&#32447;&#34892;&#20026;&#65292;&#20174;&#32593;&#32476;&#25915;&#20987;&#21040;&#21311;&#21517;&#27969;&#37327;&#21644;&#22403;&#22334;&#37038;&#20214;&#65292;&#20005;&#37325;&#24178;&#25200;&#20102;&#32593;&#32476;&#30340;&#27491;&#24120;&#36816;&#34892;&#12290;&#30001;&#20110;&#32593;&#32476;&#34892;&#20026;&#30340;&#21457;&#20214;&#20154;-&#25910;&#20214;&#20154;&#26412;&#36136;&#65292;&#24120;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#30340;&#26694;&#26550;&#26469;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27491;&#24120;&#34892;&#20026;&#21644;&#24322;&#24120;&#34892;&#20026;&#20043;&#38388;&#30340;&#36793;&#30028;&#24448;&#24448;&#27169;&#31946;&#19981;&#28165;&#12290;&#22270;&#30340;&#23616;&#37096;&#24322;&#36136;&#24615;&#24178;&#25200;&#20102;&#26816;&#27979;&#36807;&#31243;&#65292;&#32780;&#22522;&#20110;&#33410;&#28857;&#25110;&#36793;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#34920;&#31034;&#32467;&#26524;&#20013;&#24341;&#20837;&#20102;&#19981;&#24517;&#35201;&#30340;&#22122;&#22768;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhoGAD&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;PhoGAD&#21033;&#29992;&#25345;&#20037;&#21516;&#35843;&#20248;&#21270;&#26469;&#28548;&#28165;&#34892;&#20026;&#36793;&#30028;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#30456;&#37051;&#36793;&#30340;&#26435;&#37325;&#20197;&#20943;&#36731;&#23616;&#37096;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#22122;&#22768;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24418;&#24335;&#20998;&#26512;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#32806;&#30340;&#37325;&#26032;...
&lt;/p&gt;
&lt;p&gt;
A multitude of toxic online behaviors, ranging from network attacks to anonymous traffic and spam, have severely disrupted the smooth operation of networks. Due to the inherent sender-receiver nature of network behaviors, graph-based frameworks are commonly used for detecting anomalous behaviors. However, in real-world scenarios, the boundary between normal and anomalous behaviors tends to be ambiguous. The local heterophily of graphs interferes with the detection, and existing methods based on nodes or edges introduce unwanted noise into representation results, thereby impacting the effectiveness of detection. To address these issues, we propose PhoGAD, a graph-based anomaly detection framework. PhoGAD leverages persistent homology optimization to clarify behavioral boundaries. Building upon this, the weights of adjacent edges are designed to mitigate the effects of local heterophily. Subsequently, to tackle the noise problem, we conduct a formal analysis and propose a disentangled re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#23558;&#25552;&#21069;&#36864;&#20986;&#21644;&#20998;&#21106;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;I-SplitEE&#31639;&#27861;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#29615;&#22659;&#22833;&#30495;&#12290;</title><link>http://arxiv.org/abs/2401.10541</link><description>&lt;p&gt;
I-SplitEE&#65306;&#22312;&#20855;&#26377;&#25552;&#21069;&#36864;&#20986;&#21151;&#33021;&#30340;&#20998;&#21106;&#35745;&#31639;DNN&#20013;&#30340;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
I-SplitEE: Image classification in Split Computing DNNs with Early Exits. (arXiv:2401.10541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#23558;&#25552;&#21069;&#36864;&#20986;&#21644;&#20998;&#21106;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;I-SplitEE&#31639;&#27861;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#29615;&#22659;&#22833;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#28304;&#20110;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20854;&#26412;&#36523;&#24222;&#22823;&#30340;&#20307;&#31215;&#38459;&#30861;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#65288;&#22914;&#36793;&#32536;&#12289;&#31227;&#21160;&#21644;&#29289;&#32852;&#32593;&#24179;&#21488;&#65289;&#19978;&#37096;&#32626;&#36825;&#20123;&#32593;&#32476;&#12290;&#20174;&#37096;&#20998;&#20113;&#35745;&#31639;&#21368;&#36733;&#65288;&#20998;&#21106;&#35745;&#31639;&#65289;&#21040;&#22312;DNN&#23618;&#20013;&#38598;&#25104;&#25552;&#21069;&#36864;&#20986;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#23558;&#25552;&#21069;&#36864;&#20986;&#19982;&#20998;&#21106;&#35745;&#31639;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#8220;&#20998;&#21106;&#23618;&#8221;&#65292;&#21363;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#35745;&#31639;&#30340;DNN&#30340;&#26368;&#20339;&#28145;&#24230;&#65292;&#24182;&#26681;&#25454;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36890;&#20449;&#25104;&#26412;&#20915;&#23450;&#26159;&#22312;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#25512;&#29702;&#36824;&#26159;&#22312;&#20113;&#31471;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;&#20998;&#31867;&#38754;&#20020;&#30528;&#24433;&#21709;&#22240;&#32032;&#22914;&#30333;&#22825;&#26102;&#38388;&#12289;&#29031;&#26126;&#21644;&#22825;&#27668;&#31561;&#22810;&#26679;&#30340;&#29615;&#22659;&#22833;&#30495;&#12290;&#20026;&#20102;&#36866;&#24212;&#36825;&#20123;&#22833;&#30495;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;I-SplitEE&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#32570;&#23569;&#22522;&#20934;&#30340;&#22330;&#26223;&#21644;&#20855;&#26377;&#36830;&#32493;&#24615;&#30340;&#22312;&#32447;&#26080;&#30417;&#30563;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in Deep Neural Networks (DNNs) stem from their exceptional performance across various domains. However, their inherent large size hinders deploying these networks on resource-constrained devices like edge, mobile, and IoT platforms. Strategies have emerged, from partial cloud computation offloading (split computing) to integrating early exits within DNN layers. Our work presents an innovative unified approach merging early exits and split computing. We determine the 'splitting layer', the optimal depth in the DNN for edge device computations, and whether to infer on edge device or be offloaded to the cloud for inference considering accuracy, computational efficiency, and communication costs. Also, Image classification faces diverse environmental distortions, influenced by factors like time of day, lighting, and weather. To adapt to these distortions, we introduce I-SplitEE, an online unsupervised algorithm ideal for scenarios lacking ground truths and with sequentia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23457;&#26597;&#20102;&#22312;&#23391;&#21152;&#25289;&#31038;&#32676;&#20013;&#32463;&#21382;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#30340;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#33021;&#23384;&#22312;&#22522;&#20110;&#36523;&#20221;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#35686;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10535</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#8220;&#27542;&#27665;&#20914;&#21160;&#8221;: &#23391;&#21152;&#25289;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#21450;&#20854;&#22522;&#20110;&#36523;&#20221;&#30340;&#20559;&#35265;&#30340;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
The "Colonial Impulse" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases. (arXiv:2401.10535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23457;&#26597;&#20102;&#22312;&#23391;&#21152;&#25289;&#31038;&#32676;&#20013;&#32463;&#21382;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#30340;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#33021;&#23384;&#22312;&#22522;&#20110;&#36523;&#20221;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#35686;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27542;&#27665;&#20027;&#20041;&#22312;&#31038;&#20250;&#21382;&#21490;&#19978;&#23545;&#20154;&#20204;&#30340;&#36523;&#20221;&#20135;&#29983;&#20102;&#21508;&#31181;&#24433;&#21709;&#65292;&#20294;&#36825;&#20123;&#27542;&#27665;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#20173;&#36890;&#36807;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#24471;&#21040;&#20102;&#24310;&#32493;&#12290;&#19968;&#31867;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#65292;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65292;&#20063;&#21487;&#33021;&#24310;&#32493;&#27542;&#27665;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#65292;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#24037;&#20855;&#21487;&#33021;&#22914;&#20309;&#19982;&#27542;&#27665;&#20027;&#20041;&#30340;&#24310;&#32493;&#30456;&#20851;&#32852;&#30340;&#20851;&#27880;&#21364;&#36739;&#23569;&#65292;&#23613;&#31649;&#23427;&#20204;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#21508;&#31181;&#23454;&#36341;&#65288;&#20363;&#22914;&#65292;&#20869;&#23481;&#31649;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#32463;&#21382;&#21644;&#32487;&#32493;&#32463;&#21382;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#30340;&#23391;&#21152;&#25289;&#31038;&#32676;&#32972;&#26223;&#19979;&#65292;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#26681;&#25454;&#24403;&#22320;&#23391;&#21152;&#25289;&#31038;&#32676;&#20013;&#21463;&#27542;&#27665;&#20027;&#20041;&#24433;&#21709;&#26368;&#22823;&#30340;&#36523;&#20221;&#31867;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24615;&#21035;&#12289;&#23447;&#25945;&#21644;&#22269;&#31821;&#12290;&#25105;&#20204;&#23545;&#22312;Python&#21253;&#32034;&#24341;(PyPI)&#21644;GitHub&#19978;&#25552;&#20379;&#30340;&#25152;&#26377;&#23391;&#21152;&#25289;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#36827;&#34892;&#20102;&#31639;&#27861;&#23457;&#26597;&#12290;&#23613;&#31649;&#35821;&#20041;&#20869;&#23481;&#30456;&#20284;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#22312;&#22788;&#29702;&#19982;&#23391;&#21152;&#25289;&#31038;&#32676;&#26377;&#20851;&#30340;&#24773;&#24863;&#26102;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
While colonization has sociohistorically impacted people's identities across various dimensions, those colonial values and biases continue to be perpetuated by sociotechnical systems. One category of sociotechnical systems--sentiment analysis tools--can also perpetuate colonial values and bias, yet less attention has been paid to how such tools may be complicit in perpetuating coloniality, although they are often used to guide various practices (e.g., content moderation). In this paper, we explore potential bias in sentiment analysis tools in the context of Bengali communities that have experienced and continue to experience the impacts of colonialism. Drawing on identity categories most impacted by colonialism amongst local Bengali communities, we focused our analytic attention on gender, religion, and nationality. We conducted an algorithmic audit of all sentiment analysis tools for Bengali, available on the Python package index (PyPI) and GitHub. Despite similar semantic content and
&lt;/p&gt;</description></item><item><title>Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.10529</link><description>&lt;p&gt;
Mementos: &#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10529
&lt;/p&gt;
&lt;p&gt;
Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#39640;&#36229;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;MLLM&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#38745;&#24577;&#20449;&#24687;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#29616;&#20195;MLLM&#22312;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#33021;&#21147;&#65292;&#22312;&#29702;&#35299;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#36739;&#23569;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Mementos&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24207;&#21015;&#22270;&#20687;&#25512;&#29702;&#33021;&#21147;&#12290;Mementos&#21253;&#25324;4761&#20010;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#22810;&#26679;&#30340;&#22270;&#20687;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;GPT-4&#36741;&#21161;&#26041;&#27861;&#26469;&#35780;&#20272;MLLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Mementos&#20013;&#21253;&#25324;GPT-4V&#21644;Gemini&#22312;&#20869;&#30340;&#20061;&#20010;&#26368;&#26032;MLLM&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#20934;&#30830;&#25551;&#36848;&#25152;&#32473;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24448;&#24448;&#23548;&#33268;&#23545;&#35937;&#21450;&#20854;&#23545;&#24212;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#26694;&#26550;FARe&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;ReRAM&#30340;PIM&#21152;&#36895;&#22120;&#19978;&#35757;&#32451;GNN&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;FARe&#26694;&#26550;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24320;&#38144;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10522</link><description>&lt;p&gt;
FARe: &#22312;&#22522;&#20110;ReRAM&#30340;PIM&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#25925;&#38556;&#24863;&#30693;&#30340;GNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FARe: Fault-Aware GNN Training on ReRAM-based PIM Accelerators. (arXiv:2401.10522v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#26694;&#26550;FARe&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;ReRAM&#30340;PIM&#21152;&#36895;&#22120;&#19978;&#35757;&#32451;GNN&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;FARe&#26694;&#26550;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24320;&#38144;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#38459;&#24335;&#38543;&#26426;&#23384;&#20648;&#22120;(ReRAM)&#30340;&#22788;&#29702;&#20869;&#23384;(PIM)&#26550;&#26500;&#26159;&#22312;&#36793;&#32536;&#24179;&#21488;&#19978;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#19981;&#25104;&#29087;&#30340;&#21046;&#36896;&#24037;&#33402;&#21644;&#26377;&#38480;&#30340;&#20889;&#20837;&#32784;&#20037;&#24615;&#20351;&#24471;ReRAM&#23481;&#26131;&#21457;&#29983;&#30828;&#20214;&#25925;&#38556;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;GNN&#35757;&#32451;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#23481;&#38169;&#35299;&#20915;&#26041;&#26696;&#22312;&#23384;&#22312;&#25925;&#38556;&#26102;&#26080;&#27861;&#26377;&#25928;&#22320;&#35757;&#32451;GNN&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FARe&#30340;&#25925;&#38556;&#24863;&#30693;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;GNN&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#36731;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26080;&#25925;&#38556;&#24773;&#20917;&#30456;&#27604;&#65292;FARe&#26694;&#26550;&#21487;&#20197;&#23558;GNN&#27979;&#35797;&#20934;&#30830;&#24615;&#25552;&#39640;47.6%&#65292;&#24182;&#19988;&#26102;&#38388;&#24320;&#38144;&#20165;&#20026;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Resistive random-access memory (ReRAM)-based processing-in-memory (PIM) architecture is an attractive solution for training Graph Neural Networks (GNNs) on edge platforms. However, the immature fabrication process and limited write endurance of ReRAMs make them prone to hardware faults, thereby limiting their widespread adoption for GNN training. Further, the existing fault-tolerant solutions prove inadequate for effectively training GNNs in the presence of faults. In this paper, we propose a fault-aware framework referred to as FARe that mitigates the effect of faults during GNN training. FARe outperforms existing approaches in terms of both accuracy and timing overhead. Experimental results demonstrate that FARe framework can restore GNN test accuracy by 47.6% on faulty ReRAM hardware with a ~1% timing overhead compared to the fault-free counterpart.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#30340;&#21306;&#22495;&#36827;&#34892;&#26102;&#31354;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STSM&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10518</link><description>&lt;p&gt;
&#27809;&#26377;&#35266;&#27979;&#25968;&#25454;&#30340;&#21306;&#22495;&#30340;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal Forecasting for Regions without Observations. (arXiv:2401.10518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#30340;&#21306;&#22495;&#36827;&#34892;&#26102;&#31354;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STSM&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#20132;&#36890;&#39044;&#27979;&#12289;&#31354;&#27668;&#27745;&#26579;&#29289;&#39044;&#27979;&#12289;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#31561;&#12290;&#29616;&#26377;&#30340;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#24403;&#25968;&#25454;&#19981;&#23436;&#25972;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#23454;&#38469;&#24773;&#20917;&#20013;&#30001;&#20110;&#37096;&#32626;&#21644;&#32500;&#25252;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#25968;&#25454;&#19981;&#23436;&#25972;&#26159;&#24456;&#24120;&#35265;&#30340;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;&#25968;&#25454;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#22312;&#19968;&#20010;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#35201;&#20040;&#26377;&#19968;&#27573;&#26102;&#38388;&#30340;&#26576;&#20123;&#25968;&#25454;&#21487;&#29992;&#65292;&#35201;&#20040;&#22312;&#19968;&#20123;&#22320;&#28857;&#26377;&#19968;&#20123;&#25968;&#25454;&#21487;&#29992;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20219;&#20309;&#21382;&#21490;&#35266;&#27979;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#36827;&#34892;&#26102;&#31354;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#21306;&#22495;&#21457;&#23637;&#19981;&#22343;&#34913;&#12289;&#20256;&#24863;&#22120;&#36880;&#27493;&#37096;&#32626;&#25110;&#32570;&#20047;&#24320;&#25918;&#25968;&#25454;&#31561;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STSM&#30340;&#27169;&#22411;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal forecasting plays an important role in many real-world applications, such as traffic forecasting, air pollutant forecasting, crowd-flow forecasting, and so on. State-of-the-art spatial-temporal forecasting models take data-driven approaches and rely heavily on data availability. Such models suffer from accuracy issues when data is incomplete, which is common in reality due to the heavy costs of deploying and maintaining sensors for data collection. A few recent studies attempted to address the issue of incomplete data. They typically assume some data availability in a region of interest either for a short period or at a few locations. In this paper, we further study spatial-temporal forecasting for a region of interest without any historical observations, to address scenarios such as unbalanced region development, progressive deployment of sensors or lack of open data. We propose a model named STSM for the task. The model takes a contrastive learning-based approach to 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25193;&#23637;&#30340;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#21892;DRL&#20013;&#29366;&#24577;&#19982;&#22870;&#21169;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20540;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#31574;&#30053;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10516</link><description>&lt;p&gt;
&#25193;&#23637;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#30340;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25193;&#23637;&#30340;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#21892;DRL&#20013;&#29366;&#24577;&#19982;&#22870;&#21169;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20540;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#37327;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#28216;&#25103;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#30001;&#20110;&#26377;&#25928;&#31574;&#30053;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#26679;&#26412;&#65292;DRL&#20173;&#34987;&#35748;&#20026;&#26159;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#24773;&#33410;&#25511;&#21046;&#65288;EC&#65289;&#30340;&#26080;&#27169;&#22411;DRL&#26041;&#27861;&#36890;&#36807;&#20174;&#24773;&#33410;&#35760;&#24518;&#20013;&#22238;&#39038;&#36807;&#21435;&#30340;&#32463;&#39564;&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;EC&#30340;&#26041;&#27861;&#30001;&#20110;&#24573;&#30053;&#20102;&#21033;&#29992;&#65288;&#36807;&#21435;&#30340;&#65289;&#26816;&#32034;&#29366;&#24577;&#30340;&#24191;&#27867;&#20449;&#24687;&#65292;&#23384;&#22312;&#29366;&#24577;&#21644;&#22870;&#21169;&#31354;&#38388;&#20043;&#38388;&#30340;&#28508;&#22312;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20540;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#31574;&#30053;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25193;&#23637;&#29366;&#24577;-&#22870;&#21169;&#31354;&#38388;&#30340;&#39640;&#25928;EC&#22411;DRL&#26694;&#26550;&#65292;&#20854;&#20013;&#29992;&#20316;&#36755;&#20837;&#30340;&#25193;&#23637;&#29366;&#24577;&#21644;&#29992;&#20110;&#35757;&#32451;&#30340;&#25193;&#23637;&#22870;&#21169;&#37117;&#21253;&#21547;&#21382;&#21490;&#21644;&#24403;&#21069;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empowered by deep neural networks, deep reinforcement learning (DRL) has demonstrated tremendous empirical successes in various domains, including games, health care, and autonomous driving. Despite these advancements, DRL is still identified as data-inefficient as effective policies demand vast numbers of environmental samples. Recently, episodic control (EC)-based model-free DRL methods enable sample efficiency by recalling past experiences from episodic memory. However, existing EC-based methods suffer from the limitation of potential misalignment between the state and reward spaces for neglecting the utilization of (past) retrieval states with extensive information, which probably causes inaccurate value estimation and degraded policy performance. To tackle this issue, we introduce an efficient EC-based DRL framework with expanded state-reward space, where the expanded states used as the input and the expanded rewards used in the training both contain historical and current informa
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10510</link><description>&lt;p&gt;
&#22825;&#20316;&#20043;&#21512;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10510
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#33258;&#28982;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25991;&#26412;&#24207;&#21015;&#29983;&#25104;&#21644;&#36827;&#21270;&#30340;&#20849;&#21516;&#29305;&#28857;&#21644;&#26041;&#21521;&#24615;&#65292;&#38416;&#36848;&#20102;LLMs&#19982;EAs&#20043;&#38388;&#30340;&#24378;&#22823;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#19968;&#23545;&#19968;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#36825;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;&#19979;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#21253;&#25324;&#36827;&#21270;&#24494;&#35843;&#21644;LLM&#22686;&#24378;&#22411;EAs&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#22312;LLMs&#21644;EAs&#32806;&#21512;&#26041;&#38754;&#30340;&#22522;&#26412;&#30740;&#31350;&#36335;&#32447;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#29109;Oracle&#26469;&#24674;&#22797;&#22270;&#23618;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#33410;&#28857;&#30340;&#26465;&#20214;&#29109;&#21644;&#22122;&#22768;&#30340;&#26080;&#26465;&#20214;&#29109;&#65292;&#36890;&#36807;&#21024;&#38500;&#28304;&#25110;&#27719;&#28857;&#26469;&#23454;&#29616;&#33410;&#28857;&#30340;&#20998;&#31163;&#12290;&#31639;&#27861;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#27491;&#30830;&#24615;&#21644;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10495</link><description>&lt;p&gt;
&#26465;&#20214;&#29109;&#19979;&#30340;&#22240;&#26524;&#23618;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Layering via Conditional Entropy. (arXiv:2401.10495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#29109;Oracle&#26469;&#24674;&#22797;&#22270;&#23618;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#33410;&#28857;&#30340;&#26465;&#20214;&#29109;&#21644;&#22122;&#22768;&#30340;&#26080;&#26465;&#20214;&#29109;&#65292;&#36890;&#36807;&#21024;&#38500;&#28304;&#25110;&#27719;&#28857;&#26469;&#23454;&#29616;&#33410;&#28857;&#30340;&#20998;&#31163;&#12290;&#31639;&#27861;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#27491;&#30830;&#24615;&#21644;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26088;&#22312;&#20174;&#21487;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#20851;&#20110;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#22270;&#30340;&#20449;&#24687;&#12290;&#23618;&#26512;&#26159;&#23545;&#21464;&#37327;&#36827;&#34892;&#25490;&#24207;&#65292;&#23558;&#22240;&#26524;&#25918;&#22312;&#25928;&#24212;&#20043;&#21069;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36890;&#36807;&#35775;&#38382;&#26465;&#20214;&#29109;Oracle&#26469;&#24674;&#22797;&#22270;&#23618;&#26512;&#30340;&#26041;&#27861;&#65292;&#24403;&#20998;&#24067;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#19981;&#26029;&#20174;&#22270;&#20013;&#21024;&#38500;&#28304;&#25110;&#27719;&#28857;&#26469;&#24037;&#20316;&#12290;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#26465;&#20214;&#29109;&#19982;&#22122;&#22768;&#30340;&#26080;&#26465;&#20214;&#29109;&#26469;&#23558;&#28304;&#25110;&#27719;&#28857;&#19982;&#20854;&#20313;&#33410;&#28857;&#20998;&#31163;&#24320;&#26469;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#20108;&#27425;&#30340;&#65292;&#24182;&#19988;&#32463;&#36807;&#35777;&#26126;&#26159;&#27491;&#30830;&#30340;&#12290;&#20027;&#35201;&#30340;&#20551;&#35774;&#26159;&#24544;&#23454;&#24615;&#21644;&#21333;&#23556;&#22122;&#22768;&#65292;&#20197;&#21450;&#24050;&#30693;&#22122;&#22768;&#29109;&#25110;&#27839;&#30528;&#26377;&#21521;&#36335;&#24452;&#24369;&#21333;&#35843;&#36882;&#22686;&#30340;&#22122;&#22768;&#29109;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38656;&#35201;&#24544;&#23454;&#24615;&#30340;&#19968;&#20010;&#38750;&#24120;&#28201;&#21644;&#30340;&#25193;&#23637;&#65292;&#25110;&#32773;&#20005;&#26684;&#21333;&#35843;&#36882;&#22686;&#30340;&#22122;&#22768;&#29109;&#65292;&#25110;&#32773;&#25193;&#23637;&#21040;&#26080;&#38480;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery aims to recover information about an unobserved causal graph from the observable data it generates. Layerings are orderings of the variables which place causes before effects. In this paper, we provide ways to recover layerings of a graph by accessing the data via a conditional entropy oracle, when distributions are discrete. Our algorithms work by repeatedly removing sources or sinks from the graph. Under appropriate assumptions and conditioning, we can separate the sources or sinks from the remainder of the nodes by comparing their conditional entropy to the unconditional entropy of their noise. Our algorithms are provably correct and run in worst-case quadratic time. The main assumptions are faithfulness and injective noise, and either known noise entropies or weakly monotonically increasing noise entropies along directed paths. In addition, we require one of either a very mild extension of faithfulness, or strictly monotonically increasing noise entropies, or expan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Auto-Encoder&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#38477;&#32500;&#31639;&#27861;&#65292;&#29992;&#20110;&#36816;&#31639;&#31526;&#23398;&#20064;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#23398;&#20064;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31526;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10490</link><description>&lt;p&gt;
&#22522;&#20110;Auto-Encoder&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#38477;&#32500;&#31639;&#27861;&#29992;&#20110;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#27867;&#21270;&#35823;&#24046;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Guaranteed Auto-Encoder-Based Nonlinear Model Reduction for Operator Learning. (arXiv:2401.10490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Auto-Encoder&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#38477;&#32500;&#31639;&#27861;&#65292;&#29992;&#20110;&#36816;&#31639;&#31526;&#23398;&#20064;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#23398;&#20064;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31526;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#35768;&#22810;&#29289;&#29702;&#36807;&#31243;&#21487;&#20197;&#33258;&#28982;&#22320;&#30001;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#36816;&#31639;&#31526;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#25552;&#21462;&#36825;&#20123;&#29289;&#29702;&#36807;&#31243;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#26080;&#38480;&#25110;&#39640;&#32500;&#24230;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#27169;&#22411;&#38477;&#32500;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#30340;&#32500;&#24230;&#21644;&#38382;&#39064;&#30340;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#22411;&#38477;&#32500;&#20013;&#30340;&#20302;&#32500;&#38750;&#32447;&#24615;&#32467;&#26500;&#65292;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;Auto-Encoder&#30340;&#31070;&#32463;&#32593;&#32476;(AENet)&#12290;AENet&#39318;&#20808;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#30340;&#28508;&#21464;&#37327;&#65292;&#28982;&#21518;&#23398;&#20064;&#20174;&#36825;&#20123;&#28508;&#21464;&#37327;&#21040;&#30456;&#24212;&#36755;&#20986;&#25968;&#25454;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;AENet&#20934;&#30830;&#23398;&#20064;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31526;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#21644;&#32479;&#35745;&#20272;&#35745;&#29702;&#35770;&#65292;&#20998;&#26512;&#20102;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many physical processes in science and engineering are naturally represented by operators between infinite-dimensional function spaces. The problem of operator learning, in this context, seeks to extract these physical processes from empirical data, which is challenging due to the infinite or high dimensionality of data. An integral component in addressing this challenge is model reduction, which reduces both the data dimensionality and problem size. In this paper, we utilize low-dimensional nonlinear structures in model reduction by investigating Auto-Encoder-based Neural Network (AENet). AENet first learns the latent variables of the input data and then learns the transformation from these latent variables to corresponding output data. Our numerical experiments validate the ability of AENet to accurately learn the solution operator of nonlinear partial differential equations. Furthermore, we establish a mathematical and statistical estimation theory that analyzes the generalization e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#31639;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#21644;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#23454;&#29616;&#20102;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#21512;&#20316;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2401.10478</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#31639;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#21644;&#35843;&#20248;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Budgeted Online Model Selection and Fine-Tuning via Federated Learning. (arXiv:2401.10478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#31639;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#21644;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#23454;&#29616;&#20102;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#21512;&#20316;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#28041;&#21450;&#20174;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#20013;&#36873;&#25321;&#19968;&#20010;&#27169;&#22411;&#8220;&#21363;&#26102;&#8221;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#39044;&#27979;&#12290;&#20505;&#36873;&#27169;&#22411;&#30340;&#36873;&#25321;&#23545;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#20351;&#29992;&#26356;&#22823;&#30340;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#33258;&#28982;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#20855;&#26377;&#26356;&#28789;&#27963;&#24615;&#65292;&#20294;&#22312;&#39044;&#27979;&#20219;&#21153;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#26102;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#38754;&#20020;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#32852;&#37030;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#32452;&#23398;&#20064;&#32773;&#65288;&#23458;&#25143;&#31471;&#65289;&#19982;&#20855;&#26377;&#36275;&#22815;&#20869;&#23384;&#30340;&#26381;&#21153;&#22120;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#24471;&#26381;&#21153;&#22120;&#23384;&#20648;&#25152;&#26377;&#20505;&#36873;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#36873;&#25321;&#23384;&#20648;&#21487;&#20197;&#36866;&#37197;&#20854;&#20869;&#23384;&#30340;&#27169;&#22411;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#20854;&#20013;&#19968;&#20010;&#23384;&#20648;&#27169;&#22411;&#25191;&#34892;&#33258;&#24049;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#21512;&#20316;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#35843;&#25972;&#21040;&#38750;&#24179;&#31283;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online model selection involves selecting a model from a set of candidate models 'on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10474</link><description>&lt;p&gt;
LDReg: &#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23398;&#20064;&#30340;&#34920;&#31034;&#21487;&#33021;&#23481;&#26131;&#20986;&#29616;&#32500;&#24230;&#22349;&#32553;&#65292;&#20854;&#20013;&#23398;&#20064;&#30340;&#34920;&#31034;&#23376;&#31354;&#38388;&#32500;&#24230;&#26497;&#20302;&#65292;&#22240;&#27492;&#26080;&#27861;&#34920;&#31034;&#23436;&#25972;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#24577;&#12290;&#32500;&#24230;&#22349;&#32553;&#20063;&#34987;&#31216;&#20026;&#8220;&#22635;&#20805;&#19981;&#36275;&#8221;&#29616;&#35937;&#65292;&#26159;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#30740;&#31350;&#20102;SSL&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#34920;&#31034;&#21487;&#20197;&#22312;&#20840;&#23616;&#19978;&#35206;&#30422;&#39640;&#32500;&#31354;&#38388;&#65292;&#20294;&#22312;&#23616;&#37096;&#19978;&#20250;&#22349;&#32553;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#65288;LDReg&#65289;&#8221;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#26159;&#22522;&#20110;Fisher-Rao&#24230;&#37327;&#30340;&#25512;&#23548;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#20248;&#21270;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#28176;&#36827;&#23567;&#21322;&#24452;&#22788;&#30340;&#23616;&#37096;&#36317;&#31163;&#20998;&#24067;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;LDReg&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the repres
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.10467</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#24182;&#20351;&#29992;&#20998;&#25903;&#23450;&#30028;&#26041;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#23384;&#22312;MIP&#21518;&#38376;&#65292;&#21363;&#19968;&#23567;&#32452;&#21464;&#37327;&#65292;&#22914;&#26524;&#20248;&#20808;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#22312;&#23427;&#20204;&#19978;&#36827;&#34892;&#20998;&#25903;&#65292;&#21017;&#21487;&#20197;&#21152;&#24555;&#36816;&#34892;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#33021;&#25552;&#39640;&#36816;&#34892;&#26102;&#38388;&#30340;&#39640;&#36136;&#37327;&#21518;&#38376;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25490;&#21517;&#23398;&#20064;&#20272;&#35745;&#38543;&#26426;&#37319;&#26679;&#30340;&#21518;&#38376;&#30456;&#23545;&#27714;&#35299;&#22120;&#36895;&#24230;&#65292;&#28982;&#21518;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26041;&#27861;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#38543;&#26426;&#37319;&#26679;&#65292;&#24182;&#19988;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#24120;&#35265;&#30340;MIP&#38382;&#39064;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#23545;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36731;&#22411;&#24046;&#20998;DSP&#22768;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#30340;&#22768;&#23398;&#27169;&#22411;&#21644;DSP&#22768;&#30721;&#22120;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#32780;&#26080;&#38656;&#25552;&#21462;&#22768;&#36947;&#39057;&#35889;&#29305;&#24449;&#12290;&#35813;&#27169;&#22411;&#22312;&#38899;&#39057;&#36136;&#37327;&#19978;&#25509;&#36817;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#19988;&#24615;&#33021;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.10460</link><description>&lt;p&gt;
&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#30340;&#36229;&#36731;&#22411;&#31070;&#32463;&#24046;&#20998;DSP&#22768;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Ultra-lightweight Neural Differential DSP Vocoder For High Quality Speech Synthesis. (arXiv:2401.10460v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36731;&#22411;&#24046;&#20998;DSP&#22768;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#30340;&#22768;&#23398;&#27169;&#22411;&#21644;DSP&#22768;&#30721;&#22120;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#32780;&#26080;&#38656;&#25552;&#21462;&#22768;&#36947;&#39057;&#35889;&#29305;&#24449;&#12290;&#35813;&#27169;&#22411;&#22312;&#38899;&#39057;&#36136;&#37327;&#19978;&#25509;&#36817;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#19988;&#24615;&#33021;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22768;&#30721;&#22120;&#27169;&#25311;&#21407;&#22987;&#38899;&#39057;&#27874;&#24418;&#24182;&#21512;&#25104;&#39640;&#36136;&#37327;&#38899;&#39057;&#65292;&#20294;&#21363;&#20351;&#26159;&#39640;&#25928;&#29575;&#30340;&#22768;&#30721;&#22120;&#65292;&#22914;MB-MelGAN&#21644;LPCNet&#65292;&#20063;&#26080;&#27861;&#22312;&#20302;&#31471;&#35774;&#22791;&#65288;&#22914;&#26234;&#33021;&#30524;&#38236;&#65289;&#19978;&#23454;&#26102;&#36816;&#34892;&#12290;&#22522;&#20110;&#32431;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#30340;&#22768;&#30721;&#22120;&#21487;&#20197;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#26469;&#23454;&#29616;&#65292;&#22240;&#27492;&#27604;&#20219;&#20309;&#31070;&#32463;&#22768;&#30721;&#22120;&#37117;&#24555;&#24471;&#22810;&#12290;DSP&#22768;&#30721;&#22120;&#36890;&#24120;&#30001;&#20110;&#20351;&#29992;&#36807;&#24230;&#24179;&#28369;&#30340;&#22768;&#23398;&#27169;&#22411;&#39044;&#27979;&#21644;&#38899;&#36947;&#30340;&#36817;&#20284;&#34920;&#31034;&#32780;&#23548;&#33268;&#38899;&#39057;&#36136;&#37327;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36731;&#24046;&#20998;DSP&#22768;&#30721;&#22120;&#65288;DDSP&#22768;&#30721;&#22120;&#65289;&#65292;&#23427;&#20351;&#29992;&#20102;&#32463;&#36807;&#32852;&#21512;&#20248;&#21270;&#30340;&#22768;&#23398;&#27169;&#22411;&#21644;DSP&#22768;&#30721;&#22120;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#25552;&#21462;&#22768;&#36947;&#39057;&#35889;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#22312;&#20316;&#20026;DSP&#22768;&#30721;&#22120;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#31070;&#32463;&#22768;&#30721;&#22120;&#30456;&#24403;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#24179;&#22343;MOS&#39640;&#36798;4.36&#12290;&#25105;&#20204;&#30340;C++&#23454;&#29616;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#30828;&#20214;&#29305;&#23450;&#30340;&#20248;&#21270;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;15 MFLOPS&#65292;&#36229;&#36807;MB-MelGAN 3&#20010;&#24615;&#33021;&#21333;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural vocoders model the raw audio waveform and synthesize high-quality audio, but even the highly efficient ones, like MB-MelGAN and LPCNet, fail to run real-time on a low-end device like a smartglass. A pure digital signal processing (DSP) based vocoder can be implemented via lightweight fast Fourier transforms (FFT), and therefore, is a magnitude faster than any neural vocoder. A DSP vocoder often gets a lower audio quality due to consuming over-smoothed acoustic model predictions of approximate representations for the vocal tract. In this paper, we propose an ultra-lightweight differential DSP (DDSP) vocoder that uses a jointly optimized acoustic model with a DSP vocoder, and learns without an extracted spectral feature for the vocal tract. The model achieves audio quality comparable to neural vocoders with a high average MOS of 4.36 while being efficient as a DSP vocoder. Our C++ implementation, without any hardware-specific optimization, is at 15 MFLOPS, surpasses MB-MelGAN by 3
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#21453;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#21453;&#23398;&#20064;&#26679;&#26412;&#21644;&#21097;&#20313;&#26679;&#26412;&#30340;&#23884;&#20837;&#65292;&#23558;&#21453;&#23398;&#20064;&#26679;&#26412;&#25512;&#31163;&#21407;&#22987;&#31867;&#21035;&#24182;&#25289;&#21521;&#20854;&#20182;&#31867;&#21035;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#28040;&#38500;&#20854;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21097;&#20313;&#26679;&#26412;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#27604;&#21453;&#23398;&#20064;&#22312;&#21453;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2401.10458</link><description>&lt;p&gt;
&#23545;&#27604;&#21453;&#35757;&#32451;&#65306;&#19968;&#31181;&#23545;&#27604;&#26041;&#27861;&#29992;&#20110;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Unlearning: A Contrastive Approach to Machine Unlearning. (arXiv:2401.10458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#21453;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#21453;&#23398;&#20064;&#26679;&#26412;&#21644;&#21097;&#20313;&#26679;&#26412;&#30340;&#23884;&#20837;&#65292;&#23558;&#21453;&#23398;&#20064;&#26679;&#26412;&#25512;&#31163;&#21407;&#22987;&#31867;&#21035;&#24182;&#25289;&#21521;&#20854;&#20182;&#31867;&#21035;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#28040;&#38500;&#20854;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21097;&#20313;&#26679;&#26412;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#27604;&#21453;&#23398;&#20064;&#22312;&#21453;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21453;&#23398;&#20064;&#26088;&#22312;&#28040;&#38500;&#35757;&#32451;&#26679;&#26412;&#23376;&#38598;&#65288;&#21363;&#21453;&#23398;&#20064;&#26679;&#26412;&#65289;&#23545;&#24050;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#31227;&#38500;&#21453;&#23398;&#20064;&#26679;&#26412;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#24565;&#26469;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#21453;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#21453;&#23398;&#20064;&#26679;&#26412;&#30340;&#23884;&#20837;&#19982;&#21097;&#20313;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#65292;&#23558;&#23427;&#20204;&#20174;&#21407;&#22987;&#31867;&#21035;&#20013;&#25512;&#24320;&#24182;&#25289;&#21521;&#20854;&#20182;&#31867;&#21035;&#65292;&#20174;&#32780;&#28040;&#38500;&#20854;&#24433;&#21709;&#12290;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#34920;&#31034;&#31354;&#38388;&#65292;&#23427;&#26377;&#25928;&#22320;&#21435;&#38500;&#20102;&#21453;&#23398;&#20064;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20174;&#21097;&#20313;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#27604;&#21453;&#23398;&#20064;&#33021;&#22815;&#20197;&#26368;&#20302;&#30340;&#24615;&#33021;&#25439;&#22833;&#23454;&#29616;&#26368;&#22909;&#30340;&#21453;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning aims to eliminate the influence of a subset of training samples (i.e., unlearning samples) from a trained model. Effectively and efficiently removing the unlearning samples without negatively impacting the overall model performance is still challenging. In this paper, we propose a contrastive unlearning framework, leveraging the concept of representation learning for more effective unlearning. It removes the influence of unlearning samples by contrasting their embeddings against the remaining samples so that they are pushed away from their original classes and pulled toward other classes. By directly optimizing the representation space, it effectively removes the influence of unlearning samples while maintaining the representations learned from the remaining samples. Experiments on a variety of datasets and models on both class unlearning and sample unlearning showed that contrastive unlearning achieves the best unlearning effects and efficiency with the lowest perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2401.10451</link><description>&lt;p&gt;
&#23398;&#20064;&#36741;&#21161;&#30340;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#35268;&#21010;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach. (arXiv:2401.10451v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#23545;&#20110;&#21306;&#22495;&#33021;&#28304;&#31995;&#32479;&#30340;&#25104;&#26412;&#25928;&#30410;&#20302;&#30899;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#30830;&#20445;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#39044;&#26399;&#32467;&#26524;&#65292;&#24314;&#27169;&#32771;&#34385;&#21040;&#22825;&#27668;&#30456;&#20851;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#20379;&#24212;&#21644;&#33021;&#28304;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38543;&#26426;&#20248;&#21270;&#27169;&#22411;&#36890;&#24120;&#27604;&#30830;&#23450;&#24615;&#27169;&#22411;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36817;&#20284;&#35299;&#27861;&#26469;&#21487;&#34892;&#22320;&#35299;&#20915;&#20004;&#38454;&#27573;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#25628;&#32034;&#26102;&#38388;&#24207;&#21015;&#32858;&#21512;&#36229;&#21442;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#35745;&#31639;&#22312;&#20379;&#38656;&#39044;&#27979;&#30340;&#39564;&#35777;&#38598;&#19978;&#26368;&#23567;&#21270;&#25104;&#26412;&#30340;&#36817;&#20284;&#35299;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#32452;&#20445;&#30041;&#30340;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving large-scale capacity expansion problems (CEPs) is central to cost-effective decarbonization of regional-scale energy systems. To ensure the intended outcomes of CEPs, modeling uncertainty due to weather-dependent variable renewable energy (VRE) supply and energy demand becomes crucially important. However, the resulting stochastic optimization models are often less computationally tractable than their deterministic counterparts. Here, we propose a learning-assisted approximate solution method to tractably solve two-stage stochastic CEPs. Our method identifies low-cost planning decisions by constructing and solving a sequence of tractable temporally aggregated surrogate problems. We adopt a Bayesian optimization approach to searching the space of time series aggregation hyperparameters and compute approximate solutions that minimize costs on a validation set of supply-demand projections. Importantly, we evaluate solved planning outcomes on a held-out set of test projections. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;LoRA&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#32423;LoRA&#21464;&#20307;&#23548;&#33268;&#20102;&#26576;&#20123;&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.10447</link><description>&lt;p&gt;
&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#20302;&#31209;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;LoRA&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#32423;LoRA&#21464;&#20307;&#23548;&#33268;&#20102;&#26576;&#20123;&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36164;&#28304;&#26377;&#38480;&#30340;&#30828;&#20214;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#19982;&#20923;&#32467;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;LoRA&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#20844;&#24320;&#30340;Librispeech&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;3.50&#65285;&#65292;&#22312;&#28040;&#24687;&#39046;&#22495;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;3.67&#65285;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35780;&#20272;&#22522;&#20110;LoRA&#30340;&#20108;&#27425;&#20256;&#36882;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#25200;&#21160;&#28304;&#20110;&#21516;&#38899;&#23383;&#26367;&#20195;&#21644;&#19968;&#31181;&#21517;&#20026;N-best Perturbation-based Rescoring Robustness&#65288;NPRR&#65289;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#29992;&#20110;&#34913;&#37327;&#37325;&#35780;&#20998;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#23545;&#38477;&#35299;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LoRA&#30340;&#39640;&#32423;&#21464;&#20307;&#65288;&#20363;&#22914;&#21160;&#24577;&#31209;&#20998;&#37197;&#30340;LoRA&#65289;&#23548;&#33268;&#20102;$1$-best&#25200;&#21160;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of low-rank adaptation (LoRA) with frozen pretrained language models (PLMs) has become increasing popular as a mainstream, resource-efficient modeling approach for memory-constrained hardware. In this study, we first explore how to enhance model performance by introducing various LoRA training strategies, achieving relative word error rate reductions of 3.50\% on the public Librispeech dataset and of 3.67\% on an internal dataset in the messaging domain. To further characterize the stability of LoRA-based second-pass speech recognition models, we examine robustness against input perturbations. These perturbations are rooted in homophone replacements and a novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both designed to measure the relative degradation in the performance of rescoring models. Our experimental results indicate that while advanced variants of LoRA, such as dynamic rank-allocated LoRA, lead to performance degradation in $1$-best perturbati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#65292;&#24182;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#65292;&#25945;&#20250;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22122;&#22768;&#21435;&#38500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.10446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#39640;&#25928;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#65292;&#24182;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#65292;&#25945;&#20250;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22122;&#22768;&#21435;&#38500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22122;&#22768;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#65292;&#21033;&#29992;LLMs&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#25913;&#21892;&#35782;&#21035;&#32467;&#26524;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;GER&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;HyPoradise&#25968;&#25454;&#38598;&#36890;&#36807;&#39640;&#25928;&#30340;LLM&#24494;&#35843;&#20174;ASR N-best&#20551;&#35774;&#21040;&#22320;&#38754;&#30495;&#23454;&#36716;&#24405;&#30340;&#26144;&#23556;&#65292;&#36825;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22122;&#22768;&#40065;&#26834;ASR&#26041;&#38754;&#32570;&#20047;&#20855;&#20307;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20934;&#27979;&#35797;&#25193;&#23637;&#21040;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#24182;&#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#25945;&#20250;LLMs&#20687;&#22122;&#22768;&#40065;&#26834;ASR&#19968;&#26679;&#25191;&#34892;&#21435;&#22122;&#12290;&#20854;&#20013;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#22122;&#22768;&#20449;&#24687;&#20316;&#20026;&#26465;&#20214;&#22120;&#24341;&#20837;LLM&#20013;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#38899;&#39057;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#22122;&#22768;&#23884;&#20837;&#21487;&#33021;&#20250;&#23545;LLM&#24494;&#35843;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#20026;&#23384;&#22312;&#36328;&#27169;&#24577;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;N-best&#21015;&#34920;&#20013;&#25552;&#21462;&#35821;&#35328;&#31354;&#38388;&#22122;&#22768;&#23884;&#20837;&#26469;&#34920;&#31034;&#28304;&#35821;&#38899;&#30340;&#22122;&#22768;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36335;&#24452;&#26041;&#27861;&#20013;&#36335;&#24452;&#36873;&#25321;&#23545;&#20110;&#28165;&#26224;&#24402;&#22240;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#8220;&#38598;&#20013;&#21407;&#21017;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#22120;SAMP&#65292;&#24182;&#20351;&#29992;&#26080;&#31351;&#23567;&#32422;&#26463;&#21644;&#21160;&#37327;&#31574;&#30053;&#26469;&#25552;&#39640;&#20005;&#35880;&#24615;&#21644;&#20248;&#21270;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25581;&#31034;DNNs&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10442</link><description>&lt;p&gt;
&#36335;&#24452;&#36873;&#25321;&#23545;&#20110;&#36335;&#24452;&#26041;&#27861;&#20013;&#30340;&#28165;&#26224;&#24402;&#22240;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Path Choice Matters for Clear Attribution in Path Methods. (arXiv:2401.10442v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36335;&#24452;&#26041;&#27861;&#20013;&#36335;&#24452;&#36873;&#25321;&#23545;&#20110;&#28165;&#26224;&#24402;&#22240;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#8220;&#38598;&#20013;&#21407;&#21017;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#22120;SAMP&#65292;&#24182;&#20351;&#29992;&#26080;&#31351;&#23567;&#32422;&#26463;&#21644;&#21160;&#37327;&#31574;&#30053;&#26469;&#25552;&#39640;&#20005;&#35880;&#24615;&#21644;&#20248;&#21270;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25581;&#31034;DNNs&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20005;&#35880;&#24615;&#21644;&#28165;&#26224;&#24615;&#23545;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20197;&#20135;&#29983;&#20154;&#31867;&#20449;&#20219;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36335;&#24452;&#26041;&#27861;&#24120;&#29992;&#20110;&#29983;&#25104;&#28385;&#36275;&#19977;&#20010;&#20844;&#29702;&#30340;&#20005;&#35880;&#24402;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36335;&#24452;&#36873;&#25321;&#30340;&#19981;&#21516;&#65292;&#24402;&#22240;&#30340;&#21547;&#20041;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#27169;&#31946;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#38598;&#20013;&#21407;&#21017;&#8221;&#65292;&#23558;&#39640;&#24230;&#37325;&#35201;&#30340;&#29305;&#24449;&#38598;&#20013;&#20998;&#37197;&#32473;&#39640;&#24230;&#24402;&#22240;&#65292;&#20174;&#32780;&#36171;&#20104;&#32654;&#24863;&#21644;&#31232;&#30095;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#22120;&#8220;SAMP&#8221;&#65292;&#23427;&#21487;&#20197;&#20174;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#25805;&#32437;&#36335;&#24452;&#20013;&#39640;&#25928;&#22320;&#25628;&#32034;&#36817;&#20046;&#26368;&#20248;&#30340;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#31351;&#23567;&#32422;&#26463;&#65288;IC&#65289;&#21644;&#21160;&#37327;&#31574;&#30053;&#65288;MS&#65289;&#26469;&#25913;&#21892;&#20005;&#35880;&#24615;&#21644;&#20248;&#21270;&#24615;&#12290;&#21487;&#35270;&#21270;&#32467;&#26524;&#26174;&#31034;&#65292;SAMP&#21487;&#20197;&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#26174;&#33879;&#30340;&#22270;&#20687;&#20687;&#32032;&#26469;&#31934;&#30830;&#25581;&#31034;DNNs&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23450;&#37327;&#23454;&#39564;&#65292;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#23545;&#24212;&#26041;&#27861;&#12290;&#20195;&#30721;&#65306;https://github.com/zbr17/SAMP&#12290;
&lt;/p&gt;
&lt;p&gt;
Rigorousness and clarity are both essential for interpretations of DNNs to engender human trust. Path methods are commonly employed to generate rigorous attributions that satisfy three axioms. However, the meaning of attributions remains ambiguous due to distinct path choices. To address the ambiguity, we introduce \textbf{Concentration Principle}, which centrally allocates high attributions to indispensable features, thereby endowing aesthetic and sparsity. We then present \textbf{SAMP}, a model-agnostic interpreter, which efficiently searches the near-optimal path from a pre-defined set of manipulation paths. Moreover, we propose the infinitesimal constraint (IC) and momentum strategy (MS) to improve the rigorousness and optimality. Visualizations show that SAMP can precisely reveal DNNs by pinpointing salient image pixels. We also perform quantitative experiments and observe that our method significantly outperforms the counterparts. Code: https://github.com/zbr17/SAMP.
&lt;/p&gt;</description></item><item><title>A2Q+&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#32047;&#21152;&#22120;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#32422;&#26463;&#21644;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#30828;&#20214;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10432</link><description>&lt;p&gt;
A2Q+: &#25913;&#36827;&#30340;&#32047;&#21152;&#22120;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A2Q+: Improving Accumulator-Aware Weight Quantization. (arXiv:2401.10432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10432
&lt;/p&gt;
&lt;p&gt;
A2Q+&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#32047;&#21152;&#22120;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#32422;&#26463;&#21644;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#30828;&#20214;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25216;&#26415;&#36890;&#24120;&#36890;&#36807;&#38480;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#31934;&#24230;&#26469;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20943;&#23569;&#32047;&#21152;&#22120;&#30340;&#31934;&#24230;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#30828;&#20214;&#25928;&#29575;&#65292;&#20294;&#20250;&#22686;&#21152;&#25968;&#20540;&#28322;&#20986;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#36991;&#20813;&#25968;&#20540;&#28322;&#20986;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#32047;&#21152;&#22120;&#24863;&#30693;&#37327;&#21270;&#65288;A2Q&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#32422;&#26463;&#27169;&#22411;&#26435;&#37325;&#65292;&#20197;&#22312;&#25512;&#29702;&#26399;&#38388;&#23433;&#20840;&#22320;&#20351;&#29992;&#30446;&#26631;&#32047;&#21152;&#22120;&#20301;&#23485;&#12290;&#23613;&#31649;&#36825;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;A2Q&#20381;&#36182;&#20110;&#36807;&#20110;&#38480;&#21046;&#24615;&#30340;&#32422;&#26463;&#21644;&#27425;&#20248;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#36825;&#20123;&#37117;&#24341;&#20837;&#20102;&#19981;&#24517;&#35201;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20197;&#19979;&#25913;&#36827;&#65306;&#65288;1&#65289;&#19968;&#31181;&#25913;&#36827;&#30340;&#32422;&#26463;&#26041;&#24335;&#65292;&#32531;&#35299;&#32047;&#21152;&#22120;&#32422;&#26463;&#32780;&#19981;&#25439;&#23475;&#28322;&#20986;&#36991;&#20813;&#65307;&#65288;2&#65289;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization techniques commonly reduce the inference costs of neural networks by restricting the precision of weights and activations. Recent studies show that also reducing the precision of the accumulator can further improve hardware efficiency at the risk of numerical overflow, which introduces arithmetic errors that can degrade model accuracy. To avoid numerical overflow while maintaining accuracy, recent work proposed accumulator-aware quantization (A2Q), a quantization-aware training method that constrains model weights during training to safely use a target accumulator bit width during inference. Although this shows promise, we demonstrate that A2Q relies on an overly restrictive constraint and a sub-optimal weight initialization strategy that each introduce superfluous quantization error. To address these shortcomings, we introduce: (1) an improved bound that alleviates accumulator constraints without compromising overflow avoidance; and (2) a new strategy for initializing qua
&lt;/p&gt;</description></item><item><title>M3BUNet&#26159;MobileNet&#21644;U-Net&#31070;&#32463;&#32593;&#32476;&#30340;&#34701;&#21512;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;MM&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#20004;&#20010;&#38454;&#27573;&#36880;&#27493;&#32454;&#20998;&#33008;&#33146;CT&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#25513;&#30721;&#25351;&#23548;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10419</link><description>&lt;p&gt;
M3BUNet: &#29992;&#20110;CT&#25195;&#25551;&#33008;&#33146;&#20998;&#21106;&#30340;&#31227;&#21160;&#22343;&#20540;&#26368;&#22823;UNet
&lt;/p&gt;
&lt;p&gt;
M3BUNet: Mobile Mean Max UNet for Pancreas Segmentation on CT-Scans. (arXiv:2401.10419v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10419
&lt;/p&gt;
&lt;p&gt;
M3BUNet&#26159;MobileNet&#21644;U-Net&#31070;&#32463;&#32593;&#32476;&#30340;&#34701;&#21512;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;MM&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#20004;&#20010;&#38454;&#27573;&#36880;&#27493;&#32454;&#20998;&#33008;&#33146;CT&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#25513;&#30721;&#25351;&#23548;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#20998;&#21106;&#22120;&#23448;&#26159;&#22810;&#20010;&#19979;&#28216;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#24517;&#35201;&#30340;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#25163;&#21160;CT&#25195;&#25551;&#20998;&#21106;&#30001;&#25918;&#23556;&#31185;&#21307;&#29983;&#24191;&#27867;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#33008;&#33146;&#36825;&#26679;&#30340;&#22120;&#23448;&#65292;&#30001;&#20110;&#22120;&#23448;&#23610;&#23544;&#23567;&#12289;&#36974;&#25377;&#21644;&#24418;&#29366;&#21464;&#21270;&#31561;&#22240;&#32032;&#38656;&#35201;&#39640;&#27700;&#24179;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#21487;&#38752;&#30340;&#20998;&#21106;&#12290;&#24403;&#36716;&#21521;&#33258;&#21160;&#33008;&#33146;&#20998;&#21106;&#26102;&#65292;&#36825;&#20123;&#22240;&#32032;&#23548;&#33268;&#21487;&#38752;&#26631;&#27880;&#25968;&#25454;&#26377;&#38480;&#65292;&#38590;&#20197;&#35757;&#32451;&#26377;&#25928;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#33008;&#33146;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#20173;&#26410;&#36798;&#21040;&#21487;&#25509;&#21463;&#33539;&#22260;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3BUNet&#65292;&#23427;&#26159;MobileNet&#21644;U-Net&#31070;&#32463;&#32593;&#32476;&#30340;&#34701;&#21512;&#65292;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22343;&#20540;&#26368;&#22823;(MM)&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#36880;&#27493;&#32454;&#20998;&#33008;&#33146;CT&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#25513;&#30721;&#25351;&#23548;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#36229;&#36234;&#31867;&#20284;&#32593;&#32476;&#26550;&#26500;&#25152;&#23454;&#29616;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmenting organs in CT scan images is a necessary process for multiple downstream medical image analysis tasks. Currently, manual CT scan segmentation by radiologists is prevalent, especially for organs like the pancreas, which requires a high level of domain expertise for reliable segmentation due to factors like small organ size, occlusion, and varying shapes. When resorting to automated pancreas segmentation, these factors translate to limited reliable labeled data to train effective segmentation models. Consequently, the performance of contemporary pancreas segmentation models is still not within acceptable ranges. To improve that, we propose M3BUNet, a fusion of MobileNet and U-Net neural networks, equipped with a novel Mean-Max (MM) attention that operates in two stages to gradually segment pancreas CT images from coarse to fine with mask guidance for object detection. This approach empowers the network to surpass segmentation performance achieved by similar network architecture
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30456;&#32467;&#21512;&#26469;&#24212;&#23545;&#21516;&#26102;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#38544;&#31169;&#20445;&#35777;&#26041;&#38754;&#19982;&#38750;&#40065;&#26834;&#31169;&#26377;&#27169;&#22411;&#30456;&#24403;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#23545;&#21160;&#24577;&#35757;&#32451;&#33539;&#24335;&#20013;&#38544;&#31169;&#20445;&#35777;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.10405</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#32463;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Differentially Private and Adversarially Robust Machine Learning: An Empirical Evaluation. (arXiv:2401.10405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30456;&#32467;&#21512;&#26469;&#24212;&#23545;&#21516;&#26102;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#38544;&#31169;&#20445;&#35777;&#26041;&#38754;&#19982;&#38750;&#40065;&#26834;&#31169;&#26377;&#27169;&#22411;&#30456;&#24403;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#23545;&#21160;&#24577;&#35757;&#32451;&#33539;&#24335;&#20013;&#38544;&#31169;&#20445;&#35777;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#21457;&#36215;&#19968;&#31995;&#21015;&#30340;&#36867;&#36920;&#25915;&#20987;&#26469;&#25512;&#26029;&#25935;&#24863;&#20449;&#24687;&#25110;&#30772;&#22351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#21508;&#31181;&#24037;&#20316;&#33268;&#21147;&#20110;&#35299;&#20915;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#38598;&#20013;&#22312;&#20010;&#21035;&#38450;&#24481;&#19978;&#65292;&#32780;&#23454;&#38469;&#19978;&#65292;&#27169;&#22411;&#21487;&#33021;&#21516;&#26102;&#36973;&#21463;&#22810;&#20010;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#23545;&#25239;&#35757;&#32451;&#21644;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#32452;&#21512;&#65292;&#20197;&#24212;&#23545;&#21516;&#26102;&#25915;&#20987;&#12290;&#34429;&#28982;DP-Adv&#20013;&#25552;&#20986;&#30340;&#24046;&#20998;&#38544;&#31169;&#23545;&#25239;&#35757;&#32451;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#27491;&#24335;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#23454;&#35777;&#39564;&#35777;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#22522;&#20934;&#27979;&#35797;&#35813;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#24182;&#32463;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#38750;&#40065;&#26834;&#31169;&#26377;&#27169;&#22411;&#19968;&#26679;&#20855;&#26377;&#38544;&#31169;&#24615;&#12290;&#26412;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#25506;&#32034;&#21160;&#24577;&#35757;&#32451;&#33539;&#24335;&#20013;&#38544;&#31169;&#20445;&#35777;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious adversaries can attack machine learning models to infer sensitive information or damage the system by launching a series of evasion attacks. Although various work addresses privacy and security concerns, they focus on individual defenses, but in practice, models may undergo simultaneous attacks. This study explores the combination of adversarial training and differentially private training to defend against simultaneous attacks. While differentially-private adversarial training, as presented in DP-Adv, outperforms the other state-of-the-art methods in performance, it lacks formal privacy guarantees and empirical validation. Thus, in this work, we benchmark the performance of this technique using a membership inference attack and empirically show that the resulting approach is as private as non-robust private models. This work also highlights the need to explore privacy guarantees in dynamic training paradigms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;-based&#30340;&#26377;&#25439;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;Deep Dict&#65292;&#36890;&#36807;&#24341;&#20837;&#20271;&#21162;&#21033;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#65288;BTAE&#65289;&#21644;&#22833;&#30495;&#32422;&#26463;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#27604;&#21644;&#20445;&#25345;&#39044;&#23450;&#33539;&#22260;&#20869;&#30340;&#35299;&#21387;&#32553;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#37327;&#21270;&#29109;&#25439;&#22833;&#65288;QEL&#65289;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#25439;&#21387;&#32553;&#22120;&#30456;&#27604;&#65292;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.10396</link><description>&lt;p&gt;
&#28145;&#24230;&#23383;&#20856;: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#26377;&#25439;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;
&lt;/p&gt;
&lt;p&gt;
Deep Dict: Deep Learning-based Lossy Time Series Compressor for IoT Data. (arXiv:2401.10396v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10396
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;-based&#30340;&#26377;&#25439;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;Deep Dict&#65292;&#36890;&#36807;&#24341;&#20837;&#20271;&#21162;&#21033;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#65288;BTAE&#65289;&#21644;&#22833;&#30495;&#32422;&#26463;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#27604;&#21644;&#20445;&#25345;&#39044;&#23450;&#33539;&#22260;&#20869;&#30340;&#35299;&#21387;&#32553;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#37327;&#21270;&#29109;&#25439;&#22833;&#65288;QEL&#65289;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#25439;&#21387;&#32553;&#22120;&#30456;&#27604;&#65292;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#23383;&#20856;&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25439;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#21387;&#32553;&#27604;&#30340;&#21516;&#26102;&#20445;&#25345;&#35299;&#21387;&#32553;&#35823;&#24046;&#22312;&#39044;&#23450;&#33539;&#22260;&#20869;&#12290;&#28145;&#24230;&#23383;&#20856;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#20271;&#21162;&#21033;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#65288;BTAE&#65289;&#21644;&#22833;&#30495;&#32422;&#26463;&#12290;BTAE&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#20271;&#21162;&#21033;&#34920;&#31034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#65292;&#20943;&#23567;&#20102;&#34920;&#31034;&#30340;&#22823;&#23567;&#12290;&#22833;&#30495;&#32422;&#26463;&#38480;&#21046;&#20102;BTAE&#30340;&#39044;&#27979;&#35823;&#24046;&#22312;&#26399;&#26395;&#33539;&#22260;&#20869;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24120;&#35265;&#22238;&#24402;&#25439;&#22833;&#65288;&#22914;L1/L2&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#21270;&#29109;&#25439;&#22833;&#65288;QEL&#65289;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#12290;QEL&#32771;&#34385;&#20102;&#38382;&#39064;&#30340;&#29305;&#23450;&#29305;&#24615;&#65292;&#22686;&#24378;&#20102;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#36731;&#20102;&#20248;&#21270;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;10&#20010;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#24230;&#23383;&#20856;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#28145;&#24230;&#23383;&#20856;&#22312;&#21387;&#32553;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26377;&#25439;&#21387;&#32553;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Deep Dict, a deep learning-based lossy time series compressor designed to achieve a high compression ratio while maintaining decompression error within a predefined range. Deep Dict incorporates two essential components: the Bernoulli transformer autoencoder (BTAE) and a distortion constraint. BTAE extracts Bernoulli representations from time series data, reducing the size of the representations compared to conventional autoencoders. The distortion constraint limits the prediction error of BTAE to the desired range. Moreover, in order to address the limitations of common regression losses such as L1/L2, we introduce a novel loss function called quantized entropy loss (QEL). QEL takes into account the specific characteristics of the problem, enhancing robustness to outliers and alleviating optimization challenges. Our evaluation of Deep Dict across ten diverse time series datasets from various domains reveals that Deep Dict outperforms state-of-the-art lossy compressors in te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#28040;&#38500;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#65292;&#25552;&#39640;&#33258;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10394</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#31232;&#30095;&#26631;&#31614;&#22270;&#31070;&#32463;&#32593;&#32476;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#28040;&#38500;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#65292;&#25552;&#39640;&#33258;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#30417;&#30563;&#19981;&#36275;&#21644;&#28508;&#22312;&#30340;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#33258;&#35757;&#32451;&#26159;&#19968;&#31181;&#24191;&#27867;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#35757;&#32451;&#38598;&#65292;&#36890;&#36807;&#32473;&#36873;&#23450;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#37197;&#20266;&#26631;&#31614;&#12290;&#24050;&#32463;&#24320;&#23637;&#20102;&#35768;&#22810;&#22522;&#20110;&#32622;&#20449;&#24230;&#12289;&#20449;&#24687;&#22686;&#30410;&#31561;&#36873;&#25321;&#31574;&#30053;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#20010;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#27979;&#35797;&#33410;&#28857;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#20266;&#26631;&#31614;&#27493;&#39588;&#21487;&#33021;&#22686;&#21152;&#36825;&#31181;&#36716;&#31227;&#29978;&#33267;&#24341;&#20837;&#26032;&#30340;&#36716;&#31227;&#65292;&#20174;&#32780;&#38459;&#30861;&#33258;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#33258;&#35757;&#32451;&#36807;&#31243;&#20013;&#26126;&#30830;&#22320;&#28040;&#38500;&#25193;&#23637;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#22270;&#33258;&#35757;&#32451;(DC-GST)&#26694;&#26550;&#65292;&#29992;&#20110;&#36776;&#21035;
&lt;/p&gt;
&lt;p&gt;
Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.10393</link><description>&lt;p&gt;
&#33258;&#28982;&#30340;&#21151;&#29575;&#27861;&#21017;&#23398;&#20064;&#29615;&#22659;&#20013;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65288;CI&#65289;&#65306;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#19982;&#20154;&#31867;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#21487;&#20197;&#36830;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#20250;&#26126;&#26174;&#24536;&#35760;&#20808;&#21069;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#20943;&#36731;CI&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#27491;&#21017;&#21270;&#12289;&#22238;&#24518;&#12289;&#29983;&#25104;&#24615;&#22238;&#25918;&#21644;&#27987;&#32553;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#25351;&#23548;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36935;&#21040;&#20219;&#21153;&#30340;&#27010;&#29575;&#19982;&#26368;&#21518;&#19968;&#27425;&#25191;&#34892;&#20219;&#21153;&#30340;&#26102;&#38388;&#25104;&#21151;&#29575;&#27861;&#21017;&#36882;&#20943;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27169;&#25311;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20943;&#36731;CI&#25216;&#26415;&#30340;&#30495;&#23454;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#38754;&#20020;&#30340;&#21151;&#29575;&#27861;&#21017;&#29615;&#22659;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#22238;&#24518;&#26041;&#27861;&#26102;&#65292;CI&#30340;&#20943;&#36731;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#36825;&#31181;&#22522;&#20110;&#22238;&#24518;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#26816;&#27979;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#65292;&#24182;&#36890;&#36807;&#34013;&#29273;&#20256;&#36755;&#32467;&#26524;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.10386</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;
&lt;/p&gt;
&lt;p&gt;
Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning. (arXiv:2401.10386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#26816;&#27979;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#65292;&#24182;&#36890;&#36807;&#34013;&#29273;&#20256;&#36755;&#32467;&#26524;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#65288;ACS&#65289;&#26159;&#19968;&#31181;&#39592;&#31185;&#24613;&#30151;&#65292;&#30001;&#32908;&#32905;&#38388;&#23460;&#20869;&#30340;&#21387;&#21147;&#21319;&#39640;&#24341;&#36215;&#65292;&#23548;&#33268;&#27704;&#20037;&#32452;&#32455;&#25439;&#20260;&#24182;&#26368;&#32456;&#33268;&#27515;&#12290;ACS&#30340;&#35786;&#26029;&#20027;&#35201;&#20381;&#36182;&#24739;&#32773;&#25253;&#21578;&#30340;&#30151;&#29366;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#19978;&#19981;&#21487;&#38752;&#65292;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#20405;&#20837;&#24615;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#27979;&#37327;&#36827;&#34892;&#34917;&#20805;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#12289;&#23458;&#35266;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;ACS&#35786;&#26029;&#26041;&#27861;&#12290;&#35813;&#35774;&#22791;&#36890;&#36807;&#19968;&#20010;&#20351;&#29992;&#36148;&#22312;&#30382;&#32932;&#19978;&#30340;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#65288;FSR&#65289;&#30340;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;ACS&#12290;&#26368;&#32456;&#35786;&#26029;&#32467;&#26524;&#36890;&#36807;&#34013;&#29273;&#20197;&#23454;&#26102;&#26041;&#24335;&#20256;&#36755;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#39564;&#35777;&#35786;&#26029;&#32467;&#26524;&#65292;&#21019;&#24314;&#20102;&#19968;&#32452;&#21253;&#21547;FSR&#27979;&#37327;&#21644;&#30456;&#24212;&#30340;&#27169;&#25311;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#65292;&#19982;&#20405;&#20837;&#24615;&#30340;&#37329;&#26631;&#20934;&#25345;&#24179;&#12290;&#35813;&#35774;&#22791;&#22312;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#21253;&#25324;&#20934;&#30830;&#24230;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute compartment syndrome (ACS) is an orthopedic emergency, caused by elevated pressure within a muscle compartment, that leads to permanent tissue damage and eventually death. Diagnosis of ACS relies heavily on patient-reported symptoms, a method that is clinically unreliable and often supplemented with invasive intracompartmental pressure measurements. This study proposes a continuous, objective, noninvasive diagnostic for ACS. The device detects ACS through a random forest machine learning model that uses pressure readings from force-sensitive resistors (FSRs) placed on the skin. The final diagnosis is exported real-time to a web application via Bluetooth. To validate the diagnostic, a data set containing FSR measurements and the corresponding simulated intracompartmental pressure was created. The diagnostic achieved an accuracy, on par to the invasive gold standard, of 97%. The device excelled in key performance metrics including precision, sensitivity, and F1 score. Manufactured 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#32500;&#24230;&#25511;&#21046;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#39640;&#32500;PDE&#30340;&#35299;&#31639;&#31526;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#38477;&#38454;&#27169;&#22411;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#22312;&#19968;&#33324;&#30340;&#20108;&#38454;&#38750;&#32447;&#24615;PDE&#31867;&#20013;&#35777;&#26126;&#20102;&#36817;&#20284;&#31934;&#24230;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10385</link><description>&lt;p&gt;
&#39640;&#32500;PDE&#30340;&#35299;&#31639;&#31526;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximation of Solution Operators for High-dimensional PDEs. (arXiv:2401.10385v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#32500;&#24230;&#25511;&#21046;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#39640;&#32500;PDE&#30340;&#35299;&#31639;&#31526;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#38477;&#38454;&#27169;&#22411;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#22312;&#19968;&#33324;&#30340;&#20108;&#38454;&#38750;&#32447;&#24615;PDE&#31867;&#20013;&#35777;&#26126;&#20102;&#36817;&#20284;&#31934;&#24230;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#32500;&#24230;&#25511;&#21046;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#28436;&#21270;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#31639;&#31526;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#33324;&#30340;&#38477;&#38454;&#27169;&#22411;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#21442;&#25968;&#30340;&#28436;&#21270;&#19982;&#30456;&#24212;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#36712;&#36857;&#32852;&#31995;&#36215;&#26469;&#12290;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#25105;&#20204;&#23398;&#20064;&#25511;&#21046;&#21442;&#25968;&#31354;&#38388;&#65292;&#20351;&#24471;&#20174;&#20219;&#24847;&#21021;&#22987;&#28857;&#24320;&#22987;&#65292;&#21463;&#25511;&#36712;&#36857;&#33021;&#22815;&#32039;&#23494;&#36924;&#36817;PDE&#30340;&#35299;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#20108;&#38454;&#38750;&#32447;&#24615;PDE&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36817;&#20284;&#31934;&#24230;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#20960;&#20010;&#39640;&#32500;PDE&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#21253;&#25324;&#29992;&#20110;&#27714;&#35299;Hamilton-Jacobi-Bellman&#26041;&#31243;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#28436;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a finite-dimensional control-based method to approximate solution operators for evolutional partial differential equations (PDEs), particularly in high-dimensions. By employing a general reduced-order model, such as a deep neural network, we connect the evolution of the model parameters with trajectories in a corresponding function space. Using the computational technique of neural ordinary differential equation, we learn the control over the parameter space such that from any initial starting point, the controlled trajectories closely approximate the solutions to the PDE. Approximation accuracy is justified for a general class of second-order nonlinear PDEs. Numerical results are presented for several high-dimensional PDEs, including real-world applications to solving Hamilton-Jacobi-Bellman equations. These are demonstrated to show the accuracy and efficiency of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;Multi-G-UCB&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10383</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#65306;UCB&#31639;&#27861;&#21644;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis. (arXiv:2401.10383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;Multi-G-UCB&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#24314;&#27169;&#20026;Zhang&#12289;Johansson&#21644;Li&#22312;[CISS 57, 1-6 (2023)]&#20013;&#25552;&#20986;&#30340;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#22810;&#26234;&#33021;&#20307;&#25193;&#23637;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;N&#20010;&#21512;&#20316;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#36830;&#36890;&#30340;&#22270;G&#19978;&#31227;&#21160;&#65292;&#22270;G&#26377;K&#20010;&#33410;&#28857;&#12290;&#25269;&#36798;&#27599;&#20010;&#33410;&#28857;&#26102;&#65292;&#26234;&#33021;&#20307;&#35266;&#23519;&#21040;&#20174;&#19968;&#20010;&#19982;&#33410;&#28857;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#38543;&#26426;&#25277;&#21462;&#30340;&#22870;&#21169;&#12290;&#31995;&#32479;&#22870;&#21169;&#34987;&#24314;&#27169;&#20026;&#26234;&#33021;&#20307;&#35266;&#27979;&#21040;&#30340;&#22870;&#21169;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#20013;&#26435;&#37325;&#34920;&#36798;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#23545;&#21516;&#19968;&#33410;&#28857;&#36827;&#34892;&#37319;&#26679;&#30340;&#36793;&#38469;&#20943;&#23569;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;UCB&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Multi-G-UCB&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;T&#27493;&#20869;&#20854;&#26399;&#26395;&#36951;&#25022;&#34987;&#30028;&#23450;&#20026;$O(N\log(T)[\sqrt{KT} + DK])$&#65292;&#20854;&#20013;D&#26159;&#22270;G&#30340;&#30452;&#24452;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we formulate the multi-agent graph bandit problem as a multi-agent extension of the graph bandit problem introduced by Zhang, Johansson, and Li [CISS 57, 1-6 (2023)]. In our formulation, $N$ cooperative agents travel on a connected graph $G$ with $K$ nodes. Upon arrival at each node, agents observe a random reward drawn from a node-dependent probability distribution. The reward of the system is modeled as a weighted sum of the rewards the agents observe, where the weights capture the decreasing marginal reward associated with multiple agents sampling the same node at the same time. We propose an Upper Confidence Bound (UCB)-based learning algorithm, Multi-G-UCB, and prove that its expected regret over $T$ steps is bounded by $O(N\log(T)[\sqrt{KT} + DK])$, where $D$ is the diameter of graph $G$. Lastly, we numerically test our algorithm by comparing it to alternative methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#25932;&#23545;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10375</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats. (arXiv:2401.10375v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#25932;&#23545;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#35299;&#20915;&#19982;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#25968;&#25454;&#19981;&#36275;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#23646;&#26410;&#24320;&#21457;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#12290;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#26469;&#30772;&#22351;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#21517;&#27169;&#22411;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#36825;&#31181;&#26032;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses critical issues in machine learning related to data privacy and security, yet suffering from data insufficiency and imbalance under certain circumstances. The emergence of foundation models (FMs) offers potential solutions to the limitations of existing FL frameworks, e.g., by generating synthetic data for model initialization. However, due to the inherent safety concerns of FMs, integrating FMs into FL could introduce new risks, which remains largely unexplored. To address this gap, we conduct the first investigation on the vulnerability of FM integrated FL (FM-FL) under adversarial threats. Based on a unified framework of FM-FL, we introduce a novel attack strategy that exploits safety issues of FM to compromise FL client models. Through extensive experiments with well-known models and benchmark datasets in both image and text domains, we reveal the high susceptibility of the FM-FL to this new threat under various FL configurations. Furthermore, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#65292;&#24341;&#20837;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#26469;&#25552;&#39640;&#23545;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10373</link><description>&lt;p&gt;
&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#23398;&#20064;&#30340;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Harmonized Spatial and Spectral Learning for Robust and Generalized Medical Image Segmentation. (arXiv:2401.10373v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#65292;&#24341;&#20837;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#26469;&#25552;&#39640;&#23545;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#31867;&#38388;&#29420;&#31435;&#24615;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#19968;&#31867;&#22312;&#19981;&#21516;&#26679;&#26412;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#38590;&#20197;&#25429;&#25417;&#19981;&#21516;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#30340;&#38169;&#35823;&#36127;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#26469;&#22686;&#24378;&#39046;&#22495;&#36890;&#29992;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#25429;&#25417;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#36807;&#34701;&#20837;&#26377;&#20215;&#20540;&#30340;&#20809;&#35889;&#20449;&#24687;&#26469;&#34917;&#20805;&#20256;&#32479;&#30340;&#31354;&#38388;&#30446;&#26631;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20248;&#21270;&#36825;&#20010;&#30446;&#26631;&#19982;&#29616;&#26377;&#30340;UNet&#21644;TransUNet&#26550;&#26500;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has demonstrated remarkable achievements in medical image segmentation. However, prevailing deep learning models struggle with poor generalization due to (i) intra-class variations, where the same class appears differently in different samples, and (ii) inter-class independence, resulting in difficulties capturing intricate relationships between distinct objects, leading to higher false negative cases. This paper presents a novel approach that synergies spatial and spectral representations to enhance domain-generalized medical image segmentation. We introduce the innovative Spectral Correlation Coefficient objective to improve the model's capacity to capture middle-order features and contextual long-range dependencies. This objective complements traditional spatial objectives by incorporating valuable spectral information. Extensive experiments reveal that optimizing this objective with existing architectures like UNet and TransUNet significantly enhances generalization, 
&lt;/p&gt;</description></item><item><title>Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10371</link><description>&lt;p&gt;
Langevin&#36951;&#24536;&#65306;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#26426;&#22120;&#36951;&#24536;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10371
&lt;/p&gt;
&lt;p&gt;
Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37319;&#29992;&#30830;&#20445;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27861;&#24459;&#65292;&#26426;&#22120;&#36951;&#24536;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#36817;&#20284;&#36951;&#24536;&#23450;&#20041;&#65292;&#31867;&#20284;&#20110;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#23450;&#20041;&#65292;&#20854;&#20013;&#38544;&#31169;&#34987;&#23450;&#20041;&#20026;&#23545;&#37325;&#26032;&#35757;&#32451;&#30340;&#32479;&#35745;&#19981;&#21487;&#21306;&#20998;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Langevin&#36951;&#24536;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#30340;&#38544;&#31169;&#20445;&#35777;&#30340;&#36951;&#24536;&#26694;&#26550;&#12290;Langevin&#36951;&#24536;&#22312;&#31639;&#27861;&#19978;&#32479;&#19968;&#20102;DP&#23398;&#20064;&#36807;&#31243;&#21644;&#38544;&#31169;&#35748;&#35777;&#30340;&#36951;&#24536;&#36807;&#31243;&#12290;&#20854;&#20013;&#21253;&#25324;&#38750;&#20984;&#38382;&#39064;&#30340;&#36817;&#20284;&#35748;&#35777;&#36951;&#24536;&#65292;&#30456;&#23545;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#29992;&#20110;&#22810;&#20010;&#36951;&#24536;&#35831;&#27714;&#30340;&#39034;&#24207;&#21644;&#25209;&#37327;&#36951;&#24536;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Langevin&#36951;&#24536;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26465;&#20214;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;VaR&#39044;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.10370</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#65306;VaR&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Modeling for Financial Time Series with Application in VaR: A Comparative Review. (arXiv:2401.10370v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26465;&#20214;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;VaR&#39044;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#26381;&#21153;&#19994;&#20013;&#65292;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#24403;&#21069;&#24066;&#22330;&#29615;&#22659;&#26469;&#39044;&#27979;&#39118;&#38505;&#22240;&#32032;&#20998;&#24067;&#26159;&#24066;&#22330;&#39118;&#38505;&#24314;&#27169;&#21644;&#20215;&#20540;-at-risk (VaR) &#27169;&#22411;&#30340;&#20851;&#38190;&#12290;&#20316;&#20026;&#21830;&#19994;&#38134;&#34892;&#20013;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;VaR&#27169;&#22411;&#20043;&#19968;&#65292;&#21382;&#21490;&#27169;&#25311; (HS) &#20351;&#29992;&#21382;&#21490;&#31383;&#21475;&#20869;&#27599;&#26085;&#25910;&#30410;&#30340;&#32463;&#39564;&#20998;&#24067;&#20316;&#20026;&#19979;&#19968;&#22825;&#39118;&#38505;&#22240;&#32032;&#25910;&#30410;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#21464;&#21270;&#24615;&#12289;&#19982;&#21407;&#22987;&#21382;&#21490;&#25968;&#25454;&#31867;&#20284;&#20998;&#24067;&#21644;&#21160;&#24577;&#30340;&#21512;&#25104;&#25968;&#25454;&#36335;&#24452;&#12290;&#26412;&#25991;&#24212;&#29992;&#20102;&#22810;&#31181;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861; (&#20363;&#22914;CGAN&#12289;CWGAN&#12289;Diffusion&#21644;Signature WGAN) &#36827;&#34892;&#26465;&#20214;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#20004;&#31181;&#26032;&#30340;&#26465;&#20214;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#21363;Encoder-Decoder CGAN&#21644;Conditional TimeVAE&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#21450;&#20854;&#22312;VaR&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the financial services industry, forecasting the risk factor distribution conditional on the history and the current market environment is the key to market risk modeling in general and value at risk (VaR) model in particular. As one of the most widely adopted VaR models in commercial banks, Historical simulation (HS) uses the empirical distribution of daily returns in a historical window as the forecast distribution of risk factor returns in the next day. The objectives for financial time series generation are to generate synthetic data paths with good variety, and similar distribution and dynamics to the original historical data. In this paper, we apply multiple existing deep generative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for conditional time series generation, and propose and test two new methods for conditional multi-step time series generation, namely Encoder-Decoder CGAN and Conditional TimeVAE. Furthermore, we introduce a comprehensive framework with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#29305;&#21035;&#24212;&#29992;&#20110;&#35774;&#35745;RISC&#22788;&#29702;&#22120;&#12290;&#32463;&#23454;&#39564;&#35777;&#23454;&#65292;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#23384;&#22312;&#26174;&#33879;&#38169;&#35823;&#65292;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#26469;&#20462;&#22797;&#12290;LLM&#21487;&#29992;&#20110;&#36741;&#21161;&#31243;&#24207;&#21592;&#30340;&#20195;&#30721;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.10364</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;ChatGPT&#30340;LLM&#35774;&#35745;&#21644;&#23454;&#29616;RISC&#22788;&#29702;&#22120;&#65306;&#25191;&#34892;&#12289;&#25361;&#25112;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations. (arXiv:2401.10364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#29305;&#21035;&#24212;&#29992;&#20110;&#35774;&#35745;RISC&#22788;&#29702;&#22120;&#12290;&#32463;&#23454;&#39564;&#35777;&#23454;&#65292;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#23384;&#22312;&#26174;&#33879;&#38169;&#35823;&#65292;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#26469;&#20462;&#22797;&#12290;LLM&#21487;&#29992;&#20110;&#36741;&#21161;&#31243;&#24207;&#21592;&#30340;&#20195;&#30721;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#29305;&#21035;&#24212;&#29992;&#20110;&#35774;&#35745;RISC&#22788;&#29702;&#22120;&#12290;&#25991;&#31456;&#36824;&#22238;&#39038;&#20102;&#19982;&#20195;&#30721;&#29983;&#25104;&#30456;&#20851;&#30340;&#27493;&#39588;&#65292;&#22914;&#35299;&#26512;&#12289;&#26631;&#35760;&#21270;&#12289;&#32534;&#30721;&#12289;&#27880;&#24847;&#26426;&#21046;&#12289;&#25277;&#26679;&#20197;&#21450;&#20195;&#30721;&#29983;&#25104;&#26399;&#38388;&#30340;&#36845;&#20195;&#12290;&#36890;&#36807;&#27979;&#35797;&#24179;&#21488;&#21644;FPGA&#26495;&#19978;&#30340;&#30828;&#20214;&#23454;&#29616;&#39564;&#35777;&#20102;&#29983;&#25104;&#30340;RISC&#32452;&#20214;&#30340;&#20195;&#30721;&#12290;&#20351;&#29992;&#20102;&#22235;&#20010;&#24230;&#37327;&#21442;&#25968;&#65306;&#31532;&#19968;&#27425;&#36845;&#20195;&#30340;&#27491;&#30830;&#36755;&#20986;&#12289;&#20195;&#30721;&#20013;&#23884;&#20837;&#30340;&#38169;&#35823;&#25968;&#37327;&#12289;&#23454;&#29616;&#20195;&#30721;&#25152;&#38656;&#30340;&#23581;&#35797;&#27425;&#25968;&#20197;&#21450;&#19977;&#27425;&#36845;&#20195;&#21518;&#26080;&#27861;&#29983;&#25104;&#20195;&#30721;&#30340;&#22833;&#36133;&#24773;&#20917;&#65292;&#20197;&#27604;&#36739;&#20351;&#29992;LLM&#32534;&#31243;&#30340;&#25928;&#29575;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#37117;&#23384;&#22312;&#26174;&#33879;&#38169;&#35823;&#65292;&#24635;&#26159;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#20462;&#22797;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;LLM&#21487;&#29992;&#20110;&#36741;&#21161;&#31243;&#24207;&#21592;&#30340;&#20195;&#30721;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the feasibility of using Large Language Models LLM for code generation with a particular application in designing an RISC. The paper also reviews the associated steps such as parsing, tokenization, encoding, attention mechanism, sampling the tokens and iterations during code generation. The generated code for the RISC components is verified through testbenches and hardware implementation on a FPGA board. Four metric parameters Correct output on the first iteration, Number of errors embedded in the code, Number of trials required to achieve the code and Failure to generate the code after three iterations, are used to compare the efficiency of using LLM in programming. In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs. LLM can therefore be used to complement a programmer code design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#36339;&#22522;&#20110;&#38598;&#32676;&#30340;&#36710;&#32852;&#32593;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24179;&#22343;&#30456;&#23545;&#36895;&#24230;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#23545;FL&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#32452;&#21512;&#20197;&#20316;&#20026;&#32858;&#31867;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10361</link><description>&lt;p&gt;
&#22312;&#22810;&#36339;&#22522;&#20110;&#38598;&#32676;&#30340;&#36710;&#32852;&#32593;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs. (arXiv:2401.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#36339;&#22522;&#20110;&#38598;&#32676;&#30340;&#36710;&#32852;&#32593;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24179;&#22343;&#30456;&#23545;&#36895;&#24230;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#23545;FL&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#32452;&#21512;&#20197;&#20316;&#20026;&#32858;&#31867;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36710;&#32852;&#32593;&#20013;&#20351;&#29992;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#22240;&#20026;&#36890;&#36807;&#36890;&#20449;&#26412;&#22320;&#25968;&#25454;&#38598;&#26799;&#24230;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#21487;&#20197;&#20943;&#23569;&#20256;&#36755;&#24320;&#38144;&#24182;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#36710;&#32852;&#32593;&#20013;&#23454;&#26045;FL&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#36890;&#20449;&#36164;&#28304;&#12289;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#22810;&#36339;&#32858;&#31867;&#30340;&#36710;&#32852;&#32593;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;FL&#27169;&#22411;&#21442;&#25968;&#30340;&#24179;&#22343;&#30456;&#23545;&#36895;&#24230;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#21152;&#26435;&#32452;&#21512;&#20316;&#20026;&#32858;&#31867;&#24230;&#37327;&#65292;&#20197;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#39640;&#36710;&#36742;&#31227;&#21160;&#24615;&#12290;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#22330;&#26223;&#20013;&#20445;&#35777;&#26368;&#23567;&#21464;&#21270;&#31751;&#22836;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35299;&#20915;&#19982;&#35813;&#22330;&#26223;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage of federated learning (FL) in Vehicular Ad hoc Networks (VANET) has garnered significant interest in research due to the advantages of reducing transmission overhead and protecting user privacy by communicating local dataset gradients instead of raw data. However, implementing FL in VANETs faces challenges, including limited communication resources, high vehicle mobility, and the statistical diversity of data distributions. In order to tackle these issues, this paper introduces a novel framework for hierarchical federated learning (HFL) over multi-hop clustering-based VANET. The proposed method utilizes a weighted combination of the average relative speed and cosine similarity of FL model parameters as a clustering metric to consider both data diversity and high vehicle mobility. This metric ensures convergence with minimum changes in cluster heads while tackling the complexities associated with non-independent and identically distributed (non-IID) data scenarios. Additionall
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21152;&#23494;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#20013;&#38544;&#34255;&#31192;&#23494;&#36733;&#33655;&#65292;&#19988;&#19981;&#24433;&#21709;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.10360</link><description>&lt;p&gt;
&#23545;&#19981;&#36215;&#65292;&#20808;&#29983;&#65311;&#20320;&#30340;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#27844;&#28431;&#65288;&#20449;&#24687;&#65289;
&lt;/p&gt;
&lt;p&gt;
Excuse me, sir? Your language model is leaking (information). (arXiv:2401.10360v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10360
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21152;&#23494;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#20013;&#38544;&#34255;&#31192;&#23494;&#36733;&#33655;&#65292;&#19988;&#19981;&#24433;&#21709;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21152;&#23494;&#26041;&#27861;&#65292;&#23558;&#20219;&#24847;&#31192;&#23494;&#36733;&#33655;&#38544;&#34255;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21709;&#24212;&#20013;&#12290;&#25552;&#21462;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#36733;&#33655;&#38656;&#35201;&#19968;&#20010;&#31192;&#23494;&#23494;&#38053;&#65292;&#27809;&#26377;&#23494;&#38053;&#26159;&#26080;&#27861;&#21306;&#20998;&#21407;&#22987;LLM&#21644;&#38544;&#34255;&#36733;&#33655;&#30340;LLM&#21709;&#24212;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#19981;&#20250;&#21463;&#21040;&#36733;&#33655;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;Christ&#12289;Gunn&#21644;Zamir&#65288;2023&#65289;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27861;&#26816;&#27979;&#21040;&#30340;LLM&#27700;&#21360;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a cryptographic method to hide an arbitrary secret payload in the response of a Large Language Model (LLM). A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload. In particular, the quality of generated text is not affected by the payload. Our approach extends a recent result of Christ, Gunn and Zamir (2023) who introduced an undetectable watermarking scheme for LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26234;&#33021;&#20248;&#21270;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#32467;&#26500;&#25391;&#21160;&#30340;&#23454;&#39564;&#27979;&#37327;&#20540;&#21644;&#25968;&#20540;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;&#23545;&#31616;&#21333;&#32467;&#26500;&#20013;&#24322;&#24120;&#24773;&#20917;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10355</link><description>&lt;p&gt;
&#20351;&#29992;&#22320;&#38663;&#20449;&#21495;&#36827;&#34892;&#32467;&#26500;&#24322;&#24120;&#26816;&#27979;&#30340;&#26234;&#33021;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Intelligent Optimization and Machine Learning Algorithms for Structural Anomaly Detection using Seismic Signals. (arXiv:2401.10355v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26234;&#33021;&#20248;&#21270;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#32467;&#26500;&#25391;&#21160;&#30340;&#23454;&#39564;&#27979;&#37327;&#20540;&#21644;&#25968;&#20540;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;&#23545;&#31616;&#21333;&#32467;&#26500;&#20013;&#24322;&#24120;&#24773;&#20917;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#26800;&#21270;&#38567;&#36947;&#24037;&#31243;&#20013;&#32570;&#20047;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#36130;&#21153;&#25439;&#22833;&#21644;&#38075;&#21066;&#26102;&#38388;&#30340;&#20111;&#25439;&#12290;&#29616;&#22330;&#25366;&#25496;&#38656;&#35201;&#22312;&#38075;&#25506;&#20043;&#21069;&#35782;&#21035;&#20986;&#30828;&#38556;&#30861;&#29289;&#65292;&#20197;&#36991;&#20813;&#25439;&#22351;&#38567;&#36947;&#25496;&#36827;&#26426;&#24182;&#35843;&#25972;&#20256;&#25773;&#36895;&#24230;&#12290;&#21033;&#29992;&#26234;&#33021;&#20248;&#21270;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#32467;&#26500;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#23558;&#32467;&#26500;&#25391;&#21160;&#30340;&#23454;&#39564;&#27979;&#37327;&#20540;&#19982;&#25968;&#20540;&#27169;&#25311;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#26816;&#27979;&#31616;&#21333;&#32467;&#26500;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of anomaly detection methods during mechanized tunnelling can cause financial loss and deficits in drilling time. On-site excavation requires hard obstacles to be recognized prior to drilling in order to avoid damaging the tunnel boring machine and to adjust the propagation velocity. The efficiency of the structural anomaly detection can be increased with intelligent optimization techniques and machine learning. In this research, the anomaly in a simple structure is detected by comparing the experimental measurements of the structural vibrations with numerical simulations using parameter estimation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PCS&#35843;&#24230;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#39044;&#27979;&#12290;PCS&#20351;&#29992;&#21152;&#26435;&#20844;&#24179;&#38431;&#21015;&#35843;&#24230;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#36741;&#21161;&#30340;&#25628;&#32034;&#31574;&#30053;&#26469;&#23547;&#25214;&#28385;&#36275;&#29305;&#23450;&#30446;&#26631;&#30340;&#37197;&#32622;&#12290;&#36890;&#36807;&#22312;DNN&#20316;&#19994;&#35843;&#24230;&#19978;&#30340;&#23454;&#26045;&#21644;&#35780;&#20272;&#65292;PCS&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#20379;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10354</link><description>&lt;p&gt;
&#20351;&#29992;PCS&#25552;&#20379;&#21487;&#38752;&#30340;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards providing reliable job completion time predictions using PCS. (arXiv:2401.10354v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PCS&#35843;&#24230;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#39044;&#27979;&#12290;PCS&#20351;&#29992;&#21152;&#26435;&#20844;&#24179;&#38431;&#21015;&#35843;&#24230;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#36741;&#21161;&#30340;&#25628;&#32034;&#31574;&#30053;&#26469;&#23547;&#25214;&#28385;&#36275;&#29305;&#23450;&#30446;&#26631;&#30340;&#37197;&#32622;&#12290;&#36890;&#36807;&#22312;DNN&#20316;&#19994;&#35843;&#24230;&#19978;&#30340;&#23454;&#26045;&#21644;&#35780;&#20272;&#65292;PCS&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#20379;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26696;&#20363;&#65292;&#20026;&#20113;&#29992;&#25143;&#25552;&#20379;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#30340;&#39044;&#27979;&#65292;&#31867;&#20284;&#20110;&#21253;&#35065;&#30340;&#36865;&#36798;&#26085;&#26399;&#25110;&#35746;&#36710;&#30340;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25552;&#20379;&#21487;&#39044;&#27979;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#20113;&#35843;&#24230;&#31995;&#32479;&#22312;&#26435;&#34913;&#31354;&#38388;&#20013;&#20248;&#21270;&#26497;&#31471;&#28857;&#65292;&#20351;&#20854;&#35201;&#20040;&#26497;&#19981;&#21487;&#39044;&#27979;&#65292;&#35201;&#20040;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PCS&#65292;&#19968;&#20010;&#26032;&#30340;&#35843;&#24230;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20379;&#21487;&#39044;&#27979;&#24615;&#30340;&#21516;&#26102;&#24179;&#34913;&#20854;&#20182;&#20256;&#32479;&#30446;&#26631;&#12290;PCS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#21152;&#26435;&#20844;&#24179;&#38431;&#21015;&#35843;&#24230;&#65288;WFQ&#65289;&#65292;&#25214;&#21040;&#36866;&#24403;&#30340;WFQ&#21442;&#25968;&#37197;&#32622;&#65288;&#20363;&#22914;&#31867;&#21035;&#26435;&#37325;&#65289;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#21487;&#39044;&#27979;&#24615;&#30446;&#26631;&#12290;&#23427;&#20351;&#29992;&#27169;&#25311;&#36741;&#21161;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#22788;&#20110;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#26435;&#34913;&#31354;&#38388;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;WFQ&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;DNN&#20316;&#19994;&#35843;&#24230;&#30340;GP&#19978;&#23454;&#26045;&#21644;&#35780;&#20272;PCS&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we build a case for providing job completion time predictions to cloud users, similar to the delivery date of a package or arrival time of a booked ride. Our analysis reveals that providing predictability can come at the expense of performance and fairness. Existing cloud scheduling systems optimize for extreme points in the trade-off space, making them either extremely unpredictable or impractical.  To address this challenge, we present PCS, a new scheduling framework that aims to provide predictability while balancing other traditional objectives. The key idea behind PCS is to use Weighted-Fair-Queueing (WFQ) and find a suitable configuration of different WFQ parameters (e.g., class weights) that meets specific goals for predictability. It uses a simulation-aided search strategy, to efficiently discover WFQ configurations that lie on the Pareto front of the trade-off space between these objectives. We implement and evaluate PCS in the context of DNN job scheduling on GP
&lt;/p&gt;</description></item><item><title>MELODY&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#30340;&#24322;&#26500;&#24615;&#12289;&#20302;&#24310;&#36831;&#23481;&#24525;&#24230;&#12289;&#27169;&#31946;&#30340;&#24322;&#24120;&#23450;&#20041;&#21644;&#26377;&#38480;&#30340;&#30417;&#30563;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10338</link><description>&lt;p&gt;
MELODY: &#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online Anomaly Detection with Multivariate Time Series. (arXiv:2401.10338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10338
&lt;/p&gt;
&lt;p&gt;
MELODY&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#30340;&#24322;&#26500;&#24615;&#12289;&#20302;&#24310;&#36831;&#23481;&#24525;&#24230;&#12289;&#27169;&#31946;&#30340;&#24322;&#24120;&#23450;&#20041;&#21644;&#26377;&#38480;&#30340;&#30417;&#30563;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;IT&#31995;&#32479;&#20013;&#65292;&#36719;&#20214;&#37096;&#32626;&#26159;&#22312;&#32447;&#26381;&#21153;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20195;&#30721;&#32463;&#24120;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26377;&#25925;&#38556;&#30340;&#20195;&#30721;&#26356;&#25913;&#21487;&#33021;&#20250;&#38477;&#20302;&#30446;&#26631;&#26381;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19979;&#28216;&#26381;&#21153;&#20013;&#24341;&#36215;&#36830;&#38145;&#25925;&#38556;&#12290;&#22240;&#27492;&#65292;&#24212;&#20840;&#38754;&#30417;&#27979;&#36719;&#20214;&#37096;&#32626;&#65292;&#24182;&#21450;&#26102;&#26816;&#27979;&#20986;&#24322;&#24120;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#32626;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19982;&#35813;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#30456;&#20851;&#30340;&#23454;&#20307;&#32423;&#21035;&#65288;&#20363;&#22914;&#65292;&#37096;&#32626;&#65289;&#30456;&#23545;&#20110;&#26356;&#20856;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#21253;&#25324;&#37096;&#32626;&#30340;&#24322;&#26500;&#24615;&#12289;&#20302;&#24310;&#36831;&#23481;&#24525;&#24230;&#12289;&#27169;&#31946;&#30340;&#24322;&#24120;&#23450;&#20041;&#21644;&#26377;&#38480;&#30340;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#30340;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#65288;MELODY&#65289;&#12290;MELODY&#39318;&#20808;&#23558;&#19981;&#21516;&#23454;&#20307;&#30340;MTS&#36716;&#25442;&#20026;&#30456;&#21516;&#30340;&#24418;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
In large IT systems, software deployment is a crucial process in online services as their code is regularly updated. However, a faulty code change may degrade the target service's performance and cause cascading outages in downstream services. Thus, software deployments should be comprehensively monitored, and their anomalies should be detected timely. In this paper, we study the problem of anomaly detection for deployments. We begin by identifying the challenges unique to this anomaly detection problem, which is at entity-level (e.g., deployments), relative to the more typical problem of anomaly detection in multivariate time series (MTS). The unique challenges include the heterogeneity of deployments, the low latency tolerance, the ambiguous anomaly definition, and the limited supervision. To address them, we propose a novel framework, semi-supervised hybrid Model for Entity-Level Online Detection of anomalY (MELODY). MELODY first transforms the MTS of different entities to the same 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>DrugAssist&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#23376;&#20248;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#23454;&#29616;&#20248;&#21270;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10334</link><description>&lt;p&gt;
DrugAssist&#65306;&#19968;&#20010;&#29992;&#20110;&#20998;&#23376;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DrugAssist: A Large Language Model for Molecule Optimization. (arXiv:2401.10334v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10334
&lt;/p&gt;
&lt;p&gt;
DrugAssist&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#23376;&#20248;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#23454;&#29616;&#20248;&#21270;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#23581;&#35797;&#23558;LLMs&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#27969;&#31243;&#20013;&#65292;&#20998;&#23376;&#20248;&#21270;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21442;&#19982;&#24456;&#23569;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25429;&#25417;&#25968;&#25454;&#20013;&#25552;&#20379;&#30340;&#21270;&#23398;&#32467;&#26500;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#19987;&#23478;&#21453;&#39304;&#12290;&#36825;&#20123;&#38750;&#20132;&#20114;&#24335;&#26041;&#27861;&#24573;&#35270;&#20102;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#23454;&#38469;&#19978;&#38656;&#35201;&#19987;&#23478;&#32463;&#39564;&#21644;&#36845;&#20195;&#25913;&#36827;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DrugAssist&#65292;&#19968;&#20010;&#36890;&#36807;&#20154;&#26426;&#23545;&#35805;&#21033;&#29992;LLM&#30340;&#24378;&#20132;&#20114;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20998;&#23376;&#20248;&#21270;&#30340;&#20132;&#20114;&#24335;&#27169;&#22411;&#12290;DrugAssist&#22312;&#21333;&#19968;&#21644;&#22810;&#20010;&#24615;&#36136;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense pot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#38544;&#24335;&#21453;&#39304;&#20013;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#20854;&#20013;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#23618;&#26469;&#25506;&#32034;&#39640;&#38454;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#34920;&#31034;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10316</link><description>&lt;p&gt;
&#22312;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#19978;&#36890;&#36807;&#22810;&#20219;&#21153;&#25552;&#21319;&#21333;&#31867;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving One-class Recommendation with Multi-tasking on Various Preference Intensities. (arXiv:2401.10316v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#38544;&#24335;&#21453;&#39304;&#20013;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#20854;&#20013;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#23618;&#26469;&#25506;&#32034;&#39640;&#38454;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#34920;&#31034;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#31867;&#25512;&#33616;&#38382;&#39064;&#20013;&#65292;&#38656;&#35201;&#26681;&#25454;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#26469;&#36827;&#34892;&#25512;&#33616;&#65292;&#35813;&#21453;&#39304;&#26159;&#36890;&#36807;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#19981;&#34892;&#20026;&#36827;&#34892;&#25512;&#26029;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#20132;&#20114;&#26469;&#33719;&#21462;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#38544;&#24335;&#21453;&#39304;&#20013;&#30340;&#25152;&#26377;&#31215;&#26497;&#20449;&#21495;&#37117;&#21453;&#26144;&#20102;&#22266;&#23450;&#30340;&#20559;&#22909;&#24378;&#24230;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#29992;&#36825;&#20123;&#26041;&#27861;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21453;&#26144;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#30340;&#20449;&#24687;&#24615;&#23454;&#20307;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#38544;&#24335;&#21453;&#39304;&#20013;&#27599;&#20010;&#20449;&#21495;&#30340;&#19981;&#21516;&#20559;&#22909;&#24378;&#24230;&#12290;&#23454;&#20307;&#30340;&#34920;&#31034;&#38656;&#35201;&#21516;&#26102;&#28385;&#36275;&#27599;&#20010;&#23376;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#20351;&#20854;&#26356;&#21152;&#31283;&#20581;&#21644;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#23618;&#24341;&#20837;&#21040;&#29992;&#25143;-&#29289;&#21697;&#30340;&#39640;&#38454;&#20851;&#31995;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the one-class recommendation problem, it's required to make recommendations basing on users' implicit feedback, which is inferred from their action and inaction. Existing works obtain representations of users and items by encoding positive and negative interactions observed from training data. However, these efforts assume that all positive signals from implicit feedback reflect a fixed preference intensity, which is not realistic. Consequently, representations learned with these methods usually fail to capture informative entity features that reflect various preference intensities.  In this paper, we propose a multi-tasking framework taking various preference intensities of each signal from implicit feedback into consideration. Representations of entities are required to satisfy the objective of each subtask simultaneously, making them more robust and generalizable. Furthermore, we incorporate attentive graph convolutional layers to explore high-order relationships in the user-item
&lt;/p&gt;</description></item><item><title>LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.10314</link><description>&lt;p&gt;
LangProp: &#19968;&#31181;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LangProp: A code optimization framework using Language Models applied to driving. (arXiv:2401.10314v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10314
&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30417;&#30563;/&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#36845;&#20195;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#34429;&#28982;LLM&#33021;&#22815;&#38646;-shot&#22320;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#21021;&#22987;&#20195;&#30721;&#21487;&#33021;&#22312;&#26576;&#20123;&#36793;&#32536;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;LangProp&#33258;&#21160;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#20195;&#30721;&#24615;&#33021;&#65292;&#24182;&#25429;&#25417;&#20219;&#20309;&#24322;&#24120;&#65292;&#24182;&#23558;&#32467;&#26524;&#21453;&#39304;&#32473;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20351;LLM&#21487;&#20197;&#36845;&#20195;&#25913;&#36827;&#20854;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24230;&#37327;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#33539;&#24335;&#26469;&#36827;&#34892;&#20195;&#30721;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20511;&#37492;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#27169;&#20223;&#23398;&#20064;&#12289;DAgger&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#30340;&#31532;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;LangProp&#21487;&#20197;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#39550;&#39542;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
LangProp is a framework for iteratively optimizing code generated by large language models (LLMs) in a supervised/reinforcement learning setting. While LLMs can generate sensible solutions zero-shot, the solutions are often sub-optimal. Especially for code generation tasks, it is likely that the initial code will fail on certain edge cases. LangProp automatically evaluates the code performance on a dataset of input-output pairs, as well as catches any exceptions, and feeds the results back to the LLM in the training loop, so that the LLM can iteratively improve the code it generates. By adopting a metricand data-driven training paradigm for this code optimization procedure, one could easily adapt findings from traditional machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA, showing that LangProp can generate interpretable and transparent dri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#20010;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23613;&#31649;&#22270;&#20687;&#22320;&#22270;&#23545;&#20110;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#21487;&#33021;&#21482;&#26377;&#36731;&#24494;&#30340;&#36129;&#29486;&#65292;&#20294;&#20351;&#29992;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#21046;&#20316;&#30340;&#19981;&#21487;&#26816;&#27979;&#30340;&#22270;&#20687;&#22320;&#22270;&#25200;&#21160;&#21487;&#20197;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#22823;&#24133;&#22686;&#21152;&#65292;&#20174;&#32780;&#30772;&#22351;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10313</link><description>&lt;p&gt;
&#40657;&#23458;&#25915;&#20987;&#39044;&#27979;&#22120;&#24847;&#21619;&#30528;&#40657;&#23458;&#25915;&#20987;&#27773;&#36710;&#65306;&#21033;&#29992;&#25935;&#24863;&#24615;&#20998;&#26512;&#35782;&#21035;&#33258;&#21160;&#39550;&#39542;&#23433;&#20840;&#20013;&#30340;&#36712;&#36857;&#39044;&#27979;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to Identify Trajectory Prediction Vulnerabilities for Autonomous Driving Security. (arXiv:2401.10313v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#20010;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23613;&#31649;&#22270;&#20687;&#22320;&#22270;&#23545;&#20110;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#21487;&#33021;&#21482;&#26377;&#36731;&#24494;&#30340;&#36129;&#29486;&#65292;&#20294;&#20351;&#29992;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#21046;&#20316;&#30340;&#19981;&#21487;&#26816;&#27979;&#30340;&#22270;&#20687;&#22320;&#22270;&#25200;&#21160;&#21487;&#20197;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#22823;&#24133;&#22686;&#21152;&#65292;&#20174;&#32780;&#30772;&#22351;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22522;&#20110;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#24050;&#32463;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#20294;&#26159;&#20851;&#20110;&#38500;&#20102;&#29366;&#24577;&#21382;&#21490;&#20197;&#22806;&#30340;&#36712;&#36857;&#39044;&#27979;&#22120;&#36755;&#20837;&#30340;&#25200;&#21160;&#25928;&#26524;&#20197;&#21450;&#36825;&#20123;&#25915;&#20987;&#23545;&#19979;&#28216;&#35268;&#21010;&#21644;&#25511;&#21046;&#30340;&#24433;&#21709;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;Trajectron++&#21644;AgentFormer&#36827;&#34892;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#25152;&#26377;&#30340;&#36755;&#20837;&#20013;&#65292;Trajectron++&#30340;&#20960;&#20046;&#25152;&#26377;&#25200;&#21160;&#25935;&#24863;&#24615;&#20165;&#38480;&#20110;&#26368;&#36817;&#30340;&#29366;&#24577;&#21382;&#21490;&#26102;&#38388;&#28857;&#65292;&#32780;AgentFormer&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#21017;&#20998;&#24067;&#22312;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29366;&#24577;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#23613;&#31649;&#23545;&#29366;&#24577;&#21382;&#21490;&#30340;&#25200;&#21160;&#20855;&#26377;&#20027;&#23548;&#30340;&#25935;&#24863;&#24615;&#65292;&#20294;&#20351;&#29992;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#21046;&#20316;&#30340;&#19981;&#21487;&#26816;&#27979;&#30340;&#22270;&#20687;&#22320;&#22270;&#25200;&#21160;&#21487;&#20197;&#23548;&#33268;&#36825;&#20004;&#20010;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#35823;&#24046;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22270;&#20687;&#22320;&#22270;&#23545;&#20110;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#21487;&#33021;&#21482;&#26377;&#36731;&#24494;&#30340;&#36129;&#29486;&#65292;&#20294;&#36825;&#20010;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#25200;&#21160;&#22270;&#20687;&#22320;&#22270;&#26469;&#30772;&#22351;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on learning-based trajectory predictors have already been demonstrated. However, there are still open questions about the effects of perturbations on trajectory predictor inputs other than state histories, and how these attacks impact downstream planning and control. In this paper, we conduct a sensitivity analysis on two trajectory prediction models, Trajectron++ and AgentFormer. We observe that between all inputs, almost all of the perturbation sensitivities for Trajectron++ lie only within the most recent state history time point, while perturbation sensitivities for AgentFormer are spread across state histories over time. We additionally demonstrate that, despite dominant sensitivity on state history perturbations, an undetectable image map perturbation made with the Fast Gradient Sign Method can induce large prediction error increases in both models. Even though image maps may contribute slightly to the prediction output of both models, this result reveals that
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#21644;&#21496;&#27861;&#32422;&#26463;&#19979;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#23398;&#31639;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31639;&#27861;&#36879;&#26126;&#24615;&#30340;&#35201;&#27714;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#23454;&#29616;&#36879;&#26126;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10310</link><description>&lt;p&gt;
&#22312;&#31038;&#20250;&#21644;&#21496;&#27861;&#32422;&#26463;&#19979;&#23545;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#23398;&#31639;&#27861;&#35774;&#35745;: &#31639;&#27861;&#36879;&#26126;&#24615;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement. (arXiv:2401.10310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#21644;&#21496;&#27861;&#32422;&#26463;&#19979;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#23398;&#31639;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31639;&#27861;&#36879;&#26126;&#24615;&#30340;&#35201;&#27714;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#23454;&#29616;&#36879;&#26126;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#20449;&#24230;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#32570;&#38519;&#65292;&#36825;&#25551;&#36848;&#20102;&#19968;&#31181;&#21487;&#29702;&#35299;&#12289;&#20844;&#24179;&#12289;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#36890;&#36807;&#30417;&#31649;&#25351;&#21335;&#25552;&#20986;&#20102;&#19982;&#21487;&#20449;&#24230;&#30456;&#20851;&#30340;&#26126;&#30830;&#20041;&#21153;&#65292;&#20363;&#22914;,&#22312;&#27431;&#27954;AI&#27861;&#26696;&#20013;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#21487;&#20197;&#23454;&#29616;&#22810;&#22823;&#31243;&#24230;&#19978;&#30340;&#21487;&#20449;&#24230;&#28145;&#24230;&#23398;&#20064;&#12290;&#24314;&#31435;&#26500;&#25104;&#21487;&#20449;&#24230;&#30340;&#25551;&#36848;&#24615;&#23646;&#24615;&#35201;&#27714;&#33021;&#22815;&#36861;&#28335;&#24433;&#21709;&#31639;&#27861;&#35745;&#31639;&#30340;&#22240;&#32032;&#65292;&#21363;&#31639;&#27861;&#30340;&#23454;&#29616;&#26159;&#36879;&#26126;&#30340;&#12290;&#21463;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#38656;&#35201;&#25913;&#21464;&#35745;&#31639;&#25216;&#26415;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#24471;&#20986;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21487;&#33021;&#23454;&#29616;&#36879;&#26126;&#23454;&#26045;&#12290;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#21487;&#20449;&#24230;&#26694;&#26550;&#26469;&#20998;&#26512;&#25968;&#23383;&#21644;&#27169;&#25311;&#35745;&#31639;&#27169;&#22411;&#20013;&#36870;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning still has drawbacks in terms of trustworthiness, which describes a comprehensible, fair, safe, and reliable method. To mitigate the potential risk of AI, clear obligations associated to trustworthiness have been proposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a central question is to what extent trustworthy deep learning can be realized. Establishing the described properties constituting trustworthiness requires that the factors influencing an algorithmic computation can be retraced, i.e., the algorithmic implementation is transparent. Motivated by the observation that the current evolution of deep learning models necessitates a change in computing technology, we derive a mathematical framework which enables us to analyze whether a transparent implementation in a computing model is feasible. We exemplarily apply our trustworthiness framework to analyze deep learning approaches for inverse problems in digital and analog computing models represe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10306</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations. (arXiv:2401.10306v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20004;&#31181;&#31867;&#22411;&#30340;&#21453;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#31243;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#37117;&#26159;&#38750;&#32447;&#24615;&#19988;&#21464;&#21270;&#30340;&#12290;&#22312;&#31532;&#19968;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21463;&#31354;&#38388;&#21464;&#21270;&#30340;&#31995;&#32479;&#35823;&#24046;&#65288;&#21363;&#20559;&#24046;&#65292;&#20063;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65289;&#20559;&#31227;&#30340;&#25968;&#25454;&#12290;&#20219;&#21153;&#26159;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#30495;&#23454;&#29366;&#24577;&#65292;&#21363;PDE&#30340;&#35299;&#12290;&#22312;&#31532;&#20108;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;PDE&#35299;&#30340;&#31232;&#30095;&#20449;&#24687;&#12290;&#20219;&#21153;&#26159;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#31354;&#38388;&#20013;&#30340;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PC-CNN&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#30340;&#26102;&#38388;&#31383;&#21475;&#26041;&#26696;&#32422;&#26463;PDE&#26469;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;PC-CNN&#22312;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#35299;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#20197;&#21450;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#21518;&#32773;&#25551;&#36848;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#26102;&#31354;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a physics-constrained convolutional neural network (PC-CNN) to solve two types of inverse problems in partial differential equations (PDEs), which are nonlinear and vary both in space and time. In the first inverse problem, we are given data that is offset by spatially varying systematic error (i.e., the bias, also known the epistemic uncertainty). The task is to uncover from the biased data the true state, which is the solution of the PDE. In the second inverse problem, we are given sparse information on the solution of a PDE. The task is to reconstruct the solution in space with high-resolution. First, we present the PC-CNN, which constrains the PDE with a simple time-windowing scheme to handle sequential data. Second, we analyse the performance of the PC-CNN for uncovering solutions from biased data. We analyse both linear and nonlinear convection-diffusion equations, and the Navier-Stokes equations, which govern the spatiotemporally chaotic dynamics of turbulent flows. W
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#27963;&#21160;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#20102;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#30740;&#31350;&#25104;&#26524;&#20026;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26080;&#38382;&#21367;&#35843;&#26597;&#30340;&#26041;&#24335;&#23545;&#20010;&#24615;&#30456;&#20851;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25512;&#21160;&#20010;&#24615;&#30740;&#31350;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.10305</link><description>&lt;p&gt;
&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25512;&#26029;&#20010;&#24615;&#29305;&#24449;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning Approach. (arXiv:2401.10305v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#27963;&#21160;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#20102;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#30740;&#31350;&#25104;&#26524;&#20026;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26080;&#38382;&#21367;&#35843;&#26597;&#30340;&#26041;&#24335;&#23545;&#20010;&#24615;&#30456;&#20851;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25512;&#21160;&#20010;&#24615;&#30740;&#31350;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#27963;&#21160;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#20010;&#24615;&#29305;&#24449;&#12290;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#35760;&#24405;&#21644;&#36816;&#21160;&#27169;&#24335;&#35745;&#31639;&#24471;&#20986;&#19968;&#32452;&#30693;&#24773;&#25351;&#26631;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20004;&#31867;&#38382;&#39064;&#19978;&#39044;&#27979;&#29992;&#25143;&#30340;&#20010;&#24615;&#29305;&#24449;&#65292;&#36798;&#21040;&#20102;0.78&#30340;F1&#20998;&#25968;&#12290;&#37492;&#20110;&#25163;&#26426;&#25910;&#38598;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#20010;&#24615;&#29305;&#24449;&#25351;&#26631;&#20026;&#31038;&#20250;&#31185;&#23398;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;&#34892;&#20026;&#27169;&#24335;&#65292;&#23427;&#20204;&#34987;&#35777;&#26126;&#26377;&#24046;&#24322;&#24615;&#22320;&#39044;&#27979;&#20102;&#20116;&#22823;&#20154;&#26684;&#29305;&#24449;&#12290;&#23427;&#20204;&#26377;&#28508;&#21147;&#20197;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26080;&#38382;&#21367;&#35843;&#26597;&#30340;&#26041;&#24335;&#65292;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#19978;&#30740;&#31350;&#19982;&#20010;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#20016;&#23500;&#34892;&#20026;&#25968;&#25454;&#30340;&#32452;&#21512;&#22914;&#20309;&#24110;&#21161;&#25512;&#21160;&#20010;&#24615;&#30740;&#31350;&#65292;&#24182;&#19988;&#21487;&#20197;&#21521;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study provides evidence that personality can be reliably predicted from activity data collected through mobile phone sensors. Employing a set of well informed indicators calculable from accelerometer records and movement patterns, we were able to predict users' personality up to a 0.78 F1 score on a two class problem. Given the fast growing number of data collected from mobile phones, our novel personality indicators open the door to exciting avenues for future research in social sciences. Our results reveal distinct behavioral patterns that proved to be differentially predictive of big five personality traits. They potentially enable cost effective, questionnaire free investigation of personality related questions at an unprecedented scale. Overall, this paper shows how a combination of rich behavioral data obtained with smartphone sensing and the use of machine learning techniques can help to advance personality research and can inform both practitioners and researchers about th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2401.10304</link><description>&lt;p&gt;
&#35770;&#31185;&#23398;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#27491;&#21644;&#36879;&#26126;&#20351;&#29992;&#30340;&#20934;&#22791;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning. (arXiv:2401.10304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#26368;&#36817;&#30340;&#31435;&#27861;&#20030;&#25514;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#30456;&#20851;&#30740;&#31350;&#25351;&#20986;&#38656;&#35201;&#35760;&#24405;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#37325;&#22797;&#24615;&#65292;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#20849;&#20139;&#23454;&#36341;&#36817;&#24180;&#26469;&#20063;&#26377;&#20102;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#23398;&#26415;&#26426;&#26500;&#37319;&#29992;&#20102;&#36825;&#20123;&#23454;&#36341;&#65292;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#25216;&#26415;&#25991;&#20214;&#21457;&#24067;&#22312;&#21516;&#34892;&#35780;&#35758;&#30340;&#20986;&#29256;&#29289;&#19978;&#65292;&#22914;&#25968;&#25454;&#35770;&#25991;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;4041&#31687;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#35770;&#25991;&#26679;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35780;&#20272;&#20854;&#23436;&#25972;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#30740;&#31350;&#20102;&#36817;&#24180;&#26469;&#30340;&#36235;&#21183;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#22810;&#21644;&#26368;&#23569;&#34987;&#35760;&#24405;&#30340;&#26041;&#38754;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#25968;&#25454;&#21019;&#24314;&#32773;&#30340;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the fairness and trustworthiness of machine learning (ML) systems, recent legislative initiatives and relevant research in the ML community have pointed out the need to document the data used to train ML models. Besides, data-sharing practices in many scientific domains have evolved in recent years for reproducibility purposes. In this sense, the adoption of these practices by academic institutions has encouraged researchers to publish their data and technical documentation in peer-reviewed publications such as data papers. In this study, we analyze how this scientific data documentation meets the needs of the ML community and regulatory bodies for its use in ML technologies. We examine a sample of 4041 data papers of different domains, assessing their completeness and coverage of the requested dimensions, and trends in recent years, putting special emphasis on the most and least documented dimensions. As a result, we propose a set of recommendation guidelines for data creato
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10300</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#26816;&#27979;&#30340;&#20855;&#26377;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#20998;&#23618;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems. (arXiv:2401.10300v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#20132;&#20114;&#20195;&#29702;&#32452;&#25104;&#30340;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#65288;CAS&#65289;&#20013;&#65292;&#29616;&#35937;&#26159;&#19968;&#31181;&#20840;&#23616;&#23646;&#24615;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#32593;&#32476;&#23618;&#27425;&#30340;&#20132;&#36890;&#25317;&#22581;&#12290;&#26816;&#27979;&#23427;&#30340;&#24418;&#25104;&#21644;&#28040;&#25955;&#26377;&#21161;&#20110;&#30417;&#27979;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#24182;&#21457;&#20986;&#26377;&#23475;&#29616;&#35937;&#30340;&#35686;&#25253;&#20449;&#21495;&#12290;&#30001;&#20110;CAS&#27809;&#26377;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#65292;&#22522;&#20110;&#27599;&#20010;&#20195;&#29702;&#30340;&#23616;&#37096;&#35266;&#23519;&#26469;&#26816;&#27979;&#29616;&#35937;&#26159;&#21487;&#21462;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#19981;&#33021;&#25429;&#25417;&#19982;&#29616;&#35937;&#30456;&#20851;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#19988;&#26080;&#27861;&#24314;&#27169;&#20195;&#29702;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#34920;&#31034;&#21644;&#20195;&#29702;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#22120;&#38024;&#23545;&#20195;&#29702;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#31995;&#32479;&#30340;&#22797;&#26434;&#28436;&#21270;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36890;&#36807;&#20445;&#30041;&#26368;&#26032;100&#20010;&#20195;&#29702;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#29366;&#24577;&#26469;&#23398;&#20064;&#20195;&#29702;&#21644;&#31995;&#32479;&#30340;&#34920;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent's local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Especially, spatio-temporal encoders are tailored to capture agents' nonlinear relationships and the system's complex evolution. Representations of the agents and the system are learned by preserving the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#29983;&#25104;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#32500;&#30697;&#38453;&#34892;&#21015;&#24335;&#35745;&#31639;&#21644;&#31070;&#32463;&#32593;&#32476;&#21487;&#36870;&#21464;&#25442;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10299</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27969;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of generative flow. (arXiv:2401.10299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#29983;&#25104;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#32500;&#30697;&#38453;&#34892;&#21015;&#24335;&#35745;&#31639;&#21644;&#31070;&#32463;&#32593;&#32476;&#21487;&#36870;&#21464;&#25442;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20171;&#32461;&#19981;&#21516;&#20998;&#24067;&#20043;&#38388;&#30340;&#22352;&#26631;&#21644;&#27010;&#29575;&#21464;&#25442;&#30340;&#31034;&#20363;&#65292;&#31616;&#26126;&#25212;&#35201;&#22320;&#20171;&#32461;&#20102;&#24402;&#19968;&#21270;&#27969;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#20174;&#38543;&#26426;&#21464;&#37327;&#20989;&#25968;&#30340;&#20998;&#24067;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#27010;&#29575;&#21464;&#25442;&#30340;&#26412;&#36136;&#65292;&#24182;&#24341;&#20837;&#20102;&#27010;&#29575;&#21464;&#25442;&#30340;&#32553;&#25918;&#22240;&#23376;&#38597;&#21487;&#27604;&#34892;&#21015;&#24335;&#12290;&#23558;&#25968;&#25454;&#38598;&#35270;&#20026;&#26469;&#33258;&#24635;&#20307;&#30340;&#26679;&#26412;&#65292;&#33719;&#21462;&#24402;&#19968;&#21270;&#27969;&#26412;&#36136;&#19978;&#26159;&#36890;&#36807;&#37319;&#26679;&#35843;&#26597;&#23545;&#24635;&#20307;&#30340;&#25968;&#20540;&#29305;&#24449;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#24314;&#31435;&#25439;&#22833;&#20989;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24402;&#19968;&#21270;&#27969;&#22914;&#20309;&#24039;&#22937;&#22320;&#35299;&#20915;&#20102;&#39640;&#32500;&#30697;&#38453;&#34892;&#21015;&#24335;&#35745;&#31639;&#21644;&#31070;&#32463;&#32593;&#32476;&#21487;&#36870;&#21464;&#25442;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#25361;&#25112;&#12290;&#21033;&#29992;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;&#29983;&#25104;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through examples of coordinate and probability transformation between different distributions, the basic principle of normalizing flow is introduced in a simple and concise manner. From the perspective of the distribution of random variable function, the essence of probability transformation is explained, and the scaling factor Jacobian determinant of probability transformation is introduced. Treating the dataset as a sample from the population, obtaining normalizing flow is essentially through sampling surveys to statistically infer the numerical features of the population, and then the loss function is established by using the maximum likelihood estimation method. This article introduces how normalizing flow cleverly solves the two major application challenges of high-dimensional matrix determinant calculation and neural network reversible transformation. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge, constr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#27979;&#37327;&#26469;&#23545;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#65292;&#21457;&#29616;&#34913;&#37327;&#24490;&#29615;&#28857;&#23494;&#24230;&#30340;&#29305;&#24449;&#26159;&#26368;&#30456;&#20851;&#30340;&#65292;&#19988;&#35757;&#32451;&#21518;&#30340;&#31639;&#27861;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#21160;&#21147;&#23398;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.10298</link><description>&lt;p&gt;
&#20174;&#24490;&#29615;&#27979;&#37327;&#20013;&#26816;&#27979;&#21160;&#21147;&#23398;&#29366;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning approach to detect dynamical states from recurrence measures. (arXiv:2401.10298v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10298
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#27979;&#37327;&#26469;&#23545;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#65292;&#21457;&#29616;&#34913;&#37327;&#24490;&#29615;&#28857;&#23494;&#24230;&#30340;&#29305;&#24449;&#26159;&#26368;&#30456;&#20851;&#30340;&#65292;&#19988;&#35757;&#32451;&#21518;&#30340;&#31639;&#27861;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#21160;&#21147;&#23398;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#21033;&#29992;&#24490;&#29615;&#27979;&#37327;&#26469;&#23545;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#21508;&#31181;&#21160;&#21147;&#23398;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#23454;&#26045;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36923;&#36753;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#36755;&#20837;&#29305;&#24449;&#26159;&#20174;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#24490;&#29615;&#37327;&#21270;&#21644;&#23545;&#24212;&#30340;&#24490;&#29615;&#32593;&#32476;&#30340;&#29305;&#24449;&#37327;&#24471;&#20986;&#30340;&#12290;&#23545;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#25105;&#20204;&#20174;&#26631;&#20934;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20026;&#21608;&#26399;&#24615;&#65292;&#28151;&#27788;&#65292;&#36229;&#28151;&#27788;&#25110;&#22122;&#22768;&#31867;&#21035;&#26041;&#38754;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20998;&#31867;&#26041;&#26696;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#26174;&#33879;&#24615;&#65292;&#24182;&#21457;&#29616;&#34913;&#37327;&#24490;&#29615;&#28857;&#23494;&#24230;&#30340;&#29305;&#24449;&#26368;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;&#31639;&#27861;&#22914;&#20309;&#25104;&#21151;&#39044;&#27979;d
&lt;/p&gt;
&lt;p&gt;
We integrate machine learning approaches with nonlinear time series analysis, specifically utilizing recurrence measures to classify various dynamical states emerging from time series. We implement three machine learning algorithms Logistic Regression, Random Forest, and Support Vector Machine for this study. The input features are derived from the recurrence quantification of nonlinear time series and characteristic measures of the corresponding recurrence networks. For training and testing we generate synthetic data from standard nonlinear dynamical systems and evaluate the efficiency and performance of the machine learning algorithms in classifying time series into periodic, chaotic, hyper-chaotic, or noisy categories. Additionally, we explore the significance of input features in the classification scheme and find that the features quantifying the density of recurrence points are the most relevant. Furthermore, we illustrate how the trained algorithms can successfully predict the d
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#38750;&#36828;&#35265;&#21151;&#29575;&#20998;&#37197;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20020;&#26102;&#32422;&#26463;&#19979;&#30340;&#24178;&#25200;&#32593;&#32476;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#21151;&#29575;&#20998;&#37197;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#32422;&#26463;&#30340;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10297</link><description>&lt;p&gt;
&#22312;&#32422;&#26463;&#22330;&#26223;&#20013;&#23398;&#20064;&#38750;&#36828;&#35265;&#30340;&#21151;&#29575;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Learning Non-myopic Power Allocation in Constrained Scenarios. (arXiv:2401.10297v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10297
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#38750;&#36828;&#35265;&#21151;&#29575;&#20998;&#37197;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20020;&#26102;&#32422;&#26463;&#19979;&#30340;&#24178;&#25200;&#32593;&#32476;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#21151;&#29575;&#20998;&#37197;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#32422;&#26463;&#30340;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20020;&#26102;&#32422;&#26463;&#19979;&#30340;&#33258;&#32452;&#24178;&#25200;&#32593;&#32476;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#21151;&#29575;&#20998;&#37197;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#30636;&#26102;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#32473;&#23450;&#32593;&#32476;&#25928;&#29992;&#24230;&#37327;&#30340;&#26368;&#20248;&#21151;&#29575;&#20998;&#37197;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#33719;&#24471;&#24555;&#36895;&#12289;&#26377;&#25928;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25972;&#20010;&#26102;&#38388;&#32806;&#21512;&#32422;&#26463;&#19979;&#65292;&#38656;&#35201;&#20248;&#21270;&#25972;&#20010;&#24773;&#33410;&#19979;&#30340;&#25928;&#29992;&#24230;&#37327;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#35843;&#33410;&#30636;&#26102;&#21151;&#29575;&#65292;&#20197;&#20415;&#22312;&#28385;&#36275;&#25152;&#26377;&#26102;&#21051;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#20248;&#21270;&#32473;&#23450;&#30340;&#25928;&#29992;&#24230;&#12290;&#29420;&#31435;&#22320;&#35299;&#20915;&#27599;&#20010;&#23454;&#20363;&#23558;&#26159;&#36828;&#35265;&#30340;&#65292;&#22240;&#20026;&#38271;&#26399;&#32422;&#26463;&#26080;&#27861;&#35843;&#33410;&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#20854;&#26694;&#26550;&#21270;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#33719;&#24471;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#26368;&#20248;&#21151;&#29575;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learning-based framework for efficient power allocation in ad hoc interference networks under episodic constraints. The problem of optimal power allocation -- for maximizing a given network utility metric -- under instantaneous constraints has recently gained significant popularity. Several learnable algorithms have been proposed to obtain fast, effective, and near-optimal performance. However, a more realistic scenario arises when the utility metric has to be optimized for an entire episode under time-coupled constraints. In this case, the instantaneous power needs to be regulated so that the given utility can be optimized over an entire sequence of wireless network realizations while satisfying the constraint at all times. Solving each instance independently will be myopic as the long-term constraint cannot modulate such a solution. Instead, we frame this as a constrained and sequential decision-making problem, and employ an actor-critic algorithm to obtain the constrain
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;DP-SGD&#32452;&#32423;&#21035;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#20351;&#29992;&#27850;&#26494;&#25277;&#26679;&#25110;&#22266;&#23450;&#25209;&#37327;&#22823;&#23567;&#25277;&#26679;&#26102;&#26159;&#32039;&#23494;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.10294</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#26426;&#21046;&#30340;DP-SGD&#25277;&#26679;&#30340;DP&#32423;&#21035;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of Gaussians Mechanisms. (arXiv:2401.10294v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;DP-SGD&#32452;&#32423;&#21035;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#20351;&#29992;&#27850;&#26494;&#25277;&#26679;&#25110;&#22266;&#23450;&#25209;&#37327;&#22823;&#23567;&#25277;&#26679;&#26102;&#26159;&#32039;&#23494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;DP-SGD&#30340;&#32452;&#32423;&#21035;$(\epsilon, \delta)$-DP&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24403;&#20351;&#29992;&#27850;&#26494;&#25277;&#26679;&#25110;&#22266;&#23450;&#25209;&#37327;&#22823;&#23567;&#25277;&#26679;&#26102;&#12290;&#22312;&#23454;&#29616;&#20013;&#65292;&#38500;&#20102;&#31163;&#25955;&#21270;&#38169;&#35823;&#20043;&#22806;&#65292;&#36890;&#36807;&#27492;&#36807;&#31243;&#35745;&#31639;&#30340;DP&#38480;&#21046;&#26159;&#32039;&#23494;&#30340;&#65288;&#20551;&#35774;&#25105;&#20204;&#21457;&#24067;&#20102;&#27599;&#20010;&#20013;&#38388;&#36845;&#20195;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a procedure for computing group-level $(\epsilon, \delta)$-DP guarantees for DP-SGD, when using Poisson sampling or fixed batch size sampling. Up to discretization errors in the implementation, the DP guarantees computed by this procedure are tight (assuming we release every intermediate iterate).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22122;&#22768;&#23384;&#22312;&#19979;EQNN&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#31216;&#24615;&#30772;&#32570;&#38543;&#30528;&#23618;&#25968;&#21644;&#22122;&#22768;&#24378;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#30340;&#29616;&#35937;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22686;&#24378;EQNN&#27169;&#22411;&#23545;&#31216;&#24615;&#20445;&#25252;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.10293</link><description>&lt;p&gt;
&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;
&lt;/p&gt;
&lt;p&gt;
Symmetry breaking in geometric quantum machine learning in the presence of noise. (arXiv:2401.10293v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22122;&#22768;&#23384;&#22312;&#19979;EQNN&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#31216;&#24615;&#30772;&#32570;&#38543;&#30528;&#23618;&#25968;&#21644;&#22122;&#22768;&#24378;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#30340;&#29616;&#35937;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22686;&#24378;EQNN&#27169;&#22411;&#23545;&#31216;&#24615;&#20445;&#25252;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNN&#65289;&#30340;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26368;&#36817;&#25104;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#20173;&#38480;&#20110;&#29702;&#35770;&#65292;&#24182;&#19988;&#23578;&#26410;&#25506;&#32034;EQNN&#35757;&#32451;&#20013;&#30828;&#20214;&#22122;&#22768;&#30340;&#20316;&#29992;&#12290;&#26412;&#24037;&#20316;&#30740;&#31350;&#20102;EQNN&#27169;&#22411;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#23450;&#30340;EQNN&#27169;&#22411;&#21487;&#20197;&#22312;Pauli&#36890;&#36947;&#19979;&#20445;&#25345;&#31561;&#21464;&#24615;&#65292;&#32780;&#22312;&#38459;&#23612;&#36890;&#36947;&#19979;&#19981;&#21487;&#33021;&#23454;&#29616;&#12290;&#25105;&#20204;&#22768;&#31216;&#23545;&#31216;&#24615;&#30772;&#32570;&#38543;&#30528;&#23618;&#25968;&#21644;&#22122;&#22768;&#24378;&#24230;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#21644;&#39640;&#36798;64&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#30828;&#20214;&#23454;&#39564;&#25968;&#25454;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#26029;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;EQNN&#27169;&#22411;&#23545;&#31216;&#24615;&#20445;&#25252;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric quantum machine learning based on equivariant quantum neural networks (EQNN) recently appeared as a promising direction in quantum machine learning. Despite the encouraging progress, the studies are still limited to theory, and the role of hardware noise in EQNN training has never been explored. This work studies the behavior of EQNN models in the presence of noise. We show that certain EQNN models can preserve equivariance under Pauli channels, while this is not possible under the amplitude damping channel. We claim that the symmetry breaking grows linearly in the number of layers and noise strength. We support our claims with numerical data from simulations as well as hardware up to 64 qubits. Furthermore, we provide strategies to enhance the symmetry protection of EQNN models in the presence of noise.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#38752;&#22320;&#23613;&#26089;&#39044;&#27979;&#25152;&#26377;&#31867;&#22411;&#30340;&#22320;&#30913;&#26292;&#12290;&#36890;&#36807;&#34701;&#21512;&#20840;&#29699;&#22810;&#20010;&#22320;&#38754;&#31449;&#25910;&#38598;&#30340;&#20851;&#20110;&#22826;&#38451;&#27979;&#37327;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#29305;&#24449;&#36873;&#25321;&#21644;&#25968;&#25454;&#38477;&#37319;&#26679;&#30340;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#26041;&#27861;&#23545;&#36739;&#23567;&#30340;&#22320;&#30913;&#26292;&#23454;&#20363;&#36827;&#34892;&#22788;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;82.55%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10290</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#22320;&#30913;&#26292;&#30340;&#26089;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Early Prediction of Geomagnetic Storms by Machine Learning Algorithms. (arXiv:2401.10290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#38752;&#22320;&#23613;&#26089;&#39044;&#27979;&#25152;&#26377;&#31867;&#22411;&#30340;&#22320;&#30913;&#26292;&#12290;&#36890;&#36807;&#34701;&#21512;&#20840;&#29699;&#22810;&#20010;&#22320;&#38754;&#31449;&#25910;&#38598;&#30340;&#20851;&#20110;&#22826;&#38451;&#27979;&#37327;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#29305;&#24449;&#36873;&#25321;&#21644;&#25968;&#25454;&#38477;&#37319;&#26679;&#30340;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#26041;&#27861;&#23545;&#36739;&#23567;&#30340;&#22320;&#30913;&#26292;&#23454;&#20363;&#36827;&#34892;&#22788;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;82.55%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#30913;&#26292;&#26159;&#25351;&#22826;&#38451;&#39118;&#25200;&#21160;&#22320;&#29699;&#30913;&#23618;&#25152;&#24341;&#36215;&#30340;&#29616;&#35937;&#12290;&#22320;&#30913;&#26292;&#21487;&#33021;&#23545;&#21355;&#26143;&#12289;&#30005;&#32593;&#21644;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#36896;&#25104;&#20005;&#37325;&#25439;&#23475;&#12290;&#32654;&#22269;&#27599;&#22825;&#22823;&#35268;&#27169;&#22320;&#30913;&#26292;&#30340;&#32463;&#27982;&#24433;&#21709;&#36229;&#36807;400&#20159;&#32654;&#20803;&#12290;&#26089;&#26399;&#39044;&#27979;&#23545;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28798;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#21482;&#33021;&#25552;&#21069;&#20960;&#23567;&#26102;&#39044;&#27979;&#20294;&#26080;&#27861;&#35782;&#21035;&#25152;&#26377;&#31867;&#22411;&#30340;&#22320;&#30913;&#26292;&#65292;&#35201;&#20040;&#21482;&#33021;&#22312;&#22320;&#30913;&#26292;&#21457;&#29983;&#21069;&#30340;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#27979;&#65292;&#20363;&#22914;&#25552;&#21069;&#19968;&#23567;&#26102;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#38752;&#22320;&#23613;&#26089;&#39044;&#27979;&#25152;&#26377;&#31867;&#22411;&#30340;&#22320;&#30913;&#26292;&#12290;&#36890;&#36807;&#34701;&#21512;&#20840;&#29699;&#22810;&#20010;&#22320;&#38754;&#31449;&#25910;&#38598;&#30340;&#20851;&#20110;&#22826;&#38451;&#27979;&#37327;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#29305;&#24449;&#36873;&#25321;&#21644;&#25968;&#25454;&#38477;&#37319;&#26679;&#30340;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#26041;&#27861;&#23545;&#36739;&#23567;&#30340;&#22320;&#30913;&#26292;&#23454;&#20363;&#36827;&#34892;&#22788;&#29702;&#65288;&#36825;&#20123;&#23454;&#20363;&#21344;&#25454;&#20102;&#22823;&#22810;&#25968;&#25968;&#25454;&#65289;&#65292;&#25105;&#20204;&#22312;2021&#24180;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;82.55%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geomagnetic storms (GS) occur when solar winds disrupt Earth's magnetosphere. GS can cause severe damages to satellites, power grids, and communication infrastructures. Estimate of direct economic impacts of a large scale GS exceeds $40 billion a day in the US. Early prediction is critical in preventing and minimizing the hazards. However, current methods either predict several hours ahead but fail to identify all types of GS, or make predictions within short time, e.g., one hour ahead of the occurrence. This work aims to predict all types of geomagnetic storms reliably and as early as possible using big data and machine learning algorithms. By fusing big data collected from multiple ground stations in the world on different aspects of solar measurements and using Random Forests regression with feature selection and downsampling on minor geomagnetic storm instances (which carry majority of the data), we are able to achieve an accuracy of 82.55% on data collected in 2021 when making ear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20809;&#30005;&#31070;&#32463;&#22788;&#29702;&#22120;&#65292;&#29992;&#20110;&#27169;&#25311;&#32463;&#22270;&#20687;&#26816;&#27979;&#35757;&#32451;&#36807;&#30340;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#21512;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20809;&#36951;&#20256;&#23398;&#23454;&#29616;&#31934;&#30830;&#28608;&#27963;&#30340;&#21453;&#21521;&#20256;&#25773;STDP&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10289</link><description>&lt;p&gt;
&#20809;&#30005;&#31070;&#32463;&#22788;&#29702;&#22120;&#30340;&#35774;&#35745;&#19982;&#24320;&#21457;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#26426;&#22120;&#20154;&#20013;&#23454;&#29616;&#32463;&#22270;&#20687;&#26816;&#27979;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics. (arXiv:2401.10289v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20809;&#30005;&#31070;&#32463;&#22788;&#29702;&#22120;&#65292;&#29992;&#20110;&#27169;&#25311;&#32463;&#22270;&#20687;&#26816;&#27979;&#35757;&#32451;&#36807;&#30340;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#21512;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20809;&#36951;&#20256;&#23398;&#23454;&#29616;&#31934;&#30830;&#28608;&#27963;&#30340;&#21453;&#21521;&#20256;&#25773;STDP&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#12289;&#36816;&#21160;&#25511;&#21046;&#12289;&#29289;&#20307;&#26816;&#27979;&#31561;&#21508;&#31181;&#22788;&#29702;&#24212;&#29992;&#20013;&#12290;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20302;&#21151;&#32791;&#12289;&#26356;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#21644;&#29983;&#29289;&#29616;&#23454;&#24615;&#30340;&#20248;&#21183;&#12290;&#20809;&#36951;&#20256;&#23398;&#20026;&#29983;&#29289;&#31070;&#32463;&#20803;&#25552;&#20379;&#39640;&#24230;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#25511;&#21046;&#65292;&#24182;&#22312;&#35757;&#32451;&#27963;&#29983;&#29983;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#26377;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20809;&#36951;&#20256;&#23398;&#23454;&#29616;&#31934;&#30830;&#28608;&#27963;&#30340;&#21453;&#21521;&#20256;&#25773;STDP&#31639;&#27861;&#38388;&#25509;&#35757;&#32451;&#30340;&#27169;&#25311;&#27963;&#29983;&#29983;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20934;&#30830;&#24230;&#21487;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.
&lt;/p&gt;</description></item><item><title>CLAN&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#23545;&#25361;&#25112;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#24050;&#30693;&#27963;&#21160;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10288</link><description>&lt;p&gt;
CLAN:&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29992;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CLAN: A Contrastive Learning based Novelty Detection Framework for Human Activity Recognition. (arXiv:2401.10288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10288
&lt;/p&gt;
&lt;p&gt;
CLAN&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#23545;&#25361;&#25112;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#24050;&#30693;&#27963;&#21160;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#36741;&#21161;&#29983;&#27963;&#20013;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20027;&#35201;&#38598;&#20013;&#20110;&#39044;&#23450;&#20041;&#30340;&#27963;&#21160;&#65292;&#24448;&#24448;&#24573;&#30053;&#20102;&#26032;&#30340;&#27963;&#21160;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CLAN&#65292;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#36127;&#26679;&#26412;&#23545;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#35813;&#26694;&#26550;&#38024;&#23545;&#20154;&#20307;&#27963;&#21160;&#29305;&#24449;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#21253;&#25324;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12289;&#22797;&#26434;&#30340;&#27963;&#21160;&#21160;&#24577;&#12289;&#27963;&#21160;&#20043;&#38388;&#20849;&#20139;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#20256;&#24863;&#22120;&#27169;&#24577;&#30340;&#21464;&#21270;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#26500;&#24314;&#23545;&#25361;&#25112;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#24050;&#30693;&#27963;&#21160;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#20026;&#20102;&#29983;&#25104;&#21512;&#36866;&#30340;&#36127;&#26679;&#26412;&#23545;&#65292;&#23427;&#26681;&#25454;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#23545;&#27604;&#21644;&#20998;&#31867;&#25439;&#22833;&#30340;&#34920;&#31034;&#23398;&#20064;&#20197;&#21450;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#65292;&#20174;&#20013;&#23548;&#20986;&#38024;&#23545;&#26080;&#24847;&#20041;&#21160;&#24577;&#30340;&#20851;&#38190;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ambient assisted living, human activity recognition from time series sensor data mainly focuses on predefined activities, often overlooking new activity patterns. We propose CLAN, a two-tower contrastive learning-based novelty detection framework with diverse types of negative pairs for human activity recognition. It is tailored to challenges with human activity characteristics, including the significance of temporal and frequency features, complex activity dynamics, shared features across activities, and sensor modality variations. The framework aims to construct invariant representations of known activity robust to the challenges. To generate suitable negative pairs, it selects data augmentation methods according to the temporal and frequency characteristics of each dataset. It derives the key representations against meaningless dynamics by contrastive and classification losses-based representation learning and score function-based novelty detection that accommodate dynamic number
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;FermiNet&#27169;&#22411;&#38598;&#25104;&#21040;&#24320;&#28304;&#24211;DeepChem&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#23376;&#30340;&#30005;&#23376;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10287</link><description>&lt;p&gt;
&#20855;&#26377;&#31163;&#23376;&#30005;&#33655;&#21021;&#22987;&#21270;&#30340;&#24320;&#28304;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Open-Source Fermionic Neural Networks with Ionic Charge Initialization. (arXiv:2401.10287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;FermiNet&#27169;&#22411;&#38598;&#25104;&#21040;&#24320;&#28304;&#24211;DeepChem&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#23376;&#30340;&#30005;&#23376;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#29616;&#37325;&#35201;&#30340;&#20998;&#23376;&#21644;&#26448;&#26009;&#33021;&#37327;&#21644;&#29305;&#24615;&#26041;&#38754;&#65292;&#31934;&#30830;&#27714;&#35299;&#30005;&#23376;&#34203;&#23450;&#35860;&#26041;&#31243;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#30005;&#23376;&#30340;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;(VMC)&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;&#19968;&#31181;&#21517;&#20026;FermiNet&#30340;&#27169;&#22411;&#65292;&#19968;&#31181;&#21518;&#21704;&#29305;&#37324;-&#31119;&#20811;(HF)&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#65292;&#38598;&#25104;&#21040;&#19968;&#20010;&#26631;&#20934;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#24320;&#28304;&#24211;DeepChem&#20013;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#20197;&#20811;&#26381;&#19982;&#31163;&#23376;&#36229;&#39069;&#25110;&#27424;&#30005;&#23376;&#30340;&#20998;&#37197;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding accurate solutions to the electronic Schr\"odinger equation plays an important role in discovering important molecular and material energies and characteristics. Consequently, solving systems with large numbers of electrons has become increasingly important. Variational Monte Carlo (VMC) methods, especially those approximated through deep neural networks, are promising in this regard. In this paper, we aim to integrate one such model called the FermiNet, a post-Hartree-Fock (HF) Deep Neural Network (DNN) model, into a standard and widely used open source library, DeepChem. We also propose novel initialization techniques to overcome the difficulties associated with the assignment of excess or lack of electrons for ions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;EEG&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;STEM&#27963;&#21160;&#20013;&#30340;&#33041;&#27963;&#21160;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#20219;&#21153;&#30340;&#20998;&#31867;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#21491;&#39069;&#21494;&#22312;&#25968;&#23398;&#22788;&#29702;&#21644;&#35268;&#21010;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24038;&#39069;&#21494;&#22312;&#35748;&#30693;&#28789;&#27963;&#24615;&#21644;&#24515;&#29702;&#28789;&#27963;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24038;&#39070;&#39030;&#21494;&#22312;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#30740;&#31350;&#33041;&#27963;&#21160;&#19982;&#23398;&#20064;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#23545;&#33041;&#27963;&#21160;&#21644;&#23398;&#20064;&#30340;&#29702;&#35299;&#26356;&#28145;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.10285</link><description>&lt;p&gt;
&#20351;&#29992;EEG&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#33041;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Analyzing Brain Activity During Learning Tasks with EEG and Machine Learning. (arXiv:2401.10285v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;EEG&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;STEM&#27963;&#21160;&#20013;&#30340;&#33041;&#27963;&#21160;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#20219;&#21153;&#30340;&#20998;&#31867;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#21491;&#39069;&#21494;&#22312;&#25968;&#23398;&#22788;&#29702;&#21644;&#35268;&#21010;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24038;&#39069;&#21494;&#22312;&#35748;&#30693;&#28789;&#27963;&#24615;&#21644;&#24515;&#29702;&#28789;&#27963;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24038;&#39070;&#39030;&#21494;&#22312;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#30740;&#31350;&#33041;&#27963;&#21160;&#19982;&#23398;&#20064;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#23545;&#33041;&#27963;&#21160;&#21644;&#23398;&#20064;&#30340;&#29702;&#35299;&#26356;&#28145;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#21508;&#31181;STEM&#27963;&#21160;&#20013;&#30340;&#33041;&#27963;&#21160;&#65292;&#25506;&#32034;&#20998;&#31867;&#19981;&#21516;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#25910;&#38598;&#20102;&#20108;&#21313;&#20010;&#21463;&#35797;&#32773;&#21442;&#19982;&#20116;&#31181;&#35748;&#30693;&#20219;&#21153;&#26102;&#30340;EEG&#33041;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#20998;&#21106;&#20026;4&#31186;&#30340;&#29255;&#27573;&#12290;&#28982;&#21518;&#20998;&#26512;&#20102;&#33041;&#39057;&#29575;&#27874;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;XGBoost&#12289;Random Forest&#21644;Bagging Classifier&#27979;&#35797;&#19981;&#21516;&#30340;k&#38388;&#38548;&#65292;&#21457;&#29616;Random Forest&#22312;&#38388;&#38548;&#22823;&#23567;&#20026;&#20004;&#20010;&#26102;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;91.07%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#24403;&#21033;&#29992;&#25152;&#26377;&#22235;&#20010;EEG&#36890;&#36947;&#36827;&#34892;&#20998;&#26512;&#26102;&#65292;&#35748;&#30693;&#28789;&#27963;&#24615;&#26368;&#20026;&#26126;&#26174;&#12290;&#20219;&#21153;&#29305;&#23450;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#31034;&#21491;&#39069;&#21494;&#22312;&#25968;&#23398;&#22788;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24038;&#39069;&#21494;&#22312;&#35748;&#30693;&#28789;&#27963;&#24615;&#21644;&#24515;&#29702;&#28789;&#27963;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24038;&#39070;&#39030;&#21494;&#22312;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;STEM&#27963;&#21160;&#20013;&#35266;&#23519;&#21040;&#20102;&#22823;&#37327;&#39069;&#21494;&#21644;&#39070;&#39030;&#21494;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#33041;&#27963;&#21160;&#21644;&#23398;&#20064;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to analyze brain activity during various STEM activities, exploring the feasibility of classifying between different tasks. EEG brain data from twenty subjects engaged in five cognitive tasks were collected and segmented into 4-second clips. Power spectral densities of brain frequency waves were then analyzed. Testing different k-intervals with XGBoost, Random Forest, and Bagging Classifier revealed that Random Forest performed best, achieving a testing accuracy of 91.07% at an interval size of two. When utilizing all four EEG channels, cognitive flexibility was most recognizable. Task-specific classification accuracy showed the right frontal lobe excelled in mathematical processing and planning, the left frontal lobe in cognitive flexibility and mental flexibility, and the left temporoparietal lobe in connections. Notably, numerous connections between frontal and temporoparietal lobes were observed during STEM activities. This study contributes to a deeper understandi
&lt;/p&gt;</description></item><item><title>MorpheusNet&#26159;&#19968;&#20010;&#36164;&#28304;&#25928;&#29575;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#23454;&#26102;&#39044;&#27979;&#30561;&#30496;&#38454;&#27573;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#65292;&#28040;&#32791;&#23569;&#37327;&#33021;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.10284</link><description>&lt;p&gt;
MorpheusNet&#65306;&#29992;&#20110;&#23884;&#20837;&#24335;&#22312;&#32447;&#31995;&#32479;&#30340;&#36164;&#28304;&#25928;&#29575;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
MorpheusNet: Resource efficient sleep stage classifier for embedded on-line systems. (arXiv:2401.10284v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10284
&lt;/p&gt;
&lt;p&gt;
MorpheusNet&#26159;&#19968;&#20010;&#36164;&#28304;&#25928;&#29575;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#23454;&#26102;&#39044;&#27979;&#30561;&#30496;&#38454;&#27573;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#65292;&#28040;&#32791;&#23569;&#37327;&#33021;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#65288;SSC&#65289;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19987;&#23478;&#26816;&#26597;&#25968;&#23567;&#26102;&#30340;&#30005;&#29983;&#29702;&#35760;&#24405;&#36827;&#34892;&#25163;&#21160;&#20998;&#31867;&#12290;&#36825;&#22312;&#21033;&#29992;&#30561;&#30496;&#38454;&#27573;&#36827;&#34892;&#27835;&#30103;&#30446;&#30340;&#26102;&#26159;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#12290;&#38543;&#30528;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20215;&#26684;&#36234;&#26469;&#36234;&#21487;&#25215;&#21463;&#21644;&#25193;&#24352;&#65292;&#33258;&#21160;&#21270;SSC&#21487;&#33021;&#20351;&#24471;&#35268;&#27169;&#25193;&#22823;&#30340;&#30561;&#30496;&#22522;&#30784;&#27835;&#30103;&#25104;&#20026;&#21487;&#33021;&#12290;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#25163;&#21160;&#19987;&#23478;&#35780;&#20998;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23454;&#26102;&#20998;&#31867;&#21644;&#22312;&#36793;&#32536;&#37096;&#32626;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#33021;&#22815;&#22312;&#23454;&#26102;&#39044;&#27979;&#30561;&#30496;&#38454;&#27573;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#22806;&#37096;&#35745;&#31639;&#36164;&#28304;&#65288;&#20363;&#22914;&#31227;&#21160;&#25163;&#26426;&#12289;&#20113;&#65289;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20302;&#21151;&#32791;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#24335;&#30005;&#27744;&#20379;&#30005;&#31995;&#32479;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts to examine hours of electrophysiological recordings for manual classification. This is a limiting factor when it comes to leveraging sleep stages for therapeutic purposes. With increasing affordability and expansion of wearable devices, automating SSC may enable deployment of sleep-based therapies at scale. Deep Learning has gained increasing attention as a potential method to automate this process. Previous research has shown accuracy comparable to manual expert scores. However, previous approaches require sizable amount of memory and computational resources. This constrains the ability to classify in real time and deploy models on the edge. To address this gap, we aim to provide a model capable of predicting sleep stages in real-time, without requiring access to external computational sources (e.g., mobile phone, cloud). The algorithm is power efficient to enable use on embedded battery powered systems. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31383;&#21475;&#22534;&#21472;&#30340;&#20803;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#20351;&#29992;&#31383;&#21475;&#21270;&#25216;&#26415;&#26102;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#26631;&#31614;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22522;&#20934;&#20934;&#30830;&#29575;&#20174;89.8%&#25552;&#21319;&#21040;99.0%&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10283</link><description>&lt;p&gt;
&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#30340;&#31383;&#21475;&#22534;&#21472;&#20803;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Window Stacking Meta-Models for Clinical EEG Classification. (arXiv:2401.10283v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31383;&#21475;&#22534;&#21472;&#30340;&#20803;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#20351;&#29992;&#31383;&#21475;&#21270;&#25216;&#26415;&#26102;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#26631;&#31614;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22522;&#20934;&#20934;&#30830;&#29575;&#20174;89.8%&#25552;&#21319;&#21040;99.0%&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31383;&#21475;&#21270;&#26159;&#33041;&#30005;&#22270;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#21644;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#26102;&#20250;&#36935;&#21040;&#19968;&#20010;&#25361;&#25112;&#65306;&#35745;&#31639;&#24320;&#38144;&#38480;&#21046;&#20102;&#23398;&#20064;&#25972;&#20010;&#35760;&#24405;&#25110;&#19968;&#32452;&#35760;&#24405;&#30340;&#20840;&#23616;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#31383;&#21475;&#20174;&#20854;&#29238;&#35760;&#24405;&#32487;&#25215;&#30340;&#26631;&#31614;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#35813;&#31383;&#21475;&#22312;&#21333;&#29420;&#24773;&#20917;&#19979;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#38454;&#27573;&#27169;&#22411;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#38024;&#23545;&#26102;&#38388;&#31383;&#21475;&#21270;&#25968;&#25454;&#32858;&#21512;&#30340;&#20803;&#23398;&#20064;&#21407;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27979;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65306;&#24310;&#38271;&#31383;&#21475;&#21644;&#21033;&#29992;&#37325;&#21472;&#26469;&#22686;&#21152;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Temple University Hospital Abnormal EEG Corpus (TUAB)&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;&#23558;&#22522;&#20934;&#20934;&#30830;&#29575;&#20174;89.8&#65285;&#25552;&#21319;&#21040;99.0&#65285;&#12290;&#36825;&#19968;&#31361;&#30772;&#24615;&#34920;&#29616;&#36229;&#36807;&#20102;&#27492;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#24615;&#33021;&#39044;&#27979;&#65292;&#24182;&#20026;&#20020;&#24202;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Windowing is a common technique in EEG machine learning classification and other time series tasks. However, a challenge arises when employing this technique: computational expense inhibits learning global relationships across an entire recording or set of recordings. Furthermore, the labels inherited by windows from their parent recordings may not accurately reflect the content of that window in isolation. To resolve these issues, we introduce a multi-stage model architecture, incorporating meta-learning principles tailored to time-windowed data aggregation. We further tested two distinct strategies to alleviate these issues: lengthening the window and utilizing overlapping to augment data. Our methods, when tested on the Temple University Hospital Abnormal EEG Corpus (TUAB), dramatically boosted the benchmark accuracy from 89.8 percent to 99.0 percent. This breakthrough performance surpasses prior performance projections for this dataset and paves the way for clinical applications of
&lt;/p&gt;</description></item><item><title>BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10282</link><description>&lt;p&gt;
BioDiffusion&#65306;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10282
&lt;/p&gt;
&lt;p&gt;
BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12289;&#26631;&#31614;&#22797;&#26434;&#24615;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#24178;&#25200;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#32463;&#24120;&#38459;&#30861;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioDiffusion&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#21512;&#25104;&#22810;&#21464;&#37327;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#36827;&#34892;&#20248;&#21270;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;BioDiffusion&#22312;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#26080;&#26465;&#20214;&#12289;&#26631;&#31614;&#26465;&#20214;&#21644;&#20449;&#21495;&#26465;&#20214;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#20026;&#19978;&#36848;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#36827;&#34892;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24378;&#35843;&#20854;&#22312;&#19982;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#24403;&#21069;&#20027;&#27969;&#30340;&#26102;&#38388;&#31995;&#20449;&#24687;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;BioDiffusion&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEGFormer&#30340;&#33041;&#30005;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#30340;&#22797;&#21512;EEG&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#36801;&#31227;&#21644;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#20026;&#33041;&#30005;&#20449;&#21495;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.10278</link><description>&lt;p&gt;
EEGFormer: &#23454;&#29616;&#21487;&#36801;&#31227;&#21644;&#21487;&#35299;&#37322;&#30340;&#22823;&#35268;&#27169;&#33041;&#30005;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model. (arXiv:2401.10278v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEGFormer&#30340;&#33041;&#30005;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#30340;&#22797;&#21512;EEG&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#36801;&#31227;&#21644;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#20026;&#33041;&#30005;&#20449;&#21495;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#22312;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#31561;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#36866;&#29992;&#20110;&#33041;&#20449;&#21495;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#20174;&#30315;&#30187;&#26816;&#27979;&#21040;&#27874;&#24418;&#20998;&#26512;&#31561;&#21508;&#31181;&#30495;&#23454;&#21307;&#23398;&#24212;&#29992;&#20013;&#23384;&#22312;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;EEG&#24314;&#27169;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#24212;&#20110;&#21333;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#27599;&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#19978;&#65292;&#36825;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20016;&#23500;&#30340;&#25968;&#25454;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#23548;&#33268;&#32570;&#20047;&#27867;&#21270;&#24615;&#30340;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#29702;&#35299;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;EEG&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;EEGFormer&#65292;&#23427;&#22312;&#22823;&#35268;&#27169;&#22797;&#21512;EEG&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#36866;&#24212;&#24615;&#33021;&#30340;EEG&#20449;&#21495;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#24473;&#40479;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26420;&#32032;&#36125;&#21494;&#26031;&#20316;&#20026;&#20869;&#37096;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#20998;&#31867;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#29305;&#24449;&#20943;&#23569;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#23453;&#36149;&#35265;&#35299;&#65292;&#20026;&#25991;&#26412;&#20998;&#31867;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10270</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#24473;&#40479;&#20248;&#21270;&#30340;&#25991;&#26412;&#20998;&#31867;&#29305;&#24449;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Migrating Birds Optimization-Based Feature Selection for Text Classification. (arXiv:2401.10270v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#24473;&#40479;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26420;&#32032;&#36125;&#21494;&#26031;&#20316;&#20026;&#20869;&#37096;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#20998;&#31867;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#29305;&#24449;&#20943;&#23569;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#23453;&#36149;&#35265;&#35299;&#65292;&#20026;&#25991;&#26412;&#20998;&#31867;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;MBO-NB&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#21033;&#29992;&#36801;&#24473;&#40479;&#20248;&#21270;&#65288;MBO&#65289;&#19982;&#26420;&#32032;&#36125;&#21494;&#26031;&#20316;&#20026;&#20869;&#37096;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#29305;&#24449;&#36873;&#25321;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#22686;&#30410;&#31639;&#27861;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20174;&#24179;&#22343;62221&#20010;&#29305;&#24449;&#38477;&#20302;&#21040;2089&#20010;&#65292;&#20197;&#20415;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;MBO-NB&#22312;&#29305;&#24449;&#20943;&#23569;&#26041;&#38754;&#20855;&#26377;&#25928;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;&#26420;&#32032;&#36125;&#21494;&#26031;&#23884;&#20837;MBO&#20013;&#30340;&#25104;&#21151;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19982;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#36827;&#34892;&#20010;&#20307;&#23545;&#27604;&#26102;&#65292;MBO-NB&#22312;&#22235;&#31181;&#35774;&#32622;&#20013;&#24179;&#22343;&#34920;&#29616;&#20248;&#20110;6.9&#65285;&#12290;&#26412;&#30740;&#31350;&#20026;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20026;&#25991;&#26412;&#20998;&#31867;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research introduces a novel approach, MBO-NB, that leverages Migrating Birds Optimization (MBO) coupled with Naive Bayes as an internal classifier to address feature selection challenges in text classification having large number of features. Focusing on computational efficiency, we preprocess raw data using the Information Gain algorithm, strategically reducing the feature count from an average of 62221 to 2089. Our experiments demonstrate MBO-NB's superior effectiveness in feature reduction compared to other existing techniques, emphasizing an increased classification accuracy. The successful integration of Naive Bayes within MBO presents a well-rounded solution. In individual comparisons with Particle Swarm Optimization (PSO), MBO-NB consistently outperforms by an average of 6.9% across four setups. This research offers valuable insights into enhancing feature selection methods, providing a scalable and effective solution for text classification
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;</title><link>http://arxiv.org/abs/2401.10266</link><description>&lt;p&gt;
&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;: &#26041;&#27861;&#35770;&#21644;&#19981;&#30830;&#23450;&#24615;&#31649;&#29702;&#31574;&#30053;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies. (arXiv:2401.10266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#30417;&#27979;&#22312;&#29616;&#20195;&#24037;&#19994;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#26085;&#30410;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#20851;&#27880;&#30340;&#22686;&#38271;&#20027;&#39064;&#21644;&#19968;&#31181;&#24378;&#22823;&#30340;&#25925;&#38556;&#35782;&#21035;&#26041;&#24335;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#24320;&#28304;&#22522;&#20934;Tennessee Eastman Process&#65288;TEP&#65289;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#24635;&#32467;&#20102;&#29992;&#20110;&#24037;&#19994;&#21378;&#25151;&#29366;&#24577;&#30417;&#27979;&#12289;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#27599;&#31181;&#31639;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#36824;&#28085;&#30422;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#12289;&#26080;&#26631;&#35760;&#26679;&#26412;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#27604;&#36739;&#20102;&#21033;&#29992;Tennessee Eastman Process&#30340;&#19981;&#21516;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#24180;&#40836;&#24230;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#27010;&#24565;&#21644;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.10265</link><description>&lt;p&gt;
&#26368;&#20339;&#26356;&#26032;&#26102;&#38388;&#65306;&#22522;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#24180;&#40836;&#24230;&#37327;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
The Best Time for an Update: Risk-Sensitive Minimization of Age-Based Metrics. (arXiv:2401.10265v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10265
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#24180;&#40836;&#24230;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#27010;&#24565;&#21644;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#37327;&#21270;&#20256;&#36755;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#20449;&#24687;&#24180;&#40836;&#65288;Age of Information&#65292;AoI&#65289;&#65292;&#26597;&#35810;&#20449;&#24687;&#24180;&#40836;&#65288;Query Age of Information&#65292;QAoI&#65289;&#21644;&#19981;&#27491;&#30830;&#20449;&#24687;&#24180;&#40836;&#65288;Age of Incorrect Information&#65292;AoII&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#28857;&#23545;&#28857;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#65292;&#21457;&#36865;&#26041;&#30417;&#35270;&#19968;&#20010;&#36827;&#31243;&#24182;&#21521;&#25509;&#25910;&#26041;&#21457;&#36865;&#29366;&#24577;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#25361;&#25112;&#26159;&#20915;&#23450;&#26356;&#26032;&#30340;&#26368;&#20339;&#26102;&#38388;&#65292;&#24179;&#34913;&#20256;&#36755;&#33021;&#37327;&#21644;&#25509;&#25910;&#26041;&#30340;&#24180;&#40836;&#24230;&#37327;&#12290;&#30001;&#20110;&#39640;&#24180;&#40836;&#24230;&#37327;&#20540;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#31995;&#32479;&#29366;&#24577;&#31561;&#38382;&#39064;&#30340;&#22266;&#26377;&#39118;&#38505;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#30340;&#27010;&#24565;&#26469;&#34920;&#31034;&#24180;&#40836;&#24230;&#37327;&#39640;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#27010;&#24565;&#26469;&#37327;&#21270;&#21644;&#26368;&#23567;&#21270;&#39640;&#24180;&#40836;&#24230;&#37327;&#30340;&#39118;&#38505;&#65292;&#36890;&#36807;&#30452;&#25509;&#23548;&#20986;&#39118;&#38505;&#29366;&#24577;&#30340;&#39057;&#29575;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#38024;&#23545;AoI&#65292;QAoI&#21644;AoII&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#12290;&#31532;&#19968;&#31181;&#31574;&#30053;&#20351;&#29992;&#31995;&#32479;&#30693;&#35782;&#65292;
&lt;/p&gt;
&lt;p&gt;
Popular methods to quantify transmitted data quality are the Age of Information (AoI), the Query Age of Information (QAoI), and the Age of Incorrect Information (AoII). We consider these metrics in a point-to-point wireless communication system, where the transmitter monitors a process and sends status updates to a receiver. The challenge is to decide on the best time for an update, balancing the transmission energy and the age-based metric at the receiver. Due to the inherent risk of high age-based metric values causing complications such as unstable system states, we introduce the new concept of risky states to denote states with high age-based metric. We use this new notion of risky states to quantify and minimize this risk of experiencing high age-based metrics by directly deriving the frequency of risky states as a novel risk-metric. Building on this foundation, we introduce two risk-sensitive strategies for AoI, QAoI and AoII. The first strategy uses system knowledge, i.e., chann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#24182;&#38024;&#23545;&#27492;&#24369;&#28857;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#38544;&#20889;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10262</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#21450;&#20854;&#22312;&#22270;&#20687;&#38544;&#20889;&#26415;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Null Space Properties of Neural Networks with Applications to Image Steganography. (arXiv:2401.10262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#24182;&#38024;&#23545;&#27492;&#24369;&#28857;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#38544;&#20889;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;&#38646;&#31354;&#38388;&#23450;&#20041;&#20174;&#32447;&#24615;&#26144;&#23556;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#24182;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#38646;&#31354;&#38388;&#30340;&#23384;&#22312;&#12290;&#32473;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#31354;&#38388;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#21738;&#20123;&#36755;&#20837;&#25968;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27809;&#26377;&#20219;&#20309;&#36129;&#29486;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#23427;&#26469;&#27450;&#39575;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#24212;&#29992;&#65292;&#21363;&#22270;&#20687;&#38544;&#20889;&#26415;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#35832;&#22914;MNIST&#20043;&#31867;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38646;&#31354;&#38388;&#25104;&#20998;&#26469;&#24378;&#21046;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#25152;&#36873;&#30340;&#38544;&#34255;&#22270;&#20687;&#31867;&#65292;&#21363;&#20351;&#25972;&#20307;&#22270;&#20687;&#30475;&#36215;&#26469;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26174;&#31034;&#20154;&#31867;&#35266;&#23519;&#32773;&#33021;&#22815;&#30475;&#21040;&#30340;&#20869;&#23481;&#19982;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#35266;&#27979;&#8221;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the null space properties of neural networks. We extend the null space definition from linear to nonlinear maps and discuss the presence of a null space in neural networks. The null space of a given neural network can tell us the part of the input data that makes no contribution to the final prediction so that we can use it to trick the neural network. This reveals an inherent weakness in neural networks that can be exploited. One application described here leads to a method of image steganography. Through experiments on image datasets such as MNIST, we show that we can use null space components to force the neural network to choose a selected hidden image class, even though the overall image can be made to look like a completely different image. We conclude by showing comparisons between what a human viewer would see, and the part of the image that the neural network is actually using to make predictions and, hence, show that what the neural network ``sees'' is com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19978;&#35838;&#31243;&#23398;&#20064;&#65288;Curriculum Learning&#65289;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;CSNN&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#39034;&#24207;&#21644;&#31070;&#32463;&#20803;&#27963;&#21160;&#32534;&#30721;&#26041;&#38754;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10257</link><description>&lt;p&gt;
&#35838;&#31243;&#35774;&#35745;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Curriculum Design Helps Spiking Neural Networks to Classify Time Series. (arXiv:2401.10257v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19978;&#35838;&#31243;&#23398;&#20064;&#65288;Curriculum Learning&#65289;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;CSNN&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#39034;&#24207;&#21644;&#31070;&#32463;&#20803;&#27963;&#21160;&#32534;&#30721;&#26041;&#38754;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22312;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#20855;&#26377;&#26356;&#22823;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#21644;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#26356;&#22909;&#30340;&#32593;&#32476;&#32467;&#26500;&#19978;&#65292;&#24456;&#38590;&#35777;&#26126;&#23427;&#20204;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#22823;&#33041;&#21551;&#21457;&#24335;&#31185;&#23398;&#30340;&#24605;&#24819;&#65292;&#21457;&#29616;&#19981;&#20165;&#32467;&#26500;&#65292;&#23398;&#20064;&#36807;&#31243;&#20063;&#24212;&#35813;&#20687;&#20154;&#31867;&#19968;&#26679;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;CSNN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26426;&#21046;&#25506;&#32034;&#20102;&#35838;&#31243;&#23398;&#20064;(CL)&#22312;SNNs&#19978;&#30340;&#28508;&#21147;&#65306;&#20027;&#21160;-&#20241;&#30496;&#35757;&#32451;&#39034;&#24207;&#20351;&#35838;&#31243;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23398;&#20064;&#21644;&#36866;&#21512;&#33033;&#20914;&#31070;&#32463;&#20803;&#65307;&#22522;&#20110;&#20540;&#30340;&#21306;&#22495;&#32534;&#30721;&#20351;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#22312;&#23398;&#20064;&#36830;&#32493;&#25968;&#25454;&#26102;&#27169;&#25311;&#22823;&#33041;&#30340;&#35760;&#24518;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#28304;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#27169;&#25311;&#12289;&#20256;&#24863;&#22120;&#12289;&#36816;&#21160;&#21644;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have a greater potential for modeling time series data than Artificial Neural Networks (ANNs), due to their inherent neuron dynamics and low energy consumption. However, it is difficult to demonstrate their superiority in classification accuracy, because current efforts mainly focus on designing better network structures. In this work, enlighten by brain-inspired science, we find that, not only the structure but also the learning process should be human-like. To achieve this, we investigate the power of Curriculum Learning (CL) on SNNs by designing a novel method named CSNN with two theoretically guaranteed mechanisms: The active-to-dormant training order makes the curriculum similar to that of human learning and suitable for spiking neurons; The value-based regional encoding makes the neuron activity to mimic the brain memory when learning sequential data. Experiments on multiple time series sources including simulated, sensor, motion, and healthcare dem
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#39532;&#36798;&#21152;&#26031;&#21152;&#30340;&#23439;&#35266;&#32463;&#27982;&#39046;&#20808;&#25351;&#26631;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#38598;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#20256;&#32479;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10255</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#39532;&#36798;&#21152;&#26031;&#21152;&#23454;&#26102;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Nowcasting Madagascar's real GDP using machine learning algorithms. (arXiv:2401.10255v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#39532;&#36798;&#21152;&#26031;&#21152;&#30340;&#23439;&#35266;&#32463;&#27982;&#39046;&#20808;&#25351;&#26631;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#38598;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#20256;&#32479;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#39532;&#36798;&#21152;&#26031;&#21152;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#27969;&#34892;&#30340;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#32447;&#24615;&#27491;&#21017;&#21270;&#22238;&#24402;&#65288;Ridge&#65292;Lasso&#65292;Elastic-net&#65289;&#65292;&#38477;&#32500;&#27169;&#22411;&#65288;&#20027;&#25104;&#20998;&#22238;&#24402;&#65289;&#65292;k&#26368;&#36817;&#37051;&#31639;&#27861;&#65288;k-NN&#22238;&#24402;&#65289;&#65292;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;&#32447;&#24615;SVR&#65289;&#21644;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#65288;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#22238;&#24402;&#65289;&#65292;&#24182;&#23558;&#31616;&#21333;&#30340;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#26469;&#34913;&#37327;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32858;&#21512;&#20010;&#20307;&#39044;&#27979;&#24418;&#25104;&#30340;&#38598;&#25104;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the predictive power of different machine learning algorithms to nowcast Madagascar's gross domestic product (GDP). We trained popular regression models, including linear regularized regression (Ridge, Lasso, Elastic-net), dimensionality reduction model (principal component regression), k-nearest neighbors algorithm (k-NN regression), support vector regression (linear SVR), and tree-based ensemble models (Random forest and XGBoost regressions), on 10 Malagasy quarterly macroeconomic leading indicators over the period 2007Q1--2022Q4, and we used simple econometric models as a benchmark. We measured the nowcast accuracy of each model by calculating the root mean square error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). Our findings reveal that the Ensemble Model, formed by aggregating individual predictions, consistently outperforms traditional econometric models. We conclude that machine learning models can deliver more accurate and timely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#31181;NLP&#25216;&#26415;&#21644;&#35270;&#39057;&#22788;&#29702;&#25216;&#26415;&#23558;&#38271;&#35270;&#39057;&#36716;&#25442;&#20026;&#25688;&#35201;&#35270;&#39057;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35266;&#30475;/&#22238;&#39038;&#35270;&#39057;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10254</link><description>&lt;p&gt;
&#36229;&#36234;&#30011;&#38754;&#65306;&#20351;&#29992;&#29992;&#25143;&#23450;&#20041;&#30340;&#38271;&#24230;&#30340;&#21333;&#20010;&#21644;&#22810;&#20010;&#35270;&#39057;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond the Frame: Single and mutilple video summarization method with user-defined length. (arXiv:2401.10254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#31181;NLP&#25216;&#26415;&#21644;&#35270;&#39057;&#22788;&#29702;&#25216;&#26415;&#23558;&#38271;&#35270;&#39057;&#36716;&#25442;&#20026;&#25688;&#35201;&#35270;&#39057;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35266;&#30475;/&#22238;&#39038;&#35270;&#39057;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#26159;&#19968;&#31181;&#20943;&#23569;&#35270;&#39057;&#35266;&#30475;/&#22238;&#39038;&#26102;&#38388;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#38543;&#30528;&#21457;&#34920;&#30340;&#35270;&#39057;&#25968;&#37327;&#27599;&#22825;&#37117;&#22312;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21487;&#20197;&#20351;&#29992;&#22810;&#31181;&#25216;&#26415;&#65292;&#20174;&#22810;&#27169;&#24577;&#38899;&#35270;&#39057;&#25216;&#26415;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#25110;&#22810;&#20010;&#35270;&#39057;&#24635;&#32467;&#20026;&#30456;&#23545;&#36739;&#30701;&#30340;&#35270;&#39057;&#12290;&#38899;&#35270;&#39057;&#25216;&#26415;&#21487;&#29992;&#20110;&#35782;&#21035;&#37325;&#35201;&#30340;&#35270;&#35273;&#20107;&#20214;&#24182;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#32780;NLP&#25216;&#26415;&#21487;&#29992;&#20110;&#35780;&#20272;&#38899;&#39057;&#36716;&#24405;&#24182;&#20174;&#21407;&#22987;&#35270;&#39057;&#20013;&#25552;&#21462;&#20027;&#35201;&#21477;&#23376;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#30456;&#24212;&#30340;&#35270;&#39057;&#24103;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#21516;&#26102;&#20351;&#29992;&#39046;&#22495;&#20013;&#30340;&#26368;&#20339;&#25216;&#26415;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#38899;&#35270;&#39057;&#32447;&#32034;&#21644;&#35270;&#39057;&#36716;&#24405;&#26469;&#25552;&#21462;&#21644;&#24635;&#32467;&#35270;&#39057;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;NLP&#25216;&#26415;&#65288;&#25277;&#21462;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#65289;&#19982;&#35270;&#39057;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23558;&#38271;&#35270;&#39057;&#36716;&#25442;&#20026;&#25688;&#35201;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video smmarization is a crucial method to reduce the time of videos which reduces the spent time to watch/review a long video. This apporach has became more important as the amount of publisehed video is increasing everyday. A single or multiple videos can be summarized into a relatively short video using various of techniques from multimodal audio-visual techniques, to natural language processing approaches. Audiovisual techniques may be used to recognize significant visual events and pick the most important parts, while NLP techniques can be used to evaluate the audio transcript and extract the main sentences (timestamps) and corresponding video frames from the original video. Another approach is to use the best of both domain. Meaning that we can use audio-visual cues as well as video transcript to extract and summarize the video. In this paper, we combine a variety of NLP techniques (extractive and contect-based summarizers) with video processing techniques to convert a long video 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#21644;&#21487;&#36716;&#31227;&#30340;&#24102;&#23485;&#20998;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;GNN&#21644;HML&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10253</link><description>&lt;p&gt;
&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#65306;&#19968;&#31181;&#29992;&#20110;&#21487;&#25193;&#23637;&#21644;&#21487;&#36716;&#31227;&#24102;&#23485;&#20998;&#37197;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation. (arXiv:2401.10253v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#21644;&#21487;&#36716;&#31227;&#30340;&#24102;&#23485;&#20998;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;GNN&#21644;HML&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24102;&#23485;&#20998;&#37197;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;1&#65289;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65307;2&#65289;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#19979;&#36827;&#34892;&#36716;&#31227;&#65292;&#20363;&#22914;&#38750;&#24179;&#31283;&#30340;&#26080;&#32447;&#20449;&#36947;&#12289;&#19981;&#21516;&#30340;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#21644;&#21160;&#24577;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#25903;&#25345;&#21487;&#25193;&#23637;&#24615;&#65292;&#24102;&#23485;&#20998;&#37197;&#31574;&#30053;&#37319;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#31034;&#65292;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#38543;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#19981;&#21464;&#12290;&#20026;&#20102;&#23454;&#29616;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#65288;HML&#65289;&#31639;&#27861;&#65292;&#22312;&#20803;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#26469;&#35757;&#32451;GNN&#30340;&#21021;&#22987;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23545;GNN&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#36890;&#20449;&#22330;&#26223;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;HML&#26041;&#27861;&#21487;&#20197;&#23558;&#21021;&#22987;&#24615;&#33021;&#25552;&#39640;8.79&#65285;&#65292;&#24182;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;73&#65285;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by $8.79\%$, and sampling efficiency by $73\%$, compared with existing benchmarks. After fine-tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#36776;&#29575;&#33394;&#35889;&#23618;&#26512;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31895;&#31961;&#21040;&#32454;&#33268;&#34892;&#20026;&#21644;&#35774;&#35745;&#26102;&#38388;&#20381;&#36182;&#30340;&#35843;&#21046;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#24212;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10247</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#36776;&#29575;&#33394;&#35889;&#23618;&#26512;
&lt;/p&gt;
&lt;p&gt;
Resolution Chromatography of Diffusion Models. (arXiv:2401.10247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#36776;&#29575;&#33394;&#35889;&#23618;&#26512;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31895;&#31961;&#21040;&#32454;&#33268;&#34892;&#20026;&#21644;&#35774;&#35745;&#26102;&#38388;&#20381;&#36182;&#30340;&#35843;&#21046;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#24212;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#29305;&#21035;&#22320;&#65292;&#21435;&#22122;&#26041;&#27861;&#26159;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#39044;&#27979;&#26679;&#26412;&#20013;&#30340;&#22122;&#22768;&#24182;&#23545;&#20854;&#36827;&#34892;&#21435;&#22122;&#12290;&#24120;&#24120;&#35266;&#23519;&#21040;&#29983;&#25104;&#26679;&#26412;&#30340;&#20998;&#36776;&#29575;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#65292;&#24320;&#22987;&#27169;&#31946;&#21644;&#31895;&#31961;&#65292;&#28982;&#21518;&#21464;&#24471;&#26356;&#21152;&#28165;&#26224;&#21644;&#32454;&#33268;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;"&#20998;&#36776;&#29575;&#33394;&#35889;&#23618;&#26512;"&#30340;&#27010;&#24565;&#65292;&#25351;&#31034;&#20102;&#27599;&#20010;&#20998;&#36776;&#29575;&#30340;&#20449;&#21495;&#29983;&#25104;&#36895;&#29575;&#65292;&#36825;&#23545;&#20110;&#25968;&#23398;&#19978;&#35299;&#37322;&#29983;&#25104;&#36807;&#31243;&#20013;&#31895;&#31890;&#21040;&#32454;&#31890;&#34892;&#20026;&#12289;&#29702;&#35299;&#22122;&#22768;&#26102;&#38388;&#34920;&#30340;&#20316;&#29992;&#20197;&#21450;&#35774;&#35745;&#26102;&#38388;&#20381;&#36182;&#30340;&#35843;&#21046;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#21033;&#29992;&#20998;&#36776;&#29575;&#33394;&#35889;&#23618;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#20013;&#21738;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#21464;&#24471;&#20027;&#23548;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#30452;&#25509;&#24212;&#29992;&#35813;&#27010;&#24565;&#30340;&#26041;&#27861;&#65306;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#21319;&#32423;&#21040;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models generate high-resolution images through iterative stochastic processes. In particular, the denoising method is one of the most popular approaches that predicts the noise in samples and denoises it at each time step. It has been commonly observed that the resolution of generated samples changes over time, starting off blurry and coarse, and becoming sharper and finer. In this paper, we introduce "resolution chromatography" that indicates the signal generation rate of each resolution, which is very helpful concept to mathematically explain this coarse-to-fine behavior in generation process, to understand the role of noise schedule, and to design time-dependent modulation. Using resolution chromatography, we determine which resolution level becomes dominant at a specific time step, and experimentally verify our theory with text-to-image diffusion models. We also propose some direct applications utilizing the concept: upscaling pre-trained models to higher resolutions and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31574;&#30053;&#65292;&#22312;&#21516;&#27493;&#35757;&#32451;&#35821;&#20041;&#19979;&#25104;&#21151;&#23454;&#29616;&#20102;&#38646;&#31649;&#36947;&#27873;&#27819;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#35745;&#31639;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#35774;&#35745;&#20102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#26032;&#39062;&#31649;&#36947;&#35843;&#24230;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25214;&#21040;&#26368;&#20248;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#26032;&#39062;&#25216;&#26415;&#32469;&#36807;&#21516;&#27493;&#25805;&#20316;&#23454;&#29616;&#38646;&#27873;&#27819;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#20284;&#26465;&#20214;&#19979;&#65292;&#26412;&#26041;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#20248;&#20110;1F1B&#35843;&#24230;23%&#12290;</title><link>http://arxiv.org/abs/2401.10241</link><description>&lt;p&gt;
&#38646;&#27873;&#27819;&#31649;&#36947;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zero Bubble Pipeline Parallelism. (arXiv:2401.10241v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31574;&#30053;&#65292;&#22312;&#21516;&#27493;&#35757;&#32451;&#35821;&#20041;&#19979;&#25104;&#21151;&#23454;&#29616;&#20102;&#38646;&#31649;&#36947;&#27873;&#27819;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#35745;&#31639;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#35774;&#35745;&#20102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#26032;&#39062;&#31649;&#36947;&#35843;&#24230;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25214;&#21040;&#26368;&#20248;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#26032;&#39062;&#25216;&#26415;&#32469;&#36807;&#21516;&#27493;&#25805;&#20316;&#23454;&#29616;&#38646;&#27873;&#27819;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#20284;&#26465;&#20214;&#19979;&#65292;&#26412;&#26041;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#20248;&#20110;1F1B&#35843;&#24230;23%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#36947;&#24182;&#34892;&#24615;&#26159;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#25928;&#29575;&#21463;&#21040;&#34987;&#35270;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#31649;&#36947;&#27873;&#27819;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35843;&#24230;&#31574;&#30053;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#22312;&#21516;&#27493;&#35757;&#32451;&#35821;&#20041;&#19979;&#25104;&#21151;&#23454;&#29616;&#38646;&#31649;&#36947;&#27873;&#27819;&#12290;&#36825;&#20010;&#25913;&#36827;&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#21453;&#21521;&#35745;&#31639;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#19968;&#37096;&#20998;&#35745;&#31639;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#21478;&#19968;&#37096;&#20998;&#35745;&#31639;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#25163;&#24037;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#31649;&#36947;&#35843;&#24230;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#26681;&#25454;&#29305;&#23450;&#30340;&#27169;&#22411;&#37197;&#32622;&#21644;&#20869;&#23384;&#38480;&#21046;&#33258;&#21160;&#25214;&#21040;&#26368;&#20248;&#35843;&#24230;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30495;&#27491;&#23454;&#29616;&#38646;&#27873;&#27819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#32469;&#36807;&#21516;&#27493;&#25805;&#20316;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#27604;1F1B&#35843;&#24230;&#39640;&#20986;&#22810;&#36798;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 23% in throughput under a simila
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#21152;&#23494;&#36135;&#24065;&#35770;&#22363;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#20540;&#27874;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;Bitcointalk&#35770;&#22363;&#30340;&#27963;&#21160;&#19982;&#27604;&#29305;&#24065;&#30340;&#36235;&#21183;&#26377;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.10238</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#21644;&#22312;&#32447;&#37329;&#34701;&#35770;&#22363;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interplay between Cryptocurrency Transactions and Online Financial Forums. (arXiv:2401.10238v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21152;&#23494;&#36135;&#24065;&#35770;&#22363;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#20540;&#27874;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;Bitcointalk&#35770;&#22363;&#30340;&#27963;&#21160;&#19982;&#27604;&#29305;&#24065;&#30340;&#36235;&#21183;&#26377;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#26159;&#19968;&#31181;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#25552;&#20379;&#23433;&#20840;&#24615;&#21644;&#21311;&#21517;&#24615;&#30340;&#25968;&#23383;&#36135;&#24065;&#12290;&#23613;&#31649;&#21152;&#23494;&#36135;&#24065;&#20195;&#34920;&#20102;&#19968;&#39033;&#31361;&#30772;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#39118;&#38505;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#30417;&#31649;&#26426;&#26500;&#21644;&#36879;&#26126;&#24230;&#25152;&#23548;&#33268;&#30340;&#12290;&#30001;&#20110;&#34394;&#20551;&#20449;&#24687;&#21644;&#20215;&#26684;&#27874;&#21160;&#20250;&#35753;&#20010;&#20154;&#25237;&#36164;&#32773;&#24863;&#21040;&#27822;&#20007;&#65292;&#22240;&#27492;&#21152;&#23494;&#36135;&#24065;&#30340;&#20986;&#29616;&#19982;&#22312;&#32447;&#29992;&#25143;&#31038;&#21306;&#21644;&#35770;&#22363;&#30340;&#34028;&#21187;&#21457;&#23637;&#21516;&#26102;&#36827;&#34892;&#65292;&#36825;&#20123;&#22320;&#26041;&#21487;&#20197;&#20849;&#20139;&#20449;&#24687;&#20197;&#20943;&#36731;&#29992;&#25143;&#30340;&#19981;&#20449;&#20219;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#21152;&#23494;&#36135;&#24065;&#35770;&#22363;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#20540;&#27874;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26368;&#27969;&#34892;&#30340;&#21152;&#23494;&#36135;&#24065;&#27604;&#29305;&#24065;&#65288;BTC&#65289;&#21644;&#30456;&#20851;&#27963;&#36291;&#30340;&#35752;&#35770;&#31038;&#21306;Bitcointalk&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Bitcointalk&#35770;&#22363;&#30340;&#27963;&#21160;&#19982;BTC&#30340;&#36235;&#21183;&#20445;&#25345;&#30452;&#25509;&#20851;&#31995;&#65292;&#22240;&#27492;&#20998;&#26512;&#36825;&#31181;&#20132;&#20114;&#20316;&#29992;&#23558;&#26159;&#19968;&#20010;&#23436;&#32654;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies are a type of digital money meant to provide security and anonymity while using cryptography techniques. Although cryptocurrencies represent a breakthrough and provide some important benefits, their usage poses some risks that are a result of the lack of supervising institutions and transparency. Because disinformation and volatility is discouraging for personal investors, cryptocurrencies emerged hand-in-hand with the proliferation of online users' communities and forums as places to share information that can alleviate users' mistrust. This research focuses on the study of the interplay between these cryptocurrency forums and fluctuations in cryptocurrency values. In particular, the most popular cryptocurrency Bitcoin (BTC) and a related active discussion community, Bitcointalk, are analyzed. This study shows that the activity of Bitcointalk forum keeps a direct relationship with the trend in the values of BTC, therefore analysis of this interaction would be a perfec
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#35757;&#32451;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#35299;&#20915;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10191</link><description>&lt;p&gt;
&#20998;&#32780;&#19981;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#20013;&#36873;&#25321;&#24615;&#35757;&#32451;&#19987;&#23478;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10191
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#35757;&#32451;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#35299;&#20915;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#27969;&#34892;&#65292;&#27169;&#22411;&#33021;&#22815;&#25299;&#23485;&#24212;&#29992;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#24536;&#35760;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#36235;&#21183;&#26159;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#25216;&#26415;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#20849;&#21516;&#35299;&#20915;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19987;&#23478;&#36890;&#24120;&#20250;&#19968;&#27425;&#24615;&#20351;&#29992;&#25972;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#26679;&#20250;&#22686;&#21152;&#36951;&#24536;&#30340;&#39118;&#38505;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#12290;SEED&#20165;&#36873;&#25321;&#19968;&#20010;&#34987;&#35748;&#20026;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#22788;&#29702;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#23545;&#36825;&#20010;&#19987;&#23478;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#27599;&#20010;&#19987;&#23478;&#29992;&#39640;&#26031;&#20998;&#24067;&#34920;&#31034;&#27599;&#20010;&#31867;&#21035;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#36873;&#25321;&#26368;&#20248;&#19987;&#23478;&#12290;&#22240;&#27492;&#65292;SEED&#22312;&#20445;&#25345;&#38598;&#25104;&#26041;&#27861;&#30340;&#39640;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#22686;&#21152;&#20102;&#19987;&#23478;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SEED&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#23384;&#22312;&#30528;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L$&#30340;&#22686;&#38271;&#19982;$p$&#21644;$\varepsilon$&#30340;&#20851;&#31995;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.09902</link><description>&lt;p&gt;
&#28145;&#24230;&#21644;&#23485;&#24230;&#22312;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#23384;&#22312;&#30528;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L$&#30340;&#22686;&#38271;&#19982;$p$&#21644;$\varepsilon$&#30340;&#20851;&#31995;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(neural ODEs)&#24050;&#32463;&#25104;&#20026;&#20174;&#25511;&#21046;&#35282;&#24230;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#28982;&#24037;&#20855;&#65292;&#28982;&#32780;&#23545;&#20854;&#26368;&#20339;&#32467;&#26500;&#30340;&#23436;&#20840;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23485;&#24230;$p$&#21644;&#23618;&#20043;&#38388;&#30340;&#36807;&#28193;&#27425;&#25968;$L$&#65288;&#23454;&#38469;&#19978;&#26159;&#28145;&#24230;$L+1$&#65289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20197;&#20854;&#33021;&#22815;&#22312;Wasserstein&#35823;&#24046;&#36793;&#30028;$\varepsilon&gt;0$&#20869;&#25554;&#20540;&#19968;&#20010;&#21253;&#21547;$N$&#23545;&#28857;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;$D$&#25110;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#22312;$\mathbb{R}^d$&#20013;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#65292;$L$&#38543;&#30528;$O(1+N/p)$&#30340;&#27604;&#20363;&#22686;&#38271;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L=O\left(1+(p\varepsilon^d)^{-1}\right)$&#12290;&#22312;&#33258;&#20027;&#24773;&#20917;&#19979;&#65292;$L=0$&#65292;&#38656;&#35201;&#36827;&#34892;&#21333;&#29420;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#38598;&#25554;&#20540;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;$\varepsilon$-&#36817;&#20284;&#25511;&#21046;&#24615;&#30340;&#25918;&#26494;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\mathbb{R}^d$ within a Wasserstein error margin $\varepsilon&gt;0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\left(1+(p\varepsilon^d)^{-1}\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\varepsilon$-approximate controllability and establish an error 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20999;&#29255;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#37096;&#32626;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#21152;&#23494;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#31363;&#21462;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20998;&#21106;&#24494;&#35843;&#21644;&#31232;&#30095;&#21270;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#35774;&#22791;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09796</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#12289;&#39640;&#24615;&#33021;&#12289;&#23433;&#20840;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Fast, Performant, Secure Distributed Training Framework For Large Language Model. (arXiv:2401.09796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20999;&#29255;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#37096;&#32626;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#21152;&#23494;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#31363;&#21462;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20998;&#21106;&#24494;&#35843;&#21644;&#31232;&#30095;&#21270;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#35774;&#22791;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#65288;&#32852;&#37030;&#24335;&#65289;LLM&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#38548;&#31163;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#22320;&#20174;&#26381;&#21153;&#22120;&#25110;&#23458;&#25143;&#31471;&#31363;&#21462;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20999;&#29255;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;LLM&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#37117;&#37096;&#32626;&#20102;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#65292;&#24182;&#23558;&#24494;&#35843;&#30340;&#32467;&#26500;&#65288;LoRA&#25110;P-tuning v2&#30340;&#23884;&#20837;&#65289;&#25918;&#20837;TEE&#20013;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#21152;&#23494;&#65292;&#22312;TEE&#21644;&#36890;&#29992;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#35774;&#22791;&#25104;&#26412;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#24494;&#35843;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#23618;&#27425;&#36827;&#34892;LLM&#30340;&#20998;&#21106;&#65292;&#23558;&#21518;&#38754;&#30340;&#23618;&#27425;&#25918;&#22312;&#26381;&#21153;&#22120;&#31471;TEE&#20013;&#65288;&#23458;&#25143;&#31471;&#19981;&#38656;&#35201;TEE&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#31232;&#30095;&#21270;&#21442;&#25968;&#24494;&#35843;&#65288;SPF&#65289;&#19982;LoRA&#37096;&#20998;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the down
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.09691</link><description>&lt;p&gt;
&#23558;&#22270;&#20687;&#29305;&#24449;&#36755;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#24182;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#22312;&#20351;&#29992;&#30701;&#37319;&#26679;&#21608;&#26399;&#26102;&#26080;&#24847;&#20013;&#24573;&#30053;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#36755;&#20986;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#32435;&#20837;&#21040;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#21644;&#20851;&#33410;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#31616;&#21333;&#30340;&#25342;&#21462;&#25918;&#32622;&#25805;&#20316;&#30340;&#23454;&#39564;&#65292;&#21363;&#20351;&#22788;&#29702;&#26469;&#33258;&#30701;&#37319;&#26679;&#21608;&#26399;&#30340;&#25968;&#25454;&#65292;&#20063;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
&lt;/p&gt;</description></item><item><title>CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;</title><link>http://arxiv.org/abs/2401.08897</link><description>&lt;p&gt;
CFASL&#65306;&#29992;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#30340;&#22797;&#21512;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08897
&lt;/p&gt;
&lt;p&gt;
CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#21644;&#28508;&#22312;&#21521;&#37327;&#30340;&#23545;&#31216;&#24615;&#20026;VAE&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29978;&#33267;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20063;&#38656;&#35201;&#24050;&#30693;&#30340;&#22240;&#23376;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Composite Factor-Aligned Symmetry Learning (CFASL)&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;VAE&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;CFASL&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#30340;&#26032;&#29305;&#24449;&#65306;1)&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#23558;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#23545;&#40784;&#21040;&#26126;&#30830;&#21487;&#23398;&#20064;&#30340;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65307;2)&#23398;&#20064;&#19968;&#20010;&#22797;&#21512;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65292;&#26469;&#34920;&#36798;&#20004;&#20010;&#38543;&#26426;&#26679;&#26412;&#20043;&#38388;&#30340;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65307;3)&#22312;&#35757;&#32451;VAE&#26102;&#65292;&#24341;&#20837;&#20855;&#26377;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#20004;&#20010;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#39564;&#35777;&#22320;&#20174;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#27602;&#21270;&#25915;&#20987;&#20013;&#24674;&#22797;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#20197;&#21450;&#26410;&#21463;&#24694;&#24847;&#23458;&#25143;&#31471;&#24433;&#21709;&#30340;&#21382;&#21490;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#24694;&#24847;&#23458;&#25143;&#31471;&#34987;&#35782;&#21035;&#21518;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.08216</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#39564;&#35777;&#22320;&#20174;&#27602;&#21270;&#25915;&#20987;&#20013;&#24674;&#22797;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning. (arXiv:2401.08216v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#39564;&#35777;&#22320;&#20174;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#27602;&#21270;&#25915;&#20987;&#20013;&#24674;&#22797;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#20197;&#21450;&#26410;&#21463;&#24694;&#24847;&#23458;&#25143;&#31471;&#24433;&#21709;&#30340;&#21382;&#21490;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#24694;&#24847;&#23458;&#25143;&#31471;&#34987;&#35782;&#21035;&#21518;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#27602;&#21270;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#24694;&#24847;&#23458;&#25143;&#31471;&#36890;&#36807;&#25805;&#32437;&#26356;&#26032;&#26469;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#12290;&#30446;&#21069;&#23384;&#22312;&#21508;&#31181;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#36825;&#20123;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#20294;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#38656;&#35201;&#36275;&#22815;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#22240;&#27492;&#19968;&#26086;&#26816;&#27979;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#27745;&#26579;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#22312;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#21518;&#24674;&#22797;&#20934;&#30830;&#20840;&#23616;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#24674;&#22797;&#26041;&#27861;&#20381;&#36182;&#20110;&#65288;i&#65289;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#21644;&#65288;ii&#65289;&#26410;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#24433;&#21709;&#30340;&#21021;&#22987;&#27169;&#22411;&#65292;&#23548;&#33268;&#23545;&#23384;&#20648;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#24456;&#39640;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36873;&#25321;&#24615;&#21382;&#21490;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#20197;&#21450;&#22522;&#20110;&#26410;&#34987;&#24694;&#24847;&#23458;&#25143;&#31471;&#26174;&#33879;&#24433;&#21709;&#30340;&#21382;&#21490;&#27169;&#22411;&#32780;&#19981;&#26159;&#21021;&#22987;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model. Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned. Thus, a method is needed to recover an accurate global model after malicious clients are identified. Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources. In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model. In this scenario, while maintaini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Vision Transformer&#20013;&#27880;&#24847;&#21147;&#22270;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27880;&#24847;&#21147;&#20316;&#20026;&#21487;&#38752;&#30340;&#23450;&#37327;&#35777;&#25454;&#25351;&#26631;&#29992;&#20110;&#20915;&#31574;&#65292;&#24182;&#36890;&#36807;p&#20540;&#36827;&#34892;&#32479;&#35745;&#26174;&#33879;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.08169</link><description>&lt;p&gt;
Vision Transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#22270;&#32479;&#35745;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Attention Map in Vision Transformer. (arXiv:2401.08169v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Vision Transformer&#20013;&#27880;&#24847;&#21147;&#22270;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27880;&#24847;&#21147;&#20316;&#20026;&#21487;&#38752;&#30340;&#23450;&#37327;&#35777;&#25454;&#25351;&#26631;&#29992;&#20110;&#20915;&#31574;&#65292;&#24182;&#36890;&#36807;p&#20540;&#36827;&#34892;&#32479;&#35745;&#26174;&#33879;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#65288;ViT&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#27880;&#24847;&#21147;&#23545;&#20110;ViT&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#20043;&#38388;&#22797;&#26434;&#24191;&#27867;&#30340;&#20851;&#31995;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#26435;&#34913;&#22270;&#20687;&#34917;&#19969;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;ViT&#30340;&#27880;&#24847;&#21147;&#29992;&#20316;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#65288;&#22914;&#21307;&#23398;&#35786;&#26029;&#65289;&#20013;&#30340;&#35777;&#25454;&#26102;&#65292;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#27880;&#24847;&#26426;&#21046;&#21487;&#33021;&#38169;&#35823;&#22320;&#20851;&#27880;&#26080;&#20851;&#30340;&#21306;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;ViT&#27880;&#24847;&#21147;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#27880;&#24847;&#21147;&#20316;&#20026;&#21487;&#38752;&#30340;&#23450;&#37327;&#35777;&#25454;&#25351;&#26631;&#29992;&#20110;ViT&#30340;&#20915;&#31574;&#65292;&#24182;&#20005;&#26684;&#25511;&#21046;&#35823;&#24046;&#29575;&#12290;&#20351;&#29992;&#36873;&#25321;&#24615;&#25512;&#29702;&#26694;&#26550;&#65292;&#25105;&#20204;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;&#27880;&#24847;&#21147;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#29702;&#35770;&#19978;&#22522;&#20110;&#20551;&#38451;&#24615;&#26816;&#27979;&#27010;&#29575;&#37327;&#21270;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Vision Transformer (ViT) demonstrates exceptional performance in various computer vision tasks. Attention is crucial for ViT to capture complex wide-ranging relationships among image patches, allowing the model to weigh the importance of image patches and aiding our understanding of the decision-making process. However, when utilizing the attention of ViT as evidence in high-stakes decision-making tasks such as medical diagnostics, a challenge arises due to the potential of attention mechanisms erroneously focusing on irrelevant regions. In this study, we propose a statistical test for ViT's attentions, enabling us to use the attentions as reliable quantitative evidence indicators for ViT's decision-making with a rigorously controlled error rate. Using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#27010;&#29575;Lambert&#38382;&#39064;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#31561;&#39046;&#22495;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07961</link><description>&lt;p&gt;
&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs. (arXiv:2401.07961v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07961
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#27010;&#29575;Lambert&#38382;&#39064;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#31561;&#39046;&#22495;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lambert&#38382;&#39064;&#28041;&#21450;&#36890;&#36807;&#36895;&#24230;&#25511;&#21046;&#22312;&#35268;&#23450;&#30340;&#39134;&#34892;&#26102;&#38388;&#20869;&#23558;&#33322;&#22825;&#22120;&#20174;&#32473;&#23450;&#30340;&#21021;&#22987;&#20301;&#32622;&#36716;&#31227;&#21040;&#32473;&#23450;&#30340;&#32456;&#31471;&#20301;&#32622;&#65292;&#21463;&#21040;&#37325;&#21147;&#21147;&#22330;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;Lambert&#38382;&#39064;&#30340;&#27010;&#29575;&#21464;&#31181;&#65292;&#20854;&#20013;&#20301;&#32622;&#21521;&#37327;&#30340;&#31471;&#28857;&#32422;&#26463;&#30340;&#30693;&#35782;&#34987;&#23427;&#20204;&#21508;&#33258;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#25152;&#26367;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#31471;&#28857;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#32422;&#26463;&#30340;Lambert&#38382;&#39064;&#26159;&#19968;&#20010;&#24191;&#20041;&#30340;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#65288;OMT&#65289;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#36825;&#20010;&#32463;&#20856;&#30340;&#22825;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#19982;&#29616;&#20195;&#38543;&#26426;&#25511;&#21046;&#21644;&#38543;&#26426;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#26032;&#21457;&#29616;&#30340;&#36830;&#25509;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#24314;&#31435;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#21516;&#26679;&#30340;&#36830;&#25509;&#36824;&#24110;&#21161;&#36890;&#36807;&#25193;&#25955;&#27491;&#35268;&#21270;&#25968;&#20540;&#27714;&#35299;&#27010;&#29575;Lambert&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#36830;&#25509;&#26469;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#24037;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.07494</link><description>&lt;p&gt;
&#36755;&#20837;&#20984;&#24615;Lipschitz RNN: &#19968;&#31181;&#29992;&#20110;&#24037;&#31243;&#20219;&#21153;&#30340;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks. (arXiv:2401.07494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07494
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#24037;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#30495;&#23454;&#19990;&#30028;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#22312;&#21516;&#26102;&#25110;&#20998;&#21035;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#36890;&#36807;&#20174;&#33258;&#28982;&#29289;&#29702;&#31995;&#32479;&#21644;&#29616;&#26377;&#25991;&#29486;&#20013;&#33719;&#21462;&#30340;&#35265;&#35299;&#65292;&#24050;&#30693;&#36755;&#20837;&#20984;&#24615;&#32467;&#26500;&#22686;&#24378;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;Lipschitz&#32422;&#26463;&#32467;&#26500;&#22686;&#24378;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#31216;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#22522;&#20934;MNIST&#22270;&#20687;&#20998;&#31867;&#12289;&#26032;&#21152;&#22369;LHT Holdings&#20844;&#21496;&#30340;&#23454;&#38469;&#22826;&#38451;&#33021;&#20809;&#20239;&#31995;&#32479;&#35268;&#21010;&#20013;&#30340;&#23454;&#26102;&#22826;&#38451;&#36752;&#23556;&#39044;&#27979;&#65292;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#22120;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04336</link><description>&lt;p&gt;
&#28145;&#24230;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#29992;&#20110;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#24040;&#22823;&#22270;&#36890;&#24120;&#20197;&#38750;&#20013;&#24515;&#21270;&#23376;&#22270;&#30340;&#24418;&#24335;&#30001;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20998;&#25955;&#23384;&#20648;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#32771;&#34385;&#21040;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65288;subgraph FL&#65289;&#22330;&#26223;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#20854;&#20013;&#27599;&#20010;&#26412;&#22320;&#23458;&#25143;&#31471;&#25345;&#26377;&#25972;&#20010;&#20840;&#23616;&#22270;&#30340;&#23376;&#22270;&#65292;&#20197;&#33719;&#21462;&#20840;&#23616;&#19968;&#33324;&#21270;&#30340;&#22270;&#25366;&#25496;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#32570;&#23569;&#36328;&#23376;&#22270;&#37051;&#23621;&#32780;&#23548;&#33268;&#30340;&#23616;&#37096;&#23376;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#32570;&#22833;&#37051;&#23621;&#29983;&#25104;&#22120;&#21644;GNN&#30340;&#32852;&#21512;FL&#26469;&#22686;&#21152;&#26412;&#22320;&#37051;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;FL&#30340;&#25928;&#29992;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#38544;&#31169;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDEP&#26469;&#20840;&#38754;&#35299;&#20915;&#23376;&#22270;FL&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;FedDEP&#21253;&#25324;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65306;(1) &#21033;&#29992;&#28508;&#22312;&#32570;&#22833;&#37051;&#23621;&#30340;GNN&#23884;&#20837;&#36827;&#34892;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#65307;(2) Effic...
&lt;/p&gt;
&lt;p&gt;
Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2312.15591</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15591
&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#20449;&#24687;&#31995;&#32479;&#26102;&#20195;&#65292;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#32034;&#25968;&#25454;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#65288;NGDB&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#23558;&#22270;&#25968;&#25454;&#24211;&#65288;&#22270;&#24418;&#25968;&#25454;&#24211;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;&#31070;&#32463;&#23884;&#20837;&#23384;&#20648;&#21644;&#22797;&#26434;&#31070;&#32463;&#36923;&#36753;&#26597;&#35810;&#22238;&#31572;&#20026;NGDB&#25552;&#20379;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#22270;&#24418;&#19981;&#23436;&#25972;&#26102;&#65292;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#28508;&#22312;&#27169;&#24335;&#21644;&#34920;&#31034;&#26469;&#22635;&#34917;&#22270;&#32467;&#26500;&#20013;&#30340;&#31354;&#32570;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#20851;&#31995;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#32452;&#21512;&#26597;&#35810;&#25512;&#26029;&#20986;&#26356;&#22810;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#36890;&#36807;&#27604;&#36739;&#22270;&#25968;&#25454;&#24211;&#20013;Turing&#22870;&#24471;&#20027;&#30340;&#31572;&#26696;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#30340;&#22810;&#21151;&#33021;&#25439;&#22833;&#20960;&#20309;&#20803;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#38236;&#20687;&#26144;&#23556;&#26469;&#24555;&#36895;&#36866;&#24212;&#25552;&#21462;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#20415;&#22312;&#23569;&#37327;&#20248;&#21270;&#27493;&#39588;&#20869;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.13486</link><description>&lt;p&gt;
&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#30340;&#22810;&#21151;&#33021;&#25439;&#22833;&#20960;&#20309;&#20803;&#36827;&#34892;&#24555;&#36895;&#33258;&#36866;&#24212;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning with Versatile Loss Geometries for Fast Adaptation Using Mirror Descent. (arXiv:2312.13486v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#30340;&#22810;&#21151;&#33021;&#25439;&#22833;&#20960;&#20309;&#20803;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#38236;&#20687;&#26144;&#23556;&#26469;&#24555;&#36895;&#36866;&#24212;&#25552;&#21462;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#20415;&#22312;&#23569;&#37327;&#20248;&#21270;&#27493;&#39588;&#20869;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#20219;&#21153;&#19981;&#21464;&#20808;&#39564;&#30693;&#35782;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#25968;&#25454;&#35760;&#24405;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26032;&#20219;&#21153;&#12290;&#20803;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#22914;&#20309;&#24555;&#36895;&#8220;&#35843;&#25972;&#8221;&#25552;&#21462;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#20415;&#22312;&#20960;&#27425;&#20248;&#21270;&#27493;&#39588;&#20869;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#39044;&#22788;&#29702;&#22120;&#26469;&#22788;&#29702;&#36825;&#20010;&#25361;&#25112;&#65292;&#20197;&#22686;&#24378;&#27599;&#20010;&#20219;&#21153;&#35757;&#32451;&#36807;&#31243;&#30340;&#25910;&#25947;&#24615;&#12290;&#23613;&#31649;&#22312;&#34920;&#31034;&#20108;&#27425;&#35757;&#32451;&#25439;&#22833;&#26102;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#31616;&#21333;&#30340;&#32447;&#24615;&#39044;&#22788;&#29702;&#22120;&#24456;&#38590;&#25429;&#25417;&#22797;&#26434;&#30340;&#25439;&#22833;&#20960;&#20309;&#20803;&#12290;&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#38236;&#20687;&#26144;&#23556;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#35813;&#26144;&#23556;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#36317;&#31163;&#24230;&#37327;&#65292;&#21487;&#20197;&#25429;&#25417;&#21644;&#20248;&#21270;&#21508;&#31181;&#25439;&#22833;&#20960;&#20309;&#20803;&#65292;&#20174;&#32780;&#20419;&#36827;&#27599;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#22312;&#20960;&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#27979;&#35797;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing task-invariant prior knowledge extracted from related tasks, meta-learning is a principled framework that empowers learning a new task especially when data records are limited. A fundamental challenge in meta-learning is how to quickly "adapt" the extracted prior in order to train a task-specific model within a few optimization steps. Existing approaches deal with this challenge using a preconditioner that enhances convergence of the per-task training process. Though effective in representing locally a quadratic training loss, these simple linear preconditioners can hardly capture complex loss geometries. The present contribution addresses this limitation by learning a nonlinear mirror map, which induces a versatile distance metric to enable capturing and optimizing a wide range of loss geometries, hence facilitating the per-task training. Numerical tests on few-shot learning datasets demonstrate the superior expressiveness and convergence of the advocated approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;GNN&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#26377;&#30340;&#20998;&#23376;&#26500;&#35937;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#22810;&#20010;&#26500;&#35937;&#36890;&#29992;&#30340;&#28508;&#21464;&#37327;&#65292;&#20197;&#35299;&#20915;&#33719;&#21462;&#22810;&#20010;&#26500;&#35937;&#30340;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.13110</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#29627;&#23572;&#20857;&#26364;&#29983;&#25104;&#22120;&#36827;&#34892;&#20998;&#23376;GNN&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training of Molecular GNNs via Conditional Boltzmann Generator. (arXiv:2312.13110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;GNN&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#26377;&#30340;&#20998;&#23376;&#26500;&#35937;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#22810;&#20010;&#26500;&#35937;&#36890;&#29992;&#30340;&#28508;&#21464;&#37327;&#65292;&#20197;&#35299;&#20915;&#33719;&#21462;&#22810;&#20010;&#26500;&#35937;&#30340;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#34920;&#31034;&#26159;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20998;&#23376;&#26412;&#36136;&#19978;&#26159;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19977;&#32500;&#32467;&#26500;&#65307;&#27492;&#22806;&#65292;&#23427;&#20204;&#19981;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#26159;&#22312;&#19977;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#19981;&#26029;&#36816;&#21160;&#65292;&#24418;&#25104;&#19968;&#20010;&#21183;&#33021;&#34920;&#38754;&#12290;&#22240;&#27492;&#65292;&#24076;&#26395;&#20107;&#20808;&#29983;&#25104;&#22810;&#20010;&#26500;&#35937;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#26500;&#35937;&#30340;4D-QSAR&#27169;&#22411;&#25552;&#21462;&#20998;&#23376;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#22810;&#20010;&#26500;&#35937;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#33647;&#29289;&#21644;&#26448;&#26009;&#21457;&#29616;&#20219;&#21153;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#26377;&#20998;&#23376;&#26500;&#35937;&#25968;&#25454;&#38598;&#23545;&#20998;&#23376;GNN&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20174;2D&#20998;&#23376;&#22270;&#29983;&#25104;&#23545;&#22810;&#20010;&#26500;&#35937;&#36890;&#29992;&#30340;&#28508;&#21464;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#29627;&#23572;&#20857;&#26364;GNN&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26465;&#20214;&#29983;&#25104;&#27010;&#29575;&#30340;&#26465;&#20214;&#36793;&#32536;&#20284;&#28982;&#26469;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations of molecular structures using deep learning is a fundamental problem in molecular property prediction tasks. Molecules inherently exist in the real world as three-dimensional structures; furthermore, they are not static but in continuous motion in the 3D Euclidean space, forming a potential energy surface. Therefore, it is desirable to generate multiple conformations in advance and extract molecular representations using a 4D-QSAR model that incorporates multiple conformations. However, this approach is impractical for drug and material discovery tasks because of the computational cost of obtaining multiple conformations. To address this issue, we propose a pre-training method for molecular GNNs using an existing dataset of molecular conformations to generate a latent vector universal to multiple conformations from a 2D molecular graph. Our method, called Boltzmann GNN, is formulated by maximizing the conditional marginal likelihood of a conditional generative 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.10401</link><description>&lt;p&gt;
&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#22270;&#20013;&#25429;&#25417;&#19981;&#21464;&#20449;&#24687;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#25506;&#32034;&#22270;&#30340;&#32467;&#26500;&#29702;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#19981;&#21464;&#20449;&#24687;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22270;&#27169;&#22411;&#26397;&#21521;&#35299;&#37322;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#38169;&#35823;&#23398;&#20064;&#65292;&#22240;&#27492;&#23398;&#20064;&#21040;&#30340;&#22122;&#22768;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#24178;&#25200;&#20102;&#22270;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25506;&#32034;&#22270;&#30340;&#20869;&#22312;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#19978;&#36848;&#36335;&#24452;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#38416;&#26126;&#32500;&#24230;&#29702;&#35770;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#34920;&#24449;&#21160;&#21147;&#23398;&#21464;&#21270;&#30340;&#25299;&#25169;&#19981;&#21464;&#29305;&#24449;&#25552;&#21462;&#65292;&#29305;&#21035;&#20851;&#27880;&#36229;&#20020;&#30028;&#38669;&#26222;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#31995;&#32479;&#30340;&#36136;&#21464;&#21644;&#24120;&#21457;&#34892;&#20026;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2312.09234</link><description>&lt;p&gt;
&#20570;&#26102;&#38388;&#25197;&#26354;&#21543;&#65306;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25299;&#25169;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Let's do the time-warp-attend: Learning topological invariants of dynamical systems. (arXiv:2312.09234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#34920;&#24449;&#21160;&#21147;&#23398;&#21464;&#21270;&#30340;&#25299;&#25169;&#19981;&#21464;&#29305;&#24449;&#25552;&#21462;&#65292;&#29305;&#21035;&#20851;&#27880;&#36229;&#20020;&#30028;&#38669;&#26222;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#31995;&#32479;&#30340;&#36136;&#21464;&#21644;&#24120;&#21457;&#34892;&#20026;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#20174;&#30005;&#36335;&#21040;&#29983;&#24577;&#32593;&#32476;&#65292;&#24403;&#20854;&#22522;&#26412;&#21442;&#25968;&#36328;&#36234;&#38408;&#20540;&#26102;&#65292;&#20250;&#21457;&#29983;&#36136;&#21464;&#21644;&#24120;&#21457;&#24615;&#30340;&#34892;&#20026;&#21464;&#21270;&#65292;&#31216;&#20026;&#20998;&#27495;&#12290;&#29616;&#26377;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#21333;&#20010;&#31995;&#32479;&#20013;&#21363;&#23558;&#21457;&#29983;&#30340;&#28798;&#38590;&#65292;&#20294;&#20027;&#35201;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#19981;&#21516;&#31995;&#32479;&#30340;&#23450;&#24615;&#21160;&#21147;&#23398;&#21464;&#21270;&#21644;&#25512;&#24191;&#21040;&#30495;&#23454;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21160;&#21147;&#23398;&#21464;&#21270;&#36827;&#34892;&#20998;&#31867;&#24182;&#34920;&#24449;&#20998;&#27495;&#36793;&#30028;&#30340;&#25299;&#25169;&#19981;&#21464;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36229;&#20020;&#30028;&#38669;&#26222;&#20998;&#27495;&#30340;&#20856;&#22411;&#26696;&#20363;&#65292;&#20854;&#29992;&#20110;&#27169;&#25311;&#24191;&#27867;&#24212;&#29992;&#30340;&#21608;&#26399;&#24615;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;&#21367;&#31215;&#20851;&#27880;&#26041;&#27861;&#32463;&#36807;&#20102;&#25968;&#25454;&#22686;&#24378;&#35757;&#32451;&#65292;&#40723;&#21169;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#20998;&#27495;&#36793;&#30028;&#30340;&#25299;&#25169;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior, called bifurcations, when their underlying parameters cross a threshold. Existing methods predict oncoming catastrophes in individual systems but are primarily time-series-based and struggle both to categorize qualitative dynamical regimes across diverse systems and to generalize to real data. To address this challenge, we propose a data-driven, physically-informed deep-learning framework for classifying dynamical regimes and characterizing bifurcation boundaries based on the extraction of topologically invariant features. We focus on the paradigmatic case of the supercritical Hopf bifurcation, which is used to model periodic dynamics across a wide range of applications. Our convolutional attention method is trained with data augmentations that encourage the learning of topological invariants which can be used to detect bifurcation boun
&lt;/p&gt;</description></item><item><title>EZ-CLIP&#26159;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#30340;CLIP&#25913;&#36827;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#30340;&#35270;&#35273;&#25552;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20445;&#25345;&#20102;CLIP&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.08010</link><description>&lt;p&gt;
EZ-CLIP: &#39640;&#25928;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EZ-CLIP: Efficient Zeroshot Video Action Recognition. (arXiv:2312.08010v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08010
&lt;/p&gt;
&lt;p&gt;
EZ-CLIP&#26159;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#30340;CLIP&#25913;&#36827;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#30340;&#35270;&#35273;&#25552;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20445;&#25345;&#20102;CLIP&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20511;&#37492;&#36825;&#19968;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#23558;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24212;&#29992;&#20110;&#35270;&#39057;&#65292;&#25193;&#23637;&#20854;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#25913;&#36827;&#34920;&#29616;&#20986;&#20102;&#24456;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;&#26377;&#25928;&#24314;&#27169;&#35270;&#39057;&#39046;&#22495;&#20869;&#20851;&#38190;&#30340;&#26102;&#38388;&#26041;&#38754;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EZ-CLIP&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#30340;CLIP&#25913;&#36827;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;EZ-CLIP&#21033;&#29992;&#26102;&#38388;&#19978;&#30340;&#35270;&#35273;&#25552;&#31034;&#36827;&#34892;&#26080;&#32541;&#30340;&#26102;&#38388;&#36866;&#24212;&#65292;&#26080;&#38656;&#23545;&#26680;&#24515;CLIP&#26550;&#26500;&#36827;&#34892;&#22522;&#26412;&#25913;&#21160;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23545;&#26102;&#38388;&#35270;&#35273;&#25552;&#31034;&#36827;&#34892;&#24341;&#23548;&#65292;&#20197;&#19987;&#27880;&#20110;&#25429;&#25417;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the crucial temporal aspects inherent to the video domain. In this study, we present EZ-CLIP, a simple and efficient adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal visual prompting for seamless temporal adaptation, requiring no fundamental alterations to the core CLIP architecture while preserving its remarkable generalization abilities. Moreover, we introduce a novel learning objective that guides the temporal visual prompts to focus on capturing motion, thereb
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#35889;&#26041;&#27861;&#23558;&#32463;&#20856;&#35889;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35889;&#25439;&#22833;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#27714;&#23548;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#36229;&#36807;&#20043;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.05225</link><description>&lt;p&gt;
&#31070;&#32463;&#35889;&#26041;&#27861;: &#35889;&#22495;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Spectral Methods: Self-supervised learning in the spectral domain. (arXiv:2312.05225v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05225
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35889;&#26041;&#27861;&#23558;&#32463;&#20856;&#35889;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35889;&#25439;&#22833;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#27714;&#23548;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#36229;&#36807;&#20043;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#35889;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32463;&#20856;&#35889;&#26041;&#27861;&#22522;&#30784;&#19978;&#35299;&#20915;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#27491;&#20132;&#22522;&#26469;&#23398;&#20064;PDE&#35299;&#20316;&#20026;&#35889;&#31995;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#19982;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#26102;&#31354;&#22495;&#20013;&#26368;&#23567;&#21270;&#27531;&#24046;&#30340;&#25968;&#20540;&#31215;&#20998;&#26469;&#24378;&#21046;PDE&#32422;&#26463;&#26465;&#20214;&#65292;&#25105;&#20204;&#21033;&#29992;Parseval&#24658;&#31561;&#24335;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#35889;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#35889;&#25439;&#22833;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27714;&#23548;&#65292;&#24182;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24230;&#12290;&#22312;&#25512;&#29702;&#26102;&#38388;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#20445;&#25345;&#24658;&#23450;&#65292;&#19981;&#21463;&#26102;&#31354;&#22495;&#20998;&#36776;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22810;&#20010;&#19981;&#21516;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#19968;&#21040;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. Our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients. In contrast to current machine learning approaches which enforce PDE constraints by minimizing the numerical quadrature of the residuals in the spatiotemporal domain, we leverage Parseval's identity and introduce a new training strategy through a \textit{spectral loss}. Our spectral loss enables more efficient differentiation through the neural network, and substantially reduces training complexity. At inference time, the computational cost of our method remains constant, regardless of the spatiotemporal resolution of the domain. Our experimental results demonstrate that our method significantly outperforms previous machine learning approaches in terms of speed and accuracy by one to two orders of magnitude on multiple different problems. When 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#22522;&#20110;MRI&#22270;&#20687;&#39044;&#27979;&#24739;&#32773;&#19968;&#24180;&#20869;&#26159;&#21542;&#20250;&#21457;&#23637;&#20026;&#20083;&#33146;&#30284;&#65292;&#20197;&#20943;&#36731;&#31579;&#26597;&#36127;&#25285;&#24182;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2312.00067</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#20010;&#20307;&#39118;&#38505;&#35843;&#25972;MRI&#31579;&#26597;&#21644;&#26089;&#26399;&#26816;&#27979;&#30340;&#20083;&#33146;&#30284;
&lt;/p&gt;
&lt;p&gt;
Predicting breast cancer with AI for individual risk-adjusted MRI screening and early detection. (arXiv:2312.00067v2 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#22522;&#20110;MRI&#22270;&#20687;&#39044;&#27979;&#24739;&#32773;&#19968;&#24180;&#20869;&#26159;&#21542;&#20250;&#21457;&#23637;&#20026;&#20083;&#33146;&#30284;&#65292;&#20197;&#20943;&#36731;&#31579;&#26597;&#36127;&#25285;&#24182;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#39118;&#38505;&#24739;&#32773;&#36827;&#34892;&#39069;&#22806;&#24180;&#24230;&#31579;&#26597;&#24615;MRI&#20197;&#39044;&#38450;&#20083;&#33146;&#30284;&#12290;&#25105;&#20204;&#25552;&#20986;&#26681;&#25454;&#24403;&#21069;MRI&#39044;&#27979;&#19968;&#24180;&#20869;&#24739;&#20083;&#33146;&#30284;&#30340;&#39118;&#38505;&#65292;&#30446;&#30340;&#26159;&#20943;&#23569;&#31579;&#26597;&#36127;&#25285;&#21644;&#20419;&#36827;&#26089;&#26399;&#21457;&#29616;&#12290;&#22522;&#20110;12&#24180;&#32047;&#31215;&#30340;12,694&#21517;&#24739;&#32773;&#30340;53,858&#21482;&#20083;&#25151;&#19978;&#36827;&#34892;&#30340;&#31579;&#26597;&#25110;&#35786;&#26029;&#24615;MRI&#65292;&#22521;&#35757;&#20102;&#19968;&#20010;AI&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;2,331&#20363;&#30830;&#35748;&#30340;&#30284;&#30151;&#12290;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;U-Net&#26469;&#20998;&#21106;&#30149;&#21464;&#21644;&#35782;&#21035;&#20851;&#27880;&#21306;&#22495;&#12290;&#28982;&#21518;&#35757;&#32451;&#20102;&#31532;&#20108;&#20010;&#21367;&#31215;&#32593;&#32476;&#26469;&#20351;&#29992;U-Net&#25552;&#21462;&#30340;&#29305;&#24449;&#26816;&#27979;&#24694;&#24615;&#30284;&#30151;&#12290;&#28982;&#21518;&#23545;&#35813;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20272;&#35745;&#22312;&#25918;&#23556;&#23398;&#23478;&#35748;&#20026;&#27491;&#24120;&#25110;&#21487;&#33021;&#26159;&#33391;&#24615;&#30340;&#24773;&#20917;&#19979;&#19968;&#24180;&#20869;&#24739;&#30284;&#30151;&#30340;&#39118;&#38505;&#12290;&#20174;&#26410;&#34987;&#29992;&#20110;&#35757;&#32451;&#30340;&#39640;&#39118;&#38505;&#31579;&#26597;&#32676;&#20307;&#30340;9,183&#21482;&#20083;&#25151;&#36827;&#34892;&#20102;&#27492;AI&#30340;&#39118;&#38505;&#39044;&#27979;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#12290;&#32479;&#35745;&#20998;&#26512;&#20391;&#37325;&#20110;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Women with an increased life-time risk of breast cancer undergo supplemental annual screening MRI. We propose to predict the risk of developing breast cancer within one year based on the current MRI, with the objective of reducing screening burden and facilitating early detection. An AI algorithm was developed on 53,858 breasts from 12,694 patients who underwent screening or diagnostic MRI and accrued over 12 years, with 2,331 confirmed cancers. A first U-Net was trained to segment lesions and identify regions of concern. A second convolutional network was trained to detect malignant cancer using features extracted by the U-Net. This network was then fine-tuned to estimate the risk of developing cancer within a year in cases that radiologists considered normal or likely benign. Risk predictions from this AI were evaluated with a retrospective analysis of 9,183 breasts from a high-risk screening cohort, which were not used for training. Statistical analysis focused on the tradeoff betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#20043;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2311.18426</link><description>&lt;p&gt;
&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#20043;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#23548;&#25968;&#26159;&#25972;&#25968;&#38454;&#23548;&#25968;&#30340;&#19968;&#31181;&#24191;&#20041;&#25512;&#24191;&#12290;&#23545;&#20110;&#20248;&#21270;&#38382;&#39064;&#65292;&#30740;&#31350;&#20351;&#29992;&#20998;&#25968;&#23548;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#26159;&#38750;&#24120;&#26377;&#24847;&#20041;&#30340;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#22312;&#30740;&#31350;&#26041;&#27861;&#21644;&#30740;&#31350;&#29615;&#22659;&#26041;&#38754;&#37117;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#20998;&#26512;&#20102;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#21464;&#31181;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24314;&#31435;&#23558;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#32852;&#31995;&#36215;&#26469;&#30340;&#26032;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#19988;&#24378;&#20984;&#20989;&#25968;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#21644;&#23545;&#20110;&#24179;&#28369;&#19988;&#20984;&#20989;&#25968;&#30340;O(1/T)&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26356;&#36866;&#21512;&#20998;&#25968;&#23548;&#25968;&#30340;&#25193;&#23637;&#24179;&#28369;&#24230;&#27010;&#24565;-Holder&#24179;&#28369;&#24230;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#19988;&#38750;&#20984;&#20989;&#25968;&#30340;O(1/T)&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\"older smoothness - that is more natural for fractional derivative
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#20687;&#37197;&#20934;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#30340;&#36755;&#20986;&#20316;&#20026;&#20248;&#21270;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#37325;&#28857;&#22788;&#29702;&#25439;&#22833;&#26368;&#22823;&#30340;&#22270;&#20687;&#23545;&#65292;&#21462;&#24471;&#20102;1.6%&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;1.0%&#30340;&#21464;&#24418;&#22330;&#24179;&#28369;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.15497</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22270;&#20687;&#37197;&#20934;&#65306;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#20989;&#25968;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26041;&#27861;&#20197;&#25552;&#39640;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#20687;&#37197;&#20934;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#30340;&#36755;&#20986;&#20316;&#20026;&#20248;&#21270;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#37325;&#28857;&#22788;&#29702;&#25439;&#22833;&#26368;&#22823;&#30340;&#22270;&#20687;&#23545;&#65292;&#21462;&#24471;&#20102;1.6%&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;1.0%&#30340;&#21464;&#24418;&#22330;&#24179;&#28369;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#20256;&#32479;&#19978;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#20381;&#36182;&#20110;&#24378;&#22823;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24212;&#29992;&#22797;&#26434;&#25968;&#23398;&#21464;&#25442;&#26469;&#20351;&#22270;&#20687;&#21464;&#24418;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#24403;&#28982;&#65292;&#36825;&#20004;&#31181;&#33539;&#24335;&#37117;&#26377;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23558;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#32467;&#21512;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#30340;&#36755;&#20986;&#20316;&#20026;&#20248;&#21270;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#21516;&#26102;&#20248;&#20808;&#32771;&#34385;&#23545;&#25439;&#22833;&#26368;&#22823;&#30340;&#22270;&#20687;&#23545;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#25552;&#39640;&#20102;1.6%&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#19988;&#21464;&#24418;&#22330;&#24179;&#28369;&#24230;&#26041;&#38754;&#33719;&#24471;&#20102;1.0%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image registration has traditionally been done using two distinct approaches: learning based methods, relying on robust deep neural networks, and optimization-based methods, applying complex mathematical transformations to warp images accordingly. Of course, both paradigms offer advantages and disadvantages, and, in this work, we seek to combine their respective strengths into a single streamlined framework, using the outputs of the learning based method as initial parameters for optimization while prioritizing computational power for the image pairs that offer the greatest loss. Our investigations showed improvements of up to 1.6% in test data, while maintaining the same inference time, and a substantial 1.0% points performance gain in deformation field smoothness.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.12399</link><description>&lt;p&gt;
&#22270;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Meets Large Language Model: Progress and Future Directions. (arXiv:2311.12399v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#34920;&#31034;&#21644;&#20998;&#26512;&#35832;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#25968;&#25454;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23558;LLMs&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;LLMs&#22312;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#25198;&#28436;&#30340;&#35282;&#33394;(&#21363;&#22686;&#24378;&#22120;&#12289;&#39044;&#27979;&#22120;&#21644;&#23545;&#40784;&#32452;&#20214;)&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#32452;&#32455;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#20998;&#31867;&#27861;&#19977;&#20010;&#31867;&#21035;&#20013;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are 
&lt;/p&gt;</description></item><item><title>LogLead&#26159;&#19968;&#31181;&#24555;&#36895;&#19988;&#38598;&#25104;&#30340;&#26085;&#24535;&#21152;&#36733;&#22120;&#12289;&#22686;&#24378;&#22120;&#21644;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#30340;&#26085;&#24535;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#26085;&#24535;&#34920;&#31034;&#26041;&#27861;&#21644;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;LogLead&#26041;&#20415;&#20102;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#19982;&#36807;&#21435;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;LogLead&#22312;&#26085;&#24535;&#21152;&#36733;&#36895;&#24230;&#21644;Drain&#35299;&#26512;&#36895;&#24230;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.11809</link><description>&lt;p&gt;
LogLead -- &#24555;&#36895;&#19988;&#38598;&#25104;&#30340;&#26085;&#24535;&#21152;&#36733;&#22120;&#12289;&#22686;&#24378;&#22120;&#21644;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
LogLead -- Fast and Integrated Log Loader, Enhancer, and Anomaly Detector. (arXiv:2311.11809v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11809
&lt;/p&gt;
&lt;p&gt;
LogLead&#26159;&#19968;&#31181;&#24555;&#36895;&#19988;&#38598;&#25104;&#30340;&#26085;&#24535;&#21152;&#36733;&#22120;&#12289;&#22686;&#24378;&#22120;&#21644;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#30340;&#26085;&#24535;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#26085;&#24535;&#34920;&#31034;&#26041;&#27861;&#21644;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;LogLead&#26041;&#20415;&#20102;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#19982;&#36807;&#21435;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;LogLead&#22312;&#26085;&#24535;&#21152;&#36733;&#36895;&#24230;&#21644;Drain&#35299;&#26512;&#36895;&#24230;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LogLead&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#36827;&#34892;&#39640;&#25928;&#30340;&#26085;&#24535;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#12290;LogLead&#32467;&#21512;&#20102;&#26085;&#24535;&#22788;&#29702;&#30340;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#21152;&#36733;&#12289;&#22686;&#24378;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;&#39640;&#36895;DataFrame&#24211;Polars&#12290;&#25105;&#20204;&#30446;&#21069;&#24050;&#32463;&#20844;&#24320;&#25552;&#20379;&#20102;&#20843;&#20010;&#31995;&#32479;&#30340;&#21152;&#36733;&#22120;&#65288;HDFS&#12289;Hadoop&#12289;BGL&#12289;Thunderbird&#12289;Spirit&#12289;Liberty&#12289;TrainTicket&#21644;GC Webshop&#65289;&#12290;&#25105;&#20204;&#25317;&#26377;&#22810;&#20010;&#22686;&#24378;&#22120;&#65292;&#21253;&#25324;&#19977;&#20010;&#35299;&#26512;&#22120;&#65288;Drain&#12289;Spell&#12289;LenMa&#65289;&#12289;Bert&#23884;&#20837;&#21019;&#24314;&#20197;&#21450;&#35789;&#34955;&#31561;&#20854;&#20182;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#12290;LogLead&#38598;&#25104;&#20102;SKLearn&#20013;&#30340;&#20116;&#31181;&#30417;&#30563;&#23398;&#20064;&#21644;&#22235;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#26085;&#24535;&#34920;&#31034;&#26041;&#27861;&#21644;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;LogLead&#20026;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#36807;&#21435;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;LogLead&#20174;&#21407;&#22987;&#25991;&#20214;&#21152;&#36733;&#21040;DataFrame&#30340;&#36895;&#24230;&#25552;&#21319;&#20102;&#36229;&#36807;10&#20493;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;Drain&#35299;&#26512;&#36895;&#24230;&#22823;&#32422;&#25552;&#21319;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces LogLead, a tool designed for efficient log analysis benchmarking. LogLead combines three essential steps in log processing: loading, enhancing, and anomaly detection. The tool leverages Polars, a high-speed DataFrame library. We currently have Loaders for eight systems that are publicly available (HDFS, Hadoop, BGL, Thunderbird, Spirit, Liberty, TrainTicket, and GC Webshop). We have multiple enhancers with three parsers (Drain, Spell, LenMa), Bert embedding creation and other log representation techniques like bag-of-words. LogLead integrates to five supervised and four unsupervised machine learning algorithms for anomaly detection from SKLearn. By integrating diverse datasets, log representation methods and anomaly detectors, LogLead facilitates comprehensive benchmarking in log analysis research. We show that log loading from raw file to dataframe is over 10x faster with LogLead compared to past solutions. We demonstrate roughly 2x improvement in Drain parsing s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.07202</link><description>&lt;p&gt;
&#36755;&#20837;&#20984;LSTM&#65306;&#19968;&#31181;&#24555;&#36895;&#22522;&#20110;Lyapunov&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#20984;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control. (arXiv:2311.07202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;ICNN&#65289;&#65292;&#22522;&#20110;ICNN&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#36890;&#36807;&#22312;MPC&#26694;&#26550;&#20013;&#20445;&#25345;&#20984;&#24615;&#25104;&#21151;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;ICNN&#26550;&#26500;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#22797;&#26434;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#21644;&#22522;&#20110;ICNN&#30340;MPC&#65292;&#19982;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#30340;MPC&#30456;&#27604;&#38754;&#20020;&#36739;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;ICNN&#30340;&#21407;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;MPC&#65292;&#26088;&#22312;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#12289;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#24182;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#38750;&#32447;&#24615;&#21270;&#23398;&#21453;&#24212;&#22120;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#30340;&#32531;&#35299;&#21644;&#25910;&#25947;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#25910;&#25947;&#26102;&#38388;&#24179;&#22343;&#38477;&#20302;&#20102;&#19968;&#23450;&#30340;&#30334;&#20998;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive Control (MPC) successfully attains globally optimal solutions by upholding convexity within the MPC framework. However, current ICNN architectures encounter the issue of vanishing/exploding gradients, which limits their ability to serve as deep neural networks for complex tasks. Additionally, the current neural network-based MPC, including conventional neural network-based MPC and ICNN-based MPC, faces slower convergence speed when compared to MPC based on first-principles models. In this study, we leverage the principles of ICNNs to propose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific goal of reducing convergence time and mitigating the vanishing/exploding gradient problem while ensuring closed-loop stability. From a simulation study of a nonlinear chemical reactor, we observed a mitigation of vanishing/exploding gradient problem and a reduction in convergence time, with a percentage de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.03976</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#22312;&#25968;&#25454;&#25110;&#26631;&#31614;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#20445;&#25345;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#19968;&#33268;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36801;&#31227;&#12290;&#33021;&#22815;&#22312;&#20219;&#24847;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#30340;&#27169;&#22411;&#23558;&#25104;&#20026;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#25552;&#20986;&#20102;FoToM&#65292;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#30340;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;FoToM&#22312;&#22810;&#20010;&#22270;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27491;&#21521;&#36801;&#31227;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#65292;&#24615;&#33021;&#26368;&#24046;&#26102;&#19982;&#26377;&#30417;&#30563;&#22522;&#32447;&#30456;&#24403;&#65292;76%&#30340;&#25968;&#25454;&#38598;&#22312;95%&#32622;&#20449;&#24230;&#19979;&#37117;&#26174;&#33879;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#65288;P&#8804;0.01&#65289;&#65292;&#35823;&#24046;&#20943;&#23569;&#20102;8%&#33267;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30416;&#21270;DNN&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#35753;&#23458;&#25143;&#25511;&#21046;DNN&#36755;&#20986;&#30340;&#35821;&#20041;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19982;&#26631;&#20934;DNN&#20960;&#20046;&#19968;&#33268;&#65292;&#25552;&#21319;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#38544;&#31169;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13384</link><description>&lt;p&gt;
Salted Inference: &#22312;&#31227;&#21160;&#35745;&#31639;&#20013;&#25552;&#21319;&#38544;&#31169;&#24182;&#20445;&#25345;&#20998;&#21106;&#25512;&#29702;&#30340;&#25928;&#29575;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing. (arXiv:2310.13384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30416;&#21270;DNN&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#35753;&#23458;&#25143;&#25511;&#21046;DNN&#36755;&#20986;&#30340;&#35821;&#20041;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19982;&#26631;&#20934;DNN&#20960;&#20046;&#19968;&#33268;&#65292;&#25552;&#21319;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#38544;&#31169;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#25512;&#29702;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21010;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#36793;&#32536;&#37096;&#20998;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#65292;&#21518;&#32493;&#37096;&#20998;&#22312;&#20113;&#31471;&#36816;&#34892;&#12290;&#36825;&#28385;&#36275;&#20102;&#35774;&#22791;&#19978;&#26426;&#22120;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#38656;&#27714;&#65306;&#36755;&#20837;&#38544;&#31169;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#21106;&#25512;&#29702;&#20013;&#65292;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#36755;&#20986;&#38544;&#31169;&#65292;&#22240;&#20026;DNN&#30340;&#36755;&#20986;&#21487;&#35265;&#20110;&#20113;&#31471;&#12290;&#23613;&#31649;&#21152;&#23494;&#35745;&#31639;&#21487;&#20197;&#20445;&#25252;&#36755;&#20986;&#38544;&#31169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#30416;&#21270;DNN&#8221;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35753;&#23458;&#25143;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;DNN&#36755;&#20986;&#30340;&#35821;&#20041;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19982;&#26631;&#20934;DNN&#20960;&#20046;&#19968;&#33268;&#12290;&#22312;&#22270;&#20687;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#30416;&#21270;DNN&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#19982;&#26631;&#20934;DNN&#38750;&#24120;&#25509;&#36817;&#65292;&#23588;&#20854;&#26159;&#24403;&#30416;&#21270;&#23618;&#20301;&#20110;&#36739;&#26089;&#38454;&#27573;&#20197;&#28385;&#36275;&#20998;&#21106;&#25512;&#29702;&#30340;&#35201;&#27714;&#26102;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split inference partitions a deep neural network (DNN) to run the early part at the edge and the later part in the cloud. This meets two key requirements for on-device machine learning: input privacy and compute efficiency. Still, an open question in split inference is output privacy, given that the output of a DNN is visible to the cloud. While encrypted computing can protect output privacy, it mandates extensive computation and communication resources. In this paper, we introduce "Salted DNNs": a novel method that lets clients control the semantic interpretation of DNN output at inference time while maintaining accuracy and efficiency very close to that of a standard DNN. Experimental evaluations conducted on both image and sensor data show that Salted DNNs achieve classification accuracy very close to standard DNNs, particularly when the salted layer is positioned within the early part to meet the requirements of split inference. Our method is general and can be applied to various D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12955</link><description>&lt;p&gt;
&#26500;&#24314;&#20855;&#26377;&#22810;&#26679;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24378;&#21270;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#26114;&#36149;&#25110;&#19981;&#23433;&#20840;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#22122;&#22768;&#65292;&#29978;&#33267;&#21487;&#33021;&#34987;&#24694;&#24847;&#25439;&#22351;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21253;&#25324;&#29366;&#24577;&#12289;&#21160;&#20316;&#12289;&#22870;&#21169;&#21644;&#21160;&#21147;&#23398;&#22312;&#20869;&#30340;&#20840;&#38754;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#26174;&#31034;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#25239;&#25968;&#25454;&#25439;&#22351;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;IQL&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#30830;&#23450;&#20026;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#30456;&#23545;&#40065;&#26834;&#65292;&#20294;IQL&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;MultiScript&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#26085;&#24120;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.04965</link><description>&lt;p&gt;
MULTISCRIPT: &#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#29992;&#20110;&#25903;&#25345;&#24320;&#25918;&#39046;&#22495;&#30340;&#26085;&#24120;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;MultiScript&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#26085;&#24120;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#28436;&#31034;&#20013;&#33258;&#21160;&#29983;&#25104;&#33050;&#26412;&#65288;&#21363;&#25991;&#26412;&#25551;&#36848;&#30340;&#20851;&#38190;&#27493;&#39588;&#24207;&#21015;&#65289;&#24182;&#25512;&#29702;&#21518;&#32493;&#27493;&#39588;&#23545;&#20110;&#29616;&#20195;AI&#34394;&#25311;&#21161;&#25163;&#26469;&#24341;&#23548;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#38476;&#29983;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32467;&#26500;&#33391;&#22909;&#30340;&#21069;&#32622;&#27493;&#39588;&#30340;&#25991;&#26412;&#21644;/&#25110;&#22270;&#20687;&#25551;&#36848;&#65292;&#25110;&#32773;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#23548;&#33268;&#19982;&#30495;&#23454;&#19990;&#30028;&#20013;&#29992;&#25143;&#22330;&#26223;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;&#8212;&#8212;MultiScript&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#20851;&#20110;&#38754;&#21521;&#20219;&#21153;&#30340;&#22810;&#27169;&#24335;&#33050;&#26412;&#23398;&#20064;&#30340;&#26032;&#20219;&#21153;&#65306;&#65288;1&#65289;&#22810;&#27169;&#24335;&#33050;&#26412;&#29983;&#25104;&#65292;&#21644;&#65288;2&#65289;&#21518;&#32493;&#27493;&#39588;&#39044;&#27979;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#36755;&#20837;&#21253;&#25324;&#30446;&#26631;&#20219;&#21153;&#21517;&#31216;&#21644;&#28436;&#31034;&#35270;&#39057;&#65292;&#39044;&#26399;&#36755;&#20986;&#20026;&#65288;1&#65289;&#22522;&#20110;&#28436;&#31034;&#35270;&#39057;&#30340;&#32467;&#26500;&#21270;&#27493;&#39588;&#25551;&#36848;&#30340;&#24207;&#21015;&#65292;&#21644;&#65288;2&#65289;&#38024;&#23545;&#27599;&#20010;&#27493;&#39588;&#30340;&#21333;&#19968;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for th
&lt;/p&gt;</description></item><item><title>BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03320</link><description>&lt;p&gt;
BioBridge: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03320
&lt;/p&gt;
&lt;p&gt;
BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;(FMs)&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;FMs&#20027;&#35201;&#20173;&#22788;&#20110;&#21333;&#27169;&#24577;&#29366;&#24577;&#65292;&#21363;&#29420;&#31435;&#35757;&#32451;&#24182;&#29992;&#20110;&#22788;&#29702;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#23567;&#20998;&#23376;&#32467;&#26500;&#25110;&#20020;&#24202;&#25968;&#25454;&#31561;&#21333;&#19968;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#29983;&#29289;&#21307;&#23398;FMs&#30340;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;BioBridge&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;(KG)&#26469;&#23398;&#20064;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#24213;&#23618;&#21333;&#27169;&#24577;FMs&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#26725;&#25509;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;FMs&#20197;&#24314;&#31435;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21487;&#20197;&#20987;&#36133;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65288;&#24179;&#22343;&#25552;&#39640;&#32422;76.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;BioBridge&#34920;&#29616;&#20986;&#39046;&#22495;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#30340;&#27169;&#24577;&#25110;&#20851;&#31995;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03298</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#28508;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;&#26041;&#27861;&#22312;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#21644;&#35774;&#35745;&#20248;&#21270;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#20302;&#20445;&#30495;&#24230;&#65288;LF&#65289;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MF&#26041;&#27861;&#20551;&#23450;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#26159;&#21160;&#24577;&#20998;&#37197;&#36164;&#28304;&#22312;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MF&#26041;&#27861;&#20381;&#36182;&#20110;&#20445;&#30495;&#24230;&#32423;&#21035;&#30340;&#23618;&#27425;&#20551;&#35774;&#65292;&#25110;&#32773;&#26080;&#27861;&#25429;&#25417;&#22810;&#20010;&#20445;&#30495;&#24230;&#32423;&#21035;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#24182;&#21033;&#29992;&#20854;&#26469;&#37327;&#21270;&#26410;&#26469;&#26679;&#26412;&#30340;&#20215;&#20540;&#21644;&#23548;&#33322;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#23884;&#20837;&#21644;&#30456;&#20851;&#30340;&#20808;&#39564;-&#21518;&#39564;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#20197;&#26174;&#24335;&#22320;&#21033;&#29992;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;&#22635;&#20805;&#37319;&#26679;&#36845;&#20195;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#25105;&#20204;&#30830;&#23450;&#20855;&#26377;&#26368;&#22823;&#28508;&#21147;&#24433;&#21709;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential imp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#65288;U2C&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#21487;&#30693;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#38754;&#23545;&#22256;&#38590;&#26679;&#20363;&#26102;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.01202</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Unified Uncertainty Calibration. (arXiv:2310.01202v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#65288;U2C&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#21487;&#30693;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#38754;&#23545;&#22256;&#38590;&#26679;&#20363;&#26102;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#20581;&#22766;&#65292;&#20844;&#24179;&#21644;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#38754;&#23545;&#22256;&#38590;&#25110;&#36229;&#20986;&#35757;&#32451;&#31867;&#21035;&#30340;&#27979;&#35797;&#26679;&#20363;&#26102;&#65292;&#20998;&#31867;&#22120;&#33021;&#22815;&#35828;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#12290;&#26222;&#36941;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#31574;&#30053;&#26159;&#31616;&#21333;&#30340;&#8220;&#25298;&#32477;&#25110;&#20998;&#31867;&#8221;&#35268;&#21017;&#65306;&#22914;&#26524;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#39640;&#65292;&#21017;&#25918;&#24323;&#39044;&#27979;&#65292;&#21542;&#21017;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20801;&#35768;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30456;&#20114;&#36890;&#20449;&#65292;&#20250;&#20135;&#29983;&#26410;&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#19981;&#33021;&#32416;&#27491;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19977;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#65288;U2C&#65289;&#30340;&#25972;&#20307;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#21487;&#30693;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;U2C&#33021;&#22815;&#36827;&#34892;&#28165;&#26224;&#30340;&#23398;&#20064;&#29702;&#35770;&#20998;&#26512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;ImageNet&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25298;&#32477;&#25110;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To build robust, fair, and safe AI systems, we would like our classifiers to say ``I don't know'' when facing test examples that are difficult or fall outside of the training classes.The ubiquitous strategy to predict under uncertainty is the simplistic \emph{reject-or-classify} rule: abstain from prediction if epistemic uncertainty is high, classify otherwise.Unfortunately, this recipe does not allow different sources of uncertainty to communicate with each other, produces miscalibrated predictions, and it does not allow to correct for misspecifications in our uncertainty estimates. To address these three issues, we introduce \emph{unified uncertainty calibration (U2C)}, a holistic framework to combine aleatoric and epistemic uncertainties. U2C enables a clean learning-theoretical analysis of uncertainty estimation, and outperforms reject-or-classify across a variety of ImageNet benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14393</link><description>&lt;p&gt;
LLMCarbon: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30899;&#36275;&#36857;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#12289;&#23454;&#39564;&#21644;&#23384;&#20648;&#36807;&#31243;&#20013;&#30340;&#25490;&#25918;&#65292;&#21253;&#25324;&#36816;&#33829;&#21644;&#22266;&#23450;&#30899;&#25490;&#25918;&#12290;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#22312;LLMs&#35757;&#32451;&#20043;&#21069;&#20934;&#30830;&#20272;&#35745;&#20854;&#30899;&#24433;&#21709;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;GPU&#30340;&#20351;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#25253;&#21578;&#20102;LLMs&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#24037;&#20855;mlco2&#33021;&#22815;&#22312;&#23454;&#38469;&#35757;&#32451;&#20043;&#21069;&#39044;&#27979;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30899;&#36275;&#36857;&#12290;&#28982;&#32780;&#65292;mlco2&#23384;&#22312;&#19968;&#20123;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#23427;&#19981;&#33021;&#25193;&#23637;&#20854;&#23545;&#23494;&#38598;&#25110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;LLMs&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#20165;&#20851;&#27880;GPU&#65292;&#24182;&#19981;&#33021;&#24314;&#27169;&#22266;&#21270;&#30340;&#30899;&#36275;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#20026;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#19982;mlco2&#30456;&#27604;&#65292;LLMCarbon&#26174;&#33879;&#22686;&#24378;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36127;&#38754;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#39034;&#24207;&#38899;&#20048;&#25512;&#33616;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11623</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#36830;&#32493;&#38899;&#20048;&#25512;&#33616;&#36827;&#34892;&#36127;&#38754;&#20449;&#21495;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation. (arXiv:2309.11623v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36127;&#38754;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#39034;&#24207;&#38899;&#20048;&#25512;&#33616;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#20208;&#36182;&#20854;&#25512;&#33616;&#24341;&#25806;&#36830;&#32493;&#21521;&#29992;&#25143;&#25552;&#20379;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#39034;&#24207;&#25512;&#33616;&#24050;&#32463;&#24341;&#36215;&#20102;&#24403;&#21069;&#25991;&#29486;&#30340;&#30456;&#24403;&#20851;&#27880;&#65292;&#32780;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#38271;&#26399;&#21644;&#30701;&#26399;&#29992;&#25143;&#21382;&#21490;&#21644;&#39033;&#30446;&#29305;&#24449;&#65289;&#30340;&#33258;&#25105;&#20851;&#27880;&#27169;&#22411;&#19978;&#65307;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#38271;&#26684;&#24335;&#20869;&#23481;&#39046;&#22495;&#65288;&#38646;&#21806;&#12289;&#30005;&#24433;&#31561;&#65289;&#32780;&#19981;&#26159;&#30701;&#26684;&#24335;&#65292;&#20363;&#22914;&#38899;&#20048;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#30740;&#31350;&#26410;&#25506;&#32034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#34701;&#20837;&#36127;&#38754;&#20250;&#35805;&#32423;&#21453;&#39304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#30340;&#33258;&#25105;&#20851;&#27880;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23398;&#20064;&#29992;&#20110;&#39034;&#24207;&#38899;&#20048;&#25512;&#33616;&#30340;&#38544;&#24335;&#20250;&#35805;&#32423;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#34701;&#20837;&#36127;&#38754;&#21453;&#39304;&#65288;&#20363;&#22914;&#36339;&#36807;&#30340;&#26354;&#30446;&#65289;&#20197;&#20419;&#36827;&#27491;&#38754;&#21629;&#20013;&#24182;&#24809;&#32602;&#36127;&#38754;&#21629;&#20013;&#12290;&#36825;&#20010;&#20219;&#21153;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25439;&#22833;&#39033;&#65292;&#21487;&#20197;&#21152;&#20837;&#21040;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music streaming services heavily rely on their recommendation engines to continuously provide content to their consumers. Sequential recommendation consequently has seen considerable attention in current literature, where state of the art approaches focus on self-attentive models leveraging contextual information such as long and short-term user history and item features; however, most of these studies focus on long-form content domains (retail, movie, etc.) rather than short-form, such as music. Additionally, many do not explore incorporating negative session-level feedback during training. In this study, we investigate the use of transformer-based self-attentive architectures to learn implicit session-level information for sequential music recommendation. We additionally propose a contrastive learning task to incorporate negative feedback (e.g skipped tracks) to promote positive hits and penalize negative hits. This task is formulated as a simple loss term that can be incorporated in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25240;&#21472;&#27880;&#24847;&#21147;&#30340;&#25216;&#26415;&#65292;&#22312;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23569;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;24%&#12289;&#21151;&#32791;&#20943;&#23567;23%&#12290;</title><link>http://arxiv.org/abs/2309.07988</link><description>&lt;p&gt;
&#25240;&#21472;&#27880;&#24847;&#21147;&#65306;&#38754;&#21521;&#35774;&#22791;&#30340;Transformer&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#20869;&#23384;&#21644;&#21151;&#32791;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25240;&#21472;&#27880;&#24847;&#21147;&#30340;&#25216;&#26415;&#65292;&#22312;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23569;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;24%&#12289;&#21151;&#32791;&#20943;&#23567;23%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#20248;&#21270;Transformer&#25512;&#26029;&#30340;&#21162;&#21147;&#65292;&#36890;&#24120;&#38024;&#23545;&#38271;&#19978;&#19979;&#25991;&#24212;&#29992;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21270;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#19978;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36890;&#24120;&#27599;&#27425;&#21482;&#22788;&#29702;&#26377;&#38480;&#25968;&#37327;&#30340;&#20196;&#29260;&#65292;&#22240;&#27492;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24182;&#19981;&#26159;&#29942;&#39048;&#25152;&#22312;&#12290;&#30456;&#21453;&#65292;&#29942;&#39048;&#22312;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#32447;&#24615;&#25237;&#24433;&#23618;&#65292;&#23427;&#20204;&#26500;&#25104;&#20102;&#27169;&#22411;&#22823;&#23567;&#30340;&#30456;&#24403;&#37096;&#20998;&#65292;&#24182;&#23545;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#21151;&#32791;&#30340;&#20351;&#29992;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25240;&#21472;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#32447;&#24615;&#23618;&#30340;&#25216;&#26415;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;&#35774;&#22791;&#19978;&#30340;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25240;&#21472;&#27880;&#24847;&#21147;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#65288;&#21644;&#30456;&#24212;&#30340;&#20869;&#23384;&#28040;&#32791;&#65289;&#20943;&#23567;&#22810;&#36798;24%&#65292;&#24182;&#23558;&#21151;&#32791;&#20943;&#23567;&#22810;&#36798;23%&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models excel in speech recognition. Existing efforts to optimize Transformer inference, typically for long-context applications, center on simplifying attention score calculations. However, streaming speech recognition models usually process a limited number of tokens each time, making attention score calculation less of a bottleneck. Instead, the bottleneck lies in the linear projection layers of multi-head attention and feedforward networks, constituting a substantial portion of the model size and contributing significantly to computation, memory, and power usage.  To address this bottleneck, we propose folding attention, a technique targeting these linear layers, significantly reducing model size and improving memory and power efficiency. Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#38598;&#21512;&#22825;&#27668;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#23558;&#39044;&#27979;&#38598;&#21512;&#35270;&#20026;&#19968;&#32452;&#26080;&#24207;&#30340;&#25104;&#21592;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#23545;&#25104;&#21592;&#39034;&#24207;&#30340;&#25490;&#21015;&#32622;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#38142;&#25509;&#20989;&#25968;&#12290;&#22312;&#22320;&#34920;&#28201;&#24230;&#21644;&#39118;&#36895;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.04452</link><description>&lt;p&gt;
&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#38598;&#21512;&#22825;&#27668;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks. (arXiv:2309.04452v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#38598;&#21512;&#22825;&#27668;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#23558;&#39044;&#27979;&#38598;&#21512;&#35270;&#20026;&#19968;&#32452;&#26080;&#24207;&#30340;&#25104;&#21592;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#23545;&#25104;&#21592;&#39034;&#24207;&#30340;&#25490;&#21015;&#32622;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#38142;&#25509;&#20989;&#25968;&#12290;&#22312;&#22320;&#34920;&#28201;&#24230;&#21644;&#39118;&#36895;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#21518;&#22788;&#29702;&#29992;&#20110;&#23558;&#21407;&#22987;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#38598;&#21512;&#36716;&#21270;&#20026;&#21487;&#38752;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36825;&#19968;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36890;&#24120;&#22522;&#20110;&#38598;&#21512;&#27010;&#35201;&#32479;&#35745;&#20449;&#24687;&#24182;&#24573;&#30053;&#38598;&#21512;&#20998;&#24067;&#30340;&#32454;&#33410;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#23558;&#39044;&#27979;&#38598;&#21512;&#35270;&#20026;&#19968;&#32452;&#26080;&#24207;&#30340;&#25104;&#21592;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#23545;&#25104;&#21592;&#39034;&#24207;&#30340;&#25490;&#21015;&#32622;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#38142;&#25509;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#26657;&#20934;&#24230;&#21644;&#38160;&#24230;&#35780;&#20272;&#25152;&#33719;&#24471;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#36136;&#37327;&#65292;&#24182;&#23558;&#27169;&#22411;&#19982;&#32463;&#20856;&#30340;&#22522;&#20934;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#22788;&#29702;&#22320;&#34920;&#28201;&#24230;&#21644;&#39118;&#36895;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#20026;&#20102;&#21152;&#28145;&#23545;&#23398;&#20064;&#25512;&#29702;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#25442;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. In this study, we examine the use of permutation-invariant neural networks for this task. In contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks which treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. We evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness, and compare the models against classical and neural network-based benchmark methods. In case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. To deepen the understanding of the learned inference process, we further propose a permutation-based importance
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IPA&#30340;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12871</link><description>&lt;p&gt;
IPA&#65306;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency. (arXiv:2308.12871v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12871
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IPA&#30340;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#20135;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#22320;&#20248;&#21270;&#22810;&#27169;&#22411;&#25512;&#29702;&#31649;&#36947;&#20197;&#23454;&#29616;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#23545;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#31616;&#21270;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#24191;&#38420;&#32780;&#22797;&#26434;&#30340;&#26435;&#34913;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#20379;&#32773;&#36890;&#24120;&#36873;&#25321;&#32771;&#34385;&#20854;&#20013;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#21327;&#35843;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#31649;&#29702;&#25512;&#29702;&#31649;&#36947;&#20013;&#27169;&#22411;&#21464;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IPA&#65292;&#19968;&#31181;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#27599;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#27169;&#22411;&#21464;&#20307;&#26159;&#21516;&#19968;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21516;&#29256;&#26412;&#65292;&#20854;&#36164;&#28304;&#38656;&#27714;&#12289;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#26377;&#25152;&#19981;&#21516;&#12290;IPA&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#26469;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;SLA&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs
&lt;/p&gt;</description></item><item><title>TemperatureGAN&#26159;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20351;&#29992;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#25968;&#25454;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.17248</link><description>&lt;p&gt;
TemperatureGAN: &#21306;&#22495;&#22823;&#27668;&#28201;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures. (arXiv:2306.17248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17248
&lt;/p&gt;
&lt;p&gt;
TemperatureGAN&#26159;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20351;&#29992;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#25968;&#25454;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29983;&#25104;&#22120;&#23545;&#20110;&#20272;&#35745;&#27668;&#20505;&#23545;&#21508;&#20010;&#39046;&#22495;&#30340;&#24433;&#21709;&#38750;&#24120;&#26377;&#29992;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#27668;&#20505;&#39118;&#38505;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#33021;&#28304;&#31995;&#32479;&#65292;&#38656;&#35201;&#20934;&#30830;&#65288;&#19982;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#26377;&#32479;&#35745;&#30456;&#20284;&#24615;&#65289;&#12289;&#21487;&#38752;&#65288;&#19981;&#20135;&#29983;&#38169;&#35823;&#26679;&#26412;&#65289;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21271;&#32654;&#38470;&#22320;&#25968;&#25454;&#21516;&#21270;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;TemperatureGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#26376;&#20221;&#12289;&#20301;&#32622;&#21644;&#26102;&#38388;&#27573;&#20026;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#27599;&#23567;&#26102;&#20998;&#36776;&#29575;&#29983;&#25104;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#25351;&#26631;&#26469;&#34913;&#37327;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;TemperatureGAN&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#24050;&#30693;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic generators are useful for estimating climate impacts on various sectors. Projecting climate risk in various sectors, e.g. energy systems, requires generators that are accurate (statistical resemblance to ground-truth), reliable (do not produce erroneous examples), and efficient. Leveraging data from the North American Land Data Assimilation System, we introduce TemperatureGAN, a Generative Adversarial Network conditioned on months, locations, and time periods, to generate 2m above ground atmospheric temperatures at an hourly resolution. We propose evaluation methods and metrics to measure the quality of generated samples. We show that TemperatureGAN produces high-fidelity examples with good spatial representation and temporal dynamics consistent with known diurnal cycles.
&lt;/p&gt;</description></item><item><title>innsight&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;R&#21253;&#65292;&#33021;&#22815;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#35299;&#37322;&#26469;&#33258;&#20219;&#20309;R&#21253;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#20197;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21464;&#37327;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.10822</link><description>&lt;p&gt;
&#21033;&#29992;innsight&#21253;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Interpreting Deep Neural Networks with the Package innsight. (arXiv:2306.10822v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10822
&lt;/p&gt;
&lt;p&gt;
innsight&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;R&#21253;&#65292;&#33021;&#22815;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#35299;&#37322;&#26469;&#33258;&#20219;&#20309;R&#21253;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#20197;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21464;&#37327;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
R&#21253;innsight&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25152;&#35859;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21464;&#37327;&#35299;&#37322;&#12290;&#38500;&#20102;&#32479;&#19968;&#30340;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#22806;&#65292;&#35813;&#21253;&#22312;&#19977;&#20010;&#26041;&#38754;&#33073;&#39062;&#32780;&#20986;&#65306;&#39318;&#20808;&#65292;&#23427;&#36890;&#24120;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;R&#21253;&#12290;&#20854;&#27425;&#65292;&#23427;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#20801;&#35768;&#35299;&#37322;&#26469;&#33258;&#20219;&#20309;R&#21253;&#65292;&#21253;&#25324;keras&#12289;torch&#12289;neuralnet&#29978;&#33267;&#29992;&#25143;&#23450;&#20041;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#24456;&#28789;&#27963;&#65292;&#20294;innsight&#22312;&#20869;&#37096;&#20174;torch&#21253;&#30340;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#25968;&#32452;&#35745;&#31639;&#20013;&#21463;&#30410;&#65292;&#36825;&#24314;&#31435;&#22312;LibTorch&#65288;PyTorch&#30340;C++&#21518;&#31471;&#65289;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;Python&#20381;&#36182;&#12290;&#26368;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#21508;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#34920;&#26684;&#12289;&#20449;&#21495;&#12289;&#22270;&#20687;&#25968;&#25454;&#25110;&#36825;&#20123;&#25968;&#25454;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#20351;&#29992;plotly&#21253;&#20197;&#20132;&#20114;&#26041;&#24335;&#21576;&#29616;&#36825;&#20123;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The R package innsight offers a general toolbox for revealing variable-wise interpretations of deep neural networks' predictions with so-called feature attribution methods. Aside from the unified and user-friendly framework, the package stands out in three ways: It is generally the first R package implementing feature attribution methods for neural networks. Secondly, it operates independently of the deep learning library allowing the interpretation of models from any R package, including keras, torch, neuralnet, and even custom models. Despite its flexibility, innsight benefits internally from the torch package's fast and efficient array calculations, which builds on LibTorch $-$ PyTorch's C++ backend $-$ without a Python dependency. Finally, it offers a variety of visualization tools for tabular, signal, image data or a combination of these. Additionally, the plots can be rendered interactively using the plotly package.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20984;&#35268;&#21010;&#34920;&#24449;&#25152;&#26377;ReLU&#32593;&#32476;&#30340;&#26368;&#20248;&#38598;&#21512;&#21644;&#35299;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#23567;&#32593;&#32476;&#30340;&#26368;&#20248;&#21098;&#26525;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;ReLU&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#36830;&#32493;&#30340;&#26465;&#20214;&#65292;&#24182;&#20026;&#26368;&#23567;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#28789;&#25935;&#24230;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00119</link><description>&lt;p&gt;
ReLU&#32593;&#32476;&#30340;&#26368;&#20248;&#38598;&#21512;&#21644;&#35299;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Optimal Sets and Solution Paths of ReLU Networks. (arXiv:2306.00119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20984;&#35268;&#21010;&#34920;&#24449;&#25152;&#26377;ReLU&#32593;&#32476;&#30340;&#26368;&#20248;&#38598;&#21512;&#21644;&#35299;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#23567;&#32593;&#32476;&#30340;&#26368;&#20248;&#21098;&#26525;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;ReLU&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#36830;&#32493;&#30340;&#26465;&#20214;&#65292;&#24182;&#20026;&#26368;&#23567;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#28789;&#25935;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38750;&#20984;&#35757;&#32451;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20984;&#35268;&#21010;&#38382;&#39064;&#65292;&#26469;&#21051;&#30011;&#26368;&#20248;ReLU&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20984;&#21442;&#25968;&#21270;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#30001;&#19968;&#20010;&#22810;&#38754;&#20307;&#38598;&#21512;&#32473;&#20986;&#65292;&#24182;&#23558;&#36825;&#20010;&#34920;&#24449;&#25193;&#23637;&#21040;&#20102;&#38750;&#20984;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#20248;&#38598;&#12290;&#30001;&#20110;ReLU&#35757;&#32451;&#38382;&#39064;&#30340;&#25152;&#26377;&#31283;&#23450;&#28857;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#23376;&#37319;&#26679;&#20984;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25152;&#26377;&#38750;&#20984;&#30446;&#26631;&#30340;&#20020;&#30028;&#28857;&#25552;&#20379;&#20102;&#36890;&#29992;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#35745;&#31639;&#26368;&#23567;&#32593;&#32476;&#30340;&#26368;&#20248;&#21098;&#26525;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;ReLU&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#36830;&#32493;&#30340;&#26465;&#20214;&#65292;&#24182;&#20026;&#26368;&#23567;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#28789;&#25935;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an analytical framework to characterize the set of optimal ReLU neural networks by reformulating the non-convex training problem as a convex program. We show that the global optima of the convex parameterization are given by a polyhedral set and then extend this characterization to the optimal set of the non-convex training objective. Since all stationary points of the ReLU training problem can be represented as optima of sub-sampled convex programs, our work provides a general expression for all critical points of the non-convex objective. We then leverage our results to provide an optimal pruning algorithm for computing minimal networks, establish conditions for the regularization path of ReLU networks to be continuous, and develop sensitivity results for minimal ReLU networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;DARTS&#20248;&#21270;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20197;&#24448;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14402</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;iable&#30340;&#25628;&#32034;&#20307;&#31995;&#26550;&#26500;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Emotion Recognition Performance using Differentiable Architecture Search. (arXiv:2305.14402v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;DARTS&#20248;&#21270;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20197;&#24448;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;(SER)&#26159;&#23454;&#29616;&#24773;&#24863;&#24863;&#30693;&#20132;&#20114;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28145;&#24230;&#23398;&#20064;(DL)&#25913;&#21892;&#20102;SER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#35774;&#35745;DL&#20307;&#31995;&#32467;&#26500;&#38656;&#35201;&#20808;&#21069;&#30340;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#12290;&#40723;&#21169;&#22320;&#65292;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;(NAS)&#20801;&#35768;&#33258;&#21160;&#25628;&#32034;&#26368;&#20248;DL&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#21487;&#21306;&#20998;&#30340;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;(DARTS)&#26159;&#19968;&#31181;&#20351;&#29992;NAS&#25628;&#32034;&#26368;&#20248;&#21270;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DARTS&#29992;&#20110;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25913;&#21892;SER&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;CNN LSTM&#32806;&#21512;&#30340;&#21407;&#22240;&#26159;&#32467;&#26524;&#34920;&#26126;&#31867;&#20284;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;&#34429;&#28982;SER&#30740;&#31350;&#20154;&#21592;&#24050;&#23558;CNN&#21644;RNN&#20998;&#21035;&#32771;&#34385;&#65292;&#20294;DARTs&#21516;&#26102;&#29992;&#20110;CNN&#21644;LSTM&#30340;&#21487;&#34892;&#24615;&#20173;&#38656;&#35201;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;IEMOCAP&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;DA&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) is a critical enabler of emotion-aware communication in human-computer interactions. Deep Learning (DL) has improved the performance of SER models by improving model complexity. However, designing DL architectures requires prior experience and experimental evaluations. Encouragingly, Neural Architecture Search (NAS) allows automatic search for an optimum DL model. In particular, Differentiable Architecture Search (DARTS) is an efficient method of using NAS to search for optimised models. In this paper, we propose DARTS for a joint CNN and LSTM architecture for improving SER performance. Our choice of the CNN LSTM coupling is inspired by results showing that similar models offer improved performance. While SER researchers have considered CNNs and RNNs separately, the viability of using DARTs jointly for CNN and LSTM still needs exploration. Experimenting with the IEMOCAP dataset, we demonstrate that our approach outperforms best-reported results using DA
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#26263;&#29289;&#36136;&#26197;&#30340;&#28436;&#21270;&#21382;&#21490;&#19982;&#20854;&#23494;&#24230;&#20998;&#24067;&#30456;&#36830;&#25509;&#65292;&#32593;&#32476;&#21457;&#29616;&#36229;&#36807;&#28183;&#36879;&#21322;&#24452;&#30340;&#36718;&#24275;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#25551;&#36848;&#65292;&#36825;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;&#22825;&#20307;&#29289;&#29702;&#25968;&#25454;&#38598;&#20013;&#26426;&#22120;&#21327;&#21161;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03077</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26263;&#29289;&#36136;&#26197;&#30340;&#23494;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Explaining dark matter halo density profiles with neural networks. (arXiv:2305.03077v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03077
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#26263;&#29289;&#36136;&#26197;&#30340;&#28436;&#21270;&#21382;&#21490;&#19982;&#20854;&#23494;&#24230;&#20998;&#24067;&#30456;&#36830;&#25509;&#65292;&#32593;&#32476;&#21457;&#29616;&#36229;&#36807;&#28183;&#36879;&#21322;&#24452;&#30340;&#36718;&#24275;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#25551;&#36848;&#65292;&#36825;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;&#22825;&#20307;&#29289;&#29702;&#25968;&#25454;&#38598;&#20013;&#26426;&#22120;&#21327;&#21161;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#26263;&#29289;&#36136;&#26197;&#30340;&#28436;&#21270;&#21382;&#21490;&#19982;&#20854;&#23494;&#24230;&#20998;&#24067;&#30456;&#36830;&#25509;&#12290;&#35813;&#32593;&#32476;&#25429;&#33719;&#23494;&#24230;&#20998;&#24067;&#20013;&#29420;&#31435;&#30340;&#21464;&#21270;&#22240;&#32032;&#65292;&#22312;&#20302;&#32500;&#34920;&#31034;&#20013;&#29289;&#29702;&#22320;&#35299;&#37322;&#20102;&#23427;&#20204;&#65292;&#20351;&#29992;&#20102;&#20114;&#20449;&#24687;&#12290;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#32593;&#32476;&#24674;&#22797;&#20102;&#26089;&#26399;&#32452;&#35013;&#19982;&#20869;&#37096;&#36718;&#24275;&#20043;&#38388;&#30340;&#24050;&#30693;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#36229;&#36807;&#28183;&#36879;&#21322;&#24452;&#30340;&#36718;&#24275;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#25551;&#36848;&#65292;&#35813;&#21442;&#25968;&#34920;&#31034;&#26368;&#36817;&#30340;&#36136;&#37327;&#21560;&#31215;&#29575;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;&#22825;&#20307;&#29289;&#29702;&#25968;&#25454;&#38598;&#20013;&#26426;&#22120;&#21327;&#21161;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use explainable neural networks to connect the evolutionary history of dark matter halos with their density profiles. The network captures independent factors of variation in the density profiles within a low-dimensional representation, which we physically interpret using mutual information. Without any prior knowledge of the halos' evolution, the network recovers the known relation between the early time assembly and the inner profile, and discovers that the profile beyond the virial radius is described by a single parameter capturing the most recent mass accretion rate. The results illustrate the potential for machine-assisted scientific discovery in complicated astrophysical datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#40736;&#36229;&#22768;&#27874;&#22768;&#38899;&#30340;&#30417;&#30563;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.03183</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#22823;&#40736;&#36229;&#22768;&#27874;&#22768;&#38899;&#30340;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Utilizing synthetic training data for the supervised classification of rat ultrasonic vocalizations. (arXiv:2303.03183v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03183
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#40736;&#36229;&#22768;&#27874;&#22768;&#38899;&#30340;&#30417;&#30563;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#40736;&#20135;&#29983;&#39057;&#29575;&#39640;&#36798;120kHz&#30340;&#36229;&#22768;&#27874;&#22768;&#38899;&#65288;USVs&#65289;&#12290;&#36825;&#20123;&#21483;&#22768;&#22312;&#31038;&#20132;&#34892;&#20026;&#20013;&#24456;&#37325;&#35201;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;&#22768;&#38899;&#36890;&#20449;&#30340;&#21151;&#33021;&#21644;&#21151;&#33021;&#32010;&#20081;&#12290;&#25163;&#21160;&#35782;&#21035;USVs&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#23376;&#31867;&#38750;&#24120;&#32791;&#26102;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#20110;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20294;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#21487;&#33021;&#24456;&#39640;&#65292;&#24182;&#19988;&#24403;&#21069;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;&#20154;&#19982;&#20004;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#65292;DeepSqueak&#21644;VocalMat&#65292;&#22312;&#21547;&#26377;&#22823;&#40736;USVs&#30340;&#38899;&#39057;&#20013;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#23558;&#21512;&#25104;&#30340;USVs&#25554;&#20837;&#21040;VocalMat CNN&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#26159;&#21542;&#21487;&#20197;&#20943;&#23569;&#29983;&#25104;&#35757;&#32451;&#38598;&#30340;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Murine rodents generate ultrasonic vocalizations (USVs) with frequencies that extend to around 120kHz. These calls are important in social behaviour, and so their analysis can provide insights into the function of vocal communication, and its dysfunction. The manual identification of USVs, and subsequent classification into different subcategories is time consuming. Although machine learning approaches for identification and classification can lead to enormous efficiency gains, the time and effort required to generate training data can be high, and the accuracy of current approaches can be problematic. Here we compare the detection and classification performance of a trained human against two convolutional neural networks (CNNs), DeepSqueak and VocalMat, on audio containing rat USVs. Furthermore, we test the effect of inserting synthetic USVs into the training data of the VocalMat CNN as a means of reducing the workload associated with generating a training set. Our results indicate th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\alpha$-&#25955;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20272;&#35745;&#38543;&#26426;&#29109;&#20135;&#29983;&#26102;&#34920;&#29616;&#20986;&#26356;&#21152;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#24378;&#38750;&#24179;&#34913;&#39537;&#21160;&#25110;&#32773;&#32531;&#24930;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#12290;&#36873;&#25321;$\alpha=-0.5$&#33021;&#33719;&#24471;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.02901</link><description>&lt;p&gt;
$\alpha$-&#25955;&#24230;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;&#20102;&#29109;&#20135;&#29983;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-divergence Improves the Entropy Production Estimation via Machine Learning. (arXiv:2303.02901v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\alpha$-&#25955;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20272;&#35745;&#38543;&#26426;&#29109;&#20135;&#29983;&#26102;&#34920;&#29616;&#20986;&#26356;&#21152;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#24378;&#38750;&#24179;&#34913;&#39537;&#21160;&#25110;&#32773;&#32531;&#24930;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#12290;&#36873;&#25321;$\alpha=-0.5$&#33021;&#33719;&#24471;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20174;&#36712;&#36857;&#25968;&#25454;&#20272;&#35745;&#38543;&#26426;&#29109;&#20135;&#29983;&#65288;EP&#65289;&#30340;&#31639;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#36825;&#31867;&#31639;&#27861;&#30340;&#20851;&#38190;&#26159;&#25214;&#21040;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#26368;&#23567;&#21270;&#33021;&#22815;&#20445;&#35777;&#20934;&#30830;&#30340;EP&#20272;&#35745;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#37027;&#20123;&#23454;&#29616;&#20102;$\alpha$-&#25955;&#24230;&#30340;&#21464;&#20998;&#34920;&#31034;&#30340;&#20989;&#25968;&#65292;&#21487;&#20197;&#29992;&#20110;EP&#20272;&#35745;&#12290;&#36890;&#36807;&#23558;$\alpha$&#22266;&#23450;&#20026;&#22312;-1&#21040;0&#20043;&#38388;&#30340;&#20540;&#65292;$\alpha$-NEEP&#65288;Entropy Production&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#65289;&#22312;&#24378;&#38750;&#24179;&#34913;&#39537;&#21160;&#25110;&#32773;&#32531;&#24930;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#20026;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#20123;&#24773;&#20917;&#23545;&#22522;&#20110;Kullback-Leibler&#25955;&#24230;&#65288;$\alpha=0$&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#36873;&#25321;$\alpha=-0.5$&#24448;&#24448;&#33021;&#24471;&#21040;&#26368;&#20248;&#32467;&#26524;&#12290;&#20026;&#20102;&#35777;&#23454;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#31934;&#30830;&#27714;&#35299;&#30340;EP&#20272;&#35745;&#38382;&#39064;&#31616;&#21270;&#27169;&#22411;&#65292;&#20854;&#25439;&#22833;&#20989;&#25968;&#20026;land
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge of interest in the algorithmic estimation of stochastic entropy production (EP) from trajectory data via machine learning. A crucial element of such algorithms is the identification of a loss function whose minimization guarantees the accurate EP estimation. In this study, we show that there exists a host of loss functions, namely those implementing a variational representation of the $\alpha$-divergence, which can be used for the EP estimation. By fixing $\alpha$ to a value between $-1$ and $0$, the $\alpha$-NEEP (Neural Estimator for Entropy Production) exhibits a much more robust performance against strong nonequilibrium driving or slow dynamics, which adversely affects the existing method based on the Kullback-Leibler divergence ($\alpha = 0$). In particular, the choice of $\alpha = -0.5$ tends to yield the optimal results. To corroborate our findings, we present an exactly solvable simplification of the EP estimation problem, whose loss function land
&lt;/p&gt;</description></item><item><title>Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.02506</link><description>&lt;p&gt;
Prismer: &#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38598;&#21512;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02506
&lt;/p&gt;
&lt;p&gt;
Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#23427;&#20204;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Prismer&#65292;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#12290;Prismer&#21482;&#38656;&#35201;&#35757;&#32451;&#23569;&#37327;&#32452;&#20214;&#65292;&#22823;&#37096;&#20998;&#32593;&#32476;&#26435;&#37325;&#20174;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#39046;&#22495;&#19987;&#23478;&#20013;&#32487;&#25215;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20923;&#32467;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#21487;&#20197;&#26377;&#25928;&#22320;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/NVlabs/prismer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#26080;&#32447;&#30005;&#39057;&#35889;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23556;&#30005;&#25968;&#25454;&#20013;&#24555;&#36895;&#25214;&#21040;&#31867;&#20284;&#24863;&#20852;&#36259;&#20449;&#21495;&#12290;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;B-Variational Autoencoder&#20197;&#21450;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#23618;&#21644;&#38468;&#21152;&#20803;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#25628;&#32034;&#31639;&#27861;&#33021;&#22815;&#20943;&#36731;&#32321;&#37325;&#30340;&#25163;&#21160;&#20449;&#21495;&#23457;&#26597;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2302.13854</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#26080;&#32447;&#30005;&#39057;&#35889;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Neural Network Based Reverse Radio Spectrogram Search Algorithm. (arXiv:2302.13854v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#26080;&#32447;&#30005;&#39057;&#35889;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23556;&#30005;&#25968;&#25454;&#20013;&#24555;&#36895;&#25214;&#21040;&#31867;&#20284;&#24863;&#20852;&#36259;&#20449;&#21495;&#12290;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;B-Variational Autoencoder&#20197;&#21450;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#23618;&#21644;&#38468;&#21152;&#20803;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#25628;&#32034;&#31639;&#27861;&#33021;&#22815;&#20943;&#36731;&#32321;&#37325;&#30340;&#25163;&#21160;&#20449;&#21495;&#23457;&#26597;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#23556;&#30005;&#22825;&#25991;&#20202;&#22120;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#26085;&#30410;&#22797;&#26434;&#30340;&#23556;&#30005;&#39057;&#29575;&#24178;&#25200;(RFI)&#29615;&#22659;&#38656;&#35201;&#26356;&#21152;&#22797;&#26434;&#30340;RFI&#25298;&#32477;&#31639;&#27861;&#12290;&#23545;&#20110;&#30636;&#21464;&#21644;&#25216;&#26415;&#20449;&#21495;&#30340;&#25628;&#32034;&#20855;&#26377;&#8220;&#22823;&#28023;&#25438;&#38024;&#8221;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#21487;&#20197;&#30830;&#23450;&#24863;&#20852;&#36259;&#20449;&#21495;&#26159;&#21542;&#20855;&#26377;&#29420;&#29305;&#23646;&#24615;&#25110;&#26159;&#21542;&#23646;&#20110;&#26576;&#20010;&#26356;&#22823;&#30340;&#26377;&#23475;RFI&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#36807;&#21435;&#65292;&#36825;&#31181;&#23457;&#26597;&#38656;&#35201;&#32791;&#26102;&#32791;&#21147;&#22320;&#25163;&#21160;&#26816;&#26597;&#22823;&#37327;&#30340;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#27169;&#22359;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23556;&#30005;&#39057;&#35889;&#25968;&#25454;&#20013;&#25628;&#32034;&#31867;&#20284;&#24863;&#20852;&#36259;&#20449;&#21495;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#33021;&#37327;&#26816;&#27979;&#31639;&#27861;&#36820;&#22238;&#30340;&#20449;&#21495;&#23545;B-Variational Autoencoder&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32463;&#20856;&#30340;Transformer&#26550;&#26500;&#20013;&#36866;&#24212;&#20102;&#19968;&#20010;&#20301;&#32622;&#23884;&#20837;&#23618;&#65292;&#26469;&#23884;&#20837;&#38468;&#21152;&#30340;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#39057;&#29575;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;B-V&#30340;&#32534;&#30721;&#22120;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern radio astronomy instruments generate vast amounts of data, and the increasingly challenging radio frequency interference (RFI) environment necessitates ever-more sophisticated RFI rejection algorithms. The "needle in a haystack" nature of searches for transients and technosignatures requires us to develop methods that can determine whether a signal of interest has unique properties, or is a part of some larger set of pernicious RFI. In the past, this vetting has required onerous manual inspection of very large numbers of signals. In this paper we present a fast and modular deep learning algorithm to search for lookalike signals of interest in radio spectrogram data. First, we trained a B-Variational Autoencoder on signals returned by an energy detection algorithm. We then adapted a positional embedding layer from classical Transformer architecture to a embed additional metadata, which we demonstrate using a frequency-based embedding. Next we used the encoder component of the B-V
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#22522;&#20110;&#34507;&#30333;&#36136;&#20849;&#36827;&#21270;&#30340;Transformer&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#31232;&#32570;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06120</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#25509;&#35302;&#39044;&#27979;&#27169;&#22411;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#21270;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
Knowledge from Large-Scale Protein Contact Prediction Models Can Be Transferred to the Data-Scarce RNA Contact Prediction Task. (arXiv:2302.06120v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#22522;&#20110;&#34507;&#30333;&#36136;&#20849;&#36827;&#21270;&#30340;Transformer&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#31232;&#32570;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNA&#36890;&#36807;&#20854;&#32467;&#26500;&#22312;&#35768;&#22810;&#29983;&#29289;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#21151;&#33021;&#24448;&#24448;&#21463;&#20854;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#39044;&#27979;RNA&#24207;&#21015;&#20013;&#27599;&#20010;&#26680;&#33527;&#37240;&#20043;&#38388;&#30340;&#32467;&#26500;&#25509;&#36817;&#24615;&#21487;&#20197;&#34920;&#24449;RNA&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#24182;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#34507;&#30333;&#36136;&#20849;&#36827;&#21270;&#30340;Transformer&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#30001;&#20110;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#27604;RNA&#25509;&#35302;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#38543;&#21518;&#30340;&#26694;&#26550;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25968;&#25454;&#31232;&#32570;&#30340;&#29942;&#39048;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#20351;&#29992;&#20844;&#24320;&#30340;&#34507;&#30333;&#36136;&#27169;&#22411;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#25928;&#26524;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34507;&#30333;&#36136;&#23398;&#20064;&#30340;&#32467;&#26500;&#27169;&#24335;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#19978;&#65292;&#20026;RNA&#32467;&#26500;&#39044;&#27979;&#24320;&#36767;&#20102;&#28508;&#22312;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNA, whose functionality is largely determined by its structure, plays an important role in many biological activities. The prediction of pairwise structural proximity between each nucleotide of an RNA sequence can characterize the structural information of the RNA. Historically, this problem has been tackled by machine learning models using expert-engineered features and trained on scarce labeled datasets. Here, we find that the knowledge learned by a protein-coevolution Transformer-based deep neural network can be transferred to the RNA contact prediction task. As protein datasets are orders of magnitude larger than those for RNA contact prediction, our findings and the subsequent framework greatly reduce the data scarcity bottleneck. Experiments confirm that RNA contact prediction through transfer learning using a publicly available protein model is greatly improved. Our findings indicate that the learned structural patterns of proteins can be transferred to RNAs, opening up potenti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25277;&#35937;&#25688;&#35201;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AnswerSumm&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;20\%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2212.09726</link><description>&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#25552;&#39640;&#25277;&#35937;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences. (arXiv:2212.09726v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09726
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#26080;&#20851;&#21477;&#23376;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25277;&#35937;&#25688;&#35201;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AnswerSumm&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;20\%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#31995;&#32479;&#22312;&#29983;&#25104;&#30475;&#20284;&#27969;&#21033;&#30340;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#20107;&#23454;&#27491;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#26080;&#20851;&#30340;&#36755;&#20837;&#25991;&#26412;&#37096;&#20998;&#21487;&#33021;&#23548;&#33268;&#20107;&#23454;&#19981;&#19968;&#33268;&#65292;&#20316;&#20026;&#28151;&#28102;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#25928;&#24212;&#30340;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26469;&#37327;&#21270;&#28151;&#28102;&#30340;&#31243;&#24230;&#65292;&#24182;&#20934;&#30830;&#34913;&#37327;&#20854;&#23545;&#25688;&#35201;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#20174;&#29702;&#35770;&#32467;&#26524;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#65292;&#24403;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#30456;&#20851;&#21477;&#23376;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#26469;&#25511;&#21046;&#36825;&#31181;&#28151;&#28102;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#30340;&#21051;&#30011;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#30456;&#20851;&#21477;&#23376;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;AnswerSumm&#19978;&#65288;&#21442;&#32771;&#25991;&#29486;&#65306;fabbri2021answersumm&#65289;&#19978;&#24378;&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#23558;&#20934;&#30830;&#24615;&#24471;&#20998;&#25552;&#39640;&#20102;20\%&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of factual correctness is an issue that still plagues state-of-the-art summarization systems despite their impressive progress on generating seemingly fluent summaries. In this paper, we show that factual inconsistency can be caused by irrelevant parts of the input text, which act as confounders. To that end, we leverage information-theoretic measures of causal effects to quantify the amount of confounding and precisely quantify how they affect the summarization performance. Based on insights derived from our theoretical results, we design a simple multi-task model to control such confounding by leveraging human-annotated relevant sentences when available. Crucially, we give a principled characterization of data distributions where such confounding can be large thereby necessitating the use of human annotated relevant sentences to generate factual summaries. Our approach improves faithfulness scores by 20\% over strong baselines on AnswerSumm \citep{fabbri2021answersumm}, a conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#20998;&#24067;&#25311;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#23616;&#20998;&#24067;&#25311;&#21512;&#21644;&#23616;&#37096;&#20998;&#24067;&#25311;&#21512;&#65292;&#21487;&#20197;&#38480;&#21046;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01521</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#20998;&#24067;&#25311;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distribution Fitting for Combating Mode Collapse in Generative Adversarial Networks. (arXiv:2212.01521v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#20998;&#24067;&#25311;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#23616;&#20998;&#24067;&#25311;&#21512;&#21644;&#23616;&#37096;&#20998;&#24067;&#25311;&#21512;&#65292;&#21487;&#20197;&#38480;&#21046;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24335;&#23849;&#28291;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#32771;&#23519;&#20102;&#27169;&#24335;&#23849;&#28291;&#30340;&#21407;&#22240;&#12290;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#65292;&#19968;&#20123;&#23376;&#20998;&#24067;&#21487;&#33021;&#34987;&#38169;&#36807;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#29983;&#25104;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#19981;&#21516;&#65292;GAN&#30446;&#26631;&#20173;&#28982;&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24809;&#32602;&#39033;&#30340;&#20840;&#23616;&#20998;&#24067;&#25311;&#21512;&#65288;GDF&#65289;&#26041;&#27861;&#26469;&#38480;&#21046;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#24403;&#29983;&#25104;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;GDF&#23558;&#20351;&#30446;&#26631;&#21464;&#24471;&#26356;&#38590;&#36798;&#21040;&#26368;&#23567;&#20540;&#65292;&#32780;&#21407;&#22987;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#19981;&#21464;&#12290;&#38024;&#23545;&#26080;&#27861;&#33719;&#24471;&#25972;&#20307;&#30495;&#23454;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#20998;&#24067;&#25311;&#21512;&#65288;LDF&#65289;&#26041;&#27861;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;GDF&#21644;LDF&#30340;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mode collapse is a significant unsolved issue of generative adversarial networks. In this work, we examine the causes of mode collapse from a novel perspective. Due to the nonuniform sampling in the training process, some sub-distributions may be missed when sampling data. As a result, even when the generated distribution differs from the real one, the GAN objective can still achieve the minimum. To address the issue, we propose a global distribution fitting (GDF) method with a penalty term to confine the generated data distribution. When the generated distribution differs from the real one, GDF will make the objective harder to reach the minimal value, while the original global minimum is not changed. To deal with the circumstance when the overall real data is unreachable, we also propose a local distribution fitting (LDF) method. Experiments on several benchmarks demonstrate the effectiveness and competitive performance of GDF and LDF.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#25351;&#26631;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#19981;&#24847;&#21619;&#30528;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2212.00219</link><description>&lt;p&gt;
&#20320;&#26159;&#21542;&#27491;&#30830;&#20351;&#29992;&#20102;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are you using test log-likelihood correctly?. (arXiv:2212.00219v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00219
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#25351;&#26631;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#19981;&#24847;&#21619;&#30528;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#24120;&#34987;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#21516;&#19968;&#25968;&#25454;&#65292;&#25110;&#32773;&#27604;&#36739;&#25311;&#21512;&#21516;&#19968;&#27010;&#29575;&#27169;&#22411;&#30340;&#19981;&#21516;&#36817;&#20284;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#22522;&#20110;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#30340;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#30446;&#26631;&#30456;&#30683;&#30462;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20363;&#23376;&#34920;&#26126;&#65306;&#65288;i&#65289;&#36798;&#21040;&#26356;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#19981;&#24517;&#24847;&#21619;&#30528;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#65288;ii&#65289;&#22522;&#20110;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#27604;&#36739;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#32467;&#35770;&#21487;&#33021;&#19982;&#22522;&#20110;&#22343;&#26041;&#26681;&#35823;&#24046;&#30340;&#32467;&#35770;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#27169;&#24072;&#20195;&#29702;&#65292;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#25216;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#25506;&#32034;&#21644;&#25216;&#33021;&#23398;&#20064;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#21457;&#29616;&#25216;&#33021;&#65292;&#24182;&#36890;&#36807;&#20803;&#25511;&#21046;&#22120;&#36827;&#34892;&#39640;&#25928;&#30340;&#36866;&#24212;&#12290;&#24314;&#27169;&#24072;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#21644;&#21516;&#26102;&#25910;&#38598;&#25968;&#25454;&#20013;&#23398;&#20064;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.13350</link><description>&lt;p&gt;
&#24314;&#27169;&#24072;&#65306;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Choreographer: Learning and Adapting Skills in Imagination. (arXiv:2211.13350v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#27169;&#24072;&#20195;&#29702;&#65292;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#25216;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#25506;&#32034;&#21644;&#25216;&#33021;&#23398;&#20064;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#21457;&#29616;&#25216;&#33021;&#65292;&#24182;&#36890;&#36807;&#20803;&#25511;&#21046;&#22120;&#36827;&#34892;&#39640;&#25928;&#30340;&#36866;&#24212;&#12290;&#24314;&#27169;&#24072;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#21644;&#21516;&#26102;&#25910;&#38598;&#25968;&#25454;&#20013;&#23398;&#20064;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#23398;&#20064;&#26088;&#22312;&#22312;&#27809;&#26377;&#22806;&#37096;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20016;&#23500;&#30340;&#34892;&#20026;&#24211;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#25511;&#21046;&#21644;&#24433;&#21709;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#32570;&#20047;&#36866;&#24403;&#30340;&#30693;&#35782;&#21644;&#25506;&#32034;&#65292;&#25216;&#33021;&#21487;&#33021;&#21482;&#33021;&#25511;&#21046;&#29615;&#22659;&#30340;&#26377;&#38480;&#21306;&#22495;&#65292;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#21033;&#29992;&#23398;&#24471;&#30340;&#25216;&#33021;&#34892;&#20026;&#20197;&#25968;&#25454;&#39640;&#25928;&#30340;&#26041;&#24335;&#36866;&#24212;&#21518;&#32493;&#20219;&#21153;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24314;&#27169;&#24072;&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#21033;&#29992;&#20854;&#19990;&#30028;&#27169;&#22411;&#22312;&#24819;&#35937;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25506;&#32034;&#21644;&#25216;&#33021;&#23398;&#20064;&#36807;&#31243;&#35299;&#32806;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#21457;&#29616;&#25216;&#33021;&#12290;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#20195;&#29702;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#26469;&#35780;&#20272;&#21644;&#35843;&#25972;&#36890;&#36807;&#22312;&#24819;&#35937;&#20013;&#24182;&#34892;&#37096;&#32626;&#24050;&#23398;&#24471;&#30340;&#25216;&#33021;&#65292;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#23427;&#20204;&#12290;&#24314;&#27169;&#24072;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#21644;&#21516;&#26102;&#25910;&#38598;&#25968;&#25454;&#20013;&#23398;&#20064;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20445;&#35777;&#20102;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#20197;&#21450;&#38750;&#36127;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#36895;&#24230;&#21644;&#20248;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.02672</link><description>&lt;p&gt;
&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;:&#26368;&#22823;&#29109;&#21407;&#21017;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Non-negative Matrix Factorization: a Maximum-Entropy-Principle Approach. (arXiv:2210.02672v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20445;&#35777;&#20102;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#20197;&#21450;&#38750;&#36127;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#36895;&#24230;&#21644;&#20248;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;ONMF&#65289;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20004;&#20010;&#38750;&#36127;&#30697;&#38453;&#65288;&#29305;&#24449;&#30697;&#38453;&#21644;&#28151;&#21512;&#30697;&#38453;&#65289;&#30340;&#20056;&#31215;&#26469;&#36817;&#20284;&#36755;&#20837;&#25968;&#25454;&#30697;&#38453;&#65292;&#20854;&#20013;&#19968;&#20010;&#30697;&#38453;&#26159;&#27491;&#20132;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;ONMF&#35299;&#37322;&#20026;&#29305;&#23450;&#30340;&#35774;&#26045;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;ONMF&#38382;&#39064;&#37319;&#29992;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;FLP&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20445;&#35777;&#20102;&#29305;&#24449;&#30697;&#38453;&#25110;&#28151;&#21512;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#20004;&#32773;&#30340;&#38750;&#36127;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#30340;&#8220;&#30495;&#23454;&#8221;&#28508;&#22312;&#29305;&#24449;&#25968;&#37327;&#30340;&#29305;&#24449;-&#36229;&#21442;&#25968;&#29992;&#20110;ONMF&#12290;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#26631;&#20934;&#30340;&#22522;&#22240;&#33455;&#29255;&#25968;&#32452;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#21644;&#24615;&#33021;&#36895;&#24230;&#65292;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new methodology to solve the orthogonal nonnegative matrix factorization (ONMF) problem, where the objective is to approximate an input data matrix by a product of two nonnegative matrices, the features matrix and the mixing matrix, where one of them is orthogonal. We show how the ONMF can be interpreted as a specific facility-location problem (FLP), and adapt a maximum-entropy-principle based solution for FLP to the ONMF problem. The proposed approach guarantees orthogonality and sparsity of the features or the mixing matrix, while ensuring nonnegativity of both. Additionally, our methodology develops a quantitative characterization of ``true" number of underlying features - a hyperparameter required for the ONMF. An evaluation of the proposed method conducted on synthetic datasets, as well as a standard genetic microarray dataset indicates significantly better sparsity, orthogonality, and performance speed compared to similar methods in the literature, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20154;&#26426;&#20915;&#31574;&#30340;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#31639;&#27861;&#25512;&#33616;&#23545;&#36873;&#25321;&#30340;&#24433;&#21709;&#21644;&#35774;&#35745;&#65292;&#29305;&#21035;&#20851;&#27880;&#31639;&#27861;&#23545;&#20559;&#22909;&#30340;&#25913;&#21464;&#65292;&#20197;&#35299;&#20915;&#31639;&#27861;&#36741;&#21161;&#21487;&#33021;&#24102;&#26469;&#30340;&#24847;&#22806;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.07626</link><description>&lt;p&gt;
&#31639;&#27861;&#36741;&#21161;&#19979;&#30340;&#25512;&#33616;&#30456;&#20851;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Assistance with Recommendation-Dependent Preferences. (arXiv:2208.07626v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20154;&#26426;&#20915;&#31574;&#30340;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#31639;&#27861;&#25512;&#33616;&#23545;&#36873;&#25321;&#30340;&#24433;&#21709;&#21644;&#35774;&#35745;&#65292;&#29305;&#21035;&#20851;&#27880;&#31639;&#27861;&#23545;&#20559;&#22909;&#30340;&#25913;&#21464;&#65292;&#20197;&#35299;&#20915;&#31639;&#27861;&#36741;&#21161;&#21487;&#33021;&#24102;&#26469;&#30340;&#24847;&#22806;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31639;&#27861;&#25552;&#20379;&#39118;&#38505;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#23558;&#20854;&#35270;&#20026;&#23545;&#20154;&#31867;&#20915;&#31574;&#30340;&#26377;&#30410;&#36755;&#20837;&#65292;&#20363;&#22914;&#23558;&#39118;&#38505;&#35780;&#20998;&#21576;&#29616;&#32473;&#27861;&#23448;&#25110;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#20915;&#31574;&#32773;&#21487;&#33021;&#19981;&#20165;&#20165;&#21482;&#38024;&#23545;&#31639;&#27861;&#25552;&#20379;&#30340;&#20449;&#24687;&#20570;&#20986;&#21453;&#24212;&#12290;&#20915;&#31574;&#32773;&#36824;&#21487;&#33021;&#23558;&#31639;&#27861;&#25512;&#33616;&#35270;&#20026;&#40664;&#35748;&#25805;&#20316;&#65292;&#20351;&#20854;&#38590;&#20197;&#20559;&#31163;&#65292;&#20363;&#22914;&#27861;&#23448;&#22312;&#23545;&#34987;&#21578;&#36827;&#34892;&#39640;&#39118;&#38505;&#35780;&#20272;&#30340;&#26102;&#20505;&#19981;&#24895;&#24847;&#25512;&#32763;&#65292;&#25110;&#21307;&#29983;&#25285;&#24515;&#20559;&#31163;&#25512;&#33616;&#30340;&#31243;&#24207;&#20250;&#24102;&#26469;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#31639;&#27861;&#36741;&#21161;&#30340;&#36825;&#31181;&#24847;&#22806;&#21518;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20154;&#26426;&#20915;&#31574;&#30340;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31639;&#27861;&#25512;&#33616;&#23545;&#36873;&#25321;&#30340;&#24433;&#21709;&#21644;&#35774;&#35745;&#65292;&#36825;&#31181;&#24433;&#21709;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#25913;&#21464;&#20449;&#24565;&#65292;&#36824;&#36890;&#36807;&#25913;&#21464;&#20559;&#22909;&#12290;&#25105;&#20204;&#20174;&#21046;&#24230;&#22240;&#32032;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#30340;&#24050;&#26377;&#27169;&#22411;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#36825;&#20010;&#20551;&#35774;&#30340;&#21160;&#26426;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
When an algorithm provides risk assessments, we typically think of them as helpful inputs to human decisions, such as when risk scores are presented to judges or doctors. However, a decision-maker may not only react to the information provided by the algorithm. The decision-maker may also view the algorithmic recommendation as a default action, making it costly for them to deviate, such as when a judge is reluctant to overrule a high-risk assessment for a defendant or a doctor fears the consequences of deviating from recommended procedures. To address such unintended consequences of algorithmic assistance, we propose a principal-agent model of joint human-machine decision-making. Within this model, we consider the effect and design of algorithmic recommendations when they affect choices not just by shifting beliefs, but also by altering preferences. We motivate this assumption from institutional factors, such as a desire to avoid audits, as well as from well-established models in behav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#21644;&#20195;&#29702;&#27169;&#22411;&#38454;&#27573;&#37117;&#20855;&#26377;&#21019;&#26032;&#20043;&#22788;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01409</link><description>&lt;p&gt;
&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hybrid Models for Mixed Variables in Bayesian Optimization. (arXiv:2206.01409v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#21644;&#20195;&#29702;&#27169;&#22411;&#38454;&#27573;&#37117;&#20855;&#26377;&#21019;&#26032;&#20043;&#22788;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#23450;&#37327;&#65288;&#36830;&#32493;&#21644;&#25972;&#25968;&#65289;&#21644;&#23450;&#24615;&#65288;&#20998;&#31867;&#65289;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#23558;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#32467;&#26500;&#65288;MCTS&#65289;&#29992;&#20110;&#20998;&#31867;&#21464;&#37327;&#65292;&#24182;&#23558;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#12290;&#22312;&#25628;&#32034;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#23558;&#39057;&#29575;&#27966;&#30340;&#19978;&#32622;&#20449;&#24230;&#26641;&#25628;&#32034;&#65288;UCTS&#65289;&#21644;&#36125;&#21494;&#26031;&#29380;&#21033;&#20811;&#38647;&#25628;&#32034;&#31574;&#30053;&#36827;&#34892;&#23545;&#27604;&#65292;&#23637;&#31034;&#20102;&#26641;&#32467;&#26500;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#34701;&#21512;&#12290;&#22312;&#20195;&#29702;&#27169;&#22411;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#38024;&#23545;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#22312;&#32447;&#26680;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#65292;&#21253;&#25324;&#21160;&#24577;&#26680;&#36873;&#25321;&#12289;&#29420;&#29305;&#30340;UCTS&#65288;hybridM&#65289;&#21644;&#36125;&#21494;&#26031;&#26356;&#26032;&#31574;&#30053;&#65288;hybridD&#65289;&#65292;&#23558;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#23450;&#20301;&#20026;&#28151;&#21512;&#21464;&#37327;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#27493;&#12290;&#25968;&#20540;&#23454;&#39564;&#20984;&#26174;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#65292;&#20984;&#26174;&#20102;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new type of hybrid models for Bayesian optimization (BO) adept at managing mixed variables, encompassing both quantitative (continuous and integer) and qualitative (categorical) types. Our proposed new hybrid models merge Monte Carlo Tree Search structure (MCTS) for categorical variables with Gaussian Processes (GP) for continuous ones. Addressing efficiency in searching phase, we juxtapose the original (frequentist) upper confidence bound tree search (UCTS) and the Bayesian Dirichlet search strategies, showcasing the tree architecture's integration into Bayesian optimization. Central to our innovation in surrogate modeling phase is online kernel selection for mixed-variable BO. Our innovations, including dynamic kernel selection, unique UCTS (hybridM) and Bayesian update strategies (hybridD), position our hybrid models as an advancement in mixed-variable surrogate models. Numerical experiments underscore the hybrid models' superiority, highlighting their potentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#20307;&#23884;&#20837;&#25216;&#26415;&#35299;&#20915;&#20010;&#20307;&#24046;&#24322;&#24102;&#26469;&#30340;&#32676;&#20307;&#32423;&#33041;&#35299;&#30721;&#38382;&#39064;&#65292;&#24182;&#22312;&#33041;&#30913;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.14102</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#32676;&#20307;&#32423;&#33041;&#35299;&#30721;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Group-level Brain Decoding with Deep Learning. (arXiv:2205.14102v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#20307;&#23884;&#20837;&#25216;&#26415;&#35299;&#20915;&#20010;&#20307;&#24046;&#24322;&#24102;&#26469;&#30340;&#32676;&#20307;&#32423;&#33041;&#35299;&#30721;&#38382;&#39064;&#65292;&#24182;&#22312;&#33041;&#30913;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#25104;&#20687;&#25968;&#25454;&#35299;&#30721;&#22312;&#33041;&#26426;&#25509;&#21475;&#21644;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#20013;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#30001;&#20110;&#21463;&#21040;&#20010;&#20307;&#20043;&#38388;&#21464;&#24322;&#30340;&#24433;&#21709;&#65292;&#35299;&#30721;&#36890;&#24120;&#26159;&#38024;&#23545;&#20010;&#20307;&#30340;&#65292;&#24182;&#19988;&#22312;&#20010;&#20307;&#38388;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#31070;&#32463;&#31185;&#23398;&#35265;&#35299;&#65292;&#36824;&#21487;&#20197;&#20351;&#32676;&#20307;&#27169;&#22411;&#20248;&#20110;&#20010;&#20307;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#20027;&#20307;&#23884;&#20837;&#26469;&#23398;&#20064;&#21644;&#21033;&#29992;&#20010;&#20307;&#38388;&#21464;&#21270;&#30340;&#32467;&#26500;&#20316;&#20026;&#35299;&#30721;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102; WaveNet &#26550;&#26500;&#29992;&#20110;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#33041;&#30913;&#22270;&#20687;&#25968;&#25454;&#65292;15 &#20010;&#21463;&#35797;&#32773;&#35266;&#30475;&#20102; 118 &#31181;&#19981;&#21516;&#30340;&#22270;&#20687;&#65292;&#27599;&#20010;&#22270;&#20687;&#26377; 30 &#20010;&#26679;&#26412;&#65292;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#21576;&#29616;&#21518;&#30340; 1s &#31383;&#21475;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#20307;&#23884;&#20837;&#30340;&#32467;&#21512;&#23545;&#20110;&#20998;&#31867;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding brain imaging data is gaining popularity, with applications in brain-computer interfaces and the study of neural representations. Decoding is typically subject-specific and does not generalise well over subjects, due to high amounts of between subject variability. Techniques that overcome this will not only provide richer neuroscientific insights but also make it possible for group-level models to outperform subject-specific models. Here, we propose a method that uses subject embedding, analogous to word embedding in Natural Language Processing, to learn and exploit the structure in between-subject variability as part of a decoding model, our adaptation of the WaveNet architecture for classification. We apply this to magnetoencephalography data, where 15 subjects viewed 118 different images, with 30 examples per image; to classify images using the entire 1s window following image presentation. We show that the combination of deep learning and subject embedding is crucial to cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.05359</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#25506;&#32034;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections. (arXiv:2205.05359v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#26085;&#30410;&#22686;&#24378;&#65292;&#20294;&#19982;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#20854;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19979;&#38477;&#12290;&#36825;&#31181;&#25240;&#34935;&#23548;&#33268;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20986;&#29616;&#65292;&#25552;&#20379;&#20102;&#35832;&#22914;&#23616;&#37096;&#35299;&#37322;&#65288;LE&#65289;&#21644;&#23616;&#37096;&#21464;&#37327;&#24402;&#22240;&#65288;LVA&#65289;&#20043;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#39044;&#27979;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;LVA&#36890;&#24120;&#19981;&#33021;&#26377;&#25928;&#22788;&#29702;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#23558;LVA&#36716;&#25442;&#20026;&#32447;&#24615;&#25237;&#24433;&#65292;&#24182;&#20351;&#29992;&#24452;&#21521;&#28216;&#35272;&#12290;&#36825;&#23545;&#20110;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#29359;&#38169;&#65292;&#25110;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#25110;&#35266;&#27979;&#20540;&#30340;&#32858;&#31867;&#20063;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65288;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#29616;&#20195;&#27010;&#24565;&#33402;&#26415;&#20316;&#21697;&#26102;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20027;&#35201;&#20351;&#29992;&#24418;&#29366;&#21644;&#39068;&#33394;&#31561;&#23637;&#31034;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#24573;&#30053;&#20102;&#21382;&#21490;&#32972;&#26223;&#21644;&#33402;&#26415;&#23478;&#24847;&#22270;&#31561;&#38750;&#23637;&#31034;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2203.16031</link><description>&lt;p&gt;
&#20320;&#30340;&#33402;&#26415;&#26377;&#22810;&#28145;&#65306;&#19968;&#20010;&#20851;&#20110;&#21333;&#19968;&#20219;&#21153;&#12289;&#21333;&#19968;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#33402;&#26415;&#29702;&#35299;&#38480;&#21046;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#29616;&#20195;&#27010;&#24565;&#33402;&#26415;&#20316;&#21697;&#26102;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20027;&#35201;&#20351;&#29992;&#24418;&#29366;&#21644;&#39068;&#33394;&#31561;&#23637;&#31034;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#24573;&#30053;&#20102;&#21382;&#21490;&#32972;&#26223;&#21644;&#33402;&#26415;&#23478;&#24847;&#22270;&#31561;&#38750;&#23637;&#31034;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#20316;&#21697;&#30340;&#24847;&#20041;&#35745;&#31639;&#27169;&#22411;&#26159;&#22797;&#26434;&#19988;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#33402;&#26415;&#35299;&#37322;&#26159;&#22810;&#32500;&#30340;&#19988;&#39640;&#24230;&#20027;&#35266;&#30340;&#12290;&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#35843;&#26597;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNN&#65289;&#22312;&#23558;&#29616;&#20195;&#27010;&#24565;&#33402;&#26415;&#20316;&#21697;&#27491;&#30830;&#21306;&#20998;&#20026;&#33402;&#26415;&#31574;&#23637;&#20154;&#35774;&#35745;&#30340;&#30011;&#24266;&#26102;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#20551;&#35774;&#65292;&#21363;DCNN&#27169;&#22411;&#20351;&#29992;&#23637;&#31034;&#29305;&#24449;&#65288;&#22914;&#24418;&#29366;&#21644;&#39068;&#33394;&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#20351;&#29992;&#38750;&#23637;&#31034;&#29305;&#24449;&#65288;&#22914;&#21382;&#21490;&#32972;&#26223;&#21644;&#33402;&#26415;&#23478;&#24847;&#22270;&#65289;&#12290;&#21033;&#29992;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#36825;&#20004;&#20010;&#20551;&#35774;&#12290;&#20351;&#29992;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#24182;&#37492;&#21035;&#24494;&#35843;&#30340;VGG-11 DCNN&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#27010;&#24565;&#25668;&#24433;&#30011;&#24266;&#20013;&#35774;&#35745;&#30340;&#25163;&#24037;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#20102;&#36825;&#20004;&#20010;&#20551;&#35774;&#65292;&#34920;&#26126;DCNN&#27169;&#22411;&#24573;&#30053;&#20102;&#38750;&#23637;&#31034;&#29305;&#24449;&#65292;&#20165;&#20351;&#29992;&#23637;&#31034;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational modeling of artwork meaning is complex and difficult. This is because art interpretation is multidimensional and highly subjective. This paper experimentally investigated the degree to which a state-of-the-art Deep Convolutional Neural Network (DCNN), a popular Machine Learning approach, can correctly distinguish modern conceptual art work into the galleries devised by art curators. Two hypotheses were proposed to state that the DCNN model uses Exhibited Properties for classification, like shape and color, but not Non-Exhibited Properties, such as historical context and artist intention. The two hypotheses were experimentally validated using a methodology designed for this purpose. VGG-11 DCNN pre-trained on ImageNet dataset and discriminatively fine-tuned was trained on handcrafted datasets designed from real-world conceptual photography galleries. Experimental results supported the two hypotheses showing that the DCNN model ignores Non-Exhibited Properties and uses only
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#22270;&#21644;&#24352;&#37327;&#31215;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#20986;&#19968;&#31181;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26032;&#22411;&#26144;&#23556;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2201.05158</link><description>&lt;p&gt;
&#20998;&#35299;&#24335;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Decompositional Quantum Graph Neural Network. (arXiv:2201.05158v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#22270;&#21644;&#24352;&#37327;&#31215;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#20986;&#19968;&#31181;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26032;&#22411;&#26144;&#23556;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#31639;&#27861;&#21644;&#37327;&#23376;&#35745;&#31639;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#30001;&#20110;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#32570;&#20047;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#23558;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20174;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#26144;&#23556;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#37327;&#23376;&#31867;&#27604;&#25110;&#36807;&#31243;&#27169;&#25311;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#37327;&#23376;&#27604;&#29305;&#30340;&#20855;&#20307;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22522;&#20110;&#33258;&#25105;&#22270;&#30340;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;egoQGNN&#65289;&#12290;egoQGNN&#20351;&#29992;&#24352;&#37327;&#31215;&#21644;&#21333;&#20301;&#30697;&#38453;&#34920;&#31034;&#23454;&#29616;&#20102;GNN&#29702;&#35770;&#26694;&#26550;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12290;&#24403;&#34987;&#32463;&#20856;&#35745;&#31639;&#26426;&#25511;&#21046;&#26102;&#65292;egoQGNN&#21487;&#20197;&#36890;&#36807;&#20174;&#36755;&#20837;&#22270;&#30340;&#33258;&#25105;&#22270;&#20013;&#22788;&#29702;&#26469;&#23481;&#32435;&#20219;&#24847;&#22823;&#23567;&#30340;&#22270;&#65292;&#20351;&#29992;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#37327;&#23376;&#35774;&#22791;&#12290;&#35813;&#20307;&#31995;&#32467;&#26500;&#22522;&#20110;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26032;&#22411;&#26144;&#23556;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is a fast-emerging field that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as the Ego-graph based Quantum Graph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical framework using the tensor product and unity matrix representation, which greatly reduces the number of model parameters required. When controlled by a classical computer, egoQGNN can accommodate arbitrarily sized graphs by processing ego-graphs from the input graph using a modestly-sized quantum device. The architecture is based on a novel mapping from real-world data to Hilbert space. This m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#20889;&#26415;&#12289;&#25238;&#21160;&#21644;&#26368;&#20808;&#36827;&#30340;&#27973;&#23618;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#20027;&#21160;&#24674;&#22797;&#20002;&#22833;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35266;&#23519;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#36890;&#36807;&#38544;&#20889;&#26415;&#25552;&#20379;&#30340;&#36793;&#32536;&#20449;&#24687;&#30340;&#24110;&#21161;&#19979;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#38899;&#39057;&#20449;&#21495;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2111.10891</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28508;&#22312;&#20449;&#24687;&#30340;&#20027;&#21160;&#24674;&#22797;&#20002;&#22833;&#38899;&#39057;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Active Restoration of Lost Audio Signals Using Machine Learning and Latent Information. (arXiv:2111.10891v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#20889;&#26415;&#12289;&#25238;&#21160;&#21644;&#26368;&#20808;&#36827;&#30340;&#27973;&#23618;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#20027;&#21160;&#24674;&#22797;&#20002;&#22833;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35266;&#23519;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#36890;&#36807;&#38544;&#20889;&#26415;&#25552;&#20379;&#30340;&#36793;&#32536;&#20449;&#24687;&#30340;&#24110;&#21161;&#19979;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#38899;&#39057;&#20449;&#21495;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#25968;&#23383;&#37325;&#24314;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#35752;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32447;&#24615;&#25554;&#20540;&#12289;&#30456;&#20301;&#32534;&#30721;&#21644;&#38899;&#35843;&#25554;&#20837;&#25216;&#26415;&#20173;&#28982;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#34701;&#21512;&#25238;&#21160;&#12289;&#38544;&#20889;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#22120;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#37325;&#24314;&#30340;&#30740;&#31350;&#24037;&#20316;&#20013;&#25105;&#20204;&#21457;&#29616;&#24182;&#27809;&#26377;&#31867;&#20284;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38544;&#20889;&#26415;&#12289;&#21322;&#35843;&#65288;&#25238;&#21160;&#65289;&#21644;&#26368;&#20808;&#36827;&#30340;&#27973;&#23618;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;&#20351;&#29992;SPAIN&#12289;&#33258;&#22238;&#24402;&#12289;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#21644;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#32467;&#26524;&#35780;&#20272;&#12290;&#32467;&#26524;&#35266;&#23519;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#33021;&#36890;&#36807;&#38544;&#20889;&#26415;&#25552;&#20379;&#30340;&#36793;&#32536;&#20449;&#24687;&#65288;&#22914;&#28508;&#22312;&#34920;&#31034;&#65289;&#22686;&#24378;&#38899;&#39057;&#20449;&#21495;&#30340;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37325;&#24314;&#38899;&#39057;&#20449;&#21495;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital audio signal reconstruction of a lost or corrupt segment using deep learning algorithms has been explored intensively in recent years. Nevertheless, prior traditional methods with linear interpolation, phase coding and tone insertion techniques are still in vogue. However, we found no research work on reconstructing audio signals with the fusion of dithering, steganography, and machine learning regressors. Therefore, this paper proposes the combination of steganography, halftoning (dithering), and state-of-the-art shallow and deep learning methods. The results (including comparing the SPAIN, Autoregressive, deep learning-based, graph-based, and other methods) are evaluated with three different metrics. The observations from the results show that the proposed solution is effective and can enhance the reconstruction of audio signals performed by the side information (e.g., Latent representation) steganography provides. Moreover, this paper proposes a novel framework for reconstru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#38646;&#26679;&#26412;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36335;&#24452;&#20449;&#24687;&#26500;&#24314;&#20808;&#39564;&#31181;&#32676;&#65292;&#21487;&#20197;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2109.06826</link><description>&lt;p&gt;
&#20960;&#20046;&#38646;&#26679;&#26412;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Few-shot Quality-Diversity Optimization. (arXiv:2109.06826v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.06826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#38646;&#26679;&#26412;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36335;&#24452;&#20449;&#24687;&#26500;&#24314;&#20808;&#39564;&#31181;&#32676;&#65292;&#21487;&#20197;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21033;&#29992;&#20197;&#21069;&#30340;&#23398;&#20064;&#32463;&#39564;&#21644;&#35774;&#35745;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#21644;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#28041;&#21450;&#30340;&#38382;&#39064;&#39046;&#22495;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#12290;&#19968;&#20010;&#26126;&#26174;&#30340;&#20363;&#22806;&#26159;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#20248;&#21270;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#20960;&#20046;&#27809;&#26377;&#20570;&#20986;&#21162;&#21147;&#12290;QD&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27450;&#39575;&#24615;&#26497;&#23567;&#20540;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#26412;&#36136;&#19978;&#20855;&#26377;&#20302;&#26679;&#26412;&#25928;&#29575;&#30340;&#36827;&#21270;&#36807;&#31243;&#65292;&#23427;&#20204;&#20173;&#28982;&#24456;&#26114;&#36149;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20013;&#20248;&#21270;&#36335;&#24452;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26500;&#24314;&#20808;&#39564;&#31181;&#32676;&#65292;&#24403;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#20351;&#29992;&#35813;&#31181;&#32676;&#21021;&#22987;&#21270;QD&#26041;&#27861;&#26102;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#65292;&#23454;&#29616;&#36215;&#26469;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, a considerable amount of research has been dedicated to the exploitation of previous learning experiences and the design of Few-shot and Meta Learning approaches, in problem domains ranging from Computer Vision to Reinforcement Learning based control. A notable exception, where to the best of our knowledge, little to no effort has been made in this direction is Quality-Diversity (QD) optimization. QD methods have been shown to be effective tools in dealing with deceptive minima and sparse rewards in Reinforcement Learning. However, they remain costly due to their reliance on inherently sample inefficient evolutionary processes. We show that, given examples from a task distribution, information about the paths taken by optimization in parameter space can be leveraged to build a prior population, which when used to initialize QD methods in unseen environments, allows for few-shot adaptation. Our proposed method does not require backpropagation. It is simple to impl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#22823;&#24133;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#20256;&#32479;&#28857;&#31215;&#27880;&#24847;&#21147;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#39640;&#25928;&#26426;&#21046;&#30340;&#24212;&#29992;&#20351;&#24471;&#27880;&#24847;&#21147;&#27169;&#22359;&#21487;&#20197;&#26356;&#24191;&#27867;&#22320;&#38598;&#25104;&#21040;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/1812.01243</link><description>&lt;p&gt;
&#39640;&#25928;&#27880;&#24847;&#21147;&#65306;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1812.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#22823;&#24133;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#20256;&#32479;&#28857;&#31215;&#27880;&#24847;&#21147;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#39640;&#25928;&#26426;&#21046;&#30340;&#24212;&#29992;&#20351;&#24471;&#27880;&#24847;&#21147;&#27169;&#22359;&#21487;&#20197;&#26356;&#24191;&#27867;&#22320;&#38598;&#25104;&#21040;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#31215;&#27880;&#24847;&#21147;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#36755;&#20837;&#22823;&#23567;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#36825;&#31181;&#22686;&#38271;&#38480;&#21046;&#20102;&#20854;&#22312;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#19982;&#28857;&#31215;&#27880;&#24847;&#21147;&#31561;&#25928;&#65292;&#20294;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#12290;&#20854;&#36164;&#28304;&#25928;&#29575;&#20351;&#24471;&#27880;&#24847;&#21147;&#27169;&#22359;&#33021;&#26356;&#24191;&#27867;&#12289;&#28789;&#27963;&#22320;&#38598;&#25104;&#21040;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#32463;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#21183;&#30340;&#26377;&#25928;&#24615;&#12290;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#26174;&#33879;&#25552;&#21319;&#20102;&#23545;MS-COCO 2017&#19978;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#21644;&#23454;&#20363;&#20998;&#21106;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36164;&#28304;&#25928;&#29575;&#20351;&#24471;&#22797;&#26434;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#27880;&#24847;&#21147;&#65292;&#32780;&#39640;&#25104;&#26412;&#38480;&#21046;&#20102;&#20351;&#29992;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#20197;&#31435;&#20307;&#35270;&#35273;&#20026;&#20363;&#65292;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo d
&lt;/p&gt;</description></item></channel></rss>