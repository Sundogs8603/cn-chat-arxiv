<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;</title><link>http://arxiv.org/abs/2304.11692</link><description>&lt;p&gt;
BN&#19982;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#24341;&#36215;&#26799;&#24230;&#29190;&#28856;&#65292;&#20294;&#34987;&#28608;&#27963;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25152;&#25269;&#28040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations. (arXiv:2304.11692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25209;&#26631;&#20934;&#21270;&#21644;ReLU&#31561;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21021;&#26399;&#30001;&#20110;&#26102;&#38388;&#26799;&#24230;&#29190;&#28856;&#32780;&#20986;&#29616;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;ReLU&#22914;&#20309;&#27604;&#39044;&#26399;&#26356;&#22810;&#22320;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#21450;&#25209;&#26631;&#20934;&#21270;&#22914;&#20309;&#22312;&#24674;&#22797;&#26399;&#38388;&#25918;&#22823;&#26799;&#24230;&#65292;&#23548;&#33268;&#21069;&#21521;&#20256;&#25773;&#20445;&#25345;&#31283;&#23450;&#32780;&#26799;&#24230;&#29190;&#28856;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#20197;&#21450;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#29575;&#32553;&#25918;&#26041;&#27861;&#65292;&#24182;&#21487;&#26367;&#25442;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#30340;WarmUp&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks based on batch normalization and ReLU-like activation functions can experience instability during the early stages of training due to the high gradient induced by temporal gradient explosion. We explain how ReLU reduces variance more than expected, and how batch normalization amplifies the gradient during recovery, which causes gradient explosion while forward propagation remains stable. Additionally, we discuss how the dynamics of a deep neural network change during training and how the correlation between inputs can alleviate this problem. Lastly, we propose a better adaptive learning rate algorithm inspired by second-order optimization algorithms, which outperforms existing learning rate scaling methods in large batch training and can also replace WarmUp in small batch training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#24490;&#29615;&#23398;&#20064;&#26469;&#23454;&#29616;&#21487;&#25345;&#32493;&#30340;&#21387;&#32553;&#24863;&#30693;&#31995;&#32479;&#65307;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#20294;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11674</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;&#24490;&#29615;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#21487;&#25345;&#32493;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing. (arXiv:2304.11674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#24490;&#29615;&#23398;&#20064;&#26469;&#23454;&#29616;&#21487;&#25345;&#32493;&#30340;&#21387;&#32553;&#24863;&#30693;&#31995;&#32479;&#65307;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#20294;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#22312;&#20943;&#23569;&#20256;&#24863;&#31995;&#32479;&#30340;&#37319;&#26679;&#21644;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#25552;&#39640;&#37325;&#26500;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#35745;&#31639;&#25104;&#26412;&#36825;&#19968;&#38382;&#39064;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#36816;&#31639;&#31526;&#35774;&#35745;&#65292;&#23548;&#33268; CS &#25104;&#20687;&#31995;&#32479;&#20855;&#26377;&#24191;&#27867;&#30340;&#23384;&#20648;&#35201;&#27714;&#21644;&#39640;&#33021;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#24490;&#29615;&#23398;&#20064;&#26469;&#23454;&#29616;&#21487;&#25345;&#32493;&#30340; CS &#31995;&#32479;&#65307;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#20294;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#30001;&#21021;&#22987;&#37325;&#26500;&#23376;&#32593;&#32476;&#21644;&#27531;&#24046;&#37325;&#26500;&#23376;&#32593;&#32476;&#32452;&#25104;&#12290;&#21021;&#22987;&#37325;&#26500;&#23376;&#32593;&#32476;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#65292;&#36880;&#27493;&#24674;&#22797;&#22270;&#20687;&#65292;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#65307;&#32780;&#27531;&#24046;&#37325;&#26500;&#23376;&#32593;&#32476;&#21017;&#26377;&#21161;&#20110;&#20174;&#20013;&#38388;&#20272;&#35745;&#20013;&#25552;&#21462;&#24490;&#29615;&#27531;&#24046;&#29305;&#24449;&#65292;&#20351;&#32593;&#32476;&#23545;&#24213;&#23618;&#32467;&#26500;&#30340;&#25429;&#25417;&#26356;&#21152;&#26377;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#37325;&#26500;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning-based compressed sensing (CS) has achieved great success in reducing the sampling and computational cost of sensing systems and improving the reconstruction quality. These approaches, however, largely overlook the issue of the computational cost; they rely on complex structures and task-specific operator designs, resulting in extensive storage and high energy consumption in CS imaging systems. In this paper, we propose a lightweight but effective deep neural network based on recurrent learning to achieve a sustainable CS system; it requires a smaller number of parameters but obtains high-quality reconstructions. Specifically, our proposed network consists of an initial reconstruction sub-network and a residual reconstruction sub-network. While the initial reconstruction sub-network has a hierarchical structure to progressively recover the image, reducing the number of parameters, the residual reconstruction sub-network facilitates recurrent residual feature extr
&lt;/p&gt;</description></item><item><title>TinyML&#38754;&#20020;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;MCU&#26550;&#26500;&#21644;&#36164;&#28304;&#21487;&#29992;&#24615;&#38480;&#21046;&#31561;&#22810;&#37325;&#25361;&#25112;&#12290;&#26412;&#25991;&#24378;&#35843;&#35299;&#20915;TinyML&#35299;&#20915;&#26041;&#26696;&#30340;&#32593;&#32476;&#36830;&#25509;&#38382;&#39064;&#38656;&#35201;&#26356;&#21152;&#27880;&#37325;&#26631;&#20934;&#21327;&#35758;&#12289;&#35299;&#20915;&#26041;&#26696;&#30340;&#20114;&#25805;&#20316;&#24615;&#21450;&#23433;&#20840;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LwM2M&#21327;&#35758;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.11669</link><description>&lt;p&gt;
TinyML&#26223;&#35266;&#20013;&#32570;&#22833;&#30340;&#35774;&#22791;&#31649;&#29702;&#21644;&#32593;&#32476;&#36830;&#25509;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
Device management and network connectivity as missing elements in TinyML landscape. (arXiv:2304.11669v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11669
&lt;/p&gt;
&lt;p&gt;
TinyML&#38754;&#20020;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;MCU&#26550;&#26500;&#21644;&#36164;&#28304;&#21487;&#29992;&#24615;&#38480;&#21046;&#31561;&#22810;&#37325;&#25361;&#25112;&#12290;&#26412;&#25991;&#24378;&#35843;&#35299;&#20915;TinyML&#35299;&#20915;&#26041;&#26696;&#30340;&#32593;&#32476;&#36830;&#25509;&#38382;&#39064;&#38656;&#35201;&#26356;&#21152;&#27880;&#37325;&#26631;&#20934;&#21327;&#35758;&#12289;&#35299;&#20915;&#26041;&#26696;&#30340;&#20114;&#25805;&#20316;&#24615;&#21450;&#23433;&#20840;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LwM2M&#21327;&#35758;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22522;&#20110;TinyML&#30340;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#35299;&#20915;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#24494;&#25511;&#21046;&#22120;&#65288;MCU&#65289;&#26550;&#26500;&#21644;&#36164;&#28304;&#21487;&#29992;&#24615;&#38480;&#21046;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;MCU&#30340;&#25805;&#20316;&#31995;&#32479;&#22810;&#26679;&#24615;&#65292;&#26377;&#38480;&#30340;&#20869;&#23384;&#31649;&#29702;&#23454;&#29616;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#26377;&#38480;&#36719;&#20214;&#20114;&#25805;&#20316;&#24615;&#12290;&#26412;&#25991;&#35752;&#35770;&#30340;&#25361;&#25112;&#26159;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#32593;&#32476;&#36830;&#25509;&#38382;&#39064;&#65292;&#25351;&#20986;&#24212;&#26356;&#24378;&#35843;&#26631;&#20934;&#21327;&#35758;&#12289;&#35299;&#20915;&#26041;&#26696;&#30340;&#20114;&#25805;&#20316;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;LwM2M&#21327;&#35758;&#22914;&#20309;&#35299;&#20915;&#19982;&#32593;&#32476;&#36830;&#25509;&#21644;&#20114;&#25805;&#20316;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of solutions based on TinyML requires meeting several challenges. These include hardware heterogeneity, microprocessor (MCU) architectures, and resource availability constraints. Another challenge is the variety of operating systems for MCU, limited memory management implementations and limited software interoperability between devices. A number of these challenges are solved by dedicated programming libraries and the ability to compile code for specific devices. Nevertheless, the challenge discussed in the paper is the issue of network connectivity for such solutions. We point out that more emphasis should be placed on standard protocols, interoperability of solutions and security. Finally, the paper discusses how the LwM2M protocol can solve the identified challenges related to network connectivity and interoperability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#21152;&#36895;&#21452;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#37319;&#29992;&#26032;&#22411;&#21152;&#36895;&#22810;&#21160;&#37327;&#25216;&#26415;&#65292;&#27599;&#27425;&#36845;&#20195;&#20165;&#35775;&#38382;&#19968;&#23567;&#25209;&#26679;&#26412;&#21644;&#26356;&#26032;&#23569;&#37327;&#21464;&#37327;&#22352;&#26631;&#65292;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#21644;&#23567;&#20869;&#23384;&#21344;&#29992;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11665</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21152;&#36895;&#21452;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerated Doubly Stochastic Gradient Algorithm for Large-scale Empirical Risk Minimization. (arXiv:2304.11665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#21152;&#36895;&#21452;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#37319;&#29992;&#26032;&#22411;&#21152;&#36895;&#22810;&#21160;&#37327;&#25216;&#26415;&#65292;&#27599;&#27425;&#36845;&#20195;&#20165;&#35775;&#38382;&#19968;&#23567;&#25209;&#26679;&#26412;&#21644;&#26356;&#26032;&#23569;&#37327;&#21464;&#37327;&#22352;&#26631;&#65292;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#21644;&#23567;&#20869;&#23384;&#21344;&#29992;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26469;&#35828;&#65292;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#12289;&#23567;&#20869;&#23384;&#21344;&#29992;&#21644;&#20302;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#29305;&#21035;&#26377;&#21033;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26032;&#22411;&#21152;&#36895;&#22810;&#21160;&#37327;&#25216;&#26415;&#30340;&#21452;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#20139;&#26377;&#21487;&#35777;&#26126;&#30340;&#26356;&#20248;&#25910;&#25947;&#36895;&#24230;&#65292;&#20294;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#35813;&#31639;&#27861;&#20165;&#35775;&#38382;&#19968;&#23567;&#25209;&#26679;&#26412;&#65292;&#21516;&#26102;&#26356;&#26032;&#23569;&#37327;&#21464;&#37327;&#22352;&#26631;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#22312;&#28041;&#21450;&#22823;&#37327;&#26679;&#26412;&#22823;&#23567;&#21644;&#36229;&#39640;&#32500;&#24230;&#26102;&#30340;&#20869;&#23384;&#24341;&#29992;&#37327;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;&#24040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, algorithms with fast convergence, small memory footprints, and low per-iteration complexity are particularly favorable for artificial intelligence applications. In this paper, we propose a doubly stochastic algorithm with a novel accelerating multi-momentum technique to solve large scale empirical risk minimization problem for learning tasks. While enjoying a provably superior convergence rate, in each iteration, such algorithm only accesses a mini batch of samples and meanwhile updates a small block of variable coordinates, which substantially reduces the amount of memory reference when both the massive sample size and ultra-high dimensionality are involved. Empirical studies on huge scale datasets are conducted to illustrate the efficiency of our method in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#36991;&#20813;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#23618;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11663</link><description>&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Deep Equilibrium Models. (arXiv:2304.11663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#36991;&#20813;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#23618;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#22312;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#24378;&#22823;&#12290;&#20854;&#24605;&#24819;&#26159;&#29992;&#38544;&#24335;&#30340;&#22266;&#23450;&#28857;&#26041;&#31243;&#26367;&#25442;&#20256;&#32479;&#65288;&#26174;&#24335;&#30340;&#65289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#20801;&#35768;&#35299;&#32806;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#38544;&#24335;&#20989;&#25968;&#23450;&#29702;&#65292;DEQ&#23618;&#30340;&#35757;&#32451;&#21464;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#20294;&#26159;&#65292;&#36890;&#36807;DEQ&#23618;&#30340;&#21453;&#21521;&#20256;&#25773;&#20173;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#26114;&#36149;&#30340;&#22522;&#20110;Jacobian&#30340;&#26041;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#36991;&#20813;&#36825;&#31181;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;Broyden&#26041;&#27861;&#30340;Jacobian&#36817;&#20284;&#65292;&#22312;&#21069;&#21521;&#20256;&#36882;&#20043;&#21518;&#35745;&#31639;&#21453;&#21521;&#20256;&#36882;&#20013;&#30340;&#26799;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31616;&#21333;&#22320;&#37325;&#22797;&#20351;&#29992;&#36825;&#20010;&#36817;&#20284;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#19981;&#20250;&#36896;&#25104;&#24615;&#33021;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium models (DEQs) have proven to be very powerful for learning data representations. The idea is to replace traditional (explicit) feedforward neural networks with an implicit fixed-point equation, which allows to decouple the forward and backward passes. In particular, training DEQ layers becomes very memory-efficient via the implicit function theorem. However, backpropagation through DEQ layers still requires solving an expensive Jacobian-based equation. In this paper, we introduce a simple but effective strategy to avoid this computational burden. Our method relies on the Jacobian approximation of Broyden's method after the forward pass to compute the gradients during the backward pass. Experiments show that simply re-using this approximation can significantly speed up the training while not causing any performance degradation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#32454;&#31890;&#24230;&#35821;&#20041;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;FSGCL&#65289;&#65292;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#22270;&#26696;&#30340;&#22270;&#26500;&#36896;&#26041;&#27861;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#22810;&#31181;&#35821;&#20041;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25506;&#32034;&#35821;&#20041;&#32423;&#23545;&#27604;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11658</link><description>&lt;p&gt;
&#25429;&#25417;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Capturing Fine-grained Semantics in Contrastive Graph Representation Learning. (arXiv:2304.11658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#32454;&#31890;&#24230;&#35821;&#20041;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;FSGCL&#65289;&#65292;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#22270;&#26696;&#30340;&#22270;&#26500;&#36896;&#26041;&#27861;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#22810;&#31181;&#35821;&#20041;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25506;&#32034;&#35821;&#20041;&#32423;&#23545;&#27604;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#23450;&#20041;&#20102;&#19968;&#20010;&#23545;&#27604;&#30340;&#20219;&#21153;&#65292;&#20197;&#23558;&#30456;&#20284;&#30340;&#23454;&#20363;&#25289;&#36817;&#65292;&#23558;&#19981;&#30456;&#20284;&#30340;&#23454;&#20363;&#25512;&#36828;&#65292;&#23398;&#20064;&#21306;&#20998;&#24615;&#33410;&#28857;&#23884;&#20837;&#32780;&#26080;&#38656;&#30417;&#30563;&#26631;&#31614;&#65292;&#36825;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24573;&#30053;&#20102;&#22270;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#35821;&#20041;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23398;&#20064;&#20102;&#31895;&#31890;&#24230;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#20122;&#20248;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#32454;&#31890;&#24230;&#35821;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;FSGCL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FSGCL&#39318;&#20808;&#24341;&#20837;&#20102;&#22522;&#20110;&#22270;&#26696;&#30340;&#22270;&#26500;&#36896;&#26041;&#27861;&#65292;&#20174;&#36755;&#20837;&#25968;&#25454;&#30340;&#35282;&#24230;&#21033;&#29992;&#22270;&#26696;&#25552;&#21462;&#20102;&#23384;&#22312;&#20110;&#22270;&#20013;&#30340;&#22810;&#31181;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#20174;&#27169;&#22411;&#35757;&#32451;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#35821;&#20041;&#32423;&#23545;&#27604;&#20219;&#21153;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#23545;&#32454;&#31890;&#24230;&#35821;&#20041;&#30340;&#21033;&#29992;&#12290;&#22312;&#20116;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;FSGCL&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning defines a contrastive task to pull similar instances close and push dissimilar instances away. It learns discriminative node embeddings without supervised labels, which has aroused increasing attention in the past few years. Nevertheless, existing methods of graph contrastive learning ignore the differences between diverse semantics existed in graphs, which learn coarse-grained node embeddings and lead to sub-optimal performances on downstream tasks. To bridge this gap, we propose a novel Fine-grained Semantics enhanced Graph Contrastive Learning (FSGCL) in this paper. Concretely, FSGCL first introduces a motif-based graph construction, which employs graph motifs to extract diverse semantics existed in graphs from the perspective of input data. Then, the semantic-level contrastive task is explored to further enhance the utilization of fine-grained semantics from the perspective of model training. Experiments on five real-world datasets demonstrate the superio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20132;&#36890;&#32593;&#32476;&#30340;&#38543;&#26426;&#21333;&#20803;&#20256;&#36755;&#27169;&#22411;&#65292;&#36890;&#36807;&#20559;&#22909;&#20989;&#25968;&#21644;&#21487;&#25509;&#21463;&#35774;&#35745;&#26469;&#35780;&#20272;&#20132;&#36890;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25968;&#20540;&#23454;&#29616;&#32467;&#21512;&#20102;&#27169;&#25311;&#12289;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#38543;&#26426;&#25506;&#32034;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.11654</link><description>&lt;p&gt;
&#20132;&#36890;&#32593;&#32476;&#30340;&#38543;&#26426;&#21333;&#20803;&#20256;&#36755;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Stochastic Cell Transmission Models of Traffic Networks. (arXiv:2304.11654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20132;&#36890;&#32593;&#32476;&#30340;&#38543;&#26426;&#21333;&#20803;&#20256;&#36755;&#27169;&#22411;&#65292;&#36890;&#36807;&#20559;&#22909;&#20989;&#25968;&#21644;&#21487;&#25509;&#21463;&#35774;&#35745;&#26469;&#35780;&#20272;&#20132;&#36890;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25968;&#20540;&#23454;&#29616;&#32467;&#21512;&#20102;&#27169;&#25311;&#12289;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#38543;&#26426;&#25506;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#19968;&#33324;&#20132;&#36890;&#32593;&#32476;&#24341;&#20837;&#20102;&#38543;&#26426;&#21333;&#20803;&#20256;&#36755;&#27169;&#22411;&#30340;&#20005;&#26684;&#26694;&#26550;&#12290;&#36890;&#36807;&#20559;&#22909;&#20989;&#25968;&#21644;&#21487;&#25509;&#21463;&#35774;&#35745;&#35780;&#20272;&#20132;&#36890;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25968;&#20540;&#23454;&#29616;&#32467;&#21512;&#20102;&#27169;&#25311;&#12289;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#38543;&#26426;&#25506;&#32034;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a rigorous framework for stochastic cell transmission models for general traffic networks. The performance of traffic systems is evaluated based on preference functionals and acceptable designs. The numerical implementation combines simulation, Gaussian process regression, and a stochastic exploration procedure. The approach is illustrated in two case studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110; WBP &#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#31639;&#27861; A$^2$DWB&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#26412;&#22320;&#21464;&#37327;&#20197;&#20943;&#36731;&#31561;&#24453;&#24320;&#38144;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#26102;&#38388;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11653</link><description>&lt;p&gt;
&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#24335;&#31639;&#27861;&#35299;&#20915; Wasserstein Barycenter &#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Asynchronous Decentralized Algorithm for Wasserstein Barycenter Problem. (arXiv:2304.11653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110; WBP &#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#31639;&#27861; A$^2$DWB&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#26412;&#22320;&#21464;&#37327;&#20197;&#20943;&#36731;&#31561;&#24453;&#24320;&#38144;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#26102;&#38388;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Wasserstein Barycenter Problem (WBP) &#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110; WBP &#30340;&#20998;&#25955;&#24335;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#24335;&#31639;&#27861; (A$^2$DWB)&#12290;A$^2$DWB &#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#22359;&#21327;&#35843;&#19979;&#38477;&#26041;&#27861;&#26469;&#20248;&#21270;&#29109;&#27491;&#21017;&#21270; WBP &#30340;&#23545;&#20598;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;A$^2$DWB &#26159; WBP &#30340;&#31532;&#19968;&#20010;&#24322;&#27493;&#20998;&#25955;&#24335;&#31639;&#27861;&#12290;&#19982;&#20854;&#21516;&#27493;&#29256;&#26412;&#19981;&#21516;&#30340;&#26159;&#65292;A$^2$DWB &#20197;&#19968;&#31181;&#20165;&#20381;&#36182;&#20110;&#36807;&#26102;&#30340;&#37051;&#23621;&#20449;&#24687;&#30340;&#26041;&#24335;&#26356;&#26032;&#26412;&#22320;&#21464;&#37327;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#31561;&#24453;&#24320;&#38144;&#65292;&#20174;&#32780;&#23454;&#36136;&#24615;&#22320;&#25552;&#39640;&#20102;&#26102;&#38388;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26368;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#30456;&#27604;&#65292;A$^2$DWB &#30340;&#24615;&#33021;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Barycenter Problem (WBP) has recently received much attention in the field of artificial intelligence. In this paper, we focus on the decentralized setting for WBP and propose an asynchronous decentralized algorithm (A$^2$DWB). A$^2$DWB is induced by a novel stochastic block coordinate descent method to optimize the dual of entropy regularized WBP. To our knowledge, A$^2$DWB is the first asynchronous decentralized algorithm for WBP. Unlike its synchronous counterpart, it updates local variables in a manner that only relies on the stale neighbor information, which effectively alleviate the waiting overhead, and thus substantially improve the time efficiency. Empirical results validate its superior performance compared to the latest synchronous algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#22270;&#20687;&#20803;&#25968;&#25454;&#38598;SATIN&#65292;&#23427;&#30001;27&#20010;&#29616;&#26377;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#20840;&#38754;&#35780;&#20272;&#20102;&#23427;&#30340;&#38646;-shot&#36716;&#31227;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;SATIN&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#22823;&#26041;&#27861;&#30340;&#20998;&#31867;&#31934;&#24230;&#20026;52.0&#65285;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.11619</link><description>&lt;p&gt;
SATIN&#65306;&#19968;&#20010;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#22810;&#20219;&#21153;&#20803;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models. (arXiv:2304.11619v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#22270;&#20687;&#20803;&#25968;&#25454;&#38598;SATIN&#65292;&#23427;&#30001;27&#20010;&#29616;&#26377;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#20840;&#38754;&#35780;&#20272;&#20102;&#23427;&#30340;&#38646;-shot&#36716;&#31227;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;SATIN&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#22823;&#26041;&#27861;&#30340;&#20998;&#31867;&#31934;&#24230;&#20026;52.0&#65285;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#36965;&#24863;&#22270;&#20687;&#21487;&#20197;&#23454;&#29616;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#20174;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#21040;&#26862;&#26519;&#30733;&#20240;&#30417;&#27979;&#37117;&#26377;&#21487;&#33021;&#12290;&#30001;&#20110;&#22320;&#29699;&#22320;&#29702;&#22810;&#26679;&#24615;&#30340;&#23384;&#22312;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#31283;&#20581;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#21355;&#26143;&#21644;&#33322;&#31354;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#20294;&#23578;&#26410;&#26377;&#19968;&#20010;&#36866;&#21512;&#28085;&#30422;&#36825;&#31181;&#22810;&#26679;&#24615;&#30340;&#22522;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26469;&#33258;27&#20010;&#29616;&#26377;&#36965;&#24863;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#38598;SATellite ImageNet&#65288;SATIN&#65289;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#22312;SATIN&#19978;&#30340;&#38646;-shot&#36716;&#31227;&#20998;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;SATIN&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;-&#25105;&#20204;&#35780;&#20272;&#30340;&#26368;&#24378;&#26041;&#27861;&#30340;&#20998;&#31867;&#31934;&#24230;&#20026;52.0&#65285;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#65292;&#20197;&#25351;&#23548;&#21644;&#36319;&#36394;VL&#27169;&#22411;&#22312;&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting remote sensing imagery enables numerous downstream applications ranging from land-use planning to deforestation monitoring. Robustly classifying this data is challenging due to the Earth's geographic diversity. While many distinct satellite and aerial image classification datasets exist, there is yet to be a benchmark curated that suitably covers this diversity. In this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from 27 existing remotely sensed datasets, and comprehensively evaluate the zero-shot transfer classification capabilities of a broad range of vision-language (VL) models on SATIN. We find SATIN to be a challenging benchmark-the strongest method we evaluate achieves a classification accuracy of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public leaderboard}}$ to guide and track the progress of VL models in this important domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22270;&#24418;&#32454;&#21270;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#21407;&#22411;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#26680;&#20989;&#25968;&#19979;&#20122;&#20248;&#22270;&#24418;&#26500;&#24314;&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;FSL&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.11598</link><description>&lt;p&gt;
&#36845;&#20195;&#22270;&#24418;&#32454;&#21270;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#30340;&#36801;&#31227;&#23569;&#26679;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement. (arXiv:2304.11598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22270;&#24418;&#32454;&#21270;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#21407;&#22411;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#26680;&#20989;&#25968;&#19979;&#20122;&#20248;&#22270;&#24418;&#26500;&#24314;&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;FSL&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#23398;&#20064;(FSL)&#22240;&#20854;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#32780;&#21463;&#27426;&#36814;&#12290;&#19982;&#24402;&#32435;&#24335;&#23569;&#26679;&#23398;&#20064;&#30456;&#27604;&#65292;&#20256;&#23548;&#24335;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#21033;&#29992;&#26597;&#35810;&#38598;&#30340;&#25152;&#26377;&#26679;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#20004;&#20010;&#31867;&#21035;&#65292;&#22522;&#20110;&#21407;&#22411;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#21407;&#22411;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#26680;&#20989;&#25968;&#19979;&#20122;&#20248;&#22270;&#24418;&#26500;&#24314;&#30340;&#32570;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22270;&#24418;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#21644;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#26679;&#26412;&#20043;&#38388;&#12290;&#38543;&#30528;&#21407;&#22411;&#30340;&#26356;&#26032;&#65292;&#22270;&#24418;&#20063;&#20250;&#38543;&#20043;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#27599;&#20010;&#21407;&#22411;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#23558;&#21407;&#22411;&#35270;&#20026;&#31867;&#20013;&#24515;&#12290;&#22312;mini-ImageNet&#65292;tiered-ImageNet&#65292;CIFAR-FS&#21644;CUB&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;FSL&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL) is popular due to its ability to adapt to novel classes. Compared with inductive few-shot learning, transductive models typically perform better as they leverage all samples of the query set. The two existing classes of methods, prototype-based and graph-based, have the disadvantages of inaccurate prototype estimation and sub-optimal graph construction with kernel functions, respectively. In this paper, we propose a novel prototype-based label propagation to solve these issues. Specifically, our graph construction is based on the relation between prototypes and samples rather than between samples. As prototypes are being updated, the graph changes. We also estimate the label of each prototype instead of considering a prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and CUB datasets, we show the proposed method outperforms other state-of-the-art methods in transductive FSL and semi-supervised FSL when some unlabeled data accompanies the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11597</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Partial Correlation based Deep Visual Representation for Image Classification. (arXiv:2304.11597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35270;&#35273;&#34920;&#31034;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#21367;&#31215;&#29305;&#24449;&#26144;&#23556;&#20013;&#19981;&#21516;&#36890;&#36947;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23384;&#22312;&#21478;&#19968;&#20010;&#36890;&#36947;&#19982;&#24863;&#20852;&#36259;&#30340;&#20004;&#20010;&#36890;&#36947;&#30456;&#20851;&#65292;&#21017;&#25104;&#23545;&#30456;&#20851;&#24615;&#23558;&#21464;&#24471;&#35823;&#23548;&#20154;&#65292;&#23548;&#33268;&#8220;&#28151;&#28102;&#8221;&#25928;&#24212;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#24212;&#35813;&#20272;&#35745;&#8220;&#20559;&#30456;&#20851;&#8221;&#65292;&#20197;&#28040;&#38500;&#28151;&#28102;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#20272;&#35745;&#20559;&#30456;&#20851;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#31232;&#30095;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#65288;SICE&#65289;&#12290;&#22914;&#20309;&#23558;&#27492;&#36807;&#31243;&#34701;&#20837;CNN&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;SICE&#21046;&#23450;&#20026;CNN&#30340;&#19968;&#20010;&#26032;&#32467;&#26500;&#23618;&#12290;&#20026;&#30830;&#20445;&#31471;&#21040;&#31471;&#30340;&#21487;&#35757;&#32451;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#22312;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#27493;&#39588;&#20013;&#35299;&#20915;&#19978;&#36848;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33719;&#24471;&#20102;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;Segment Non-Euclidean Anything&#65288;SNA&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#25193;&#22823;&#35821;&#20041;&#20998;&#21106;&#30340;&#33539;&#22260;&#65292;&#23548;&#33268;&#26356;&#22810;&#30340;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.11595</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#20998;&#21106;&#38382;&#39064;&#65306;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Segment Anything in Non-Euclidean Domains: Challenges and Opportunities. (arXiv:2304.11595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;Segment Non-Euclidean Anything&#65288;SNA&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#25193;&#22823;&#35821;&#20041;&#20998;&#21106;&#30340;&#33539;&#22260;&#65292;&#23548;&#33268;&#26356;&#22810;&#30340;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#31216;&#20026;Segment Anything&#65288;SA&#65289;&#22312;&#23558;&#35821;&#20041;&#20998;&#21106;&#30340;&#36793;&#30028;&#25512;&#21521;&#22522;&#30784;&#27169;&#22411;&#26102;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;SA&#30340;&#24433;&#21709;&#24341;&#21457;&#20102;&#26497;&#20026;&#27963;&#36291;&#30340;&#35752;&#35770;&#65292;&#24182;&#24341;&#39046;&#20102;&#19968;&#27874;&#26032;&#30340;&#21457;&#23637;&#28010;&#28526;&#65292;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#29289;&#20307;&#26816;&#27979;&#21644;&#22270;&#20687;&#20462;&#22797;&#12290;&#23613;&#31649;SA&#25152;&#24102;&#26469;&#30340;&#36827;&#23637;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#35813;&#27010;&#24565;&#23578;&#26410;&#25193;&#23637;&#21040;&#38750;&#27431;&#20960;&#37324;&#24471;&#22270;&#24418;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31216;&#20026;Segment Non-Euclidean Anything&#65288;SNA&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#21508;&#31181;&#22270;&#24418;&#25968;&#25454;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#35797;&#22270;&#25193;&#22823;SA&#30340;&#33539;&#22260;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19982;SA&#30456;&#20851;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#23601;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#23558;SA&#27010;&#24565;&#24212;&#29992;&#20110;&#22270;&#24418;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent work known as Segment Anything (SA) has made significant strides in pushing the boundaries of semantic segmentation into the era of foundation models. The impact of SA has sparked extremely active discussions and ushered in an encouraging new wave of developing foundation models for the diverse tasks in the Euclidean domain, such as object detection and image inpainting. Despite the promising advances led by SA, the concept has yet to be extended to the non-Euclidean graph domain. In this paper, we explore a novel Segment Non-Euclidean Anything (SNA) paradigm that strives to develop foundation models that can handle the diverse range of graph data within the non-Euclidean domain, seeking to expand the scope of SA and lay the groundwork for future research in this direction. To achieve this goal, we begin by discussing the recent achievements in foundation models associated with SA. We then shed light on the unique challenges that arise when applying the SA concept to graph a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;System III&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#23433;&#20840;&#24615;&#30340;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#24418;&#24335;&#65292;&#36890;&#36807;p&#33539;&#25968;&#35780;&#20272;&#32422;&#26463;&#30340;&#28385;&#36275;&#31243;&#24230;&#65292;&#29992;&#20110;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#20197;&#24110;&#21161;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#65292;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.11593</link><description>&lt;p&gt;
&#31995;&#32479;III&#65306;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
System III: Learning with Domain Knowledge for Safety Constraints. (arXiv:2304.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;System III&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#23433;&#20840;&#24615;&#30340;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#24418;&#24335;&#65292;&#36890;&#36807;p&#33539;&#25968;&#35780;&#20272;&#32422;&#26463;&#30340;&#28385;&#36275;&#31243;&#24230;&#65292;&#29992;&#20110;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#20197;&#24110;&#21161;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#65292;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#22823;&#37327;&#25506;&#32034;&#33258;&#28982;&#22320;&#23398;&#20064;&#12290;&#25506;&#32034;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#21487;&#33021;&#19981;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#20197;&#24110;&#21161;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;System III&#65292;&#21463;&#24515;&#29702;&#23398;&#23478;&#20851;&#20110;&#22823;&#33041;System I&#21644;System II&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#23433;&#20840;&#24615;&#30340;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#24418;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#29366;&#24577;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;p&#33539;&#25968;&#35780;&#20272;&#36825;&#20123;&#32422;&#26463;&#30340;&#28385;&#36275;&#31243;&#24230;&#12290;&#22312;&#25105;&#20204;&#30340;&#20844;&#24335;&#20013;&#65292;&#32422;&#26463;&#31867;&#20284;&#20110;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#24517;&#39035;&#36991;&#20813;&#30340;&#29366;&#24577;&#30340;&#21361;&#38505;&#65292;&#23545;&#35937;&#21644;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#20195;&#29702;&#38656;&#35201;&#22312;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#30896;&#25758;&#30340;&#21516;&#26102;&#23433;&#20840;&#23548;&#33322;&#36712;&#36857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#23433;&#20840;&#22320;&#24341;&#23548;&#25506;&#32034;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning agents naturally learn from extensive exploration. Exploration is costly and can be unsafe in $\textit{safety-critical}$ domains. This paper proposes a novel framework for incorporating domain knowledge to help guide safe exploration and boost sample efficiency. Previous approaches impose constraints, such as regularisation parameters in neural networks, that rely on large sample sets and often are not suitable for safety-critical domains where agents should almost always avoid unsafe actions. In our approach, called $\textit{System III}$, which is inspired by psychologists' notions of the brain's $\textit{System I}$ and $\textit{System II}$, we represent domain expert knowledge of safety in form of first-order logic. We evaluate the satisfaction of these constraints via p-norms in state vector space. In our formulation, constraints are analogous to hazards, objects, and regions of state that have to be avoided during exploration. We evaluated the effectiveness o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;GPS&#36712;&#36857;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#36712;&#36857;&#36880;&#28176;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#20877;&#20174;&#22122;&#22768;&#37325;&#26500;&#20266;&#36896;&#30340;&#36712;&#36857;&#65292;&#20197;&#36798;&#21040;&#29983;&#25104;&#38544;&#31169;&#20449;&#24687;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#36712;&#36857;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11582</link><description>&lt;p&gt;
GPS&#36712;&#36857;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model for GPS Trajectory Generation. (arXiv:2304.11582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;GPS&#36712;&#36857;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#36712;&#36857;&#36880;&#28176;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#20877;&#20174;&#22122;&#22768;&#37325;&#26500;&#20266;&#36896;&#30340;&#36712;&#36857;&#65292;&#20197;&#36798;&#21040;&#29983;&#25104;&#38544;&#31169;&#20449;&#24687;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#36712;&#36857;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPS&#35774;&#22791;&#21644;&#25968;&#25454;&#37319;&#38598;&#25216;&#26415;&#30340;&#37096;&#32626;&#65292;&#22823;&#37327;&#30340;GPS&#36712;&#36857;&#25968;&#25454;&#20026;&#25512;&#36827;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30740;&#31350;&#25552;&#20379;&#20102;&#26680;&#24515;&#25903;&#25345;&#12290;&#20294;&#26159;&#65292;GPS&#36712;&#36857;&#21253;&#25324;&#20010;&#20154;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#28041;&#21450;&#21040;&#38544;&#31169;&#38382;&#39064;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#36712;&#36857;&#29983;&#25104;&#65292;&#29992;&#29983;&#25104;&#30340;&#26080;&#38544;&#31169;&#20449;&#24687;&#26367;&#25442;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#22797;&#26434;&#21644;&#38543;&#26426;&#34892;&#20026;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36712;&#36857;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#29983;&#25104;&#65288;Diff-Traj&#65289;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;&#36712;&#36857;&#30340;&#26102;&#31354;&#29305;&#24449;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21069;&#21521;&#36712;&#36857;&#22122;&#22768;&#22788;&#29702;&#36880;&#28176;&#23558;&#30495;&#23454;&#36712;&#36857;&#36716;&#25442;&#20026;&#22122;&#22768;&#12290;&#28982;&#21518;&#65292;Diff-Traj&#20174;&#22122;&#22768;&#37325;&#26500;&#20266;&#36896;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the deployment of GPS-enabled devices and data acquisition technology, the massively generated GPS trajectory data provide a core support for advancing spatial-temporal data mining research. Nonetheless, GPS trajectories comprise personal geo-location information, rendering inevitable privacy concerns on plain data. One promising solution to this problem is trajectory generation, replacing the original data with the generated privacy-free ones. However, owing to the complex and stochastic behavior of human activities, generating high-quality trajectories is still in its infancy. To achieve the objective, we propose a diffusion-based trajectory generation (Diff-Traj) framework, effectively integrating the generation capability of the diffusion model and learning from the spatial-temporal features of trajectories. Specifically, we gradually convert real trajectories to noise through a forward trajectory noising process. Then, Diff-Traj reconstructs forged trajectories from the noise
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#21644;&#20869;&#23481;&#24863;&#30693;2D&#34920;&#31034;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35782;&#21035;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24182;&#23545;&#40657;&#27934;&#25968;&#25454;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11560</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20869;&#23481;&#24863;&#30693;&#20108;&#32500;&#34920;&#31034;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#38543;&#26426;&#24615;&#65306;&#40657;&#27934;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying Stochasticity in Time-Series with Autoencoder-Based Content-aware 2D Representation: Application to Black Hole Data. (arXiv:2304.11560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11560
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#21644;&#20869;&#23481;&#24863;&#30693;2D&#34920;&#31034;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35782;&#21035;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24182;&#23545;&#40657;&#27934;&#25968;&#25454;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;2D&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20026;&#38543;&#26426;&#25110;&#38750;&#38543;&#26426;&#65292;&#20197;&#20102;&#35299;&#20854;&#28508;&#22312;&#30340;&#29289;&#29702;&#36807;&#31243;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23481;&#24863;&#30693;&#30340;1D&#26102;&#38388;&#24207;&#21015;&#21040;2D&#34920;&#31034;&#30340;&#36716;&#25442;&#65292;&#21516;&#26102;&#21033;&#29992;&#26102;&#38388;&#21644;&#39057;&#22495;&#29305;&#24449;&#12290;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#65288;&#21516;&#26102;&#20351;&#29992;&#26102;&#38388;&#21644;&#39057;&#22495;&#65289;&#34920;&#31034;&#65292;&#26088;&#22312;&#26159;&#26102;&#38388;&#19981;&#21464;&#30340;&#12290;&#26102;&#38388;&#24207;&#21015;&#30340;&#27599;&#20010;&#20803;&#32032;&#37117;&#34920;&#31034;&#20026;&#19968;&#20010;&#20803;&#32452;&#65292;&#27599;&#20010;&#20803;&#32452;&#20174;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#20013;&#21508;&#21462;&#19968;&#20010;&#20803;&#32032;&#65292;&#24418;&#25104;&#19968;&#20010;&#20108;&#36827;&#21046;&#22270;&#20687;&#12290;&#22312;&#36825;&#20010;&#20108;&#36827;&#21046;&#22270;&#20687;&#20013;&#65292;&#20195;&#34920;&#26102;&#38388;&#24207;&#21015;&#28857;&#30340;&#37027;&#20123;&#20803;&#32452;&#32452;&#21512;&#22312;&#19968;&#36215;&#24418;&#25104;&#20102;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#8220;&#28508;&#31354;&#38388;&#31614;&#21517;&#8221;&#65288;LSS&#65289;&#12290;&#33719;&#24471;&#30340;&#20108;&#36827;&#21046;LSS&#22270;&#20687;&#34987;&#36865;&#20837;&#20998;&#31867;&#32593;&#32476;&#12290;&#20351;&#29992;421&#20010;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;EfficientNetv2-S&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#38543;&#26426;&#21644;&#38750;&#38543;&#26426;&#24207;&#21015;&#65292;&#20197;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we report an autoencoder-based 2D representation to classify a time-series as stochastic or non-stochastic, to understand the underlying physical process. Content-aware conversion of 1D time-series to 2D representation, that simultaneously utilizes time- and frequency-domain characteristics, is proposed. An autoencoder is trained with a loss function to learn latent space (using both time- and frequency domains) representation, that is designed to be, time-invariant. Every element of the time-series is represented as a tuple with two components, one each, from latent space representation in time- and frequency-domains, forming a binary image. In this binary image, those tuples that represent the points in the time-series, together form the ``Latent Space Signature" (LSS) of the input time-series. The obtained binary LSS images are fed to a classification network. The EfficientNetv2-S classifier is trained using 421 synthetic time-series, with fair representation from both
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20851;&#31995;&#21644;&#22810;&#23454;&#20307;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#12290;&#36890;&#36807;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#24615;&#32423;&#21035;&#30340;&#23618;&#27425;&#22270;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#27169;&#25311;&#33410;&#28857;-&#33410;&#28857;&#21644;&#20851;&#31995;-&#20851;&#31995;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11533</link><description>&lt;p&gt;
&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bi-Level Attention Graph Neural Networks. (arXiv:2304.11533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11533
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20851;&#31995;&#21644;&#22810;&#23454;&#20307;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#12290;&#36890;&#36807;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#24615;&#32423;&#21035;&#30340;&#23618;&#27425;&#22270;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#27169;&#25311;&#33410;&#28857;-&#33410;&#28857;&#21644;&#20851;&#31995;-&#20851;&#31995;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#21516;&#36136;&#22270;(HoGs)&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#24322;&#26500;&#22270;(HeGs)&#30340;GNNs&#65292;&#22312;&#22788;&#29702;&#27880;&#24847;&#21147;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;&#22823;&#22810;&#25968;&#22788;&#29702;HeGs&#30340;GNNs&#21482;&#23398;&#20064;&#33410;&#28857;&#32423;&#21035;&#25110;&#20851;&#31995;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#65292;&#32780;&#19981;&#26159;&#20004;&#32773;&#20860;&#22791;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;HeGs&#20013;&#30340;&#37325;&#35201;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#26159;&#29616;&#26377;&#23398;&#20064;&#20004;&#31181;&#32423;&#21035;&#27880;&#24847;&#21147;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20063;&#23384;&#22312;&#20551;&#23450;&#22270;&#20851;&#31995;&#26159;&#29420;&#31435;&#30340;&#65292;&#24182;&#19988;&#20854;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#24573;&#30053;&#20102;&#36825;&#31181;&#20381;&#36182;&#20851;&#32852;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#27169;&#25311;&#22810;&#20851;&#31995;&#21644;&#22810;&#23454;&#20307;&#30340;&#22823;&#35268;&#27169;HeGs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;(BA-GNN)&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;(NNs)&#65292;&#36890;&#36807;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#24615;&#32423;&#21035;&#30340;&#23618;&#27425;&#22270;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#27169;&#25311;&#20102;&#33410;&#28857;-&#33410;&#28857;&#21644;&#20851;&#31995;-&#20851;&#31995;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23398;&#20250;&#20102;&#32771;&#34385;&#22270;&#20851;&#31995;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent graph neural networks (GNNs) with the attention mechanism have historically been limited to small-scale homogeneous graphs (HoGs). However, GNNs handling heterogeneous graphs (HeGs), which contain several entity and relation types, all have shortcomings in handling attention. Most GNNs that learn graph attention for HeGs learn either node-level or relation-level attention, but not both, limiting their ability to predict both important entities and relations in the HeG. Even the best existing method that learns both levels of attention has the limitation of assuming graph relations are independent and that its learned attention disregards this dependency association. To effectively model both multi-relational and multi-entity large-scale HeGs, we present Bi-Level Attention Graph Neural Networks (BA-GNN), scalable neural networks (NNs) that use a novel bi-level graph attention mechanism. BA-GNN models both node-node and relation-relation interactions in a personalized way, by hier
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TriSIM4Rec&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#21160;&#24577;&#20132;&#20114;&#22270;&#65292;&#21516;&#26102;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20849;&#29616;&#12289;&#29992;&#25143;&#20132;&#20114;&#26102;&#24207;&#20449;&#24687;&#21644;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#19977;&#31181;&#32467;&#26500;&#20449;&#24687;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2304.11528</link><description>&lt;p&gt;
&#19977;&#20803;&#32467;&#26500;&#20449;&#24687;&#24314;&#27169;&#29992;&#20110;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Triple Structural Information Modelling for Accurate, Explainable and Interactive Recommendation. (arXiv:2304.11528v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TriSIM4Rec&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#21160;&#24577;&#20132;&#20114;&#22270;&#65292;&#21516;&#26102;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20849;&#29616;&#12289;&#29992;&#25143;&#20132;&#20114;&#26102;&#24207;&#20449;&#24687;&#21644;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#19977;&#31181;&#32467;&#26500;&#20449;&#24687;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#20132;&#20114;&#22270;&#20013;&#65292;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#20132;&#20114;&#36890;&#24120;&#36981;&#24490;&#24322;&#26500;&#27169;&#24335;&#65292;&#34920;&#31034;&#20026;&#19981;&#21516;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#29992;&#25143;-&#29289;&#21697;&#20849;&#29616;&#12289;&#29992;&#25143;&#20132;&#20114;&#30340;&#26102;&#24207;&#20449;&#24687;&#21644;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#21033;&#29992;&#36825;&#19977;&#31181;&#32467;&#26500;&#20449;&#24687;&#65292;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TriSIM4Rec&#65292;&#19968;&#31181;&#22522;&#20110;&#19977;&#20803;&#32467;&#26500;&#20449;&#24687;&#24314;&#27169;&#30340;&#21160;&#24577;&#20132;&#20114;&#22270;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#25512;&#33616;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;TriSIM4Rec&#21253;&#25324;1)&#19968;&#20010;&#21160;&#24577;&#29702;&#24819;&#20302;&#36890;&#22270;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#22686;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#21160;&#24577;&#22320;&#25366;&#25496;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20013;&#30340;&#20849;&#29616;&#20449;&#24687;&#65307;2)&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#25429;&#33719;&#29992;&#25143;&#20132;&#20114;&#30340;&#26102;&#24207;&#20449;&#24687;&#65307;&#21644;3)&#19968;&#20010;&#29289;&#21697;&#36716;&#31227;&#30697;&#38453;&#20197;&#23384;&#20648;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic interaction graphs, user-item interactions usually follow heterogeneous patterns, represented by different structural information, such as user-item co-occurrence, sequential information of user interactions and the transition probabilities of item pairs. However, the existing methods cannot simultaneously leverage all three structural information, resulting in suboptimal performance. To this end, we propose TriSIM4Rec, a triple structural information modeling method for accurate, explainable and interactive recommendation on dynamic interaction graphs. Specifically, TriSIM4Rec consists of 1) a dynamic ideal low-pass graph filter to dynamically mine co-occurrence information in user-item interactions, which is implemented by incremental singular value decomposition (SVD); 2) a parameter-free attention module to capture sequential information of user interactions effectively and efficiently; and 3) an item transition matrix to store the transition probabilities of item pairs.
&lt;/p&gt;</description></item><item><title>HWA&#26159;&#19968;&#31181;&#23558;&#22312;&#32447;WA&#21644;&#31163;&#32447;WA&#30456;&#32467;&#21512;&#30340;&#36890;&#29992;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#25552;&#39640;SGD&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.11519</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#21152;&#26435;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Weight Averaging for Deep Neural Networks. (arXiv:2304.11519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11519
&lt;/p&gt;
&lt;p&gt;
HWA&#26159;&#19968;&#31181;&#23558;&#22312;&#32447;WA&#21644;&#31163;&#32447;WA&#30456;&#32467;&#21512;&#30340;&#36890;&#29992;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#25552;&#39640;SGD&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#38750;&#24120;&#31616;&#21333;&#65292;&#20294;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35757;&#32451;&#38750;&#24120;&#25104;&#21151;&#12290;&#22312;&#21508;&#31181;&#35797;&#22270;&#25913;&#36827;SGD&#30340;&#23581;&#35797;&#20013;&#65292;&#21152;&#26435;&#24179;&#22343;&#65288;WA&#65289;&#36817;&#26469;&#22312;&#25991;&#29486;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#23427;&#24179;&#22343;&#20102;&#22810;&#20010;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;WA&#20998;&#20026;&#20004;&#31867;&#65306;&#22312;&#32447;WA&#21644;&#31163;&#32447;WA&#12290;&#25105;&#20204;&#22312;&#26412;&#24037;&#20316;&#20013;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23618;&#27425;&#21152;&#26435;&#24179;&#22343;&#65288;HWA&#65289;&#30340;&#36890;&#29992;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HWA&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22312;&#32447;&#21644;&#31163;&#32447;WA&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;SGD&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#21644;&#23454;&#38469;&#19978;&#30740;&#31350;&#20102;HWA&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HWA&#21487;&#20197;&#25345;&#32493;&#25913;&#21892;SGD&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the simplicity, stochastic gradient descent (SGD)-like algorithms are successful in training deep neural networks (DNNs). Among various attempts to improve SGD, weight averaging (WA), which averages the weights of multiple models, has recently received much attention in the literature. Broadly, WA falls into two categories: 1) online WA, which averages the weights of multiple models trained in parallel, is designed for reducing the gradient communication overhead of parallel mini-batch SGD, and 2) offline WA, which averages the weights of one model at different checkpoints, is typically used to improve the generalization ability of DNNs. Though online and offline WA are similar in form, they are seldom associated with each other. Besides, these methods typically perform either offline parameter averaging or online parameter averaging, but not both. In this work, we firstly attempt to incorporate online and offline WA into a general training framework termed Hierarchical Weight 
&lt;/p&gt;</description></item><item><title>LayerNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#20998;&#20026;&#22810;&#20010;&#30446;&#26631;&#65292;&#24182;&#23558;&#25628;&#32034;&#25104;&#26412;&#21644;&#22870;&#21169;&#20803;&#32032;&#20998;&#24320;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#21457;&#29616;&#20248;&#36234;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.11517</link><description>&lt;p&gt;
LayerNAS&#65306;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LayerNAS: Neural Architecture Search in Polynomial Complexity. (arXiv:2304.11517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11517
&lt;/p&gt;
&lt;p&gt;
LayerNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#20998;&#20026;&#22810;&#20010;&#30446;&#26631;&#65292;&#24182;&#23558;&#25628;&#32034;&#25104;&#26412;&#21644;&#22870;&#21169;&#20803;&#32032;&#20998;&#24320;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#21457;&#29616;&#20248;&#36234;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#24050;&#25104;&#20026;&#21457;&#29616;&#26377;&#25928;&#27169;&#22411;&#26550;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#30446;&#26631;&#30828;&#20214;&#32780;&#35328;&#12290;&#22240;&#27492;&#65292;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#25214;&#21040;&#26368;&#20339;&#26550;&#26500;&#30340;NAS&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayerNAS&#65292;&#23558;&#22810;&#30446;&#26631;NAS&#30340;&#25361;&#25112;&#36716;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#23558;&#25628;&#32034;&#22797;&#26434;&#24230;&#38480;&#21046;&#20026;&#22810;&#39033;&#24335;&#12290;&#23545;&#20110;&#20855;&#26377;$L$&#23618;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#19968;&#23618;&#25191;&#34892;&#36880;&#23618;&#25628;&#32034;&#65292;&#20174;&#19968;&#20010;&#25628;&#32034;&#36873;&#39033;&#38598;$\mathbb{S}$&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;LayerNAS&#26681;&#25454;&#19968;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#27169;&#22411;&#22823;&#23567;&#25110;&#24310;&#36831;&#65292;&#23545;&#27169;&#22411;&#20505;&#36873;&#36827;&#34892;&#20998;&#32452;&#65292;&#24182;&#22522;&#20110;&#21478;&#19968;&#20010;&#30446;&#26631;&#25628;&#32034;&#26368;&#20339;&#27169;&#22411;&#65292;&#20174;&#32780;&#20998;&#21106;&#20102;&#25628;&#32034;&#30340;&#25104;&#26412;&#21644;&#22870;&#21169;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25628;&#32034;&#22797;&#26434;&#24230;&#38480;&#21046;&#22312;$O(H \cdot |\mathbb{S}| \cdot L)$&#65292;&#20854;&#20013;$H$&#26159;&#22312;LayerNAS&#20013;&#35774;&#23450;&#30340;&#24120;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LayerNAS&#33021;&#22815;&#25345;&#32493;&#21457;&#29616;&#20248;&#36234;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has become a popular method for discovering effective model architectures, especially for target hardware. As such, NAS methods that find optimal architectures under constraints are essential. In our paper, we propose LayerNAS to address the challenge of multi-objective NAS by transforming it into a combinatorial optimization problem, which effectively constrains the search complexity to be polynomial.  For a model architecture with $L$ layers, we perform layerwise-search for each layer, selecting from a set of search options $\mathbb{S}$. LayerNAS groups model candidates based on one objective, such as model size or latency, and searches for the optimal model based on another objective, thereby splitting the cost and reward elements of the search. This approach limits the search complexity to $ O(H \cdot |\mathbb{S}| \cdot L) $, where $H$ is a constant set in LayerNAS.  Our experiments show that LayerNAS is able to consistently discover superior models
&lt;/p&gt;</description></item><item><title>QuMoS&#26159;&#19968;&#20010;&#20445;&#25252; QML &#27169;&#22411;&#23433;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32463;&#20856;&#21152;&#23494;&#12289;&#37327;&#23376;&#28151;&#28102;&#21644;&#35825;&#39285;&#26679;&#26412;&#31561;&#22810;&#31181;&#25216;&#26415;&#26469;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#31363;&#21462;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11511</link><description>&lt;p&gt;
QuMoS: &#20445;&#25252;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23433;&#20840;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QuMoS: A Framework for Preserving Security of Quantum Machine Learning Model. (arXiv:2304.11511v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11511
&lt;/p&gt;
&lt;p&gt;
QuMoS&#26159;&#19968;&#20010;&#20445;&#25252; QML &#27169;&#22411;&#23433;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32463;&#20856;&#21152;&#23494;&#12289;&#37327;&#23376;&#28151;&#28102;&#21644;&#35825;&#39285;&#26679;&#26412;&#31561;&#22810;&#31181;&#25216;&#26415;&#26469;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#31363;&#21462;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#22914;&#25910;&#38598;&#30456;&#20851;&#26679;&#26412;&#12289;&#26631;&#35760;&#25968;&#25454;&#21644;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#31561;&#65292;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#26159;&#26368;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#32780;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#65292;&#36825;&#26679;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#20063;&#23384;&#22312;&#65292;&#29978;&#33267;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#21152;&#23494;&#26041;&#27861;&#24456;&#38590;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#37327;&#23376;&#35745;&#31639;&#36164;&#28304;&#65292;&#36817;&#26399;&#22521;&#35757; QML &#27169;&#22411;&#30340;&#36135;&#24065;&#25104;&#26412;&#29978;&#33267;&#21487;&#33021;&#27604;&#32463;&#20856;&#27169;&#22411;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#19968;&#23478;&#20844;&#21496;&#24320;&#21457;&#30340;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340; QML &#27169;&#22411;&#21487;&#20197;&#34987;&#22996;&#27966;&#32473;&#37327;&#23376;&#20113;&#25552;&#20379;&#21830;&#20316;&#20026;&#26381;&#21153;&#65292;&#20379;&#26222;&#36890;&#29992;&#25143;&#20351;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#20113;&#25552;&#20379;&#21830;&#21463;&#21040;&#25915;&#20987;&#65292;QML &#27169;&#22411;&#23558;&#27844;&#28431;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363; QuMoS&#65292;&#29992;&#20110;&#20445;&#25252; QML &#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;QuMoS &#21253;&#25324;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#32463;&#20856;&#21152;&#23494;&#12289;&#37327;&#23376;&#28151;&#28102;&#21644;&#35825;&#39285;&#26679;&#26412;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20351;&#29992; PennyLane &#36719;&#20214;&#24211;&#21644; Google Cirq &#21253;&#30340;&#20855;&#20307; QuMoS &#23454;&#29616;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#27490; QML &#27169;&#22411;&#34987;&#30423;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security has always been a critical issue in machine learning (ML) applications. Due to the high cost of model training -- such as collecting relevant samples, labeling data, and consuming computing power -model-stealing attack is one of the most fundamental but vitally important issues. When it comes to quantum computing, such a quantum machine learning (QML) model-stealing attack also exists and it is even more severe because the traditional encryption method can hardly be directly applied to quantum computation. On the other hand, due to the limited quantum computing resources, the monetary cost of training QML model can be even higher than classical ones in the near term. Therefore, a well-tuned QML model developed by a company can be delegated to a quantum cloud provider as a service to be used by ordinary users. In this case, the QML model will be leaked if the cloud provider is under attack. To address such a problem, we propose a novel framework, namely QuMoS, to preserve mod
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#20132;&#36890;&#20107;&#25925;&#25253;&#21578;&#20013;&#21487;&#20197;&#33719;&#21462;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#24182;&#20248;&#21270;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#30340;&#36164;&#28304;&#37096;&#32626;&#65292;&#20174;&#32780;&#20943;&#23569;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#39640;&#20844;&#20247;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2304.11507</link><description>&lt;p&gt;
&#29992;&#20110;&#19968;&#20307;&#21270;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Machine learning framework for end-to-end implementation of Incident duration prediction. (arXiv:2304.11507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11507
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#20132;&#36890;&#20107;&#25925;&#25253;&#21578;&#20013;&#21487;&#20197;&#33719;&#21462;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#24182;&#20248;&#21270;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#30340;&#36164;&#28304;&#37096;&#32626;&#65292;&#20174;&#32780;&#20943;&#23569;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#39640;&#20844;&#20247;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32463;&#24120;&#24615;&#20107;&#20214;&#65292;&#22914;&#36710;&#36742;&#30896;&#25758;&#21644;&#38556;&#30861;&#29289;&#25152;&#24341;&#36215;&#30340;&#20132;&#36890;&#25317;&#22581;&#26159;&#20132;&#36890;&#31649;&#29702;&#20013;&#24515;(TMCs)&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#21450;&#26102;&#28165;&#38500;&#20107;&#20214;&#23545;&#20110;&#25913;&#21892;&#23433;&#20840;&#12289;&#20943;&#23569;&#24310;&#35823;&#21644;&#25490;&#25918;&#23545;&#20844;&#20247;&#20986;&#34892;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;TMCs&#21644;&#20854;&#20182;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#20107;&#20214;&#30340;&#25345;&#32493;&#26102;&#38388;(&#31561;&#21040;&#36335;&#38754;&#24471;&#21040;&#28165;&#29702;)&#65292;&#20351;&#24471;&#20915;&#31574;&#22914;&#20309;&#37096;&#32626;&#36164;&#28304;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#26512;&#26694;&#26550;&#21644;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#20107;&#20214;&#25253;&#21578;&#21518;&#31435;&#21363;&#21487;&#29992;&#30340;&#20449;&#24687;&#39044;&#27979;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#12290;&#23545;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#30340;&#36136;&#37327;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;TMCs&#21644;&#20854;&#20182;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#37319;&#21462;&#20027;&#21160;&#25514;&#26045;&#65292;&#37096;&#32626;&#25937;&#25588;&#26381;&#21153;&#65292;&#22914;&#25302;&#36710;&#12289;&#32500;&#25252;&#22242;&#38431;&#25110;&#28608;&#27963;&#22791;&#36873;&#36335;&#32447;&#12290;&#39044;&#27979;&#20351;&#29992;&#20998;&#31867;&#21644;&#22238;&#24402;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#30340;&#32452;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#26469;&#33258;&#20315;&#32599;&#37324;&#36798;&#20132;&#36890;&#37096;&#30340;&#23454;&#38469;&#25968;&#25454;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#24456;&#26377;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion caused by non-recurring incidents such as vehicle crashes and debris is a key issue for Traffic Management Centers (TMCs). Clearing incidents in a timely manner is essential for improving safety and reducing delays and emissions for the traveling public. However, TMCs and other responders face a challenge in predicting the duration of incidents (until the roadway is clear), making decisions of what resources to deploy difficult. To address this problem, this research developed an analytical framework and end-to-end machine-learning solution for predicting incident duration based on information available as soon as an incident report is received. Quality predictions of incident duration can help TMCs and other responders take a proactive approach in deploying responder services such as tow trucks, maintenance crews or activating alternative routes. The predictions use a combination of classification and regression machine learning modules. The performance of the devel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#38480;&#21046;&#26469;&#25913;&#21892;&#37329;&#34701;&#26426;&#26500;&#20013;&#30340;&#23458;&#25143;&#27969;&#22833;&#22240;&#26524;&#20998;&#26512;&#65292;&#20197;&#21457;&#29616;&#19982;&#29420;&#31435;&#21464;&#37327;&#30456;&#20851;&#19988;&#19982;&#24433;&#21709;&#27969;&#22833;&#30340;&#22240;&#21464;&#37327;&#22240;&#26524;&#30456;&#20851;&#30340;&#28151;&#28102;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.11503</link><description>&lt;p&gt;
&#37329;&#34701;&#26426;&#26500;&#20013;&#30340;&#23458;&#25143;&#27969;&#22833;&#22240;&#26524;&#20998;&#26512;&#30340;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#25928;&#26524;&#32422;&#26463;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Churn Causal Analysis Through Restrained High-Dimensional Feature Space Effects in Financial Institutions. (arXiv:2304.11503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#38480;&#21046;&#26469;&#25913;&#21892;&#37329;&#34701;&#26426;&#26500;&#20013;&#30340;&#23458;&#25143;&#27969;&#22833;&#22240;&#26524;&#20998;&#26512;&#65292;&#20197;&#21457;&#29616;&#19982;&#29420;&#31435;&#21464;&#37327;&#30456;&#20851;&#19988;&#19982;&#24433;&#21709;&#27969;&#22833;&#30340;&#22240;&#21464;&#37327;&#22240;&#26524;&#30456;&#20851;&#30340;&#28151;&#28102;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#27969;&#22833;&#25351;&#32456;&#27490;&#19982;&#20225;&#19994;&#30340;&#20851;&#31995;&#25110;&#22312;&#29305;&#23450;&#26399;&#38388;&#20943;&#23569;&#23458;&#25143;&#21442;&#19982;&#12290;&#30001;&#20110;&#33719;&#21462;&#26032;&#23458;&#25143;&#30340;&#25104;&#26412;&#21487;&#33021;&#26159;&#20445;&#30041;&#29616;&#26377;&#23458;&#25143;&#25104;&#26412;&#30340;&#20116;&#21040;&#20845;&#20493;&#65292;&#22240;&#27492;&#22312;&#26377;&#27969;&#22833;&#39118;&#38505;&#30340;&#23458;&#25143;&#19978;&#36827;&#34892;&#25237;&#36164;&#26159;&#26126;&#26234;&#30340;&#12290;&#23458;&#25143;&#27969;&#22833;&#27169;&#22411;&#30340;&#22240;&#26524;&#20998;&#26512;&#21487;&#20197;&#39044;&#27979;&#23458;&#25143;&#22312;&#21487;&#39044;&#35265;&#30340;&#26410;&#26469;&#26159;&#21542;&#20250;&#27969;&#22833;&#65292;&#24182;&#30830;&#23450;&#23548;&#33268;&#27969;&#22833;&#30340;&#25928;&#26524;&#21644;&#21487;&#33021;&#30340;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#20197;&#21457;&#29616;&#19982;&#29420;&#31435;&#21464;&#37327;&#30456;&#20851;&#19988;&#19982;&#24433;&#21709;&#27969;&#22833;&#30340;&#22240;&#21464;&#37327;&#22240;&#26524;&#30456;&#20851;&#30340;&#28151;&#28102;&#29305;&#24449;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;SMOTE&#12289;&#38598;&#25104;ANN&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#22312;&#37329;&#34701;&#26426;&#26500;&#20013;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#37329;&#34701;&#25968;&#25454;&#30340;&#23458;&#25143;&#27969;&#22833;&#39044;&#27979;&#38382;&#39064;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#30001;&#23458;&#25143;&#20851;&#31995;&#31649;&#29702;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;&#21306;&#38388;&#30340;&#29305;&#24449;&#20135;&#29983;&#12290;&#36824;&#35780;&#20272;&#20102;&#32500;&#24230;&#35781;&#21650;&#21644;&#32500;&#24230;&#30340;&#31119;&#38899;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer churn describes terminating a relationship with a business or reducing customer engagement over a specific period. Customer acquisition cost can be five to six times that of customer retention, hence investing in customers with churn risk is wise. Causal analysis of the churn model can predict whether a customer will churn in the foreseeable future and identify effects and possible causes for churn. In general, this study presents a conceptual framework to discover the confounding features that correlate with independent variables and are causally related to those dependent variables that impact churn. We combine different algorithms including the SMOTE, ensemble ANN, and Bayesian networks to address churn prediction problems on a massive and high-dimensional finance data that is usually generated in financial institutions due to employing interval-based features used in Customer Relationship Management systems. The effects of the curse and blessing of dimensionality assessed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;PG-GAN&#65289;&#30340;&#27169;&#22411;&#65292;&#20197;&#21028;&#26029;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#65292;&#35299;&#20915;&#20102;DNNs&#36755;&#20986;&#19981;&#28385;&#36275;&#29289;&#29702;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11488</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#29289;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-guided generative adversarial network to learn physical models. (arXiv:2304.11488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;PG-GAN&#65289;&#30340;&#27169;&#22411;&#65292;&#20197;&#21028;&#26029;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#65292;&#35299;&#20915;&#20102;DNNs&#36755;&#20986;&#19981;&#28385;&#36275;&#29289;&#29702;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#21512;&#29702;&#30340;&#29289;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;DNNs&#24191;&#27867;&#29992;&#20110;&#39044;&#27979;&#29289;&#29702;&#21644;&#26426;&#26800;&#29616;&#35937;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;DNNs&#30340;&#36755;&#20986;&#24182;&#19981;&#24635;&#26159;&#28385;&#36275;&#29289;&#29702;&#26041;&#31243;&#12290;&#32771;&#34385;&#29289;&#29702;&#26041;&#31243;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#26041;&#31243;&#27531;&#24046;&#28155;&#21152;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#12290;PINN&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#29289;&#29702;&#26041;&#31243;&#21450;&#20854;&#27531;&#24046;&#24517;&#39035;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#27531;&#24046;&#24182;&#19981;&#24635;&#26159;&#25910;&#25947;&#21040;&#24456;&#23567;&#30340;&#20540;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;PG-GAN&#65289;&#65292;&#23427;&#20351;&#29992;GAN&#26550;&#26500;&#65292;&#22312;&#20854;&#20013;&#29289;&#29702;&#26041;&#31243;&#34987;&#29992;&#26469;&#21028;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#29289;&#29702;&#23398;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#31616;&#21333;&#38382;&#39064;&#65292;&#20197;&#35780;&#20272;&#20854;&#28508;&#22312;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short note describes the concept of guided training of deep neural networks (DNNs) to learn physically reasonable solutions. DNNs are being widely used to predict phenomena in physics and mechanics. One of the issues of DNNs is that their output does not always satisfy physical equations. One approach to consider physical equations is adding a residual of equations into the loss function; this is called physics-informed neural network (PINN). One feature of PINNs is that the physical equations and corresponding residual must be implemented as part of a neural network model. In addition, the residual does not always converge to a small value. The proposed model is a physics-guided generative adversarial network (PG-GAN) that uses a GAN architecture in which physical equations are used to judge whether the neural network's output is consistent with physics. The proposed method was applied to a simple problem to assess its potential usability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26377;&#25928;&#35782;&#21035;&#20102;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#36164;&#28304;&#24110;&#21161;&#30340;&#20154;&#32676;&#65292;&#25299;&#23637;&#20102;&#31038;&#21306;&#25104;&#21592;&#29031;&#39038;&#30340;&#33539;&#30068;&#12290;</title><link>http://arxiv.org/abs/2304.11485</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#24110;&#21161;&#31038;&#21306;&#25104;&#21592;&#30340;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#20132;&#27969;&#20013;&#30340;&#35789;&#27719;&#20559;&#24046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Lexical Biases when Identifying Gang-related Social Media Communications. (arXiv:2304.11485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26377;&#25928;&#35782;&#21035;&#20102;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#36164;&#28304;&#24110;&#21161;&#30340;&#20154;&#32676;&#65292;&#25299;&#23637;&#20102;&#31038;&#21306;&#25104;&#21592;&#29031;&#39038;&#30340;&#33539;&#30068;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#24110;&#27966;&#27963;&#21160;&#30340;&#20154;&#20351;&#29992;&#21253;&#25324;Facebook&#21644;Twitter&#22312;&#20869;&#30340;&#20027;&#27969;&#31038;&#20132;&#23186;&#20307;&#26469;&#34920;&#36798;&#22066;&#35773;&#21644;&#23041;&#32961;&#20197;&#21450;&#21696;&#24764;&#21644;&#32426;&#24565;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#24110;&#27966;&#30456;&#20851;&#27963;&#21160;&#30340;&#24433;&#21709;&#20197;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20026;&#31038;&#21306;&#25104;&#21592;&#25552;&#20379;&#24110;&#21161;&#26159;&#20855;&#26377;&#29420;&#29305;&#25361;&#25112;&#30340;&#65292;&#36825;&#21253;&#25324;&#36947;&#24503;&#19978;&#35782;&#21035;&#21463;&#24110;&#27966;&#27963;&#21160;&#24433;&#21709;&#30340;&#20010;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#21644;&#38656;&#35201;&#32771;&#34385;&#36825;&#20123;&#20010;&#20307;&#22312;&#25512;&#25991;&#20013;&#24120;&#29992;&#30340;&#38750;&#26631;&#20934;&#35821;&#35328;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#29031;&#39038;&#36164;&#28304;&#65292;&#22914;&#39038;&#38382;&#12289;&#20914;&#31361;&#35843;&#35299;&#32773;&#25110;&#23398;&#26415;/&#19987;&#19994;&#22521;&#35757;&#35745;&#21010;&#30340;&#20010;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20108;&#20803;&#36923;&#36753;&#20998;&#31867;&#22120;&#22312;&#35782;&#21035;&#21463;&#24110;&#27966;&#30456;&#20851;&#26292;&#21147;&#24433;&#21709;&#30340;&#20010;&#20307;&#26102;&#65292;&#22312;&#20351;&#29992;&#19982;2015&#24180;&#24052;&#23572;&#30340;&#25705;&#26292;&#21160;&#30456;&#20851;&#30340;&#24110;&#27966;&#30456;&#20851;&#25512;&#25991;&#26679;&#26412;&#26102;&#65292;&#20248;&#20110;&#22522;&#32447;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individuals involved in gang-related activity use mainstream social media including Facebook and Twitter to express taunts and threats as well as grief and memorializing. However, identifying the impact of gang-related activity in order to serve community member needs through social media sources has a unique set of challenges. This includes the difficulty of ethically identifying training data of individuals impacted by gang activity and the need to account for a non-standard language style commonly used in the tweets from these individuals. Our study provides evidence of methods where natural language processing tools can be helpful in efficiently identifying individuals who may be in need of community care resources such as counselors, conflict mediators, or academic/professional training programs. We demonstrate that our binary logistic classifier outperforms baseline standards in identifying individuals impacted by gang-related violence using a sample of gang-related tweets associ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.11473</link><description>&lt;p&gt;
(&#21521;&#37327;)&#31354;&#38388;&#19981;&#26159;&#26368;&#21518;&#30340;&#30086;&#22495;&#65306;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#24040;&#39069;&#25237;&#36164;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#34429;&#28982;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#20027;&#23472;&#20102;&#20135;&#21697;&#25628;&#32034;&#20013;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#21521;&#37327;&#21270;&#26412;&#36523;&#20063;&#21457;&#29983;&#20102;&#24040;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#31435;&#22330;&#35770;&#25991;&#20197;&#30456;&#21453;&#30340;&#26041;&#24335;&#20027;&#24352;&#65292;&#21363;&#31243;&#24207;&#21512;&#25104;&#23545;&#35768;&#22810;&#26597;&#35810;&#21644;&#24066;&#22330;&#20013;&#30340;&#22823;&#37327;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#34892;&#19994;&#37325;&#35201;&#24615;&#65292;&#27010;&#36848;&#20102;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#22522;&#20110;&#25105;&#20204;&#22312;Tooso&#26500;&#24314;&#31867;&#20284;&#31995;&#32479;&#30340;&#32463;&#39564;&#65292;&#22238;&#31572;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#21453;&#23545;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26469;&#33258;&#20843;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;PTMs&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#21457;&#23637;&#36895;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11472</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#38899;&#39057;&#23884;&#20837;&#19982;&#24773;&#24863;&#35782;&#21035;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition. (arXiv:2304.11472v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26469;&#33258;&#20843;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;PTMs&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#21457;&#23637;&#36895;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#22312;&#35821;&#38899;&#21644;&#38899;&#39057;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#30340;&#23884;&#20837;&#21487;&#20197;&#20316;&#20026;&#36755;&#20837;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#24773;&#24863;&#35782;&#21035;&#65292;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#23545;&#39038;&#23458;&#21628;&#21483;&#30340;&#21160;&#24577;&#20998;&#26512;&#12289;&#24515;&#29702;&#20581;&#24247;&#35780;&#20272;&#21644;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#31561;&#12290;PTM&#23884;&#20837;&#26377;&#21161;&#20110;&#25512;&#21160;&#24773;&#24863;&#35782;&#21035;&#30340;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#30340;&#32508;&#21512;&#27604;&#36739;&#65292;&#20363;&#22914;&#23884;&#20837;&#27169;&#22411;&#26550;&#26500;&#12289;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#20197;&#21450;&#39044;&#35757;&#32451;&#36807;&#31243;&#31561;&#12290;PTM&#23884;&#20837;&#30340;&#24443;&#24213;&#27604;&#36739;&#23558;&#26377;&#21161;&#20110;&#26356;&#24555;&#65292;&#26356;&#39640;&#25928;&#22320;&#24320;&#21457;&#27169;&#22411;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#23545;&#26469;&#33258;&#20843;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;PTMs&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65288;&#21253;&#25324;wav2vec 2.0&#65292;data2vec&#65292;wavLM&#65292;UniSpeec&#65289;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models (PTMs) have shown great promise in the speech and audio domain. Embeddings leveraged from these models serve as inputs for learning algorithms with applications in various downstream tasks. One such crucial task is Speech Emotion Recognition (SER) which has a wide range of applications, including dynamic analysis of customer calls, mental health assessment, and personalized language learning. PTM embeddings have helped advance SER, however, a comprehensive comparison of these PTM embeddings that consider multiple facets such as embedding model architecture, data used for pre-training, and the pre-training procedure being followed is missing. A thorough comparison of PTM embeddings will aid in the faster and more efficient development of models and enable their deployment in real-world scenarios. In this work, we exploit this research gap and perform a comparative analysis of embeddings extracted from eight speech and audio PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BAxUS&#65292;&#36890;&#36807;&#21033;&#29992;&#23884;&#22871;&#23376;&#31354;&#38388;&#26469;&#36991;&#20813;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#39118;&#38505;&#24182;&#30830;&#20445;&#39640;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#26356;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11468</link><description>&lt;p&gt;
&#23398;&#20064;&#26102;&#25193;&#22823;&#33539;&#22260;&#65306;&#23884;&#22871;&#23376;&#31354;&#38388;&#20013;&#30340;&#33258;&#36866;&#24212;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Increasing the Scope as You Learn: Adaptive Bayesian Optimization in Nested Subspaces. (arXiv:2304.11468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BAxUS&#65292;&#36890;&#36807;&#21033;&#29992;&#23884;&#22871;&#23376;&#31354;&#38388;&#26469;&#36991;&#20813;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#39118;&#38505;&#24182;&#30830;&#20445;&#39640;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#26356;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#20960;&#21313;&#20010;&#32500;&#24230;&#30340;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#65292;&#24182;&#28212;&#26395;&#22312;&#29983;&#21629;&#31185;&#23398;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#23454;&#29616;&#37325;&#22823;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;HDBO&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#30340;&#26356;&#28145;&#20837;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#32500;&#24230;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#65292;&#29978;&#33267;&#26377;&#22833;&#36133;&#39118;&#38505;&#65292;&#22914;&#26524;&#19981;&#28385;&#36275;&#26576;&#20123;&#26080;&#27861;&#39564;&#35777;&#30340;&#20551;&#35774;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;BAxUS&#65292;&#23427;&#21033;&#29992;&#19968;&#26063;&#26032;&#39062;&#30340;&#23884;&#22871;&#38543;&#26426;&#23376;&#31354;&#38388;&#26469;&#20351;&#20854;&#20248;&#21270;&#30340;&#31354;&#38388;&#36866;&#24212;&#38382;&#39064;&#12290;&#36825;&#30830;&#20445;&#20102;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#28040;&#38500;&#20102;&#22833;&#36133;&#30340;&#39118;&#38505;&#12290;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;BAxUS&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have extended the scope of Bayesian optimization (BO) to expensive-to-evaluate black-box functions with dozens of dimensions, aspiring to unlock impactful applications, for example, in the life sciences, neural architecture search, and robotics. However, a closer examination reveals that the state-of-the-art methods for high-dimensional Bayesian optimization (HDBO) suffer from degrading performance as the number of dimensions increases or even risk failure if certain unverifiable assumptions are not met. This paper proposes BAxUS that leverages a novel family of nested random subspaces to adapt the space it optimizes over to the problem. This ensures high performance while removing the risk of failure, which we assert via theoretical guarantees. A comprehensive evaluation demonstrates that BAxUS achieves better results than the state-of-the-art methods for a broad set of applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#25945;&#31243;&#21644;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#21452;&#21521;RNN&#12289;&#21452;&#21521;LSTM&#21644;ELMo&#32593;&#32476;&#31561;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11461</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65306;&#25945;&#31243;&#21644;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks and Long Short-Term Memory Networks: Tutorial and Survey. (arXiv:2304.11461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#25945;&#31243;&#21644;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#21452;&#21521;RNN&#12289;&#21452;&#21521;LSTM&#21644;ELMo&#32593;&#32476;&#31561;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#25945;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#21160;&#24577;&#31995;&#32479;&#21644;RNN&#30340;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#24320;&#22987;&#35762;&#36848;&#65292;&#28982;&#21518;&#35752;&#35770;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25509;&#36817;&#21333;&#20301;&#26435;&#37325;&#30697;&#38453;&#12289;&#38271;&#24310;&#36831;&#12289;&#27844;&#28431;&#21333;&#20803;&#21644;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LSTM&#38376;&#21644;&#21333;&#20803;&#12289;LSTM&#30340;&#21382;&#21490;&#21644;&#21464;&#20307;&#20197;&#21450;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#21452;&#21521;RNN&#12289;&#21452;&#21521;LSTM&#21644;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;&#65288;ELMo&#65289;&#32593;&#32476;&#65292;&#20197;&#22312;&#20004;&#20010;&#26041;&#21521;&#19978;&#22788;&#29702;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a tutorial paper on Recurrent Neural Network (RNN), Long Short-Term Memory Network (LSTM), and their variants. We start with a dynamical system and backpropagation through time for RNN. Then, we discuss the problems of gradient vanishing and explosion in long-term dependencies. We explain close-to-identity weight matrix, long delays, leaky units, and echo state networks for solving this problem. Then, we introduce LSTM gates and cells, history and variants of LSTM, and Gated Recurrent Units (GRU). Finally, we introduce bidirectional RNN, bidirectional LSTM, and the Embeddings from Language Model (ELMo) network, for processing a sequence in both directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#31361;&#21464;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#35753;&#26234;&#33021;&#20307;&#33719;&#24471;&#26368;&#20248;&#30340;&#38271;&#26399;&#25240;&#25187;&#22238;&#25253;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#22238;&#25253;-&#26816;&#27979;&#24179;&#34913;&#65292;&#24182;&#20351;&#29992;&#26368;&#24555;&#30340;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#26469;&#26816;&#27979;&#27169;&#22411;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.11460</link><description>&lt;p&gt;
&#27169;&#22411;&#31361;&#21464;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with an Abrupt Model Change. (arXiv:2304.11460v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#31361;&#21464;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#35753;&#26234;&#33021;&#20307;&#33719;&#24471;&#26368;&#20248;&#30340;&#38271;&#26399;&#25240;&#25187;&#22238;&#25253;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#22238;&#25253;-&#26816;&#27979;&#24179;&#34913;&#65292;&#24182;&#20351;&#29992;&#26368;&#24555;&#30340;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#26469;&#26816;&#27979;&#27169;&#22411;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29615;&#22659;&#25110;&#32773;&#27169;&#22411;&#21457;&#29983;&#21464;&#21270;&#26102;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#35753;&#26234;&#33021;&#20307;&#24212;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#33719;&#24471;&#26368;&#20248;&#30340;&#38271;&#26399;&#25240;&#25187;&#22238;&#25253;&#12290;&#35813;&#31639;&#27861;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#24182;&#19988;&#24050;&#32463;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#24378;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#20063;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#38382;&#39064;&#20013;&#19968;&#20010;&#22522;&#26412;&#30340;&#22238;&#25253;-&#26816;&#27979;&#24179;&#34913;&#65292;&#24182;&#20351;&#29992;&#20102;&#26368;&#24555;&#30340;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#26469;&#26816;&#27979;&#27169;&#22411;&#21464;&#21270;&#12290;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#26816;&#27979;&#27169;&#22411;&#21464;&#21270;&#21644;&#26234;&#33021;&#21021;&#22987;&#21270;&#31574;&#30053;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of reinforcement learning is considered where the environment or the model undergoes a change. An algorithm is proposed that an agent can apply in such a problem to achieve the optimal long-time discounted reward. The algorithm is model-free and learns the optimal policy by interacting with the environment. It is shown that the proposed algorithm has strong optimality properties. The effectiveness of the algorithm is also demonstrated using simulation results. The proposed algorithm exploits a fundamental reward-detection trade-off present in these problems and uses a quickest change detection algorithm to detect the model change. Recommendations are provided for faster detection of model changes and for smart initialization strategies.
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#36873;&#25321;&#36866;&#24403;&#30340;&#26080;&#30417;&#30563;AD&#31639;&#27861;&#12290;&#20803;&#23398;&#20064;&#22120;&#30340;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.11438</link><description>&lt;p&gt;
&#26500;&#24314;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Constructing a meta-learner for unsupervised anomaly detection. (arXiv:2304.11438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11438
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#36873;&#25321;&#36866;&#24403;&#30340;&#26080;&#30417;&#30563;AD&#31639;&#27861;&#12290;&#20803;&#23398;&#20064;&#22120;&#30340;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#23545;&#20110;&#20174;&#32593;&#32476;&#23433;&#20840;&#21040;&#21307;&#30103;&#24037;&#20855;&#31561;&#19968;&#31995;&#21015;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;&#22810;&#26679;&#24615;&#65292;&#27809;&#26377;&#21333;&#19968;&#31639;&#27861;&#34987;&#21457;&#29616;&#22312;&#25152;&#26377;AD&#20219;&#21153;&#20013;&#37117;&#20248;&#36234;&#12290;&#22312;&#26377;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#21644;AutoML&#65292;&#36873;&#25321;&#31639;&#27861;&#65288;&#20063;&#31216;&#20026;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65288;ASP&#65289;&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#26080;&#30417;&#30563;AD&#20219;&#21153;&#21364;&#23569;&#26377;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#20174;&#26410;&#26631;&#35760;&#30340;&#36755;&#20837;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#20803;&#29305;&#24449;&#65292;&#35782;&#21035;&#20986;&#36866;&#24403;&#30340;&#26080;&#30417;&#30563;AD&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20803;&#23398;&#20064;&#22120;&#30340;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36824;&#36827;&#34892;&#20102;&#28151;&#21512;&#27169;&#22411;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#26816;&#39564;&#20803;&#23398;&#20064;&#22120;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20803;&#27169;&#22411;&#12289;&#20803;&#29305;&#24449;&#21644;&#22522;&#30784;AD&#31639;&#27861;&#38598;&#21512;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly detection (AD) is critical for a wide range of practical applications, from network security to health and medical tools. Due to the diversity of problems, no single algorithm has been found to be superior for all AD tasks. Choosing an algorithm, otherwise known as the Algorithm Selection Problem (ASP), has been extensively examined in supervised classification problems, through the use of meta-learning and AutoML, however, it has received little attention in unsupervised AD tasks. This research proposes a new meta-learning approach that identifies an appropriate unsupervised AD algorithm given a set of meta-features generated from the unlabelled input dataset. The performance of the proposed meta-learner is superior to the current state of the art solution. In addition, a mixed model statistical analysis has been conducted to examine the impact of the meta-learner components: the meta-model, meta-features, and the base set of AD algorithms, on the overall performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2304.11436</link><description>&lt;p&gt;
&#36890;&#36807;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#24674;&#22797;&#22270;&#20687;&#30340;FedMD
&lt;/p&gt;
&lt;p&gt;
Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;&#27169;&#22411;&#33976;&#39311;&#65288;FedMD&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21327;&#20316;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20165;&#20256;&#36755;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986;logits&#20316;&#20026;&#33976;&#39311;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#20256;&#36882;&#26131;&#21463;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#31169;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24050;&#30693;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#21363;&#20351;&#20849;&#20139;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986; logit&#27604;&#30452;&#25509;&#20849;&#20139;&#26799;&#24230;&#26356;&#23433;&#20840;&#65292;&#20173;&#23384;&#22312;&#22240;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#23548;&#33268;&#30340;&#25968;&#25454;&#26333;&#20809;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#21453;&#28436;&#31070;&#32463;&#32593;&#32476;&#26469;&#21033;&#29992;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#32622;&#20449;&#24230;&#24046;&#65292;&#38024;&#23545;FedMD&#21450;&#20854;&#21464;&#31181;&#36827;&#34892;PLI&#65288;&#37197;&#23545;logits&#21453;&#28436;&#65289;&#25915;&#20987;&#12290;&#22312;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31867;&#20284;&#20110;FedMD&#30340;&#26041;&#26696;&#20013;&#65292;&#20165;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#37197;&#23545;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;logits&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#33021;&#22815;&#37325;&#26500;&#31169;&#26377;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65292;&#22312;&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#23545;&#27599;&#20010;&#35270;&#35282;&#36827;&#34892;&#32858;&#31867;&#24182;&#25552;&#21462;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.11435</link><description>&lt;p&gt;
&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyper-Laplacian Regularized Concept Factorization in Low-rank Tensor Space for Multi-view Clustering. (arXiv:2304.11435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65292;&#22312;&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#23545;&#27599;&#20010;&#35270;&#35282;&#36827;&#34892;&#32858;&#31867;&#24182;&#25552;&#21462;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#33021;&#22815;&#35780;&#20272;&#39640;&#38454;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#32858;&#31867;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#25968;&#23384;&#22312;&#20004;&#20010;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#33258;&#34920;&#31034;&#30340;&#24352;&#37327;&#23376;&#31354;&#38388;&#23398;&#20064;&#36890;&#24120;&#24341;&#36215;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#24863;&#30693;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#12290;&#20854;&#27425;&#65292;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#27169;&#22411;&#23558;&#27599;&#20010;&#22855;&#24322;&#20540;&#31561;&#20998;&#22320;&#37325;&#26032;&#20998;&#24067;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#65288;HLRCF&#65289;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#24212;&#29992;&#20110;&#25506;&#32034;&#27599;&#20010;&#35270;&#35282;&#30340;&#28508;&#22312;&#32858;&#31867;&#34920;&#31034;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#36229;&#22270;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#36171;&#20104;&#20102;&#35813;&#27169;&#22411;&#25552;&#21462;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor-oriented multi-view subspace clustering has achieved significant strides in assessing high-order correlations and improving clustering analysis of multi-view data. Nevertheless, most of existing investigations are typically hampered by the two flaws. First, self-representation based tensor subspace learning usually induces high time and space complexity, and is limited in perceiving nonlinear local structure in the embedding space. Second, the tensor singular value decomposition (t-SVD) model redistributes each singular value equally without considering the diverse importance among them. To well cope with the issues, we propose a hyper-Laplacian regularized concept factorization (HLRCF) in low-rank tensor space for multi-view clustering. Specifically, we adopt the concept factorization to explore the latent cluster-wise representation of each view. Further, the hypergraph Laplacian regularization endows the model with the capability of extracting the nonlinear local structures i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#35821;&#26009;&#24211;&#23558;BERT&#27169;&#22411;&#36716;&#25442;&#25104;SBERT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#27431;&#35821;&#35328;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11434</link><description>&lt;p&gt;
L3Cube-IndicSBERT: &#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23398;&#20064;&#36328;&#35821;&#35328;&#21477;&#23376;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT. (arXiv:2304.11434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#35821;&#26009;&#24211;&#23558;BERT&#27169;&#22411;&#36716;&#25442;&#25104;SBERT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#27431;&#35821;&#35328;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#21477;&#23376;BERT (SBERT) &#27169;&#22411;&#23558;&#19981;&#21516;&#35821;&#35328;&#26144;&#23556;&#21040;&#20849;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#23545;&#20110;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#35821;&#26009;&#24211;&#23558;&#26222;&#36890;&#30340;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36716;&#25442;&#25104;&#22810;&#35821;&#35328;&#21477;&#23376;BERT&#27169;&#22411;&#12290;&#25105;&#20204;&#31616;&#21333;&#22320;&#32858;&#21512;&#20302;&#36164;&#28304;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793; NLI &#25110; STS &#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26222;&#36890;&#30340;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36827;&#34892;&#31867;&#20284;SBERT&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#20855;&#26377;&#20869;&#22312;&#30340;&#36328;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#24494;&#35843;&#26041;&#27861;&#27809;&#26377;&#26174;&#24335;&#30340;&#36328;&#35821;&#35328;&#35757;&#32451;&#65292;&#21364;&#20135;&#29983;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#38750;&#21360;&#27431;&#35821;&#35328;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;L3Cube-IndicSBERT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21360;&#24230;&#35821;&#35328;&#21360;&#22320;&#35821;&#21644;&#39532;&#26469;&#35821;&#30340;&#22810;&#35821;&#35328;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multilingual Sentence-BERT (SBERT) models map different languages to common representation space and are useful for cross-language similarity and mining tasks. We propose a simple yet effective approach to convert vanilla multilingual BERT models into multilingual sentence BERT models using synthetic corpus. We simply aggregate translated NLI or STS datasets of the low-resource target languages together and perform SBERT-like fine-tuning of the vanilla multilingual BERT model. We show that multilingual BERT models are inherent cross-lingual learners and this simple baseline fine-tuning approach without explicit cross-lingual training yields exceptional cross-lingual properties. We show the efficacy of our approach on 10 major Indic languages and also show the applicability of our approach to non-Indic languages German and French. Using this approach, we further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20855;&#26377;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11433</link><description>&lt;p&gt;
&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Conditional Denoising Diffusion for Sequential Recommendation. (arXiv:2304.11433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20855;&#26377;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#23398;&#20064;&#20869;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#24182;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#29983;&#25104;&#27169;&#22411;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20004;&#31181;&#20027;&#35201;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#22312;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23384;&#22312;&#25361;&#25112;&#65292;GANs&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#20248;&#21270;&#65292;&#32780;VAEs&#21017;&#23481;&#26131;&#21457;&#29983;&#21518;&#39564;&#23849;&#22604;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#29983;&#25104;&#12290;&#39034;&#24207;&#25512;&#33616;&#30340;&#31232;&#30095;&#21644;&#22024;&#26434;&#30340;&#29305;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#20132;&#21449;&#27880;&#24847;&#21435;&#22122;&#35299;&#30721;&#22120;&#21644;&#36880;&#27493;&#25193;&#25955;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;&#23450;&#37327;&#25351;&#26631;&#19978;&#36824;&#26159;&#22312;&#23450;&#24615;&#25351;&#26631;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have attracted significant interest due to their ability to handle uncertainty by learning the inherent data distributions. However, two prominent generative models, namely Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs), exhibit challenges that impede achieving optimal performance in sequential recommendation tasks. Specifically, GANs suffer from unstable optimization, while VAEs are prone to posterior collapse and over-smoothed generations. The sparse and noisy nature of sequential recommendation further exacerbates these issues. In response to these limitations, we present a conditional denoising diffusion model, which includes a sequence encoder, a cross-attentive denoising decoder, and a step-wise diffuser. This approach streamlines the optimization and generation process by dividing it into easier and tractable steps in a conditional autoregressive manner. Furthermore, we introduce a novel optimization schema that incorporates both cro
&lt;/p&gt;</description></item><item><title>Pipeline MoE&#26159;&#19968;&#31181;&#21033;&#29992;&#31649;&#36947;&#24182;&#34892;&#24615;&#26550;&#26500;&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#36890;&#20449;&#24320;&#38144;&#21644;&#38480;&#21046;&#24615;&#25193;&#23637;&#38382;&#39064;&#30340;&#28789;&#27963;&#23454;&#29616;&#65292; &#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#29616;&#26377;&#26368;&#26032;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#36798;17%&#30340;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.11414</link><description>&lt;p&gt;
Pipeline MoE:&#19968;&#31181;&#20855;&#26377;&#31649;&#36947;&#24182;&#34892;&#24615;&#30340;&#28789;&#27963;MoE &#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism. (arXiv:2304.11414v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11414
&lt;/p&gt;
&lt;p&gt;
Pipeline MoE&#26159;&#19968;&#31181;&#21033;&#29992;&#31649;&#36947;&#24182;&#34892;&#24615;&#26550;&#26500;&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#36890;&#20449;&#24320;&#38144;&#21644;&#38480;&#21046;&#24615;&#25193;&#23637;&#38382;&#39064;&#30340;&#28789;&#27963;&#23454;&#29616;&#65292; &#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#29616;&#26377;&#26368;&#26032;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#36798;17%&#30340;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35268;&#27169;&#21270;&#65292;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#22240;&#20854;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#20122;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#32780;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MoE&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65306;1&#65289;&#20840;&#37096;&#35843;&#24230;&#21644;&#25910;&#38598;&#24341;&#20837;&#20102;&#24040;&#22823;&#30340;&#20869;&#37096;&#33410;&#28857;&#21644;&#33410;&#28857;&#38388;&#36890;&#20449;&#24320;&#38144;&#65292;2&#65289;&#25968;&#25454;&#24182;&#34892;&#21644;&#19987;&#23478;&#24182;&#34892;&#32500;&#24230;&#21463;&#38480;&#65292;&#26080;&#27861;&#22312;&#19987;&#23478;&#32500;&#24230;&#19978;&#36827;&#34892;&#25193;&#23637;&#12290;&#26412;&#25991;&#20174;&#24182;&#34892;&#26694;&#26550;&#30340;&#35282;&#24230;&#31995;&#32479;&#20998;&#26512;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Pipeline MoE&#65288;PPMoE&#65289;&#30340;&#26032;&#22411;MoE&#26550;&#26500;&#26469;&#35299;&#20915;&#23427;&#20204;&#12290;PPMoE&#20197;&#24352;&#37327;&#24182;&#34892;&#20026;&#22522;&#30784;&#26500;&#24314;&#19987;&#23478;&#24182;&#34892;&#65292;&#24182;&#29992;&#31616;&#21333;&#30340;&#24352;&#37327;&#32034;&#24341;&#20999;&#29255;&#21644;&#20869;&#37096;&#33410;&#28857;&#20840;&#23616;&#27719;&#32858;&#20195;&#26367;&#20102;&#36890;&#20449;&#23494;&#38598;&#30340;&#20840;&#37096;&#35843;&#24230;&#19982;&#25910;&#38598;&#12290;&#27492;&#22806;&#65292;PPMoE&#36824;&#21487;&#20197;&#26041;&#20415;&#22320;&#38598;&#25104;&#31649;&#36947;&#24182;&#34892;&#20197;&#36827;&#19968;&#27493;&#25193;&#23637;&#39592;&#24178;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PPMoE&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;17&#65285;&#30340;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#65292;&#24182;&#22312;&#19987;&#23478;&#25968;&#37327;&#21644;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26368;&#26032;&#27169;&#22411;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixture of Experts (MoE) model becomes an important choice of large language models nowadays because of its scalability with sublinear computational complexity for training and inference. However, existing MoE models suffer from two critical drawbacks, 1) tremendous inner-node and inter-node communication overhead introduced by all-to-all dispatching and gathering, and 2) limited scalability for the backbone because of the bound data parallel and expert parallel to scale in the expert dimension. In this paper, we systematically analyze these drawbacks in terms of training efficiency in the parallel framework view and propose a novel MoE architecture called Pipeline MoE (PPMoE) to tackle them. PPMoE builds expert parallel incorporating with tensor parallel and replaces communication-intensive all-to-all dispatching and gathering with a simple tensor index slicing and inner-node all-reduce. Besides, it is convenient for PPMoE to integrate pipeline parallel to further scale the backbo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;Lyapunov&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30899;&#20013;&#21644;&#36793;&#32536;&#35745;&#31639;&#65292;&#38477;&#20302;&#30899;&#25490;&#25918;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#26381;&#21153;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.11374</link><description>&lt;p&gt;
&#26397;&#21521;&#30899;&#20013;&#21644;&#36793;&#32536;&#35745;&#31639;&#65306;&#21033;&#29992;&#29616;&#36135;&#21644;&#26410;&#26469;&#30899;&#24066;&#22330;&#32511;&#33394;&#36793;&#32536;AI
&lt;/p&gt;
&lt;p&gt;
Towards Carbon-Neutral Edge Computing: Greening Edge AI by Harnessing Spot and Future Carbon Markets. (arXiv:2304.11374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;Lyapunov&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30899;&#20013;&#21644;&#36793;&#32536;&#35745;&#31639;&#65292;&#38477;&#20302;&#30899;&#25490;&#25918;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#26381;&#21153;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#30899;&#24847;&#35782;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21368;&#36733;&#21644;&#38480;&#23450;&#30899;&#25490;&#25918;&#37327;&#20174;&#32780;&#23454;&#29616;&#32511;&#33394;&#36793;&#32536;AI&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21368;&#36733;&#21644;&#30899;&#25490;&#25918;&#26435;&#36141;&#20080;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36164;&#28304;&#20215;&#26684;&#12289;&#30899;&#25490;&#25918;&#26435;&#20215;&#26684;&#12289;&#21508;&#20010;&#31449;&#28857;&#30340;&#30899;&#25490;&#25918;&#24378;&#24230;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21040;&#36798;&#26102;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24456;&#38590;&#22312;&#32447;&#20915;&#23450;&#38271;&#26399;&#20248;&#21270;&#25919;&#31574;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;Lyapunov&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#21453;&#39304;&#65292;&#22312;&#21453;&#24212;&#24615;&#26041;&#24335;&#19979;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21368;&#36733;&#21644;&#30899;&#25490;&#25918;&#26435;&#36141;&#20080;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provisioning dynamic machine learning (ML) inference as a service for artificial intelligence (AI) applications of edge devices faces many challenges, including the trade-off among accuracy loss, carbon emission, and unknown future costs. Besides, many governments are launching carbon emission rights (CER) for operators to reduce carbon emissions further to reverse climate change. Facing these challenges, to achieve carbon-aware ML task offloading under limited carbon emission rights thus to achieve green edge AI, we establish a joint ML task offloading and CER purchasing problem, intending to minimize the accuracy loss under the long-term time-averaged cost budget of purchasing the required CER. However, considering the uncertainty of the resource prices, the CER purchasing prices, the carbon intensity of sites, and ML tasks' arrivals, it is hard to decide the optimal policy online over a long-running period time. To overcome this difficulty, we leverage the two-timescale Lyapunov opt
&lt;/p&gt;</description></item><item><title>GEDI&#26159;&#19968;&#31181;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#20110;&#20284;&#28982;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#23427;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#32852;&#21512;&#35757;&#32451;&#65292;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#25110;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#35777;&#26126;GEDI&#21487;&#20197;&#22312;&#32858;&#31867;&#24615;&#33021;&#19978;&#26174;&#33879;&#36229;&#36234;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#23567;&#25968;&#25454;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.11357</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35757;&#32451;&#23398;&#20064;&#31526;&#21495;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training. (arXiv:2304.11357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11357
&lt;/p&gt;
&lt;p&gt;
GEDI&#26159;&#19968;&#31181;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#20110;&#20284;&#28982;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#23427;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#32852;&#21512;&#35757;&#32451;&#65292;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#25110;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#35777;&#26126;GEDI&#21487;&#20197;&#22312;&#32858;&#31867;&#24615;&#33021;&#19978;&#26174;&#33879;&#36229;&#36234;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#23567;&#25968;&#25454;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GEDI&#65292;&#23427;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#19982;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#27604;&#29420;&#31435;&#35299;&#20915;&#26041;&#26696;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;GEDI&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#24182;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#32852;&#21512;&#35757;&#32451;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#25110;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21253;&#25324;SVHN&#12289;CIFAR10&#21644;CIFAR100&#22312;&#20869;&#30340;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;GEDI&#22312;&#32858;&#31867;&#24615;&#33021;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#12290;&#31526;&#21495;&#32452;&#20214;&#36827;&#19968;&#27493;&#20801;&#35768;&#23427;&#21033;&#29992;&#36923;&#36753;&#32422;&#26463;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#23567;&#25968;&#25454;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GEDI, a Bayesian framework that combines existing self-supervised learning objectives with likelihood-based generative models. This framework leverages the benefits of both GEnerative and DIscriminative approaches, resulting in improved symbolic representations over standalone solutions. Additionally, GEDI can be easily integrated and trained jointly with existing neuro-symbolic frameworks without the need for additional supervision or costly pre-training steps. We demonstrate through experiments on real-world data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a significant margin. The symbolic component further allows it to leverage knowledge in the form of logical constraints to improve performance in the small data regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.11332</link><description>&lt;p&gt;
&#21033;&#29992;SAM&#30340;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;: &#20197;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#20026;&#22522;&#30784;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#26159;&#19968;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;. SAM&#20351;&#29992;&#20102;&#36229;&#36807;1&#20159;&#20010;&#25513;&#27169;&#30340;1100&#19975;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;&#33258;&#28982;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#23545;&#35937;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23613;&#31649;SAM&#24182;&#27809;&#26377;&#31435;&#21363;&#20026;&#21307;&#23398;&#22270;&#20687;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#65292;&#20294;&#20854;&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#23545;&#20110;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;SAM&#26469;&#22686;&#24378;&#32463;&#20856;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65288;&#22914;U-Net&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical images, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image inputs for a commonly-used medical image segmentation model (e.g., U-Net). Experiments on two datasets show the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.11328</link><description>&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37319;&#26679;&#31574;&#30053;&#23581;&#35797;&#26377;&#25928;&#22320;&#35299;&#20915;&#21453;&#21521;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#25152;&#24471;ODE&#27714;&#35299;&#22120;&#30340;&#31995;&#25968;&#30001;ODE&#20844;&#24335;&#65292;&#21453;&#21521;&#31163;&#25955;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#20351;&#29992;&#30340;ODE&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#20248;&#21270;&#26576;&#20123;&#31995;&#25968;&#26469;&#21152;&#36895;&#20960;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;ODE&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#12290;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;&#19968;&#32452;&#32454;&#31890;&#24230;&#26102;&#38388;&#27493;&#38271;&#30340;&#21407;&#22987;ODE&#27714;&#35299;&#22120;&#26500;&#36896;MSE&#65292;&#20174;&#21407;&#29702;&#19978;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#31215;&#20998;&#36924;&#36817;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#25193;&#25955;&#38544;&#34255;&#29366;&#24577;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;
&lt;/p&gt;
&lt;p&gt;
One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;ERM&#26412;&#36136;&#19978;&#21516;&#26102;&#23398;&#20064;&#20102;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;ERM&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#36136;&#37327;&#24433;&#21709;&#20102;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#65292;&#26410;&#33021;&#25429;&#33719;&#25152;&#26377;&#28508;&#22312;&#30340;&#26377;&#29992;&#29305;&#24449;&#23558;&#38480;&#21046;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11327</link><description>&lt;p&gt;
&#25506;&#32034;&#22806;&#37096;&#20998;&#24067;&#24191;&#20041;&#21270;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Feature Learning in Out-of-Distribution Generalization. (arXiv:2304.11327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11327
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;ERM&#26412;&#36136;&#19978;&#21516;&#26102;&#23398;&#20064;&#20102;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;ERM&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#36136;&#37327;&#24433;&#21709;&#20102;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#65292;&#26410;&#33021;&#25429;&#33719;&#25152;&#26377;&#28508;&#22312;&#30340;&#26377;&#29992;&#29305;&#24449;&#23558;&#38480;&#21046;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#24191;&#20041;&#21270;&#30340;&#22833;&#36133;&#65292;&#24120;&#35265;&#30340;&#35299;&#37322;&#26159;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#27169;&#22411;&#23398;&#20064;&#21040;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#32780;&#19981;&#26159;&#26399;&#26395;&#30340;&#19981;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#20960;&#39033;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35299;&#37322;&#65292;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#21487;&#33021;&#24050;&#32463;&#23398;&#21040;&#20102;&#36275;&#22815;&#22909;&#30340;&#29305;&#24449;&#36827;&#34892;OOD&#24191;&#20041;&#21270;&#12290;&#36825;&#22330;&#36777;&#35770;&#25193;&#23637;&#21040;&#20102;&#35768;&#22810;OOD&#24191;&#20041;&#21270;&#20219;&#21153;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#32452;&#32455;&#21644;OOD&#24615;&#33021;&#30456;&#20851;&#24615;&#20013;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#20284;&#20046;&#30456;&#20114;&#30683;&#30462;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;ERM&#26412;&#36136;&#19978;&#21516;&#26102;&#23398;&#20064;&#20102;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;ERM&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#36136;&#37327;&#26174;&#33879;&#24433;&#21709;&#20102;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#65292;&#22240;&#20026;OOD&#23545;&#35937;&#24456;&#23569;&#23398;&#20064;&#21040;&#26032;&#21151;&#33021;&#12290;&#26410;&#33021;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#25429;&#33719;&#25152;&#26377;&#28508;&#22312;&#30340;&#26377;&#29992;&#29305;&#24449;&#23558;&#36827;&#19968;&#27493;&#38480;&#21046;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common explanation for the failure of out-of-distribution (OOD) generalization is that the model trained with empirical risk minimization (ERM) learns spurious features instead of the desired invariant features. However, several recent studies challenged this explanation and found that deep networks may have already learned sufficiently good features for OOD generalization. The debate extends to the in-distribution and OOD performance correlations along with training or fine-tuning neural nets across a variety of OOD generalization tasks. To understand these seemingly contradicting phenomena, we conduct a theoretical investigation and find that ERM essentially learns both spurious features and invariant features. On the other hand, the quality of learned features during ERM pre-training significantly affects the final OOD performance, as OOD objectives rarely learn new features. Failing to capture all the underlying useful features during pre-training will further limit the final OOD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#65292;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#36866;&#24212;&#26426;&#21046;&#65292;&#32467;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#35328;&#26426;&#65292;&#23454;&#26102;&#20272;&#35745;&#19981;&#21305;&#37197;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#19981;&#21305;&#37197;&#21644;&#26377;&#30028;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.11315</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25903;&#25345;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#19981;&#21305;&#37197;&#19981;&#30830;&#23450;&#24615;&#32531;&#35299;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unmatched uncertainty mitigation through neural network supported model predictive control. (arXiv:2304.11315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#65292;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#36866;&#24212;&#26426;&#21046;&#65292;&#32467;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#35328;&#26426;&#65292;&#23454;&#26102;&#20272;&#35745;&#19981;&#21305;&#37197;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#19981;&#21305;&#37197;&#21644;&#26377;&#30028;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#19981;&#21305;&#37197;&#21644;&#26377;&#30028;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20316;&#20026;&#23398;&#20064;&#22522;&#30784;MPC&#65288;LBMPC&#65289;&#20013;&#30340;&#39044;&#35328;&#26426;&#65292;&#20197;&#20272;&#35745;&#19981;&#21305;&#37197;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#30001;&#20110;&#23454;&#26102;&#20272;&#35745;&#20854;&#31995;&#25968;&#30340;&#25216;&#26415;&#22256;&#38590;&#65292;&#36890;&#24120;&#35748;&#20026;LBMPC&#38590;&#20197;&#20351;&#29992;&#38750;&#21442;&#25968;&#39044;&#35328;&#26426;&#65292;&#20363;&#22914;DNN&#12290;&#25105;&#20204;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#36866;&#24212;&#26426;&#21046;&#65292;&#22312;&#20869;&#37096;&#23618;&#20197;&#32531;&#24930;&#30340;&#26102;&#38388;&#23610;&#24230;&#20351;&#29992;&#22312;&#32447;&#25910;&#38598;&#24182;&#26377;&#36873;&#25321;&#22320;&#23384;&#20648;&#22312;&#32531;&#20914;&#21306;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23454;&#26102;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21943;&#27668;&#21457;&#21160;&#26426;&#21387;&#32553;&#31995;&#32479;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#20197;&#23454;&#26102;&#23454;&#29616;&#30340;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning based model predictive control (MPC) algorithm for systems with unmatched and bounded state-action dependent uncertainties of unknown structure. We utilize a deep neural network (DNN) as an oracle in the underlying optimization problem of learning based MPC (LBMPC) to estimate unmatched uncertainties. Generally, non-parametric oracles such as DNN are considered difficult to employ with LBMPC due to the technical difficulties associated with estimation of their coefficients in real time. We employ a dual-timescale adaptation mechanism, where the weights of the last layer of the neural network are updated in real time while the inner layers are trained on a slower timescale using the training data collected online and selectively stored in a buffer. Our results are validated through a numerical experiment on the compression system model of jet engine. These results indicate that the proposed approach is implementable in real time and carries the theore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#25216;&#26415;&#26469;&#20248;&#21270;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#20272;&#35745;&#36827;&#34892;&#22806;&#25512;&#26469;&#35745;&#31639;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;DNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11312</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Lookahead Diffusion Probabilistic Models for Refining Mean Estimation. (arXiv:2304.11312v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#25216;&#26415;&#26469;&#20248;&#21270;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#20272;&#35745;&#36827;&#34892;&#22806;&#25512;&#26469;&#35745;&#31639;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;DNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;LA-DPMs&#65289;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#20013;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#20043;&#21518;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#20248;&#21270;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23545;&#20004;&#20010;$\boldsymbol{x}$&#20272;&#35745;&#36827;&#34892;&#22806;&#25512;&#26469;&#35745;&#31639;&#26356;&#20934;&#30830;&#30340;$\boldsymbol{x}$&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29616;&#26377;DPMs&#30340;&#21518;&#21521;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#36830;&#25509;&#26469;&#36731;&#26494;&#23558;&#20854;&#38598;&#25104;&#21040;&#21518;&#21521;&#36807;&#31243;&#20013;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#23545;DNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;LA-DPMs&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose lookahead diffusion probabilistic models (LA-DPMs) to exploit the correlation in the outputs of the deep neural networks (DNNs) over subsequent timesteps in diffusion probabilistic models (DPMs) to refine the mean estimation of the conditional Gaussian distributions in the backward process. A typical DPM first obtains an estimate of the original data sample $\boldsymbol{x}$ by feeding the most recent state $\boldsymbol{z}_i$ and index $i$ into the DNN model and then computes the mean vector of the conditional Gaussian distribution for $\boldsymbol{z}_{i-1}$. We propose to calculate a more accurate estimate for $\boldsymbol{x}$ by performing extrapolation on the two estimates of $\boldsymbol{x}$ that are obtained by feeding $(\boldsymbol{z}_{i+1},i+1)$ and $(\boldsymbol{z}_{i},i)$ into the DNN model. The extrapolation can be easily integrated into the backward process of existing DPMs by introducing an additional connection over two consecutive timesteps, and fine-tuning is n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#30340;&#19981;&#21516;&#25216;&#26415;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.11292</link><description>&lt;p&gt;
&#20851;&#20110;&#24212;&#29992;&#35780;&#35770;&#20013;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On the Identification of the Energy related Issues from the App Reviews. (arXiv:2304.11292v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#30340;&#19981;&#21516;&#25216;&#26415;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#31243;&#24207;&#30340;&#33021;&#28304;&#25928;&#29575;&#38382;&#39064;&#21487;&#33021;&#20250;&#23545;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#36896;&#25104;&#37325;&#22823;&#38382;&#39064;&#65292;&#24182;&#22312;&#24212;&#29992;&#21830;&#24215;&#24191;&#27867;&#35752;&#35770;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30740;&#31350;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#20197;&#30830;&#23450;&#33021;&#28304;&#30456;&#20851;&#29992;&#25143;&#21453;&#39304;&#30340;&#20027;&#35201;&#21407;&#22240;&#25110;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#26377;&#25928;&#22320;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#20197;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#21644;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19982;&#30456;&#20851;&#29305;&#24449;&#32452;&#21512;&#21644;&#30456;&#23545;&#36739;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#24635;&#20849;&#27604;&#36739;&#20102;60&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20845;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#19977;&#31181;&#21333;&#35789;&#23884;&#20837;&#27169;&#22411;&#26500;&#24314;&#30340;30&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#35813;&#24037;&#20855;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36941;&#21382;&#36825;&#20010;&#22823;&#35268;&#27169;&#30340;&#32467;&#26524;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The energy inefficiency of the apps can be a major issue for the app users which is discussed on App Stores extensively. Previous research has shown the importance of investigating the energy related app reviews to identify the major causes or categories of energy related user feedback. However, there is no study that efficiently extracts the energy related app reviews automatically. In this paper, we empirically study different techniques for automatic extraction of the energy related user feedback. We compare the accuracy, F1-score and run time of numerous machine-learning models with relevant feature combinations and relatively modern Neural Network-based models. In total, 60 machine learning models are compared to 30 models that we build using six neural network architectures and three word embedding models. We develop a visualization tool for this study through which a developer can traverse through this large-scale result set. The results show that neural networks outperform the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#21270;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27700;&#21360;&#12289;&#25351;&#32441;&#12289;&#27169;&#22411;&#35775;&#38382;&#21644;&#25915;&#20987;&#30340;&#20445;&#25252;&#25216;&#26415;&#65292;&#24182;&#26500;&#24314;&#20102;&#32508;&#21512;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.11285</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36866;&#24403;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26426;&#21046;&#30340;&#36776;&#21035;&#65306;&#38024;&#23545;&#27700;&#21360;&#12289;&#25351;&#32441;&#12289;&#27169;&#22411;&#35775;&#38382;&#21644;&#25915;&#20987;&#30340;&#31995;&#32479;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Identifying Appropriate Intellectual Property Protection Mechanisms for Machine Learning Models: A Systematization of Watermarking, Fingerprinting, Model Access, and Attacks. (arXiv:2304.11285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#21270;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27700;&#21360;&#12289;&#25351;&#32441;&#12289;&#27169;&#22411;&#35775;&#38382;&#21644;&#25915;&#20987;&#30340;&#20445;&#25252;&#25216;&#26415;&#65292;&#24182;&#26500;&#24314;&#20102;&#32508;&#21512;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#21830;&#19994;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#21450;&#65307;&#21516;&#26102;&#65292;ML&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#32780;&#19988;&#35757;&#32451;&#25104;&#26412;&#36234;&#26469;&#36234;&#39640;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#65288;IPP&#65289;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;&#39046;&#22495;&#21487;&#20197;&#24314;&#31435;&#22312;&#23545;&#20854;IP&#36827;&#34892;&#20445;&#25252;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#25514;&#26045;&#26377;&#30528;&#28145;&#21051;&#29702;&#35299;&#30340;&#22522;&#30784;&#19978;&#19981;&#21516;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#38646;&#25955;&#12290;&#36825;&#20063;&#26159;&#30001;&#20110;&#32570;&#23569;&#32479;&#19968;&#30340;&#35270;&#35282;&#20197;&#21450;&#36825;&#20123;&#26041;&#38754;&#30340;&#20849;&#21516;&#20998;&#31867;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#25105;&#20204;&#22312;ML&#20013;&#20851;&#20110;IPP&#30340;&#21457;&#29616;&#65292;&#21516;&#26102;&#19987;&#27880;&#20110;&#32534;&#20889;&#26102;&#24050;&#30830;&#35748;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#25514;&#26045;&#12290;&#25105;&#20204;&#20026;ML&#20013;&#30340;IP&#24314;&#31435;&#20102;&#32508;&#21512;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#23558;&#25915;&#20987;&#21644;&#38450;&#24481;&#25514;&#26045;&#20998;&#31867;&#21040;&#32479;&#19968;&#21644;&#25972;&#21512;&#30340;&#20998;&#31867;&#27861;&#20013;&#65292;&#20174;&#32780;&#25645;&#36215;&#20102;&#26469;&#33258;ML&#21644;&#23433;&#20840;&#31038;&#21306;&#30340;&#30740;&#31350;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
The commercial use of Machine Learning (ML) is spreading; at the same time, ML models are becoming more complex and more expensive to train, which makes Intellectual Property Protection (IPP) of trained models a pressing issue. Unlike other domains that can build on a solid understanding of the threats, attacks and defenses available to protect their IP, the ML-related research in this regard is still very fragmented. This is also due to a missing unified view as well as a common taxonomy of these aspects.  In this paper, we systematize our findings on IPP in ML, while focusing on threats and attacks identified and defenses proposed at the time of writing. We develop a comprehensive threat model for IP in ML, categorizing attacks and defenses within a unified and consolidated taxonomy, thus bridging research from both the ML and security communities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11277</link><description>&lt;p&gt;
PyTorch FSDP&#65306;&#20840;&#38754;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#35268;&#27169;&#21270;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. (arXiv:2304.11277v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#24320;&#21457;&#21644;&#25506;&#32034;&#22823;&#22411;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#20173;&#21463;&#38480;&#20110;&#23569;&#25968;&#39640;&#32423;&#29992;&#25143;&#21644;&#34892;&#19994;&#39046;&#34966;&#65292;&#23548;&#33268;&#25216;&#26415;&#19978;&#30340;&#38544;&#21547;&#22721;&#22418;&#38459;&#30861;&#24191;&#27867;&#31038;&#21306;&#35775;&#38382;&#21644;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyTorch Fully Sharded Data Parallel&#65288;FSDP&#65289;&#20316;&#20026;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#30340;&#20135;&#19994;&#32423;&#35299;&#20915;&#26041;&#26696;&#12290;FSDP&#24050;&#19982;&#20960;&#20010;&#20851;&#38190;PyTorch&#26680;&#24515;&#32452;&#20214;&#65288;&#21253;&#25324;&#24352;&#37327;&#23454;&#29616;&#12289;&#20998;&#21457;&#22120;&#31995;&#32479;&#21644;CUDA&#20869;&#23384;&#32531;&#23384;&#20998;&#37197;&#22120;&#65289;&#23494;&#20999;&#21327;&#20316;&#65292;&#20197;&#25552;&#20379;&#38750;&#20405;&#20837;&#24335;&#29992;&#25143;&#20307;&#39564;&#21644;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;FSDP&#26412;&#22320;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#21644;&#35774;&#32622;&#65292;&#20248;&#21270;&#20102;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31995;&#21015;&#36890;&#36807;GPU-aware&#20248;&#21270;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#37197;&#22791;GPU&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#26497;&#24555;&#30340;&#25512;&#35770;&#24310;&#36831;&#65292;&#25193;&#22823;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#33539;&#22260;&#24182;&#25913;&#21892;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.11267</link><description>&lt;p&gt;
&#36895;&#24230;&#21363;&#19968;&#20999;&#65306;&#36890;&#36807;GPU-aware&#20248;&#21270;&#22312;&#35774;&#22791;&#19978;&#21152;&#36895;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31995;&#21015;&#36890;&#36807;GPU-aware&#20248;&#21270;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#37197;&#22791;GPU&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#26497;&#24555;&#30340;&#25512;&#35770;&#24310;&#36831;&#65292;&#25193;&#22823;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#33539;&#22260;&#24182;&#25913;&#21892;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#24212;&#29992;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#21644;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#27604;&#22914;&#38477;&#20302;&#26381;&#21153;&#22120;&#25104;&#26412;&#12289;&#31163;&#32447;&#21151;&#33021;&#21644;&#25913;&#21892;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#35774;&#22791;&#19978;&#20849;&#21516;&#30340;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#36229;&#36807;10&#20159;&#30340;&#21442;&#25968;&#65292;&#30001;&#20110;&#35774;&#22791;&#30340;&#21463;&#38480;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20248;&#21270;&#65292;&#20197;&#22312;&#37197;&#22791;GPU&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;&#36804;&#20170;&#20026;&#27490;&#26368;&#24555;&#30340;&#25512;&#35770;&#24310;&#36831;&#65288;&#23545;&#20110;&#19968;&#20010;512x512&#30340;&#22270;&#20687;&#65292;&#22312;&#19977;&#26143;S23 Ultra&#19978;&#30340;"&#31283;&#23450;&#25193;&#25955;1.4"&#19979;&#65292;&#36827;&#34892;20&#27425;&#36845;&#20195;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;int8&#37327;&#21270;&#65292;&#25512;&#35770;&#30340;&#24310;&#36831;&#23567;&#20110;12&#31186;&#65289;&#12290;&#36825;&#20123;&#20248;&#21270;&#25193;&#22823;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#33539;&#22260;&#24182;&#25913;&#21892;&#20102;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#25972;&#20307;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;InceptionTime&#21644;ROCKET&#26041;&#27861;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#20197;&#30417;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#26041;&#27861;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26174;&#31034;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11265</link><description>&lt;p&gt;
&#25163;&#33109;&#21160;&#20316;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Classification for Detecting Parkinson's Disease from Wrist Motions. (arXiv:2304.11265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11265
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;InceptionTime&#21644;ROCKET&#26041;&#27861;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#20197;&#30417;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#26041;&#27861;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26174;&#31034;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19968;&#31181;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20855;&#26377;&#39057;&#32321;&#21464;&#21270;&#30340;&#36816;&#21160;&#30151;&#29366;&#65292;&#25345;&#32493;&#30340;&#30151;&#29366;&#30417;&#27979;&#21487;&#20197;&#23454;&#29616;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#27835;&#30103;&#12290;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#36827;&#34892;PD&#30151;&#29366;&#30417;&#27979;&#26102;&#24615;&#33021;&#26377;&#38480;&#65292;&#22240;&#20026;PD&#36816;&#21160;&#27169;&#24335;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#20294;&#25968;&#25454;&#38598;&#24456;&#23567;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;InceptionTime&#21644;RandOm&#21367;&#31215;&#26680;&#21464;&#25442;&#65288;ROCKET&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;TSC&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;&#20110;PD&#30151;&#29366;&#30417;&#27979;&#38750;&#24120;&#26377;&#21069;&#26223;&#65306;InceptionTime&#30340;&#39640;&#23398;&#20064;&#33021;&#21147;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#36816;&#21160;&#27169;&#24335;&#65292;&#32780;ROCKET&#36866;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25628;&#32034;&#25214;&#21040;&#20102;&#26368;&#39640;&#24471;&#20998;&#30340;InceptionTime&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#19982;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;ROCKET&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#29992;&#20110;PD&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;TSC&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a neurodegenerative disease with frequently changing motor symptoms where continuous symptom monitoring enables more targeted treatment. Classical time series classification (TSC) and deep learning techniques have limited performance for PD symptom monitoring using wearable accelerometer data because PD movement patterns are complex, but datasets are small. We investigate InceptionTime and RandOm Convolutional KErnel Transform (ROCKET) because they are state-of-the-art for TSC and promising for PD symptom monitoring: InceptionTime's high learning capacity is suited to modeling complex movement patterns while ROCKET is suited to small datasets. We used a random search to find the highest-scoring InceptionTime architecture and compared it to ROCKET with a ridge classifier and a multi-layer perceptron (MLP) on wrist motions of PD patients. We find that all approaches are suitable for estimating tremor severity and bradykinesia presence but struggle with detecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11263</link><description>&lt;p&gt;
&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#19979;&#20302;&#26679;&#26412;&#31283;&#20581;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#26356;&#22909;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#32463;&#21462;&#24471;&#20102;&#38024;&#23545;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#40065;&#26834;&#24615;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24494;&#35843;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#24403;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#39640;&#26102;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#26368;&#20808;&#36827;&#30340;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#21363;&#20351;&#22312;&#23436;&#25972;&#26679;&#26412;&#19979;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#28608;&#21169;&#31038;&#21306;&#20851;&#27880;&#36825;&#20010;&#23454;&#38469;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24605;&#24819;&#26469;&#25913;&#36827;&#36125;&#21494;&#26031;&#35745;&#31639;&#30340;&#28508;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#20960;&#20010;&#20855;&#20307;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.11251</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#35745;&#31639;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and the Future of Bayesian Computation. (arXiv:2304.11251v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24605;&#24819;&#26469;&#25913;&#36827;&#36125;&#21494;&#26031;&#35745;&#31639;&#30340;&#28508;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#20960;&#20010;&#20855;&#20307;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#26159;&#30740;&#31350;&#22797;&#26434;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20801;&#35768;&#20998;&#26512;&#20154;&#21592;&#32534;&#30721;&#20016;&#23500;&#30340;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#24182;&#21033;&#29992;&#20808;&#39564;&#20449;&#24687;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#36890;&#36807;&#21518;&#39564;&#20998;&#24067;&#20419;&#36827;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#23436;&#25972;&#34920;&#24449;&#12290;&#23454;&#29992;&#30340;&#21518;&#39564;&#35745;&#31639;&#36890;&#24120;&#36890;&#36807;MCMC&#36827;&#34892;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#35266;&#27979;&#20540;&#30340;&#39640;&#32500;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#21487;&#33021;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24605;&#24819;&#26469;&#25913;&#36827;&#21518;&#39564;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#30340;&#26410;&#26469;&#26041;&#21521;&#22312;&#27491;&#24577;&#27969;&#12289;&#36125;&#21494;&#26031;&#26680;&#24515;&#38598;&#12289;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#21464;&#20998;&#25512;&#26029;&#30340;vignettes&#20013;&#24471;&#21040;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian models are a powerful tool for studying complex data, allowing the analyst to encode rich hierarchical dependencies and leverage prior information. Most importantly, they facilitate a complete characterization of uncertainty through the posterior distribution. Practical posterior computation is commonly performed via MCMC, which can be computationally infeasible for high dimensional models with many observations. In this article we discuss the potential to improve posterior computation using ideas from machine learning. Concrete future directions are explored in vignettes on normalizing flows, Bayesian coresets, distributed Bayesian inference, and variational inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;&#8212;&#8212;eWaSR&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#26816;&#27979;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#36816;&#34892;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#20063;&#20248;&#20110;&#20854;&#20182;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;&#23601;&#32490;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.11249</link><description>&lt;p&gt;
eWaSR&#8212;&#8212;&#19968;&#31181;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
eWaSR -- an embedded-compute-ready maritime obstacle detection network. (arXiv:2304.11249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;&#8212;&#8212;eWaSR&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#26816;&#27979;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#36816;&#34892;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#20063;&#20248;&#20110;&#20854;&#20182;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;&#23601;&#32490;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#23545;&#20110;&#33258;&#20027;&#27700;&#38754;&#33337;&#33334;&#65288;ASV&#65289;&#30340;&#23433;&#20840;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#24050;&#32463;&#22823;&#22823;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#38459;&#27490;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;WaSR&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#22312;&#20998;&#26512;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#26367;&#25442;&#26368;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;&#38454;&#27573;&#24182;&#25552;&#20986;&#20854;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#21464;&#20307;eWaSR&#12290;&#29305;&#21035;&#22320;&#65292;&#26032;&#35774;&#35745;&#36981;&#24490;&#20102;&#22522;&#20110;transformer&#30340;&#36731;&#37327;&#32423;&#32593;&#32476;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;eWaSR&#22312;&#20165;&#26377;0.52&#65285;&#30340;F1&#24471;&#20998;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;WaSR&#30456;&#24403;&#30340;&#26816;&#27979;&#32467;&#26524;&#65292;&#24182;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;&#23601;&#32490;&#26550;&#26500;&#39640;&#36798;9.74&#65285;&#12290;&#22312;&#26631;&#20934;GPU&#19978;&#65292;eWaSR&#30340;&#36816;&#34892;&#36895;&#24230;&#27604;&#21407;&#22987;WaSR&#24555;10&#20493;&#65288;115 FPS vs 11 FPS&#65289;&#12290;&#22312;OAK-D&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23613;&#31649;WaSR&#30001;&#20110;&#20869;&#23384;&#38480;&#21046;&#32780;&#26080;&#27861;&#36816;&#34892;&#65292;&#20294;eWaSR&#20173;&#28982;&#33021;&#22815;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime obstacle detection is critical for safe navigation of autonomous surface vehicles (ASVs). While the accuracy of image-based detection methods has advanced substantially, their computational and memory requirements prohibit deployment on embedded devices. In this paper we analyze the currently best-performing maritime obstacle detection network WaSR. Based on the analysis we then propose replacements for the most computationally intensive stages and propose its embedded-compute-ready variant eWaSR. In particular, the new design follows the most recent advancements of transformer-based lightweight networks. eWaSR achieves comparable detection results to state-of-the-art WaSR with only 0.52% F1 score performance drop and outperforms other state-of-the-art embedded-ready architectures by over 9.74% in F1 score. On a standard GPU, eWaSR runs 10x faster than the original WaSR (115 FPS vs 11 FPS). Tests on a real embedded device OAK-D show that, while WaSR cannot run due to memory re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;21%&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11247</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22312;&#22797;&#26434;&#24418;&#29366;&#20013;&#27169;&#25311;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes. (arXiv:2304.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;21%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#27969;&#20307;&#30340;&#36895;&#24230;&#21644;&#21387;&#21147;&#20998;&#24067;&#65288;&#36890;&#36807;&#35299;&#20915;&#32435;&#32500;&#23572;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65289;&#26159;&#21270;&#23398;&#12289;&#33021;&#28304;&#12289;&#21046;&#33647;&#24037;&#19994;&#20197;&#21450;&#26426;&#26800;&#24037;&#31243;&#21644;&#31649;&#36947;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;&#65288;&#22914;OpenFOAM&#21644;Ansys&#65289;&#22312;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#27599;&#24403;&#20960;&#20309;&#21442;&#25968;&#25110;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#34987;&#25913;&#21464;&#12290;&#29289;&#29702;&#23398;&#20449;&#36182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#27969;&#20307;&#27969;&#21160;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36866;&#24212;&#20960;&#20309;&#24418;&#29366;&#21644;&#32593;&#26684;&#23450;&#20041;&#30340;&#21464;&#21270;&#65292;&#20801;&#35768;&#36328;&#19981;&#21516;&#24418;&#29366;&#36827;&#34892;&#27010;&#25324;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28151;&#21512;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#27169;&#25311;&#19977;&#32500; Y &#22411;&#28151;&#21512;&#22120;&#20013;&#30340;&#23618;&#27969;&#27969;&#20307;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#37327;&#23376;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982; PINN &#30340;&#28789;&#27963;&#24615;&#30456;&#32467;&#21512;&#65292;&#31934;&#24230;&#27604;&#26222;&#36890; PINN &#25552;&#39640;&#20102; 21&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the distribution of the velocities and pressures of a fluid (by solving the Navier-Stokes equations) is a principal task in the chemical, energy, and pharmaceutical industries, as well as in mechanical engineering and the design of pipeline systems. With existing solvers, such as OpenFOAM and Ansys, simulations of fluid dynamics in intricate geometries are computationally expensive and require re-simulation whenever the geometric parameters or the initial and boundary conditions are altered. Physics-informed neural networks (PINNs) are a promising tool for simulating fluid flows in complex geometries, as they can adapt to changes in the geometry and mesh definitions, allowing for generalization across different shapes. We present a hybrid quantum physics-informed neural network that simulates laminar fluid flows in 3D Y-shaped mixers. Our approach combines the expressive power of a quantum model with the flexibility of a PINN, resulting in a 21% higher accuracy compared to a pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoNeRF&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#20027;&#20307;&#20195;&#29702;&#25910;&#38598;&#35757;&#32451;NeRF&#25152;&#38656;&#25968;&#25454;&#65292;&#35757;&#32451;NeRF&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2304.11241</link><description>&lt;p&gt;
AutoNeRF: &#33258;&#20027;&#20195;&#29702;&#35757;&#32451;&#38544;&#24335;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AutoNeRF: Training Implicit Scene Representations with Autonomous Agents. (arXiv:2304.11241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoNeRF&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#20027;&#20307;&#20195;&#29702;&#25910;&#38598;&#35757;&#32451;NeRF&#25152;&#38656;&#25968;&#25454;&#65292;&#35757;&#32451;NeRF&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#34920;&#31034;&#65292;&#22914;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#26032;&#35270;&#35282;&#32508;&#21512;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#25910;&#38598;&#25968;&#25454;&#24182;&#36827;&#34892;&#32454;&#33268;&#30340;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoNeRF&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#20027;&#20307;&#20195;&#29702;&#25910;&#38598;&#35757;&#32451;NeRF&#25152;&#38656;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#26377;&#25928;&#22320;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#24182;&#20351;&#29992;&#32463;&#39564;&#33258;&#20027;&#22320;&#26500;&#24314;&#30456;&#24212;&#30340;&#38544;&#24335;&#22320;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#22522;&#20110;&#21069;&#27839;&#30340;&#25506;&#32034;&#21644;&#30001;&#32463;&#36807;&#35757;&#32451;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#21644;&#32463;&#20856;&#30340;&#20302;&#32423;&#36335;&#24452;&#36861;&#36394;&#22120;&#32452;&#25104;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65306;&#32463;&#20856;&#35270;&#35282;&#28210;&#26579;&#12289;&#22320;&#22270;&#37325;&#24314;&#12289;&#35268;&#21010;&#21644;&#23039;&#24577;&#24494;&#35843;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#20027;&#20195;&#29702;AutoNeRF&#21487;&#20197;&#25104;&#21151;&#22320;&#35757;&#32451;NeRF&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit representations such as Neural Radiance Fields (NeRF) have been shown to be very effective at novel view synthesis. However, these models typically require manual and careful human data collection for training. In this paper, we present AutoNeRF, a method to collect data required to train NeRFs using autonomous embodied agents. Our method allows an agent to explore an unseen environment efficiently and use the experience to build an implicit map representation autonomously. We compare the impact of different exploration strategies including handcrafted frontier-based exploration and modular approaches composed of trained high-level planners and classical low-level path followers. We train these models with different reward functions tailored to this problem and evaluate the quality of the learned representations on four different downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. Empirical results show that NeRFs can be trained 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#22810;&#31181;&#37319;&#38598;&#26465;&#20214;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20174;&#27424;&#37319;&#26679;&#24179;&#34892;MRI&#25968;&#25454;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#37325;&#24314;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;&#26435;&#37325;&#23545;CNN&#29305;&#24449;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#36827;&#34892;&#32553;&#25918;&#65292;&#20351;&#27169;&#22411;&#36866;&#24212;&#27599;&#20010;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2304.11238</link><description>&lt;p&gt;
&#36866;&#24212;&#22810;&#31181;&#37319;&#38598;&#26465;&#20214;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;Ada-MoDL
&lt;/p&gt;
&lt;p&gt;
Adapting model-based deep learning to multiple acquisition conditions: Ada-MoDL. (arXiv:2304.11238v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#22810;&#31181;&#37319;&#38598;&#26465;&#20214;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20174;&#27424;&#37319;&#26679;&#24179;&#34892;MRI&#25968;&#25454;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#37325;&#24314;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;&#26435;&#37325;&#23545;CNN&#29305;&#24449;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#36827;&#34892;&#32553;&#25918;&#65292;&#20351;&#27169;&#22411;&#36866;&#24212;&#27599;&#20010;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26412;&#25991;&#26088;&#22312;&#24341;&#20837;&#19968;&#31181;&#21333;&#19968;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#21487;&#20174;&#20351;&#29992;&#22810;&#31181;&#24207;&#21015;&#12289;&#37319;&#38598;&#35774;&#32622;&#21644;&#30913;&#22330;&#24378;&#24230;&#33719;&#21462;&#30340;&#27424;&#37319;&#26679;&#24179;&#34892;MRI&#25968;&#25454;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#37325;&#24314;&#12290;&#26041;&#27861;&#65306;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#23637;&#24320;&#26550;&#26500;&#65292;&#21487;&#20026;&#22810;&#31181;&#37319;&#38598;&#35774;&#32622;&#25552;&#20379;&#33391;&#22909;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;&#26435;&#37325;&#23545;CNN&#29305;&#24449;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#36827;&#34892;&#32553;&#25918;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#36866;&#24212;&#27599;&#20010;&#35774;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21521;&#37327;&#20174;&#22810;&#23618;&#24863;&#30693;&#26426;&#27169;&#22411;&#27966;&#29983;&#32553;&#25918;&#26435;&#37325;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#35813;&#26465;&#20214;&#21521;&#37327;&#20195;&#34920;&#29305;&#23450;&#30340;&#37319;&#38598;&#35774;&#32622;&#12290;&#21516;&#26102;&#20351;&#29992;&#26469;&#33258;&#22810;&#31181;&#37319;&#38598;&#35774;&#32622;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#23618;&#24863;&#30693;&#26426;&#21442;&#25968;&#21644;CNN&#26435;&#37325;&#30340;&#32852;&#21512;&#35757;&#32451;&#12290;&#20351;&#29992;&#19981;&#21516;&#37319;&#38598;&#35774;&#32622;&#37319;&#38598;&#30340;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#26465;&#20214;&#32593;&#32476;&#12290;&#32467;&#26524;&#65306;&#36866;&#24212;&#33021;&#21147;&#22909;&#30340;&#22810;&#27169;&#24577;&#40644;&#37329;&#26631;&#20934;&#35780;&#20272;&#26174;&#31034;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#26356;&#39640;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: The aim of this work is to introduce a single model-based deep network that can provide high-quality reconstructions from undersampled parallel MRI data acquired with multiple sequences, acquisition settings and field strengths.  Methods: A single unrolled architecture, which offers good reconstructions for multiple acquisition settings, is introduced. The proposed scheme adapts the model to each setting by scaling the CNN features and the regularization parameter with appropriate weights. The scaling weights and regularization parameter are derived using a multi-layer perceptron model from conditional vectors, which represents the specific acquisition setting. The perceptron parameters and the CNN weights are jointly trained using data from multiple acquisition settings, including differences in field strengths, acceleration, and contrasts. The conditional network is validated using datasets acquired with different acquisition settings.  Results: The comparison of the adaptiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;$L_0$&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;BinMask&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#30830;&#23450;&#24615;&#20108;&#36827;&#21046;&#25513;&#30721;&#20056;&#20197;&#26435;&#37325;&#24182;&#20351;&#29992;&#26631;&#35782;&#30452;&#36890;&#20272;&#35745;&#22120;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;Binmask&#19981;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#35843;&#25972;&#21363;&#21487;&#22312;&#29305;&#24449;&#36873;&#25321;&#12289;&#32593;&#32476;&#31232;&#30095;&#21270;&#21644;&#27169;&#22411;&#27491;&#21017;&#21270;&#31561;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;&#35299;&#32806;&#26435;&#37325;&#19982;&#25513;&#30721;&#20248;&#21270;&#26159;&#23454;&#29616;&#26377;&#25928;$L_0$&#27491;&#21017;&#21270;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.11237</link><description>&lt;p&gt;
&#37319;&#29992;BinMask&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;$L_0$&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Effective Neural Network $L_0$ Regularization With BinMask. (arXiv:2304.11237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;$L_0$&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;BinMask&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#30830;&#23450;&#24615;&#20108;&#36827;&#21046;&#25513;&#30721;&#20056;&#20197;&#26435;&#37325;&#24182;&#20351;&#29992;&#26631;&#35782;&#30452;&#36890;&#20272;&#35745;&#22120;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;Binmask&#19981;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#35843;&#25972;&#21363;&#21487;&#22312;&#29305;&#24449;&#36873;&#25321;&#12289;&#32593;&#32476;&#31232;&#30095;&#21270;&#21644;&#27169;&#22411;&#27491;&#21017;&#21270;&#31561;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;&#35299;&#32806;&#26435;&#37325;&#19982;&#25513;&#30721;&#20248;&#21270;&#26159;&#23454;&#29616;&#26377;&#25928;$L_0$&#27491;&#21017;&#21270;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$L_0$&#27491;&#21017;&#21270;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#38500;&#20102;&#20026;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#32780;&#27491;&#21017;&#21270;&#27169;&#22411;&#22806;&#65292;$L_0$&#27491;&#21017;&#21270;&#36824;&#36866;&#29992;&#20110;&#36873;&#25321;&#36755;&#20837;&#29305;&#24449;&#21644;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#12290;&#30456;&#20851;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#30456;&#24403;&#22797;&#26434;&#12290;&#26412;&#25991;&#26174;&#31034;&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#30830;&#23450;&#24615;&#20108;&#36827;&#21046;&#25513;&#30721;&#23558;&#26435;&#37325;&#20056;&#20197;&#24182;&#20351;&#29992;&#26631;&#35782;&#30452;&#36890;&#20272;&#35745;&#22120;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#30340;BinMask&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;$L_0$&#27491;&#21017;&#21270;&#22120;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;BinMask: &#29305;&#24449;&#36873;&#25321;&#65292;&#32593;&#32476;&#31232;&#30095;&#21270;&#21644;&#27169;&#22411;&#27491;&#21017;&#21270;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#30456;&#27604;&#20026;&#27599;&#20010;&#20219;&#21153;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;BinMask&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#33021;&#22815;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#32806;&#26435;&#37325;&#19982;&#25513;&#30721;&#20248;&#21270;&#65288;&#36825;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65289;&#26159;&#23454;&#29616;&#26377;&#25928;$L_0$&#27491;&#21017;&#21270;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
$L_0$ regularization of neural networks is a fundamental problem. In addition to regularizing models for better generalizability, $L_0$ regularization also applies to selecting input features and training sparse neural networks. There is a large body of research on related topics, some with quite complicated methods. In this paper, we show that a straightforward formulation, BinMask, which multiplies weights with deterministic binary masks and uses the identity straight-through estimator for backpropagation, is an effective $L_0$ regularizer. We evaluate BinMask on three tasks: feature selection, network sparsification, and model regularization. Despite its simplicity, BinMask achieves competitive performance on all the benchmarks without task-specific tuning compared to methods designed for each task. Our results suggest that decoupling weights from mask optimization, which has been widely adopted by previous work, is a key component for effective $L_0$ regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#28151;&#20957;&#22303;&#37197;&#27604;&#35774;&#35745;&#21644;&#24615;&#33021;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#28151;&#20957;&#22303;&#24314;&#31569;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#24378;&#24230;&#12289;&#23494;&#24230;&#21644;&#25104;&#26412;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.11226</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#20957;&#22303;&#27010;&#29575;&#36873;&#25321;&#21450;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic selection and design of concrete using machine learning. (arXiv:2304.11226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#28151;&#20957;&#22303;&#37197;&#27604;&#35774;&#35745;&#21644;&#24615;&#33021;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#28151;&#20957;&#22303;&#24314;&#31569;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#24378;&#24230;&#12289;&#23494;&#24230;&#21644;&#25104;&#26412;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#20957;&#22303;&#26448;&#26009;&#30340;&#22266;&#26377;&#24046;&#24322;&#21644;&#21487;&#33021;&#24615;&#32452;&#20998;&#30340;&#22810;&#26679;&#24615;&#19979;&#65292;&#24320;&#21457;&#20855;&#26377;&#26356;&#20302;&#29615;&#22659;&#24433;&#21709;&#30340;&#20581;&#22766;&#28151;&#20957;&#22303;&#37197;&#26041;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21487;&#38752;&#30340;&#24615;&#33021;&#39044;&#27979;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#24615;&#33021;&#30340;&#28151;&#20957;&#22303;&#35268;&#33539;&#30340;&#21046;&#23450;&#65292;&#20943;&#23569;&#26448;&#26009;&#30340;&#28010;&#36153;&#24182;&#25552;&#39640;&#28151;&#20957;&#22303;&#24314;&#31569;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20013;&#38388;&#30446;&#26631;&#21464;&#37327;&#21450;&#20854;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#39044;&#27979;&#26368;&#32456;&#30446;&#26631;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#25351;&#23450;&#20855;&#26377;&#39640;&#25239;&#30899;&#21270;&#24615;&#30340;&#28151;&#20957;&#22303;&#37197;&#21512;&#27604;&#65292;&#20197;&#21450;&#20855;&#26377;&#20302;&#29615;&#22659;&#24433;&#21709;&#30340;&#28151;&#20957;&#22303;&#37197;&#21512;&#27604;&#12290;&#36825;&#20004;&#31181;&#28151;&#20957;&#22303;&#37197;&#27604;&#20063;&#28385;&#36275;&#24378;&#24230;&#12289;&#23494;&#24230;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#30446;&#26631;&#12290;&#25351;&#23450;&#30340;&#28151;&#20957;&#22303;&#37197;&#27604;&#24050;&#32463;&#24471;&#21040;&#23454;&#39564;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#26041;&#27861;&#20351;&#22122;&#22768;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21487;&#29992;&#20110;&#32467;&#26500;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of robust concrete mixes with a lower environmental impact is challenging due to natural variability in constituent materials and a multitude of possible combinations of mix proportions. Making reliable property predictions with machine learning can facilitate performance-based specification of concrete, reducing material inefficiencies and improving the sustainability of concrete construction. In this work, we develop a machine learning algorithm that can utilize intermediate target variables and their associated noise to predict the final target variable. We apply the methodology to specify a concrete mix that has high resistance to carbonation, and another concrete mix that has low environmental impact. Both mixes also fulfill targets on the strength, density, and cost. The specified mixes are experimentally validated against their predictions. Our generic methodology enables the exploitation of noise in machine learning, which has a broad range of applications in struct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DP-Adam&#65292;&#35299;&#20915;&#20102;&#22312;Adam&#20248;&#21270;&#22120;&#20013;&#20256;&#32479;DP&#30340;&#20351;&#29992;&#24341;&#20837;&#30340;&#20559;&#24046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11208</link><description>&lt;p&gt;
DP-Adam: &#25913;&#27491;Adam&#20108;&#38454;&#21160;&#37327;&#20272;&#35745;&#20013;&#30340;DP&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
DP-Adam: Correcting DP Bias in Adam's Second Moment Estimation. (arXiv:2304.11208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DP-Adam&#65292;&#35299;&#20915;&#20102;&#22312;Adam&#20248;&#21270;&#22120;&#20013;&#20256;&#32479;DP&#30340;&#20351;&#29992;&#24341;&#20837;&#30340;&#20559;&#24046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;Adam&#20248;&#21270;&#22120;&#20013;&#20256;&#32479;DP&#30340;&#20351;&#29992;&#24341;&#20837;&#20102;&#20559;&#24046;&#21040;&#20108;&#38454;&#21160;&#37327;&#20272;&#35745;&#20013;&#65292;&#22240;&#20026;&#26799;&#24230;&#35745;&#31639;&#20013;&#21152;&#20837;&#20102;&#29420;&#31435;&#22122;&#22768;&#12290;&#36825;&#20010;&#20559;&#24046;&#23548;&#33268;&#20102;&#23545;&#20110;&#20302;&#26041;&#24046;&#21442;&#25968;&#26356;&#26032;&#30340;&#19981;&#21516;&#32553;&#25918;&#65292;&#36825;&#19982;&#38750;&#31169;&#26377;Adam&#21644;Adam&#30340;&#31526;&#21495;&#19979;&#38477;&#35299;&#37322;&#30340;&#34892;&#20026;&#19981;&#19968;&#33268;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32416;&#27491;DP&#22122;&#22768;&#24341;&#20837;&#30340;&#20559;&#24046;&#26174;&#30528;&#25552;&#39640;&#20102;DP-Adam&#30340;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe that the traditional use of DP with the Adam optimizer introduces a bias in the second moment estimation, due to the addition of independent noise in the gradient computation. This bias leads to a different scaling for low variance parameter updates, that is inconsistent with the behavior of non-private Adam, and Adam's sign descent interpretation. Empirically, correcting the bias introduced by DP noise significantly improves the optimization performance of DP-Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#23450;&#21046;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21517;&#20026;&#24555;&#36895;GraspNeXt&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24555;&#36895;GraspNeXt&#22312;&#31934;&#24230;&#21644;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.11196</link><description>&lt;p&gt;
&#24555;&#36895;GraspNeXt&#65306;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#23398;&#20064;&#30340;&#24555;&#36895;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge. (arXiv:2304.11196v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#23450;&#21046;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21517;&#20026;&#24555;&#36895;GraspNeXt&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24555;&#36895;GraspNeXt&#22312;&#31934;&#24230;&#21644;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#35270;&#35273;&#31995;&#32479;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39640;&#26550;&#26500;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#38469;&#21046;&#36896;&#21644;&#20179;&#24211;&#29615;&#22659;&#20013;&#36890;&#24120;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#33218;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#25928;&#26524;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#39640;&#25928;&#30340;&#12289;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#23450;&#21046;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#20110;&#22312;&#21046;&#36896;&#29615;&#22659;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#38750;&#24120;&#24517;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;GraspNeXt&#65292;&#19968;&#31181;&#38024;&#23545;&#23884;&#20837;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24555;&#36895;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#21644;&#19968;&#32452;&#29992;&#20110;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#24230;&#24179;&#34913;&#30340;&#26550;&#26500;&#32422;&#26463;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#24555;&#36895;GraspNeXt&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24555;&#36895;GraspNeXt&#22312;&#31934;&#24230;&#21644;&#36895;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning has shown considerable promise for improving the performance of deep learning-driven vision systems for the purpose of robotic grasping. However, high architectural and computational complexity can result in poor suitability for deployment on embedded devices that are typically leveraged in robotic arms for real-world manufacturing and warehouse environments. As such, the design of highly efficient multi-task deep neural network architectures tailored for computer vision tasks for robotic grasping on the edge is highly desired for widespread adoption in manufacturing environments. Motivated by this, we propose Fast GraspNeXt, a fast self-attention neural network architecture tailored for embedded multi-task learning in computer vision tasks for robotic grasping. To build Fast GraspNeXt, we leverage a generative network architecture search strategy with a set of architectural constraints customized to achieve a strong balance between multi-task learning performance a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#33258;&#21160;&#35782;&#21035;&#21160;&#24577;&#35268;&#24459;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36739;&#20026;&#20934;&#30830;&#22320;&#35782;&#21035;&#19977;&#32500;&#31995;&#32479;&#65292;&#20855;&#26377;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11182</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automatically identifying dynamical systems from data. (arXiv:2304.11182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#33258;&#21160;&#35782;&#21035;&#21160;&#24577;&#35268;&#24459;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36739;&#20026;&#20934;&#30830;&#22320;&#35782;&#21035;&#19977;&#32500;&#31995;&#32479;&#65292;&#20855;&#26377;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#25551;&#36848;&#31995;&#32479;&#21160;&#24577;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#26159;&#24403;&#20195;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#21160;&#24577;&#35268;&#24459;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#21435;&#22122;&#25216;&#26415;&#12289;&#31232;&#30095;&#22238;&#24402;&#21644;&#33258;&#21161;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#32452;&#65292;&#26102;&#24207;&#21576;&#25351;&#25968;&#22686;&#38271;&#21644;&#21508;&#31181;&#20449;&#22122;&#27604;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19968;&#33268;&#35782;&#21035;&#19977;&#32500;&#31995;&#32479;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#36866;&#24230;&#30340;&#21644;&#39640;&#20449;&#21495;&#36136;&#37327;&#30456;&#23545;&#20110;&#32972;&#26223;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#12290;&#36890;&#36807;&#20934;&#30830;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#24433;&#21709;&#21508;&#31181;&#39046;&#22495;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#23398;&#20197;&#21450;&#24037;&#31243;&#23398;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering nonlinear differential equations that describe system dynamics from empirical data is a fundamental challenge in contemporary science. Here, we propose a methodology to automatically identify dynamical laws by integrating denoising techniques, sparse regression, and bootstrap confidence intervals. We evaluate our method on well-known ordinary differential equations with an ensemble of random initial conditions, time series of increasing length, and varying signal-to-noise ratios. Our algorithm consistently identifies three-dimensional systems, given moderately-sized time series and high signal quality levels relative to background noise. By accurately identifying dynamical systems, our methodology has the potential to impact diverse fields, such as the physical and biological sciences, as well as engineering, where understanding complex systems is crucial.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#8221;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#29983;&#25104;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#20351;&#29992;&#30417;&#30563;&#35774;&#32622;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22810;&#23454;&#20363;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11173</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Adaptive Pseudo Labeling for Transductive Meta-Learning. (arXiv:2304.11173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#8221;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#29983;&#25104;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#20351;&#29992;&#30417;&#30563;&#35774;&#32622;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22810;&#23454;&#20363;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#25903;&#25345;&#38598;&#26469;&#36827;&#34892;&#36866;&#24212;&#24615;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26679;&#26412;&#20559;&#24046;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24863;&#30693;&#24615;&#23398;&#20064;&#35270;&#35282;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#8221;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21033;&#29992;&#26631;&#35760;&#30340;&#25903;&#25345;&#38598;&#29983;&#25104;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#30340;&#20266;&#26631;&#31614;&#65292;&#20511;&#27492;&#20351;&#24471;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#20197;&#37319;&#29992;&#30417;&#30563;&#35774;&#32622;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#12290;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#27604;&#24863;&#30693;&#24615;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#26356;&#22810;&#30340;&#31034;&#20363;&#65292;&#20174;&#32780;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#23558;&#20219;&#21153;&#33258;&#36866;&#24212;&#24212;&#29992;&#20110;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;5&#36335;1-shot few-shot&#20013;&#32988;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning performs adaptation through a limited amount of support set, which may cause a sample bias problem. To solve this problem, transductive meta-learning is getting more and more attention, going beyond the conventional inductive learning perspective. This paper proposes so-called task-adaptive pseudo labeling for transductive meta-learning. Specifically, pseudo labels for unlabeled query sets are generated from labeled support sets through label propagation. Pseudo labels enable to adopt the supervised setting as it is and also use the unlabeled query set in the adaptation process. As a result, the proposed method is able to deal with more examples in the adaptation process than inductive ones, which can result in better classification performance of the model. Note that the proposed method is the first approach of applying task adaptation to pseudo labeling. Experiments show that the proposed method outperforms the state-of-the-art (SOTA) technique in 5-way 1-shot few-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11116</link><description>&lt;p&gt;
Graph-ToolFormer: &#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#65292;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#23545;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#24403;&#21069;&#65292;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#25193;&#23637;&#20063;&#24050;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;LLMs&#30001;&#20110;&#22312;&#25191;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12289;&#31934;&#30830;&#30340;&#25968;&#23398;&#35745;&#31639;&#20197;&#21450;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#24369;&#28857;&#65292;&#22240;&#27492;&#21576;&#29616;&#20986;&#38750;&#24120;&#20005;&#37325;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#35843;&#26597;&#25506;&#32034;&#36171;&#20104;&#29616;&#26377;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#30340;&#21407;&#29702;&#12289;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#36825;&#23558;&#23545;LLMs&#21644;&#22270;&#24418;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#21463;&#26368;&#26032;&#30340;ChatGPT&#21644;Toolformer&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Graph-ToolFormer&#65288;&#38754;&#21521;&#22270;&#24418;&#25512;&#29702;&#30340;Toolformer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#33258;&#36523;&#65292;&#26088;&#22312;&#22521;&#20859;&#20182;&#20204;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#21033;&#29992;&#23610;&#24230;&#25935;&#24863;&#30340;Vapnik&#32500;&#24230;&#26469;&#23398;&#20064;$[0,1]$&#20540;&#20989;&#25968;&#31867;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#26399;&#26395;&#32477;&#23545;&#35823;&#24046;&#30340;&#19968;&#33324;&#19978;&#38480;&#12290;&#25991;&#20013;&#35777;&#26126;&#35813;&#19978;&#38480;&#19981;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25913;&#21892;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#26080;&#20559;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25552;&#39640;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.11059</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#23398;&#20064;&#12289;&#19968;&#33268;&#25910;&#25947;&#21644;&#23610;&#24230;&#25935;&#24863;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Prediction, Learning, Uniform Convergence, and Scale-sensitive Dimensions. (arXiv:2304.11059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#21033;&#29992;&#23610;&#24230;&#25935;&#24863;&#30340;Vapnik&#32500;&#24230;&#26469;&#23398;&#20064;$[0,1]$&#20540;&#20989;&#25968;&#31867;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#26399;&#26395;&#32477;&#23545;&#35823;&#24046;&#30340;&#19968;&#33324;&#19978;&#38480;&#12290;&#25991;&#20013;&#35777;&#26126;&#35813;&#19978;&#38480;&#19981;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25913;&#21892;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#26080;&#20559;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25552;&#39640;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#27979;&#27169;&#22411;&#30340;&#25512;&#24191;&#20013;&#23398;&#20064;$[0,1]$&#20540;&#20989;&#25968;&#31867;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#33324;&#24615;&#30340;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#21453;&#26144;&#20102;&#30001;Alon&#12289;Ben-David&#12289;Cesa-Bianchi&#21644;Haussler&#25552;&#20986;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;Vapnik&#32500;&#24230;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19979;&#38480;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#19978;&#38480;&#19981;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25913;&#21892;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#24212;&#29992;&#27492;&#32467;&#26524;&#21644;Haussler&#20197;&#21450;Benedek&#21644;Itai&#30340;&#25216;&#26415;&#65292;&#20197;&#21033;&#29992;&#36825;&#31181;&#23610;&#24230;&#25935;&#24863;&#30340;&#32500;&#24230;&#27010;&#24565;&#33719;&#24471;&#26032;&#30340;&#22635;&#20805;&#25968;&#19978;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;Kearns&#21644;Schapire&#30340;fat-shattering&#20989;&#25968;&#24471;&#21040;&#20102;&#26032;&#30340;&#22635;&#20805;&#25968;&#19978;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20004;&#31181;&#22635;&#20805;&#19978;&#38480;&#26469;&#33719;&#24471;&#23545;&#26080;&#20559;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25913;&#36827;&#19968;&#33324;&#24615;&#19978;&#38480;&#12290;&#23545;&#20110;&#27599;&#20010;$\epsilon &gt; 0$&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#30340;&#36275;&#22815;&#26465;&#20214;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new general-purpose algorithm for learning classes of $[0,1]$-valued functions in a generalization of the prediction model, and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi and Haussler. We give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general. We apply this result, together with techniques due to Haussler and to Benedek and Itai, to obtain new upper bounds on packing numbers in terms of this scale-sensitive notion of dimension. Using a different technique, we obtain new bounds on packing numbers in terms of Kearns and Schapire's fat-shattering function. We show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning. For each $\epsilon &gt; 0$, we establish weaker sufficient and stronger necessary conditions for a class of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11028</link><description>&lt;p&gt;
&#39044;&#27979;&#20013;&#30340;&#22806;&#22312;&#25968;&#25454;&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#32852;&#35780;&#20272;&#30340;FARM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exogenous Data in Forecasting: FARM -- An Approach for Relevance Evaluation. (arXiv:2304.11028v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22312;&#25968;&#25454;&#34987;&#35748;&#20026;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38024;&#23545;&#24688;&#24403;&#30340;&#36873;&#25321;&#65292;&#20840;&#38754;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#31532;&#19968;&#27493;&#65292;&#20174;&#22806;&#22312;&#25968;&#25454;&#19982;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#24320;&#22987;&#12290;&#21463;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#25351;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#65288;&#21069;&#21521;&#35282;&#30456;&#20851;&#24230;&#37327;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#30340;&#21069;&#21521;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#35282;&#24230;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21033;&#29992;&#21518;&#32493;&#25968;&#25454;&#28857;&#30340;&#21464;&#21270;&#27604;&#36739;&#26469;&#23545;&#40784;&#32463;&#36807;&#26102;&#38388;&#21464;&#24418;&#30340;&#24207;&#21015;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#26412;&#22320;&#21644;&#20840;&#23616;&#25351;&#26631;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#12290;&#36825;&#23548;&#33268;&#23558;&#37096;&#20998;&#12289;&#20013;&#38388;&#21305;&#37197;&#20063;&#35270;&#20026;&#22806;&#22312;&#25968;&#25454;&#24207;&#21015;&#37325;&#35201;&#25351;&#26631;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#39564;&#35777;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;FARM&#26041;&#27861;&#23545;&#21512;&#25104;&#20294;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20449;&#21495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#23637;&#31034;&#20102;FARM&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exogenous data is believed to play a key role for increasing forecasting accuracy. For an appropriate selection, a throughout relevance analysis is a fundamental first step, starting from the exogenous data similarity with the reference time series. Inspired by existing metrics for time series similarity, we introduce a new approach named FARM - Forward Angular Relevance Measure, able to effectively deal with real-time data streams. Our forward method relies on an angular feature that compares changes in subsequent data points to align time-warped series in an efficient way. The proposed algorithm combines local and global measures to provide a balanced relevance measure. This results in considering also partial, intermediate matches as relevant indicators for exogenous data series significance. As a first validation step, we present the application of our FARM approach to both synthetic but representative signals and real-world time series recordings. While demonstrating the improved 
&lt;/p&gt;</description></item><item><title>InfAdapter&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#39640;&#20934;&#30830;&#24615;&#12289;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#25928;&#30410;&#20043;&#38388;&#26435;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#24102;&#26377;&#36164;&#28304;&#20998;&#37197;&#30340; ML &#27169;&#22411;&#21464;&#20307;&#26469;&#28385;&#36275;&#24310;&#36831; SLO&#65292;&#24182;&#26368;&#22823;&#21270;&#30001;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#32452;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#38477;&#20302;&#20102; SLO &#36829;&#35268;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.10892</link><description>&lt;p&gt;
&#21327;&#35843;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#30340;&#39640;&#20934;&#30830;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20302;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems. (arXiv:2304.10892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10892
&lt;/p&gt;
&lt;p&gt;
InfAdapter&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#39640;&#20934;&#30830;&#24615;&#12289;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#25928;&#30410;&#20043;&#38388;&#26435;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#24102;&#26377;&#36164;&#28304;&#20998;&#37197;&#30340; ML &#27169;&#22411;&#21464;&#20307;&#26469;&#28385;&#36275;&#24310;&#36831; SLO&#65292;&#24182;&#26368;&#22823;&#21270;&#30001;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#32452;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#38477;&#20302;&#20102; SLO &#36829;&#35268;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#27491;&#22312;&#24613;&#21095;&#22686;&#21152;&#12290;ML&#25512;&#29702;&#26381;&#21153;&#19982;&#29992;&#25143;&#30452;&#25509;&#20132;&#20114;&#65292;&#38656;&#35201;&#24555;&#36895;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26381;&#21153;&#38754;&#20020;&#19981;&#26029;&#21464;&#21270;&#30340;&#35831;&#27714;&#24037;&#20316;&#36127;&#36733;&#65292;&#38656;&#35201;&#35843;&#25972;&#20854;&#35745;&#31639;&#36164;&#28304;&#12290;&#35745;&#31639;&#36164;&#28304;&#19981;&#21512;&#29702;&#20250;&#23548;&#33268;&#24310;&#36831;&#26381;&#21153;&#32423;&#21035;&#30446;&#26631; (SLOs) &#36829;&#35268;&#25110;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#32771;&#34385;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#21644;&#36164;&#28304;&#25104;&#26412;&#31561;&#26041;&#38754;&#30340;&#25152;&#26377;&#22240;&#32032;&#26469;&#36866;&#24212;&#21160;&#24577;&#24037;&#20316;&#36127;&#36733;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; InfAdapter&#65292;&#23427;&#20250;&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#24102;&#26377;&#36164;&#28304;&#20998;&#37197;&#30340; ML &#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#28385;&#36275;&#24310;&#36831; SLO&#65292;&#24182;&#26368;&#22823;&#21270;&#30001;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#32452;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#30456;&#36739;&#20110;&#27969;&#34892;&#30340;&#34892;&#19994;&#33258;&#21160;&#32553;&#25918;&#22120; (Kubernetes Vertical Pod Autoscaler)&#65292;InfAdapter &#20998;&#21035;&#38477;&#20302;&#20102; SLO &#36829;&#35268;&#21644;&#25104;&#26412;&#36798; 65% &#21644; 33%&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose InfAdapter, which proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.10558</link><description>&lt;p&gt;
&#20351;&#29992;Z3&#36827;&#34892;FNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24418;&#24335;&#21270;&#24314;&#27169;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Using Z3 for Formal Modeling and Verification of FNN Global Robustness. (arXiv:2304.10558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#39564;&#35777;FNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#25216;&#26415;&#37117;&#38598;&#20013;&#22312;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#23616;&#37096;&#25200;&#21160;&#37051;&#22495;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#19978;&#12290;&#20840;&#23616;&#40065;&#26834;&#24615;&#20998;&#26512;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;DeepGlobal&#26159;&#19968;&#31181;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#30830;&#23450;FNN&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#23545;&#25239;&#21361;&#38505;&#21306;&#22495;&#65288;ADR&#65289;&#65292;&#19981;&#38480;&#20110;&#27979;&#35797;&#38598;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepGlobal&#30340;&#23436;&#25972;&#35268;&#33539;&#21644;&#23454;&#29616;&#65292;&#21033;&#29992;SMT&#27714;&#35299;&#22120;Z3&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#39033;&#25913;&#36827;&#20197;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Feedforward Neural Networks (FNNs) have achieved remarkable success in various tasks, they are vulnerable to adversarial examples. Several techniques have been developed to verify the adversarial robustness of FNNs, but most of them focus on robustness verification against the local perturbation neighborhood of a single data point. There is still a large research gap in global robustness analysis. The global-robustness verifiable framework DeepGlobal has been proposed to identify \textit{all} possible Adversarial Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In this paper, we propose a complete specification and implementation of DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and propose several improvements to DeepGlobal for more efficient verification. To evaluate the effectiveness of our implementation and improvements, we conduct extensive experiments on a set of benchmark datasets. Visualization of our experiment results s
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#20139;&#20102;&#20316;&#32773;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#23545;&#36164;&#20135;&#31649;&#29702;&#20135;&#29983;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#29305;&#23450;&#22522;&#37329;&#26159;&#21542;&#30495;&#27491;&#24320;&#21457;&#20102;AI&#30340;&#31616;&#21333;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.10212</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#23545;&#36164;&#20135;&#31649;&#29702;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The impact of the AI revolution on asset management. (arXiv:2304.10212v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#20139;&#20102;&#20316;&#32773;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#23545;&#36164;&#20135;&#31649;&#29702;&#20135;&#29983;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#29305;&#23450;&#22522;&#37329;&#26159;&#21542;&#30495;&#27491;&#24320;&#21457;&#20102;AI&#30340;&#31616;&#21333;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#26426;&#22120;&#20855;&#22791;&#20102;&#26480;&#20986;&#30340;&#33021;&#21147;&#65306;&#23427;&#20204;&#21487;&#20197;&#38405;&#35835;&#21644;&#29702;&#35299;&#33258;&#30001;&#27969;&#21160;&#30340;&#25991;&#26412;&#65292;&#19982;&#20154;&#31867;&#36827;&#34892;&#25512;&#29702;&#21644;&#20132;&#28041;&#65292;&#32763;&#35793;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#65292;&#23398;&#20064;&#22914;&#20309;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#31561;&#31561;&#12290;&#22914;&#20170;&#65292;&#26426;&#22120;&#24050;&#32463;&#22312;&#30284;&#30151;&#26816;&#27979;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#33647;&#29289;&#35774;&#35745;&#12289;&#26680;&#32858;&#21464;&#21453;&#24212;&#22534;&#25511;&#21046;&#31561;&#26041;&#38754;&#23454;&#29616;&#20102;&#38761;&#21629;&#24615;&#31361;&#30772;&#12290;&#34429;&#28982;&#36825;&#20123;&#33021;&#21147;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#20294;&#23427;&#20204;&#22312;&#25345;&#32493;&#23436;&#21892;&#21644;&#24212;&#29992;&#20013;&#30340;&#25216;&#26415;&#24433;&#21709;&#20960;&#20046;&#23558;&#22312;&#20154;&#31867;&#27963;&#21160;&#30340;&#20960;&#20046;&#25152;&#26377;&#31038;&#20250;&#21644;&#32463;&#27982;&#39046;&#22495;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#36825;&#26159;&#25105;&#20204;&#20197;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#12290;&#26412;&#25991;&#23558;&#20998;&#20139;&#25105;&#30340;&#35266;&#28857;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24433;&#21709;&#36164;&#20135;&#31649;&#29702;&#65292;&#25105;&#23558;&#25552;&#20379;&#19968;&#20010;&#24605;&#32500;&#26694;&#26550;&#65292;&#20351;&#35835;&#32773;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#26631;&#20934;&#26469;&#35780;&#20272;&#19968;&#20010;&#29305;&#23450;&#22522;&#37329;&#26159;&#21542;&#30495;&#27491;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Recent progress in deep learning, a special form of machine learning, has led to remarkable capabilities machines can now be endowed with: they can read and understand free flowing text, reason and bargain with human counterparts, translate texts between languages, learn how to take decisions to maximize certain outcomes, etc. Today, machines have revolutionized the detection of cancer, the prediction of protein structures, the design of drugs, the control of nuclear fusion reactors etc. Although these capabilities are still in their infancy, it seems clear that their continued refinement and application will result in a technological impact on nearly all social and economic areas of human activity, the likes of which we have not seen before. In this article, I will share my view as to how AI will likely impact asset management in general and I will provide a mental framework that will equip readers with a simple criterion to assess whether and to what degree a given fund really exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702; (2M)&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#23398;&#20064;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10098</link><description>&lt;p&gt;
&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Two-Memory Reinforcement Learning. (arXiv:2304.10098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702; (2M)&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#23398;&#20064;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#22870;&#21169;&#20449;&#24687;&#20256;&#25773;&#21644;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#26356;&#26032;&#30340;&#36895;&#24230;&#36739;&#24930;&#65292;&#23427;&#20542;&#21521;&#20110;&#23398;&#20064;&#24471;&#27604;&#36739;&#24930;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38750;&#21442;&#25968;&#21270;&#30340;&#24773;&#33410;&#35760;&#24518;&#25552;&#20379;&#20102;&#30456;&#23545;&#36739;&#24555;&#30340;&#23398;&#20064;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#19981;&#38656;&#35201;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26368;&#22823;&#24773;&#33410;&#22238;&#25253;&#20316;&#20026;&#29366;&#24577;-&#21160;&#20316;&#20540;&#36827;&#34892;&#34892;&#21160;&#36873;&#25321;&#12290;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#37117;&#26377;&#21508;&#33258;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#31867;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22810;&#20010;&#35760;&#24518;&#31995;&#32479;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#20174;&#20013;&#33719;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;2M&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290; 2M &#20195;&#29702;&#21033;&#29992;&#24773;&#33410;&#35760;&#24518;&#37096;&#20998;&#30340;&#36895;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#37096;&#20998;&#30340;&#26368;&#20248;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#30456;&#20114;&#34917;&#20805;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>FastMRI&#21069;&#21015;&#33146;&#25512;&#20986;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21452;&#21442;&#25968;MRI&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;MRI&#22270;&#20687;&#37325;&#24314;&#21644;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;MRI&#22312;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#21644;&#35780;&#20272;&#19978;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09254</link><description>&lt;p&gt;
FastMRI&#21069;&#21015;&#33146;&#65306;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21452;&#21442;&#25968;MRI&#25968;&#25454;&#38598;&#65292;&#20197;&#25512;&#36827;&#21069;&#21015;&#33146;&#30284;&#25104;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
FastMRI Prostate: A Publicly Available, Biparametric MRI Dataset to Advance Machine Learning for Prostate Cancer Imaging. (arXiv:2304.09254v1 [physics.med-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09254
&lt;/p&gt;
&lt;p&gt;
FastMRI&#21069;&#21015;&#33146;&#25512;&#20986;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21452;&#21442;&#25968;MRI&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;MRI&#22270;&#20687;&#37325;&#24314;&#21644;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;MRI&#22312;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#21644;&#35780;&#20272;&#19978;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
fastMRI&#22823;&#33041;&#21644;&#33181;&#30422;&#25968;&#25454;&#38598;&#36890;&#36807;&#26032;&#39062;&#12289;&#20020;&#24202;&#30456;&#20851;&#30340;&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#24471;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30340;&#36895;&#24230;&#21644;&#22270;&#20687;&#36136;&#37327;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;fastMRI&#25968;&#25454;&#38598;&#30340;2023&#24180;4&#26376;&#25193;&#23637;&#65292;&#21253;&#25324;&#23545;&#20020;&#24202;&#20154;&#32676;&#36827;&#34892;&#30340;&#21452;&#21442;&#25968;&#21069;&#21015;&#33146;MRI&#25968;&#25454;&#33719;&#21462;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;T2&#21152;&#26435;&#21644;&#25193;&#25955;&#21152;&#26435;&#24207;&#21015;&#30340;&#21407;&#22987;k-&#31354;&#38388;&#21644;&#37325;&#24314;&#22270;&#20687;&#65292;&#20197;&#21450;&#25351;&#31034;&#21069;&#21015;&#33146;&#30284;&#23384;&#22312;&#21644;&#20998;&#32423;&#30340;&#20999;&#29255;&#32423;&#26631;&#31614;&#12290;&#19982;fastMRI&#19968;&#26679;&#65292;&#25552;&#39640;&#23545;&#21407;&#22987;&#21069;&#21015;&#33146;MRI&#25968;&#25454;&#30340;&#21487;&#35775;&#38382;&#24615;&#23558;&#36827;&#19968;&#27493;&#20419;&#36827;MR&#22270;&#20687;&#37325;&#24314;&#21644;&#35780;&#20272;&#30340;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#25913;&#36827;MRI&#22312;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#21644;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#30340;&#26356;&#22823;&#30446;&#26631;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://fastmri.med.nyu.edu &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fastMRI brain and knee dataset has enabled significant advances in exploring reconstruction methods for improving speed and image quality for Magnetic Resonance Imaging (MRI) via novel, clinically relevant reconstruction approaches. In this study, we describe the April 2023 expansion of the fastMRI dataset to include biparametric prostate MRI data acquired on a clinical population. The dataset consists of raw k-space and reconstructed images for T2-weighted and diffusion-weighted sequences along with slice-level labels that indicate the presence and grade of prostate cancer. As has been the case with fastMRI, increasing accessibility to raw prostate MRI data will further facilitate research in MR image reconstruction and evaluation with the larger goal of improving the utility of MRI for prostate cancer detection and evaluation. The dataset is available at https://fastmri.med.nyu.edu.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2304.05365</link><description>&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#21527;&#65311;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20010;&#24615;&#21270;&#27835;&#30103;&#24207;&#21015;&#20197;&#25903;&#25345;&#29992;&#25143;&#37319;&#21462;&#26356;&#20581;&#24247;&#30340;&#34892;&#20026;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;&#28041;&#21450;&#21040;&#22522;&#20110;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#20808;&#21069;&#30340;&#27963;&#21160;&#27700;&#24179;&#12289;&#20301;&#32622;&#31561;&#65289;&#22312;&#20309;&#26102;&#27835;&#30103;&#20197;&#21450;&#22914;&#20309;&#27835;&#30103;&#30340;&#20915;&#23450;&#12290;&#22312;&#32447;RL&#31639;&#27861;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#21382;&#21490;&#21453;&#39304;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20010;&#24615;&#21270;&#36825;&#20123;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35201;&#20915;&#23450;&#26159;&#21542;&#24212;&#22312;&#23454;&#38469;&#37096;&#32626;&#30340;&#8220;&#20248;&#21270;&#8221;&#24178;&#39044;&#20013;&#21253;&#21547;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25968;&#25454;&#35777;&#25454;&#65292;&#34920;&#26126;RL&#31639;&#27861;&#23454;&#38469;&#19978;&#27491;&#22312;&#23558;&#27835;&#30103;&#20010;&#24615;&#21270;&#36866;&#24212;&#20854;&#29992;&#25143;&#12290;&#30001;&#20110;RL&#31639;&#27861;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#23545;&#20854;&#22312;&#26576;&#20123;&#29366;&#24577;&#19979;&#30340;&#23398;&#20064;&#24182;&#20351;&#29992;&#27492;&#23398;&#20064;&#26469;&#25552;&#20379;&#29305;&#23450;&#27835;&#30103;&#30340;&#33021;&#21147;&#20135;&#29983;&#35823;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#23450;&#20041;&#30340;&#20010;&#24615;&#21270;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#37325;&#22797;&#37319;&#26679;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#22312;&#32447;RL&#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.05116</link><description>&lt;p&gt;
&#19981;&#21516;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#22312;&#22522;&#20110;&#22270;&#30340;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction. (arXiv:2304.05116v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05116
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#35843;&#24615;&#39640;&#65292;&#25104;&#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39640;&#24230;&#28789;&#27963;&#24615;&#20276;&#38543;&#30340;&#26159;&#35299;&#37322;&#24615;&#32570;&#22833;&#21644;&#21487;&#33021;&#36829;&#21453;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#20351;&#29992;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#26469;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#21487;&#20197;&#20316;&#20026;&#19982;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#37197;&#21512;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411; MTP-GO&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#36816;&#21160;&#27169;&#22411;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#33719;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#65292;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25968;&#20540;&#27714;&#35299;&#22120;&#21487;&#20197;&#23545;&#36816;&#21160;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given their adaptability and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex ones, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact o
&lt;/p&gt;</description></item><item><title>Wav2code&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;ASR&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;&#26080;&#22833;&#30495;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.04974</link><description>&lt;p&gt;
Wav2code&#65306;&#36890;&#36807;&#30721;&#26412;&#26597;&#25214;&#24674;&#22797;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;ASR
&lt;/p&gt;
&lt;p&gt;
Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR. (arXiv:2304.04974v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04974
&lt;/p&gt;
&lt;p&gt;
Wav2code&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;ASR&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;&#26080;&#22833;&#30495;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#22024;&#26434;&#29615;&#22659;&#19979;&#65292;&#20854;&#24615;&#33021;&#36890;&#24120;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#35821;&#38899;&#22686;&#24378;(SE)&#24341;&#20837;&#20316;&#20026;&#21069;&#31471;&#26469;&#25552;&#39640;&#35821;&#38899;&#36136;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#30001;&#20110;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;&#65292;&#21487;&#33021;&#23545;&#19979;&#28216;ASR&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#26368;&#26032;&#30340;&#24037;&#20316;&#23558;SE&#21644;&#24403;&#21069;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#32467;&#21512;&#36215;&#26469;&#26469;&#32531;&#35299;&#22833;&#30495;&#38382;&#39064;&#24182;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#26377;&#25928;&#24615;&#65292;&#20294;&#20256;&#32479;SE&#24341;&#36215;&#30340;&#35821;&#38899;&#22833;&#30495;&#20173;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wav2code&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;ASR&#30340;&#26080;&#22833;&#30495;&#24191;&#20041;SE&#12290;&#39318;&#20808;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20174;SSL&#27169;&#22411;&#33719;&#24471;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#36890;&#36807;&#26368;&#36817;&#37051;&#29305;&#24449;&#21305;&#37197;&#26597;&#25214;&#31163;&#25955;&#30721;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#24471;&#21040;&#30340;&#20195;&#30721;&#24207;&#21015;&#26469;&#37325;&#26500;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#20197;&#33719;&#24471;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65307;&#25509;&#30528;&#65292;&#35813;&#20195;&#30721;&#24207;&#21015;&#34987;&#29992;&#20110;&#26080;&#22833;&#30495;&#22320;&#22686;&#24378;&#24102;&#22122;&#35821;&#38899;&#20197;&#20415;&#20110;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) has gained a remarkable success thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be completely eliminated. In this paper, we propose a self-supervised framework named Wav2code to implement a generalized SE without distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the origin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31639;&#23376;&#21644;/&#25110;&#38543;&#26426;&#32422;&#26463;&#30340;&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#24403;FCVI&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#38750;&#20809;&#28369;&#30340;&#25110;&#38543;&#26426;&#30340;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#31639;&#23376;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04778</link><description>&lt;p&gt;
&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
First-order methods for Stochastic Variational Inequality problems with Function Constraints. (arXiv:2304.04778v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31639;&#23376;&#21644;/&#25110;&#38543;&#26426;&#32422;&#26463;&#30340;&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#24403;FCVI&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#38750;&#20809;&#28369;&#30340;&#25110;&#38543;&#26426;&#30340;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#31639;&#23376;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#20276;&#38543;&#30528;&#21487;&#33021;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#20989;&#25968;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#25237;&#24433;&#31639;&#23376;&#30340;&#35745;&#31639;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#21253;&#25324;&#20855;&#26377;&#38543;&#26426;&#31639;&#23376;&#21644;/&#25110;&#38543;&#26426;&#32422;&#26463;&#30340;&#20809;&#28369;&#25110;&#38750;&#20809;&#28369;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#19968;&#38454;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;{\texttt{OpConEx}}&#26041;&#27861;&#21450;&#20854;&#38543;&#26426;&#21464;&#20307;&#65292;&#23427;&#20204;&#37319;&#29992;&#31639;&#23376;&#21644;&#38480;&#21046;&#35780;&#20272;&#30340;&#22806;&#25512;&#26469;&#26356;&#26032;&#21464;&#37327;&#21644;Lagrangian&#20056;&#25968;&#12290;&#24403;FCVI&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#38750;&#20809;&#28369;&#30340;&#25110;&#38543;&#26426;&#30340;&#65288;&#21253;&#25324;&#20809;&#28369;&#25110;&#38750;&#20809;&#28369;&#30340;&#38543;&#26426;&#32422;&#26463;&#65289;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#31639;&#23376;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31616;&#21333;&#30340;&#21333;&#24490;&#29615;&#31243;&#24207;&#65292;&#19981;&#38656;&#35201;&#30693;&#36947;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#23601;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The monotone Variational Inequality (VI) is an important problem in machine learning. In numerous instances, the VI problems are accompanied by function constraints which can possibly be data-driven, making the projection operator challenging to compute. In this paper, we present novel first-order methods for function constrained VI (FCVI) problem under various settings, including smooth or nonsmooth problems with a stochastic operator and/or stochastic constraints. First, we introduce the~{\texttt{OpConEx}} method and its stochastic variants, which employ extrapolation of the operator and constraint evaluations to update the variables and the Lagrangian multipliers. These methods achieve optimal operator or sample complexities when the FCVI problem is either (i) deterministic nonsmooth, or (ii) stochastic, including smooth or nonsmooth stochastic constraints. Notably, our algorithms are simple single-loop procedures and do not require the knowledge of Lagrange multipliers to attain th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.03518</link><description>&lt;p&gt;
SSS&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#35770;&#25991;&#65306;&#20351;&#29992;&#25237;&#31080;&#32454;&#35843;&#21464;&#21387;&#22120;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#12290; (arXiv&#65306;2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023&#20219;&#21153;10&#20013;&#25552;&#20132;&#30340;&#20316;&#21697;-&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65288;EDOS&#65289;&#65292;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#23548;&#33268;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#38754;&#20020;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#12290;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#21464;&#24471;&#27604;&#20197;&#24448;&#26356;&#21152;&#37325;&#35201;&#65292;&#20197;&#20351;&#31038;&#20132;&#23186;&#20307;&#23545;&#22899;&#24615;&#26356;&#21152;&#23433;&#20840;&#21644;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23454;&#39564;&#21644;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#38598;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#21333;&#20010;&#22522;&#32447;&#27169;&#22411;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;A&#20013;&#23454;&#29616;&#20102;&#23439;F1&#20998;&#25968;0.8392&#65292;&#22312;&#20219;&#21153;B&#20013;&#20026;0.6092&#65292;&#22312;&#20219;&#21153;C&#20013;&#20026;0.4319&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
&lt;/p&gt;</description></item><item><title>TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.01951</link><description>&lt;p&gt;
TransPimLib&#65306;&#29992;&#20110;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01951
&lt;/p&gt;
&lt;p&gt;
TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#65288;PIM&#65289;&#25215;&#35834;&#20943;&#36731;&#29616;&#20195;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30495;&#23454;PIM&#31995;&#32479;&#26377;&#19968;&#20010;&#20869;&#22312;&#30340;&#21155;&#21183;&#65292;&#21363;&#23427;&#20204;&#30340;&#30828;&#20214;&#27604;&#20256;&#32479;&#30340;&#22788;&#29702;&#22120;&#65288;CPU&#12289;GPU&#65289;&#26356;&#21152;&#21463;&#38480;&#65292;&#22240;&#20026;&#22312;&#20869;&#23384;&#38468;&#36817;&#25110;&#20869;&#37096;&#26500;&#24314;&#22788;&#29702;&#20803;&#20214;&#30340;&#38590;&#24230;&#21644;&#25104;&#26412;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#36890;&#29992;&#30340;PIM&#26550;&#26500;&#25903;&#25345;&#30456;&#24403;&#26377;&#38480;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#36229;&#36234;&#20989;&#25968;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#30340;&#25805;&#20316;&#65288;&#20363;&#22914;&#24179;&#26041;&#26681;&#65289;&#12290;&#36825;&#20123;&#25805;&#20316;&#23545;&#20110;&#19968;&#20123;&#29616;&#20195;&#24037;&#20316;&#36127;&#36733;&#23588;&#20854;&#37325;&#35201;&#65292;&#20363;&#22914;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#36890;&#29992;&#30340;PIM&#31995;&#32479;&#20013;&#25552;&#20379;&#23545;&#36229;&#36234;&#65288;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#65289;&#20989;&#25968;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TransPimLib&#65292;&#36825;&#26159;&#19968;&#20010;&#24211;&#65292;&#25552;&#20379;&#22522;&#20110;CORDIC&#21644;LUT&#30340;&#19977;&#35282;&#20989;&#25968;&#12289;&#21452;&#26354;&#20989;&#25968;&#12289;&#25351;&#25968;&#12289;&#23545;&#25968;&#12289;&#24179;&#26041;&#26681;&#31561;&#38590;&#20197;&#35745;&#31639;&#30340;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28909;&#25104;&#20687;&#30340;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#36731;&#37327;&#32423;&#21644;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#19981;&#21463;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01243</link><description>&lt;p&gt;
CoReFusion: &#22522;&#20110;&#23545;&#27604;&#27491;&#21017;&#21270;&#34701;&#21512;&#23454;&#29616;&#28909;&#32418;&#22806;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution. (arXiv:2304.01243v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28909;&#25104;&#20687;&#30340;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#36731;&#37327;&#32423;&#21644;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#19981;&#21463;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#25104;&#20687;&#30001;&#20110;&#22312;&#20302;&#20809;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#32780;&#27604;&#19968;&#33324;&#21487;&#35265;&#20809;&#25104;&#20687;&#20855;&#26377;&#26356;&#22810;&#20248;&#21183;&#12290; &#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20302;&#25104;&#26412;&#12289;&#20302;&#20998;&#36776;&#29575;&#30340;&#28909;&#25104;&#20687;&#20256;&#24863;&#22120;&#27979;&#37327;&#26469;&#22797;&#21046;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#28909;&#25104;&#20687;&#22270;&#29255;&#65292;&#32780;&#19988;&#22312;&#20851;&#38190;&#22320;&#21306;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#21487;&#35265;&#20809;&#20877;&#29983;&#33021;&#21147;&#30340;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#20687;&#20809;&#35889;&#33539;&#22260;&#19981;&#19968;&#33268;&#65292;&#20351;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#36827;&#34892;&#28909;&#25104;&#20687;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#24456;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28909;&#25104;&#20687;&#30340;&#24341;&#23548;&#36229;&#20998;&#36776;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20307;&#31995;&#32467;&#26500;&#22312;&#20002;&#22833;&#19968;&#20010;&#27169;&#24577;&#65288;&#39640;&#20998;&#36776;&#29575;RGB&#22270;&#20687;&#25110;&#26356;&#20302;&#20998;&#36776;&#29575;&#30340;&#28909;&#25104;&#20687;&#65289;&#26102;&#20173;&#20855;&#22791;&#35745;&#31639;&#25928;&#29575;&#21644;&#36731;&#37327;&#32423;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#23458;&#35266;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thermal imaging has numerous advantages over regular visible-range imaging since it performs well in low-light circumstances. Super-Resolution approaches can broaden their usefulness by replicating accurate high-resolution thermal pictures using measurements from low-cost, low-resolution thermal sensors. Because of the spectral range mismatch between the images, Guided Super-Resolution of thermal images utilizing visible range images is difficult. However, In case of failure to capture Visible Range Images can prevent the operations of applications in critical areas. We present a novel data fusion framework and regularization technique for Guided Super Resolution of Thermal images. The proposed architecture is computationally in-expensive and lightweight with the ability to maintain performance despite missing one of the modalities, i.e., high-resolution RGB image or the lower-resolution thermal image, and is designed to be robust in the presence of missing data. The proposed method pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16317</link><description>&lt;p&gt;
PCA-Net&#65306;&#25805;&#20316;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#19978;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;PCA-Net&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#23427;&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#36924;&#36817;&#28508;&#22312;&#30340;&#31639;&#23376;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25913;&#36827;&#24182;&#26174;&#30528;&#25193;&#23637;&#20102;&#27492;&#26041;&#21521;&#30340;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312;&#23450;&#24615;&#30028;&#38480;&#26041;&#38754;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#26032;&#39062;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#22312;&#23545;&#28508;&#22312;&#31639;&#23376;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#26368;&#23567;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#12290;&#22312;&#23450;&#37327;&#38480;&#21046;&#26041;&#38754;&#65292;&#26412;&#25991;&#35782;&#21035;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#20004;&#20010;&#28508;&#22312;&#38556;&#30861;&#65292;&#36890;&#36807;&#23548;&#20986;&#19979;&#30028;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#65292;&#31532;&#19968;&#20010;&#38556;&#30861;&#19982;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#26377;&#20851;&#65292;&#30001;PCA&#29305;&#24449;&#20540;&#30340;&#32531;&#24930;&#34928;&#20943;&#26469;&#34913;&#37327;&#65307;&#21478;&#19968;&#20010;&#38556;&#30861;&#28041;&#21450;&#26080;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28784;&#31665; MDP &#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;&#21306;&#38388; MDP &#20316;&#20026;&#20869;&#37096;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#32467;&#21512;&#19979;&#32622;&#20449;&#21306;&#38388;&#25506;&#32034;&#21644;&#34892;&#21160;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#26377;&#38480;&#37319;&#26679;&#19979;&#30340;&#38382;&#39064;&#65292;&#29992;&#20110;&#21512;&#25104;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.12718</link><description>&lt;p&gt;
&#26377;&#38480;&#37319;&#26679;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#31574;&#30053;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Strategy Synthesis in Markov Decision Processes Under Limited Sampling Access. (arXiv:2303.12718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28784;&#31665; MDP &#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;&#21306;&#38388; MDP &#20316;&#20026;&#20869;&#37096;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#32467;&#21512;&#19979;&#32622;&#20449;&#21306;&#38388;&#25506;&#32034;&#21644;&#34892;&#21160;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#26377;&#38480;&#37319;&#26679;&#19979;&#30340;&#38382;&#39064;&#65292;&#29992;&#20110;&#21512;&#25104;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25511;&#21046;&#29702;&#35770;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#24418;&#24335;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#20219;&#21153;&#26159;&#20026;&#22312;&#37096;&#20998;&#26410;&#30693;&#29615;&#22659;&#19979;&#25805;&#20316;&#30340;&#20195;&#29702;&#21512;&#25104;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#31574;&#30053;&#12290;&#22312;&#28784;&#31665;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (MDPs) &#27169;&#22411;&#20013;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#24433;&#21709;&#20197;&#21518;&#30340;&#29366;&#24577;&#32780;&#19981;&#26159;&#28041;&#21450;&#21040;&#30340;&#27010;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#28784;&#31665; MDP &#21512;&#25104;&#31574;&#30053;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#37096;&#27169;&#22411;&#20026;&#21306;&#38388; MDP &#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#12290;&#20026;&#20102;&#24212;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#38480;&#37319;&#26679;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20004;&#20010;&#26032;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;&#31639;&#27861;&#20013;&#65292;&#19987;&#27880;&#20110;&#24555;&#36895;&#25104;&#21151;&#30340;&#23398;&#20064;&#32780;&#19981;&#26159;&#38543;&#26426;&#20445;&#35777;&#21644;&#26368;&#20248;&#24615;&#65306;&#19979;&#32622;&#20449;&#21306;&#38388;&#25506;&#32034;&#21152;&#24378;&#24050;&#32463;&#23398;&#20064;&#30340;&#21487;&#34892;&#31574;&#30053;&#30340;&#21464;&#20307;&#65292;&#34892;&#21160;&#21010;&#20998;&#23558;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#32553;&#23567;&#21040;&#26377;&#21069;&#36884;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#36890;&#36807;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23454;&#20363;&#35828;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central task in control theory, artificial intelligence, and formal methods is to synthesize reward-maximizing strategies for agents that operate in partially unknown environments. In environments modeled by gray-box Markov decision processes (MDPs), the impact of the agents' actions are known in terms of successor states but not the stochastics involved. In this paper, we devise a strategy synthesis algorithm for gray-box MDPs via reinforcement learning that utilizes interval MDPs as internal model. To compete with limited sampling access in reinforcement learning, we incorporate two novel concepts into our algorithm, focusing on rapid and successful learning rather than on stochastic guarantees and optimality: lower confidence bound exploration reinforces variants of already learned practical strategies and action scoping reduces the learning action space to promising actions. We illustrate benefits of our algorithms by means of a prototypical implementation applied on examples fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#37319;&#26679;&#30340;&#36890;&#29992;&#38598;&#25104;&#26694;&#26550;&#65292;&#30001;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#32852;&#21512;&#20351;&#29992;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11419</link><description>&lt;p&gt;
EPiC: &#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#38598;&#25104;&#30340;&#40065;&#26834;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EPiC: Ensemble of Partial Point Clouds for Robust Classification. (arXiv:2303.11419v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#37319;&#26679;&#30340;&#36890;&#29992;&#38598;&#25104;&#26694;&#26550;&#65292;&#30001;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#32852;&#21512;&#20351;&#29992;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;&#30340;&#28857;&#20113;&#20998;&#31867;&#26469;&#35828;&#65292;&#30001;&#20110;&#28040;&#36153;&#22411;3D&#20256;&#24863;&#22120;&#36890;&#24120;&#37319;&#38598;&#30340;&#26159;&#37096;&#20998;&#21644;&#24102;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#19988;&#20250;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#32780;&#21464;&#24471;&#38477;&#36136;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#37319;&#26679;&#30340;&#36890;&#29992;&#38598;&#25104;&#26694;&#26550;&#12290;&#27599;&#20010;&#38598;&#25104;&#25104;&#21592;&#21482;&#26292;&#38706;&#20110;&#37096;&#20998;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#22522;&#20110;&#34917;&#19969;&#21644;&#26354;&#32447;&#30340;&#23616;&#37096;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#19968;&#31181;&#20840;&#23616;&#30340;&#38543;&#26426;&#37319;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21508;&#31181;&#23616;&#37096;&#21644;&#20840;&#23616;&#27745;&#26579;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#39030;&#32423;&#20998;&#31867;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#20351;&#29992;&#20102;Ren&#31561;&#20154;[24]&#24341;&#20837;&#30340;&#26368;&#26032;ModelNet-C&#25968;&#25454;&#24211;&#65292;&#22312;&#19981;&#32463;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#32463;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#37117;&#36798;&#21040;&#20102;&#26368;&#20248;&#65288;SOTA&#65289;&#12290;&#25105;&#20204;&#30340;&#26410;&#32463;&#25968;&#25454;&#22686;&#24378;&#30340;&#22343;&#20540;&#33104;&#34432;&#35823;&#24046;&#65288;mCE&#65289;&#20026;0.64&#65288;&#24403;&#21069;SOTA&#20026;0.86&#65289;&#65292;&#32463;&#36807;&#25968;&#25454;&#22686;&#24378;&#21518;&#20026;0.50&#65288;&#24403;&#21069;SOTA&#20026;0.57&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#24615;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#20102;&#36825;&#20123;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust point cloud classification is crucial for real-world applications, as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts. In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. Three sampling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling. We demonstrate the robustness of our method to various local and global degradations. We show that our framework significantly improves the robustness of top classification netowrks by a large margin. Our experimental setting uses the recently introduced ModelNet-C database by Ren et al.[24], where we reach SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current SOTA is 0.57). We analyze and explain these remarkable results through diversity an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.02468</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#30340;Lon-ea&#65306;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#28608;&#27963;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#23398;&#20064;&#19981;&#21516;&#24847;&#20219;&#21153;&#30340;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#23618;&#20013;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#36719;&#26631;&#31614;&#26469;&#37327;&#21270;&#19981;&#21516;&#24847;&#37327;&#12290;&#20026;&#20102;&#39044;&#27979;&#36719;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#39044;&#22788;&#29702;&#22120;&#21644;&#32534;&#30721;&#22120;&#65292;&#24182;&#25913;&#21464;&#36755;&#20986;&#23618;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21442;&#25968;&#19981;&#21464;&#12290;&#28982;&#21518;&#23558;&#36719;&#26631;&#31614;&#29992;&#20110;&#30828;&#26631;&#31614;&#39044;&#27979;&#12290;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#21253;&#25324;sigmoid&#20989;&#25968;&#20197;&#21450;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#30340;&#38454;&#36291;&#20989;&#25968;&#21644;&#26412;&#25991;&#20013;&#39318;&#27425;&#20171;&#32461;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#30340;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02384</link><description>&lt;p&gt;
&#21033;&#29992;&#26089;&#26399;&#36864;&#20986;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Training of Deep Neural Networks Using Early Exiting. (arXiv:2303.02384v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#30340;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#35270;&#35273;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#36828;&#31163;&#33719;&#21462;&#25968;&#25454;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20113;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#22686;&#21152;&#20102;&#36890;&#20449;&#25104;&#26412;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#22312;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#20043;&#38388;&#20998;&#21106;&#26550;&#26500;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks provide state-of-the-art accuracy for vision tasks but they require significant resources for training. Thus, they are trained on cloud servers far from the edge devices that acquire the data. This issue increases communication cost, runtime and privacy concerns. In this study, a novel hierarchical training method for deep neural networks is proposed that uses early exits in a divided architecture between edge and cloud workers to reduce the communication cost, training runtime and privacy concerns. The method proposes a brand-new use case for early exits to separate the backward pass of neural networks between the edge and the cloud during the training phase. We address the issues of most available methods that due to the sequential nature of the training phase, cannot train the levels of hierarchy simultaneously or they do it with the cost of compromising privacy. In contrast, our method can use both edge and cloud workers simultaneously, does not share the raw i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25918;&#24323;&#20102;&#25945;&#24072;&#34920;&#29616;&#33391;&#22909;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#34701;&#21512;&#20219;&#24847;&#25945;&#24072;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36712;&#36857;&#30340;&#20215;&#20540;&#20272;&#35745;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23433;&#20840;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2303.01728</link><description>&lt;p&gt;
&#26377;&#32570;&#38519;&#22312;&#32447;&#28436;&#31034;&#30340;&#20445;&#25252;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Guarded Policy Optimization with Imperfect Online Demonstrations. (arXiv:2303.01728v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25918;&#24323;&#20102;&#25945;&#24072;&#34920;&#29616;&#33391;&#22909;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#34701;&#21512;&#20219;&#24847;&#25945;&#24072;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36712;&#36857;&#30340;&#20215;&#20540;&#20272;&#35745;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#8221;&#65288;TSF&#65289;&#26159;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#25945;&#24072;&#20195;&#29702;&#36890;&#36807;&#24178;&#39044;&#21644;&#25552;&#20379;&#22312;&#32447;&#28436;&#31034;&#26469;&#30417;&#30563;&#23398;&#29983;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#25918;&#23485;&#20102;&#25945;&#24072;&#34920;&#29616;&#33391;&#22909;&#30340;&#20551;&#35774;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#34701;&#21512;&#20219;&#24847;&#25945;&#24072;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;,&#21363;&#25945;&#24072;-&#23398;&#29983;&#20849;&#20139;&#25511;&#21046;&#65288;TS2C&#65289;&#65292;&#23427;&#22522;&#20110;&#22522;&#20110;&#36712;&#36857;&#30340;&#20215;&#20540;&#20272;&#35745;&#26469;&#34701;&#20837;&#25945;&#24072;&#24178;&#39044;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;TS2C&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#21487;&#38752;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#32780;&#19981;&#20250;&#36807;&#20998;&#20381;&#36182;&#25945;&#24072;&#31574;&#30053;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without be
&lt;/p&gt;</description></item><item><title>FuNVol&#26159;&#19968;&#20010;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#29983;&#25104;&#30495;&#23454;&#21382;&#21490;&#20215;&#26684;&#30340;IV&#34920;&#38754;&#24207;&#21015;&#65292;&#24182;&#22312;&#26080;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#20135;&#29983;&#19968;&#33268;&#30340;&#24066;&#22330;&#24773;&#26223;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.00859</link><description>&lt;p&gt;
FuNVol&#65306;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#30340;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00859
&lt;/p&gt;
&lt;p&gt;
FuNVol&#26159;&#19968;&#20010;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#29983;&#25104;&#30495;&#23454;&#21382;&#21490;&#20215;&#26684;&#30340;IV&#34920;&#38754;&#24207;&#21015;&#65292;&#24182;&#22312;&#26080;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#20135;&#29983;&#19968;&#33268;&#30340;&#24066;&#22330;&#24773;&#26223;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#32467;&#21512;&#27010;&#29575;&#31215;&#20998;&#21464;&#25442;&#24809;&#32602;&#26469;&#29983;&#25104;&#22810;&#20010;&#36164;&#20135;&#30340;&#38544;&#21547;&#27874;&#21160;&#29575;&#34920;&#38754;&#24207;&#21015;&#65292;&#35813;&#26041;&#27861;&#24544;&#23454;&#20110;&#21382;&#21490;&#20215;&#26684;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;IV&#34920;&#38754;&#21644;&#20215;&#26684;&#30340;&#32852;&#21512;&#21160;&#24577;&#20135;&#29983;&#30340;&#24066;&#22330;&#24773;&#26223;&#19982;&#21382;&#21490;&#29305;&#24449;&#19968;&#33268;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#20250;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Here, we introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&amp;L) distributions that are consistent with realised P&amp;Ls.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;DP-SGD&#30340;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;&#20102;&#22914;&#20309;&#35774;&#32622;&#38544;&#31169;&#21442;&#25968;&#20197;&#20445;&#25252;&#20813;&#21463;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25915;&#20987;&#30340;&#19978;&#38480;&#21644;&#21305;&#37197;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2302.07225</link><description>&lt;p&gt;
DP-SGD&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bounding Training Data Reconstruction in DP-SGD. (arXiv:2302.07225v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;DP-SGD&#30340;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;&#20102;&#22914;&#20309;&#35774;&#32622;&#38544;&#31169;&#21442;&#25968;&#20197;&#20445;&#25252;&#20813;&#21463;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25915;&#20987;&#30340;&#19978;&#38480;&#21644;&#21305;&#37197;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#36890;&#24120;&#34987;&#35299;&#37322;&#20026;&#23545;&#25239;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#20445;&#25252;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#21482;&#38656;&#35201;&#20445;&#25252;&#20813;&#21463;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#37027;&#20040;&#31169;&#26377;&#27169;&#22411;&#30340;&#25928;&#29992;&#21487;&#20197;&#25913;&#21892;&#65292;&#22240;&#20026;&#20445;&#25252;&#20813;&#21463;&#36825;&#20123;&#26356;&#26377;&#37326;&#24515;&#30340;&#25915;&#20987;&#38656;&#35201;&#26356;&#23569;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#22312;DP-SGD&#30340;&#19978;&#19979;&#25991;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;DP-SGD&#30340;&#20219;&#20309;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#30340;&#19978;&#38480;&#20197;&#21450;&#19982;&#25105;&#20204;&#36793;&#30028;&#39044;&#27979;&#30456;&#21305;&#37197;&#30340;&#25915;&#20987;&#12290;&#36825;&#20004;&#20010;&#32467;&#26524;&#20026;&#35774;&#32622;DP-SGD&#30340;&#38544;&#31169;&#21442;&#25968;&#20197;&#20445;&#25252;&#20813;&#21463;&#37325;&#24314;&#25915;&#20987;&#24320;&#36767;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Fin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#23616;&#37096;&#26368;&#20248;&#25511;&#21046;&#20989;&#25968;&#30340;&#27010;&#24565;&#26469;&#34920;&#24449;&#36845;&#20195;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05816</link><description>&lt;p&gt;
&#24102;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Policy Gradient Framework for Stochastic Optimal Control Problems with Global Convergence Guarantee. (arXiv:2302.05816v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#23616;&#37096;&#26368;&#20248;&#25511;&#21046;&#20989;&#25968;&#30340;&#27010;&#24565;&#26469;&#34920;&#24449;&#36845;&#20195;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#20013;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#25511;&#21046;&#30340;&#26799;&#24230;&#27969;&#65292;&#23558;&#20854;&#35270;&#20026;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#19968;&#20123;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;&#20998;&#26512;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#23616;&#37096;&#26368;&#20248;&#25511;&#21046;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34920;&#24449;&#36845;&#20195;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider policy gradient methods for stochastic optimal control problem in continuous time. In particular, we analyze the gradient flow for the control, viewed as a continuous time limit of the policy gradient method. We prove the global convergence of the gradient flow and establish a convergence rate under some regularity assumptions. The main novelty in the analysis is the notion of local optimal control function, which is introduced to characterize the local optimality of the iterate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26368;&#28201;&#21644;&#19978;&#21319;&#21160;&#21147;&#23398;&#25193;&#23637;&#21040;&#28857;&#20113;&#23450;&#20041;&#30340;&#27969;&#24418;&#19978;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#39537;&#21160;&#31995;&#32479;&#20174;&#21021;&#22987;&#26500;&#22411;&#21040;&#36798;&#38797;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.04426</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#37319;&#26679;&#28857;&#20113;&#23450;&#20041;&#30340;&#27969;&#24418;&#19978;&#30340;&#26368;&#28201;&#21644;&#19978;&#21319;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Gentlest ascent dynamics on manifolds defined by adaptively sampled point-clouds. (arXiv:2302.04426v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26368;&#28201;&#21644;&#19978;&#21319;&#21160;&#21147;&#23398;&#25193;&#23637;&#21040;&#28857;&#20113;&#23450;&#20041;&#30340;&#27969;&#24418;&#19978;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#39537;&#21160;&#31995;&#32479;&#20174;&#21021;&#22987;&#26500;&#22411;&#21040;&#36798;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#20998;&#23376;&#31995;&#32479;&#30340;&#31232;&#26377;&#20107;&#20214;&#30740;&#31350;&#20013;&#65292;&#23547;&#25214;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#38797;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#26368;&#28201;&#21644;&#19978;&#21319;&#21160;&#21147;&#23398;&#65288;GAD&#65289;&#26159;&#19968;&#31181;&#35797;&#22270;&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#23547;&#25214;&#38797;&#28857;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;&#23427;&#36890;&#36807;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#20351;&#21407;&#31995;&#32479;&#30340;&#38797;&#28857;&#21464;&#20026;&#31283;&#23450;&#24179;&#34913;&#28857;&#12290;GAD&#26368;&#36817;&#24050;&#32463;&#25512;&#24191;&#21040;&#25551;&#36848;&#31561;&#24335;&#32422;&#26463;&#30340;&#27969;&#24418;&#65288;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65289;&#19978;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#24182;&#20197;&#22806;&#37096;&#24418;&#24335;&#32473;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;GAD&#25193;&#23637;&#21040;&#30001;&#28857;&#20113;&#23450;&#20041;&#30340;&#27969;&#24418;&#19978;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#20869;&#22312;&#35266;&#28857;&#36827;&#34892;&#20844;&#24335;&#21270;&#12290;&#36825;&#20123;&#28857;&#20113;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#39537;&#21160;&#31995;&#32479;&#20174;&#21021;&#22987;&#26500;&#22411;&#65288;&#36890;&#24120;&#22312;&#31283;&#23450;&#24179;&#34913;&#28857;&#38468;&#36817;&#65289;&#21040;&#36798;&#38797;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#21453;&#24212;&#29289;&#65288;&#21021;&#22987;&#26500;&#22411;&#65289;&#65292;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;&#32447;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding saddle points of dynamical systems is an important problem in practical applications such as the study of rare events of molecular systems. Gentlest ascent dynamics (GAD) is one of a number of algorithms in existence that attempt to find saddle points in dynamical systems. It works by deriving a new dynamical system in which saddle points of the original system become stable equilibria. GAD has been recently generalized to the study of dynamical systems on manifolds (differential algebraic equations) described by equality constraints and given in an extrinsic formulation. In this paper, we present an extension of GAD to manifolds defined by point-clouds, formulated using the intrinsic viewpoint. These point-clouds are adaptively sampled during an iterative process that drives the system from the initial conformation (typically in the neighborhood of a stable equilibrium) to a saddle point. Our method requires the reactant (initial conformation), does not require the explicit co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Python&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#20132;&#36890;&#22330;&#26223;&#20013;&#25552;&#21462;&#26631;&#20934;&#21270;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#65292;&#20197;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20363;&#22914;&#36712;&#36857;&#39044;&#27979;&#65292;&#20026;&#33258;&#20027;&#39550;&#39542;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;GNN&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2302.01259</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65306;&#36890;&#36807;CommonRoad-Geometric&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Geometric Deep Learning for Autonomous Driving: Unlocking the Power of Graph Neural Networks With CommonRoad-Geometric. (arXiv:2302.01259v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Python&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#20132;&#36890;&#22330;&#26223;&#20013;&#25552;&#21462;&#26631;&#20934;&#21270;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#65292;&#20197;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20363;&#22914;&#36712;&#36857;&#39044;&#27979;&#65292;&#20026;&#33258;&#20027;&#39550;&#39542;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;GNN&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#21487;&#20197;&#20026;&#20132;&#36890;&#25552;&#20379;&#24378;&#22823;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#27169;&#25311;&#19981;&#21516;&#25968;&#37327;&#30340;&#20132;&#36890;&#21442;&#19982;&#32773;&#21644;&#22522;&#30784;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#25928;&#24212;&#12290;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22270;&#32467;&#26500;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#36712;&#36857;&#39044;&#27979;&#12290;&#20316;&#20026;&#39318;&#20010;Python&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#19988;&#23436;&#20840;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#20174;&#20132;&#36890;&#22330;&#26223;&#20013;&#25552;&#21462;&#26631;&#20934;&#21270;&#22270;&#24418;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;GNN&#30340;&#33258;&#20027;&#39550;&#39542;&#30740;&#31350;&#24179;&#21488;&#65292;&#23427;&#25552;&#39640;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#24615;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#19987;&#27880;&#20110;&#27169;&#22411;&#23454;&#29616;&#32780;&#19981;&#26159;&#25968;&#25454;&#38598;&#30340;&#31574;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs offer powerful data representations for traffic, given their ability to model the complex interaction effects among a varying number of traffic participants and the underlying road infrastructure. With the recent advent of graph neural networks (GNNs) as the accompanying deep learning framework, the graph structure can be efficiently leveraged for various machine learning applications such as trajectory prediction. As a first of its kind, our proposed Python framework offers an easy-to-use and fully customizable data processing pipeline to extract standardized graph datasets from traffic scenarios. Providing a platform for GNN-based autonomous driving research, it improves comparability between approaches and allows researchers to focus on model implementation instead of dataset curation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#28431;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00539</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#35782;&#21035;&#20449;&#24687;&#27844;&#38706;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Analyzing Leakage of Personally Identifiable Information in Language Models. (arXiv:2302.00539v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#28431;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#20250;&#36890;&#36807;&#21477;&#23376;&#32423;&#25104;&#21592;&#25512;&#26029;&#21644;&#37325;&#26500;&#25915;&#20987;&#27844;&#28431;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#27844;&#38706;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#20102;&#35299;&#19981;&#36275;&#12290;&#30446;&#21069;&#24050;&#32463;&#20551;&#35774;&#25968;&#25454;&#38598;&#25972;&#29702;&#25216;&#26415;&#65288;&#22914;&#25968;&#25454;&#28165;&#27927;&#65289;&#36275;&#20197;&#38450;&#27490;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#27844;&#38706;&#65292;&#20294;&#36825;&#19968;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#25968;&#25454;&#28165;&#27927;&#25216;&#26415;&#21487;&#20197;&#20943;&#23569;Pll&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#32477;&#23545;&#22320;&#38450;&#27490;&#27844;&#38706;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#27844;&#28431;&#30340;&#20005;&#26684;&#22522;&#20110;&#21338;&#24328;&#30340;&#23450;&#20041;&#65292;&#36890;&#36807;API&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Bagging&#25216;&#26415;&#21487;&#25552;&#20379;&#26080;&#20559;&#24046;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#21644;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.12600</link><description>&lt;p&gt;
Bagging&#25552;&#20379;&#26080;&#20559;&#24046;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bagging Provides Assumption-free Stability. (arXiv:2301.12600v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Bagging&#25216;&#26415;&#21487;&#25552;&#20379;&#26080;&#20559;&#24046;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#21644;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bagging&#26159;&#31283;&#23450;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20219;&#20309;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#25512;&#23548;&#20102;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#23545;&#25968;&#25454;&#20998;&#24067;&#12289;&#22522;&#26412;&#31639;&#27861;&#30340;&#23646;&#24615;&#25110;&#21327;&#21464;&#37327;&#30340;&#32500;&#25968;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#36866;&#29992;&#20110;&#22810;&#31181;&#21464;&#20307;&#30340;Bagging&#65292;&#24182;&#19988;&#26159;&#26368;&#20248;&#30340;&#24120;&#25968;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#34920;&#26126;Bagging&#25104;&#21151;&#31283;&#23450;&#20102;&#21363;&#20351;&#26159;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#22522;&#26412;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bagging is an important technique for stabilizing machine learning models. In this paper, we derive a finite-sample guarantee on the stability of bagging for any model. Our result places no assumptions on the distribution of the data, on the properties of the base algorithm, or on the dimensionality of the covariates. Our guarantee applies to many variants of bagging and is optimal up to a constant. Empirical results validate our findings, showing that bagging successfully stabilizes even highly unstable base algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#37319;&#29992;&#26446;&#20195;&#25968;&#26694;&#26550;&#26816;&#27979;&#21644;&#23884;&#20837;&#23545;&#31216;&#24615;&#65292;&#20197;&#20445;&#30041;&#36763;&#31995;&#32479;&#32467;&#26500;&#12290;&#32771;&#34385;&#25670;&#22312;&#23567;&#36710;&#19978;&#21644;&#22825;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#20108;&#20307;&#38382;&#39064;&#20316;&#20026;&#26696;&#20363;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.07928</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#21160;&#23545;&#31216;&#24615;&#26816;&#27979;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hamiltonian Neural Networks with Automatic Symmetry Detection. (arXiv:2301.07928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#37319;&#29992;&#26446;&#20195;&#25968;&#26694;&#26550;&#26816;&#27979;&#21644;&#23884;&#20837;&#23545;&#31216;&#24615;&#65292;&#20197;&#20445;&#30041;&#36763;&#31995;&#32479;&#32467;&#26500;&#12290;&#32771;&#34385;&#25670;&#22312;&#23567;&#36710;&#19978;&#21644;&#22825;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#20108;&#20307;&#38382;&#39064;&#20316;&#20026;&#26696;&#20363;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#65288;HNN&#65289;&#65292;&#20197;&#22312;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#26041;&#31243;&#26102;&#32435;&#20837;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#36763;&#31995;&#32479;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#20445;&#30041;&#23545;&#31216;&#24615;&#38656;&#35201;&#39069;&#22806;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29992;&#26446;&#20195;&#25968;&#26694;&#26550;&#22686;&#24378;&#20102;HNN&#65292;&#20197;&#26816;&#27979;&#21644;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#21516;&#26102;&#23398;&#20064;&#23545;&#31216;&#24615;&#32676;&#30340;&#20316;&#29992;&#21644;&#31995;&#32479;&#30340;&#24635;&#33021;&#37327;&#12290;&#20197;&#25670;&#22312;&#23567;&#36710;&#19978;&#21644;&#22825;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#20108;&#20307;&#38382;&#39064;&#20026;&#20363;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Hamiltonian neural networks (HNN) have been introduced to incorporate prior physical knowledge when learning the dynamical equations of Hamiltonian systems. Hereby, the symplectic system structure is preserved despite the data-driven modeling approach. However, preserving symmetries requires additional attention. In this research, we enhance HNN with a Lie algebra framework to detect and embed symmetries in the neural network. This approach allows to simultaneously learn the symmetry group action and the total energy of the system. As illustrating examples, a pendulum on a cart and a two-body problem from astrodynamics are considered.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#20803;&#28789;&#24863;&#21551;&#21457;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FEIH($\phi$)-GNN&#65289;&#65292;&#33021;&#22815;&#27169;&#25311;&#23616;&#37096;&#21018;&#24230;&#30697;&#38453;&#30340;&#35745;&#31639;&#36807;&#31243;&#65292;&#20855;&#22791;&#26059;&#36716;&#31561;&#21464;&#24615;&#65292;&#24182;&#33021;&#20934;&#30830;&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#26102;&#38388;&#28378;&#21160;&#12290;</title><link>http://arxiv.org/abs/2212.14545</link><description>&lt;p&gt;
&#26377;&#38480;&#20803;&#28789;&#24863;&#21551;&#21457;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Finite Element-Inspired Hypergraph Neural Network: Application to Fluid Dynamics Simulations. (arXiv:2212.14545v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#20803;&#28789;&#24863;&#21551;&#21457;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FEIH($\phi$)-GNN&#65289;&#65292;&#33021;&#22815;&#27169;&#25311;&#23616;&#37096;&#21018;&#24230;&#30697;&#38453;&#30340;&#35745;&#31639;&#36807;&#31243;&#65292;&#20855;&#22791;&#26059;&#36716;&#31561;&#21464;&#24615;&#65292;&#24182;&#33021;&#20934;&#30830;&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#26102;&#38388;&#28378;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#23398;&#20064;&#26694;&#26550;&#25805;&#20316;&#30340;&#26159;&#27599;&#26465;&#36793;&#36830;&#25509;&#20004;&#20010;&#33410;&#28857;&#30340;&#22270;&#12290;&#21463;&#21040;&#26377;&#38480;&#20803;&#26041;&#27861;&#20013;&#25968;&#25454;&#32852;&#36890;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#20803;&#32032;&#32780;&#19981;&#26159;&#36793;&#32536;&#36830;&#25509;&#33410;&#28857;&#26469;&#26500;&#24314;&#36229;&#22270;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#26679;&#30340;&#33410;&#28857;-&#20803;&#32032;&#36229;&#22270;&#19978;&#23450;&#20041;&#20102;&#19968;&#31181;&#36229;&#22270;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#27169;&#25311;&#23616;&#37096;&#21018;&#24230;&#30697;&#38453;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#26377;&#38480;&#20803;&#28789;&#24863;&#21551;&#21457;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31616;&#31216;FEIH($\phi$)-GNN&#12290;&#25105;&#20204;&#36824;&#20026;&#35813;&#32593;&#32476;&#26500;&#24314;&#20102;&#26059;&#36716;&#31561;&#21464;&#24615;&#65292;&#24182;&#25506;&#32034;&#20854;&#23545;&#20110;&#24314;&#27169;&#38750;&#23450;&#24120;&#27969;&#20307;&#27969;&#21160;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#35813;&#32593;&#32476;&#26550;&#26500;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;&#21363;&#22278;&#26609;&#21608;&#22260;&#30340;&#27969;&#20307;&#27969;&#21160;&#21644;&#32764;&#22411;&#32467;&#26500;&#20013;&#23637;&#31034;&#20102;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#26102;&#38388;&#28378;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging trend in deep learning research focuses on the applications of graph neural networks (GNNs) for mesh-based continuum mechanics simulations. Most of these learning frameworks operate on graphs wherein each edge connects two nodes. Inspired by the data connectivity in the finite element method, we present a method to construct a hypergraph by connecting the nodes by elements rather than edges. A hypergraph message-passing network is defined on such a node-element hypergraph that mimics the calculation process of local stiffness matrices. We term this method a finite element-inspired hypergraph neural network, in short FEIH($\phi$)-GNN. We further equip the proposed network with rotation equivariance, and explore its capability for modeling unsteady fluid flow systems. The effectiveness of the network is demonstrated on two common benchmark problems, namely the fluid flow around a circular cylinder and airfoil configurations. Stabilized and accurate temporal roll-out predictio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LeMDA&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#33258;&#21160;&#23398;&#20064;&#32852;&#21512;&#22686;&#24378;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.14453</link><description>&lt;p&gt;
&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LeMDA&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#33258;&#21160;&#23398;&#20064;&#32852;&#21512;&#22686;&#24378;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#32852;&#21512;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#65288;&#22914;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#65289;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#23613;&#31649;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26469;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#25968;&#25454;&#22686;&#24378;&#30340;&#24040;&#22823;&#25104;&#21151;&#20173;&#28982;&#23616;&#38480;&#20110;&#21333;&#27169;&#24577;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#30830;&#23454;&#65292;&#22312;&#20445;&#30041;&#25968;&#25454;&#30340;&#25972;&#20307;&#35821;&#20041;&#32467;&#26500;&#30340;&#21516;&#26102;&#22686;&#24378;&#27599;&#20010;&#27169;&#24577;&#26159;&#29305;&#21035;&#22256;&#38590;&#30340;&#65307;&#20363;&#22914;&#65292;&#22312;&#24212;&#29992;&#20102;&#26631;&#20934;&#22686;&#24378;&#26041;&#27861;&#65288;&#20363;&#22914;&#32763;&#35793;&#65289;&#20043;&#21518;&#65292;&#26631;&#39064;&#21487;&#33021;&#19981;&#20877;&#26159;&#22270;&#20687;&#30340;&#33391;&#22909;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#20173;&#28982;&#24456;&#38590;&#25351;&#23450;&#19981;&#38024;&#23545;&#29305;&#23450;&#27169;&#24577;&#30340;&#21512;&#29702;&#21464;&#25442;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;LeMDA&#65288;Learning Multimodal Data Augmentation&#65289;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#33258;&#21160;&#23398;&#20064;&#32852;&#21512;&#22686;&#24378;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#19981;&#38480;&#21046;&#27169;&#24577;&#30340;&#36523;&#20221;&#25110;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;LeMDA&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#38899;&#35782;&#21035;&#65289;&#20013;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.13679</link><description>&lt;p&gt;
CC-FedAvg&#65306;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CC-FedAvg: Computationally Customized Federated Averaging. (arXiv:2212.13679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#24335;&#65292;&#36890;&#36807;&#20998;&#24067;&#22312;&#20247;&#22810;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#23427;&#22312;&#26412;&#36136;&#19978;&#20551;&#35774;&#21442;&#19982;&#32773;&#30340;&#35745;&#31639;&#33021;&#21147;&#30456;&#21516;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#33021;&#28304;&#39044;&#31639;&#25110;&#24182;&#34892;&#25191;&#34892;&#30340;&#20219;&#21153;&#19981;&#21516;&#65292;&#21442;&#19982;&#32773;&#35745;&#31639;&#36164;&#28304;&#23384;&#22312;&#30528;&#24046;&#24322;&#12290;&#32570;&#20047;&#35745;&#31639;&#39044;&#31639;&#30340;&#21442;&#19982;&#32773;&#24517;&#39035;&#36866;&#24403;&#35268;&#21010;&#20854;&#21463;&#38480;&#35745;&#31639;&#36164;&#28304;&#30340;&#20351;&#29992;&#65292;&#21542;&#21017;&#20182;&#20204;&#23558;&#26080;&#27861;&#23436;&#25104;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#31639;&#26412;&#22320;&#27169;&#22411;&#32780;&#26080;&#38656;&#35745;&#31639;&#23494;&#38598;&#36845;&#20195;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;(CC-FedAvg)&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#24403;&#21069;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#27599;&#20010;&#36718;&#27425;&#20013;&#20915;&#23450;&#26159;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#36824;&#26159;&#27169;&#22411;&#20272;&#31639;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30456;&#27604;&#65292;CC-FedAvg&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging paradigm to train model with distributed data from numerous Internet of Things (IoT) devices. It inherently assumes a uniform capacity among participants. However, due to different conditions such as differing energy budgets or executing parallel unrelated tasks, participants have diverse computational resources in practice. Participants with insufficient computation budgets must plan for the use of restricted computational resources appropriately, otherwise they would be unable to complete the entire training procedure, resulting in model performance decline. To address the this issue, we propose a strategy for estimating local models without computationally intensive iterations. Based on it, we propose Computationally Customized Federated Averaging (CC-FedAvg), which allows participants to determine whether to perform traditional local training or model estimation in each round based on their current computational budgets. Both theoretical analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2212.12061</link><description>&lt;p&gt;
MN-DS&#65306;&#26032;&#38395;&#25991;&#31456;&#23618;&#27425;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#20174;2019&#24180;1&#26376;1&#26085;&#21040;2019&#24180;12&#26376;31&#26085;&#30340;&#23618;&#27425;&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#26681;&#25454;17&#20010;&#19968;&#32423;&#31867;&#21035;&#21644;109&#20010;&#20108;&#32423;&#31867;&#21035;&#30340;&#23618;&#27425;&#20998;&#31867;&#25163;&#21160;&#26631;&#35760;&#20102;&#36825;&#20123;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#33258;&#21160;&#25353;&#20027;&#39064;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20174;&#20107;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#26681;&#25454;&#21457;&#24067;&#30340;&#26032;&#38395;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#30828;&#20214;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;PyGFI&#26694;&#26550;&#65292;&#21033;&#29992;&#25925;&#38556;&#27880;&#20837;&#29983;&#25104;&#21512;&#25104;&#25925;&#38556;&#26469;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;GNN&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#24525;&#21508;&#31181;&#31867;&#22411;&#30340;&#30828;&#20214;&#25925;&#38556;&#65292;&#26377;&#25928;&#25552;&#39640;GNN&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;GNN&#35757;&#32451;&#27969;&#31243;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.03475</link><description>&lt;p&gt;
PyGFI&#65306;&#20998;&#26512;&#21644;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#30828;&#20214;&#38169;&#35823;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
PyGFI: Analyzing and Enhancing Robustness of Graph Neural Networks Against Hardware Errors. (arXiv:2212.03475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#30828;&#20214;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;PyGFI&#26694;&#26550;&#65292;&#21033;&#29992;&#25925;&#38556;&#27880;&#20837;&#29983;&#25104;&#21512;&#25104;&#25925;&#38556;&#26469;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;GNN&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#24525;&#21508;&#31181;&#31867;&#22411;&#30340;&#30828;&#20214;&#25925;&#38556;&#65292;&#26377;&#25928;&#25552;&#39640;GNN&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;GNN&#35757;&#32451;&#27969;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#24182;&#24050;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#31561;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#25104;&#21151;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#19968;&#26679;&#65292;GNN&#34987;&#37096;&#32626;&#22312;&#20808;&#36827;&#30340;&#29616;&#20195;&#30828;&#20214;&#31995;&#32479;&#20197;&#21450;&#19987;&#29992;&#21152;&#36895;&#22120;&#19978;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;GNN&#30340;&#26222;&#21450;&#21644;&#23558;&#20854;&#24341;&#20837;&#30828;&#20214;&#30340;&#26368;&#36817;&#21162;&#21147;&#65292;GNN&#30340;&#23481;&#38169;&#24615;&#21644;&#38887;&#24615;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#21463;DL&#26041;&#27861;&#20869;&#22312;&#30340;&#31639;&#27861;&#38887;&#24615;&#21551;&#21457;&#65292;&#39318;&#27425;&#23545;GNN&#38887;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#29702;&#35299;&#30828;&#20214;&#25925;&#38556;&#19982;GNN&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;PyTorch&#20043;&#19978;&#24320;&#21457;&#23450;&#21046;&#30340;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;GNN&#27169;&#22411;&#21644;&#24212;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25925;&#38556;&#27880;&#20837;&#23454;&#39564;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;&#20256;&#32479;&#22522;&#20110;&#22270;&#20687;&#30340;DL&#27169;&#22411;&#30456;&#27604;&#65292;GNN&#30340;&#23481;&#38169;&#33021;&#21147;&#26174;&#33879;&#36739;&#20302;&#65292;&#24182;&#19988;&#24120;&#35265;&#30340;&#30828;&#20214;&#25925;&#38556;&#20250;&#26174;&#33879;&#38477;&#20302;GNN&#30340;&#20934;&#30830;&#24615;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PyGFI&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;GNN&#23545;&#30828;&#20214;&#38169;&#35823;&#40065;&#26834;&#24615;&#30340;&#26032;&#26694;&#26550;&#12290;PyGFI&#21033;&#29992;&#25925;&#38556;&#27880;&#20837;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#29983;&#25104;&#21512;&#25104;&#25925;&#38556;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#26356;&#21152;&#40065;&#26834;&#30340;GNN&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#24525;&#21508;&#31181;&#31867;&#22411;&#30340;&#30828;&#20214;&#25925;&#38556;&#12290;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PyGFI&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;GNN&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;GNN&#35757;&#32451;&#27969;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have recently emerged as a promising learning paradigm in learning graph-structured data and have demonstrated wide success across various domains such as recommendation systems, social networks, and electronic design automation (EDA). Like other deep learning (DL) methods, GNNs are being deployed in sophisticated modern hardware systems, as well as dedicated accelerators. However, despite the popularity of GNNs and the recent efforts of bringing GNNs to hardware, the fault tolerance and resilience of GNNs have generally been overlooked. Inspired by the inherent algorithmic resilience of DL methods, this paper conducts, for the first time, a large-scale and empirical study of GNN resilience, aiming to understand the relationship between hardware faults and GNN accuracy. By developing a customized fault injection tool on top of PyTorch, we perform extensive fault injection experiments on various GNN models and application datasets. We observe that the error 
&lt;/p&gt;</description></item><item><title>Edge Impulse&#26159;&#19968;&#20010;&#38754;&#21521;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;MLOps&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#23884;&#20837;&#24335;&#21644;&#36793;&#32536;ML&#31995;&#32479;&#30340;&#36719;&#30828;&#20214;&#20248;&#21270;&#25903;&#25345;&#65292;&#35299;&#20915;TinyML&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.03332</link><description>&lt;p&gt;
Edge Impulse: &#38754;&#21521;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;MLOps&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge Impulse: An MLOps Platform for Tiny Machine Learning. (arXiv:2212.03332v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03332
&lt;/p&gt;
&lt;p&gt;
Edge Impulse&#26159;&#19968;&#20010;&#38754;&#21521;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;MLOps&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#23884;&#20837;&#24335;&#21644;&#36793;&#32536;ML&#31995;&#32479;&#30340;&#36719;&#30828;&#20214;&#20248;&#21270;&#25903;&#25345;&#65292;&#35299;&#20915;TinyML&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Edge Impulse&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;(MLOps)&#24179;&#21488;&#65292;&#29992;&#20110;&#24320;&#21457;&#21487;&#20197;&#37096;&#32626;&#21040;&#21508;&#31181;&#30828;&#20214;&#30446;&#26631;&#30340;&#23884;&#20837;&#24335;&#21644;&#36793;&#32536;ML(&#24494;&#22411;ML)&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;TinyML&#24037;&#20316;&#27969;&#31243;&#23384;&#22312;&#30528;&#36719;&#20214;&#22534;&#26632;&#30862;&#29255;&#21270;&#21644;&#24322;&#26500;&#37096;&#32626;&#30828;&#20214;&#31561;&#38382;&#39064;&#65292;&#20351;&#24471;ML&#27169;&#22411;&#20248;&#21270;&#22256;&#38590;&#19988;&#19981;&#20855;&#22791;&#21487;&#31227;&#26893;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Edge Impulse&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;MLOps&#24179;&#21488;&#65292;&#21487;&#23454;&#29616;&#22823;&#35268;&#27169;&#24320;&#21457;TinyML&#31995;&#32479;&#12290;Edge Impulse&#36890;&#36807;&#25903;&#25345;&#21508;&#31181;&#36719;&#20214;&#21644;&#30828;&#20214;&#20248;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#21487;&#25193;&#23637;&#19988;&#21487;&#31227;&#26893;&#30340;&#36719;&#20214;&#22534;&#26632;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#23884;&#20837;&#24335;&#31995;&#32479;&#12290; &#25130;&#33267;2022&#24180;10&#26376;&#65292;Edge Impulse&#25176;&#31649;&#20102;&#26469;&#33258;50,953&#21517;&#24320;&#21457;&#20154;&#21592;&#30340;118,185&#20010;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge Impulse is a cloud-based machine learning operations (MLOps) platform for developing embedded and edge ML (TinyML) systems that can be deployed to a wide range of hardware targets. Current TinyML workflows are plagued by fragmented software stacks and heterogeneous deployment hardware, making ML model optimizations difficult and unportable. We present Edge Impulse, a practical MLOps platform for developing TinyML systems at scale. Edge Impulse addresses these challenges and streamlines the TinyML design cycle by supporting various software and hardware optimizations to create an extensible and portable software stack for a multitude of embedded systems. As of Oct. 2022, Edge Impulse hosts 118,185 projects from 50,953 developers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.01346</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#30340;&#33258;&#28982;&#32422;&#26463;&#25285;&#20445;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#21644;&#25511;&#21046;&#24212;&#29992;&#30340;&#20027;&#35201;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20316;&#20026;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#31867;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21448;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#20027;&#31995;&#32479;&#12290;&#22312;&#21307;&#30103;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#25968;&#25454;&#33021;&#22815;&#34987;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#23545;&#26469;&#33258;&#33258;&#28982;&#31185;&#23398;&#30340;&#24050;&#26377;&#30693;&#35782;&#30340;&#31526;&#21512;&#24615;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#65292;&#25110;&#32773;&#21487;&#20197;&#34987;&#27987;&#32553;&#25104;&#65288;&#21487;&#33021;&#26159;&#40657;&#30418;&#30340;&#65289;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;F1&#36187;&#36710;&#24212;&#31526;&#21512;&#29275;&#39039;&#23450;&#24459;&#65288;&#36825;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#21333;&#36718;&#27169;&#22411;&#20013;&#65289;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#32771;&#34385;&#20197;&#19979;&#38382;&#39064;&#8212;&#8212;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;M&#21644;&#19968;&#20010;&#29366;&#24577;&#36716;&#31227;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#36317;&#31163;M&#30340;&#33539;&#22260;&#20869;&#26368;&#22909;&#22320;&#36817;&#20284;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#25285;&#20445;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#23558;&#25968;&#25454;&#38598;&#27987;&#32553;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have emerged as the workhorse for a large section of robotics and control applications, especially as models for dynamical systems. Such data-driven models are in turn used for designing and verifying autonomous systems. They are particularly useful in modeling medical systems where data can be leveraged to individualize treatment. In safety-critical applications, it is important that the data-driven model is conformant to established knowledge from the natural sciences. Such knowledge is often available or can often be distilled into a (possibly black-box) model. For instance, an F1 racing car should conform to Newton's laws (which are encoded within a unicycle model). In this light, we consider the following problem - given a model $M$ and a state transition dataset, we wish to best approximate the system model while being a bounded distance away from $M$. We propose a method to guarantee this conformance. Our first step is to distill the dataset into a few repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#24320;&#21457;&#21644;&#25506;&#32034;&#25910;&#36141;&#20989;&#25968;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25670;&#33073;&#20102;&#23616;&#37096;&#26368;&#20248;&#21644;&#38797;&#28857;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16667</link><description>&lt;p&gt;
&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#26435;&#34913;&#30340;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off. (arXiv:2211.16667v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#24320;&#21457;&#21644;&#25506;&#32034;&#25910;&#36141;&#20989;&#25968;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25670;&#33073;&#20102;&#23616;&#37096;&#26368;&#20248;&#21644;&#38797;&#28857;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#22823;&#37327;&#21442;&#25968;&#38459;&#30861;&#20102;&#23427;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#26222;&#21450;&#65292;&#24182;&#23545;&#29615;&#22659;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#38750;&#38646;&#26435;&#37325;&#26469;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#35757;&#32451;&#25104;&#26412;&#65292;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#25110;&#36138;&#23146;&#30340;&#20943;&#23569;&#21644;&#22686;&#38271;&#31574;&#30053;&#65292;&#23548;&#33268;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#20302;&#31934;&#24230;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#35270;&#20026;&#31232;&#30095;&#36830;&#36890;&#24615;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#24320;&#21457;&#21644;&#25506;&#32034;&#25910;&#36141;&#20989;&#25968;&#26469;&#25670;&#33073;&#23616;&#37096;&#26368;&#20248;&#21644;&#38797;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#25910;&#36141;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#24182;&#38416;&#26126;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31243;&#24207;&#33719;&#24471;&#20102;&#22810;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#21487;&#27604;&#30340;&#31934;&#24230;&#65288;&#39640;&#36798;98&#65285;&#30340;&#31232;&#30095;&#24230;&#65289;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Although effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize environmental impact. Sparse training (using a fixed number of nonzero weights in each iteration) could significantly mitigate the training costs by reducing the model size. However, existing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local minimal and low accuracy. In this work, we consider the dynamic sparse training as a sparse connectivity search problem and design an exploitation and exploration acquisition function to escape from local optima and saddle points. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence property. Experimental results show that sparse models (up to 98\% sparsity) obtained by our pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#27969;&#24418;&#36827;&#34892;&#25512;&#26029;&#30340;&#28508;&#22312;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21160;&#24577;&#23398;&#20064;&#38382;&#39064;&#20869;&#22312;&#30340;&#22270;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;Riemannian&#20960;&#20309;&#23398;&#21644;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#24182;&#20135;&#29983;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.16199</link><description>&lt;p&gt;
&#21033;&#29992;&#31215;&#27969;&#24418;&#36827;&#34892;&#28508;&#22312;&#22270;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Inference using Product Manifolds. (arXiv:2211.16199v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#27969;&#24418;&#36827;&#34892;&#25512;&#26029;&#30340;&#28508;&#22312;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21160;&#24577;&#23398;&#20064;&#38382;&#39064;&#20869;&#22312;&#30340;&#22270;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;Riemannian&#20960;&#20309;&#23398;&#21644;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#24182;&#20135;&#29983;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20381;&#36182;&#20110;&#22270;&#25299;&#25169;&#32467;&#26500;&#23545;&#32593;&#32476;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20248;&#24615;&#12290;&#32780;&#28508;&#22312;&#22270;&#25512;&#26029;&#20801;&#35768;&#27169;&#22411;&#21160;&#24577;&#23398;&#20064;&#38382;&#39064;&#20869;&#22312;&#30340;&#22270;&#32467;&#26500;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#36830;&#36890;&#24615;&#27169;&#24335;&#21487;&#33021;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#12290;&#26412;&#25991;&#23558;&#31163;&#25955;&#21487;&#24494;&#22270;&#27169;&#22359;&#65288;dDGM&#65289;&#25512;&#24191;&#21040;&#28508;&#22312;&#22270;&#23398;&#20064;&#12290;&#21407;&#22987;&#30340;dDGM&#26550;&#26500;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#24179;&#38754;&#26469;&#32534;&#30721;&#28508;&#22312;&#29305;&#24449;&#65292;&#22522;&#20110;&#27492;&#29983;&#25104;&#28508;&#22312;&#22270;&#12290;&#36890;&#36807;&#23558;&#40654;&#26364;&#20960;&#20309;&#24341;&#20837;&#27169;&#22411;&#24182;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28508;&#22312;&#22270;&#25512;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#24120;&#26354;&#29575;&#27169;&#22411;&#31354;&#38388;&#30340;&#31215;&#27969;&#24418;&#65292;&#21487;&#20197;&#32534;&#30721;&#19981;&#21516;&#32467;&#26500;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#26144;&#23556;&#21040;&#25512;&#26029;&#20986;&#30340;&#31215;&#27969;&#24418;&#19978;&#30340;&#28508;&#22312;&#34920;&#31034;&#29992;&#20110;&#35745;&#31639;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24314;&#31435;&#40065;&#26834;&#24615;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#20811;&#26381;&#26631;&#31614;&#22122;&#22768;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#21019;&#24314;&#36716;&#25442;&#30697;&#38453;&#20272;&#35745;&#22120;&#12289;&#30740;&#31350;&#20004;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#65292;&#25581;&#31034;&#20102;&#20004;&#20010;FashionMINIST&#25968;&#25454;&#38598;&#30340;&#40065;&#26834;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#26631;&#31614;&#22122;&#22768;&#24102;&#26469;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.15279</link><description>&lt;p&gt;
&#24314;&#31435;&#40065;&#26834;&#24615;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#20811;&#26381;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Establishment of Neural Networks Robust to Label Noise. (arXiv:2211.15279v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24314;&#31435;&#40065;&#26834;&#24615;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#20811;&#26381;&#26631;&#31614;&#22122;&#22768;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#21019;&#24314;&#36716;&#25442;&#30697;&#38453;&#20272;&#35745;&#22120;&#12289;&#30740;&#31350;&#20004;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#65292;&#25581;&#31034;&#20102;&#20004;&#20010;FashionMINIST&#25968;&#25454;&#38598;&#30340;&#40065;&#26834;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#26631;&#31614;&#22122;&#22768;&#24102;&#26469;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#38556;&#30861;&#65292;&#23588;&#20854;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#24433;&#21709;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#35760;&#24518;&#22024;&#26434;&#30340;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#26041;&#27861;&#30340;&#22522;&#26412;&#27010;&#24565;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#36716;&#25442;&#30697;&#38453;&#20272;&#35745;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#20110;&#23454;&#38469;&#36716;&#25442;&#30697;&#38453;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65288;LeNet&#21644;AlexNet&#35774;&#35745;&#65289;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#20004;&#20010;FashionMINIST&#25968;&#25454;&#38598;&#37117;&#23637;&#29616;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#19981;&#33021;&#26377;&#25928;&#22320;&#23637;&#31034;&#36716;&#25442;&#30697;&#38453;&#22122;&#22768;&#26657;&#27491;&#23545;&#40065;&#26834;&#24615;&#22686;&#24378;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#24314;&#31435;&#33021;&#22815;&#20811;&#26381;&#26631;&#31614;&#22122;&#22768;&#25361;&#25112;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise is a significant obstacle in deep learning model training. It can have a considerable impact on the performance of image classification models, particularly deep neural networks, which are especially susceptible because they have a strong propensity to memorise noisy labels. In this paper, we have examined the fundamental concept underlying related label noise approaches. A transition matrix estimator has been created, and its effectiveness against the actual transition matrix has been demonstrated. In addition, we examined the label noise robustness of two convolutional neural network classifiers with LeNet and AlexNet designs. The two FashionMINIST datasets have revealed the robustness of both models. We are not efficiently able to demonstrate the influence of the transition matrix noise correction on robustness enhancements due to our inability to correctly tune the complex convolutional neural network model due to time and computing resource constraints. There is a need
&lt;/p&gt;</description></item><item><title>CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15183</link><description>&lt;p&gt;
&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous Episodic Control. (arXiv:2211.15183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15183
&lt;/p&gt;
&lt;p&gt;
CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#24555;&#36895;&#38145;&#23450;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#39640;&#22870;&#21169;&#30340;&#32463;&#39564;&#12290;&#19982;&#21442;&#25968;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21442;&#25968;&#38656;&#35201;&#32531;&#24930;&#22320;&#21453;&#21521;&#20256;&#36882;&#22870;&#21169;&#20449;&#21495;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21457;&#29616;&#19968;&#27425;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#21518;&#23601;&#21487;&#20197;&#21453;&#22797;&#35299;&#20915;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24773;&#26223;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#23384;&#20648;&#22312;&#31163;&#25955;&#34920;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36804;&#20170;&#21482;&#24212;&#29992;&#20110;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;&#65288;CEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#38271;&#26399;&#24615;&#33021;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;CEC&#21487;&#20197;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#8212;&#8212;&#26368;&#20248;&#20998;&#31867;&#26862;&#26519;&#65292;&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#26500;&#24314;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#27604;&#38543;&#26426;&#26862;&#26519;&#20351;&#29992;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#26641;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.10502</link><description>&lt;p&gt;
&#26368;&#20248;&#20998;&#31867;&#26862;&#26519;&#30340;&#25968;&#23398;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Mathematical Programming Approach to Optimal Classification Forests. (arXiv:2211.10502v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#8212;&#8212;&#26368;&#20248;&#20998;&#31867;&#26862;&#26519;&#65292;&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#26500;&#24314;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#27604;&#38543;&#26426;&#26862;&#26519;&#20351;&#29992;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#26641;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#26063;&#32676;&#8212;&#8212;&#26368;&#20248;&#20998;&#31867;&#26862;&#26519;&#65292;&#21033;&#29992;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#26469;&#24471;&#20986;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25968;&#23398;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#26500;&#24314;&#32473;&#23450;&#25968;&#37327;&#30340;&#26641;&#65292;&#27599;&#20010;&#26641;&#20026;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#35266;&#27979;&#20540;&#25552;&#20379;&#19968;&#20010;&#39044;&#27979;&#31867;&#21035;&#12290;&#36890;&#36807;&#22312;&#26862;&#26519;&#20013;&#30340;&#27599;&#26869;&#26641;&#20013;&#36873;&#20986;&#34987;&#26368;&#39057;&#32321;&#39044;&#27979;&#30340;&#31867;&#21035;&#26469;&#27714;&#24471;&#20998;&#31867;&#35268;&#21017;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20844;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35745;&#31639;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26641;&#30340;&#20998;&#31867;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#21516;&#31561;&#25110;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#65292;&#27604;&#38543;&#26426;&#26862;&#26519;&#20351;&#29992;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#26641;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Optimal Classification Forests, a new family of classifiers that takes advantage of an optimal ensemble of decision trees to derive accurate and interpretable classifiers. We propose a novel mathematical optimization-based methodology in which a given number of trees are simultaneously constructed, each of them providing a predicted class for the observations in the feature space. The classification rule is derived by assigning to each observation its most frequently predicted class among the trees in the forest. We provide a mixed integer linear programming formulation for the problem. We report the results of our computational experiments, from which we conclude that our proposed method has equal or superior performance compared with state-of-the-art tree-based classification methods. More importantly, it achieves high prediction accuracy with, for example, orders of magnitude fewer trees than random forests. We also present three real-world case studies s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26497;&#31471;&#25991;&#26412;&#22270;&#20687;&#21435;&#27169;&#31946;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#21069;&#21521;&#27169;&#22411;&#20272;&#35745;&#21644;U-Net&#23454;&#29616;&#21435;&#27169;&#31946;&#65292;&#35813;&#26041;&#27861;&#22312;2021&#24180;&#36203;&#23572;&#36763;&#22522;&#21435;&#27169;&#31946;&#25361;&#25112;&#36187;&#20013;&#33719;&#32988;&#12290;</title><link>http://arxiv.org/abs/2211.10103</link><description>&lt;p&gt;
Let's Enhance&#65306;&#19968;&#31181;&#29992;&#20110;&#26497;&#31471;&#25991;&#26412;&#22270;&#20687;&#21435;&#27169;&#31946;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Let's Enhance: A Deep Learning Approach to Extreme Deblurring of Text Images. (arXiv:2211.10103v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26497;&#31471;&#25991;&#26412;&#22270;&#20687;&#21435;&#27169;&#31946;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#21069;&#21521;&#27169;&#22411;&#20272;&#35745;&#21644;U-Net&#23454;&#29616;&#21435;&#27169;&#31946;&#65292;&#35813;&#26041;&#27861;&#22312;2021&#24180;&#36203;&#23572;&#36763;&#22522;&#21435;&#27169;&#31946;&#25361;&#25112;&#36187;&#20013;&#33719;&#32988;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#27169;&#31946;&#36870;&#38382;&#39064;&#22788;&#29702;&#27969;&#31243;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#12290;&#20316;&#32773;&#30340;&#25104;&#26524;&#22522;&#20110;&#20182;&#20204;&#22312;2021&#24180;&#36203;&#23572;&#36763;&#22522;&#21435;&#27169;&#31946;&#25361;&#25112;&#36187;&#20013;&#30340;&#33719;&#32988;&#32467;&#26524;&#65292;&#35813;&#25361;&#25112;&#36187;&#26088;&#22312;&#25506;&#32034;&#26368;&#20808;&#36827;&#30340;&#21435;&#27169;&#31946;&#31639;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;&#26497;&#38480;&#12290;&#25361;&#25112;&#20219;&#21153;&#26159;&#21435;&#27169;&#31946;&#38543;&#26426;&#25991;&#26412;&#30340;&#22833;&#28966;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#35780;&#20998;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#26469;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#21069;&#21521;&#27169;&#22411;&#20272;&#35745;&#65292;&#29992;&#20110;&#25551;&#36848;&#27169;&#31946;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#20135;&#29983;&#19968;&#31995;&#21015;&#21512;&#25104;&#25968;&#25454;&#65292;&#21160;&#24577;&#29983;&#25104;&#22320;&#38754;&#23454;&#20917;&#22270;&#20687;&#21644;&#27169;&#31946;&#22270;&#20687;&#23545;&#65292;&#29992;&#20110;&#22686;&#24378;&#25361;&#25112;&#25968;&#25454;&#12290;&#23454;&#38469;&#21435;&#27169;&#31946;&#30340;&#31649;&#36947;&#21253;&#25324;&#24452;&#21521;&#36879;&#38236;&#22833;&#30495;&#30340;&#36817;&#20284;&#21453;&#28436;&#65288;&#30001;&#20272;&#35745;&#30340;&#21069;&#21521;&#27169;&#22411;&#30830;&#23450;&#65289;&#21644;U-Net&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel deep-learning-based pipeline for the inverse problem of image deblurring, leveraging augmentation and pre-training with synthetic data. Our results build on our winning submission to the recent Helsinki Deblur Challenge 2021, whose goal was to explore the limits of state-of-the-art deblurring algorithms in a real-world data setting. The task of the challenge was to deblur out-of-focus images of random text, thereby in a downstream task, maximizing an optical-character-recognition-based score function. A key step of our solution is the data-driven estimation of the physical forward model describing the blur process. This enables a stream of synthetic data, generating pairs of ground-truth and blurry images on-the-fly, which is used for an extensive augmentation of the small amount of challenge data provided. The actual deblurring pipeline consists of an approximate inversion of the radial lens distortion (determined by the estimated forward model) and a U-Net 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.08413</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#26694;&#26550;&#12289;&#36235;&#21183;&#21644;&#25361;&#25112; (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#19981;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#33258;&#38382;&#19990;&#20197;&#26469;&#65292;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#20013;&#24515;&#21270;&#23454;&#20307;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20013;&#24515;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;&#29942;&#39048;&#22686;&#21152;&#12289;&#31995;&#32479;&#25925;&#38556;&#39118;&#38505;&#22686;&#39640;&#65292;&#24433;&#21709;&#36127;&#36131;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#30340;&#23454;&#20307;&#30340;&#21487;&#20449;&#24230;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#36890;&#36807;&#25512;&#24191;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#24182;&#26368;&#23567;&#21270;&#23545;&#20013;&#24515;&#21270;&#26550;&#26500;&#30340;&#20381;&#36182;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#22312;DFL&#26041;&#38754;&#26377;&#25152;&#21162;&#21147;&#65292;&#25991;&#29486;&#36824;&#27809;&#26377;&#30740;&#31350;(i)DFL&#21644;CFL&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;;(ii)&#20998;&#26512;DFL&#26694;&#26550;&#20197;&#21019;&#24314;&#21644;&#35780;&#20272;&#26032;&#35299;&#20915;&#26041;&#26696;;(iii)&#22238;&#39038;&#20351;&#29992;DFL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22312;&#32852;&#37030;&#26550;&#26500;&#12289;&#23433;&#20840;&#24615;&#12289;&#36890;&#20449;&#31561;&#26041;&#38754;&#35782;&#21035;&#24182;&#20998;&#26512;&#20102;DFL&#30340;&#20027;&#35201;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#30340; MMD-B-Fair &#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.07907</link><description>&lt;p&gt;
&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#30340;MMD-B-Fair&#65306;&#23398;&#20064;&#20844;&#24179;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MMD-B-Fair: Learning Fair Representations with Statistical Testing. (arXiv:2211.07907v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07907
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#30340; MMD-B-Fair &#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#21452;&#26679;&#26412;&#27979;&#35797;&#23398;&#20064;&#25968;&#25454;&#20844;&#24179;&#34920;&#31034;&#30340;&#26041;&#27861;MMD-B-Fair&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#25968;&#25454;&#30340;&#31070;&#32463;&#29305;&#24449;&#65292;&#20854;&#20013;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#27979;&#35797;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#25935;&#24863;&#32452;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#20851;&#30446;&#26631;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22359;&#27979;&#35797;&#26041;&#26696;&#30340;&#31616;&#21333;&#28176;&#36817;&#24615;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#20844;&#24179;&#34920;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#29616;&#26377;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22797;&#26434;&#23545;&#25239;&#24615;&#20248;&#21270;&#25110;&#29983;&#25104;&#24314;&#27169;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20854;&#33021;&#22815;&#8220;&#38544;&#34255;&#8221;&#26377;&#20851;&#25935;&#24863;&#23646;&#24615;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;&#19979;&#28216;&#20256;&#36755;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method, MMD-B-Fair, to learn fair representations of data via kernel two-sample testing. We find neural features of our data where a maximum mean discrepancy (MMD) test cannot distinguish between representations of different sensitive groups, while preserving information about the target attributes. Minimizing the power of an MMD test is more difficult than maximizing it (as done in previous work), because the test threshold's complex behavior cannot be simply ignored. Our method exploits the simple asymptotics of block testing schemes to efficiently find fair representations without requiring complex adversarial optimization or generative modelling schemes widely used by existing work on fair representation learning. We evaluate our approach on various datasets, showing its ability to ``hide'' information about sensitive attributes, and its effectiveness in downstream transfer tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;LinkFormer&#65292;&#19968;&#31181;&#22522;&#20110;Transformer &#26550;&#26500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#36719;&#20214;&#38382;&#39064;&#21644;&#25552;&#20132;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;LinkFormer &#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#27844;&#38706;&#21644;&#21152;&#24378;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#38142;&#25509;&#24674;&#22797;&#25216;&#26415;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2211.00381</link><description>&lt;p&gt;
&#25968;&#25454;&#27844;&#38706;&#21644;&#21487;&#36801;&#31227;&#24615;&#23545;&#38382;&#39064;&#21644;&#25552;&#20132;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Data Leakage and Generalizability of Link Prediction Models for Issues and Commits. (arXiv:2211.00381v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;LinkFormer&#65292;&#19968;&#31181;&#22522;&#20110;Transformer &#26550;&#26500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#36719;&#20214;&#38382;&#39064;&#21644;&#25552;&#20132;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;LinkFormer &#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#27844;&#38706;&#21644;&#21152;&#24378;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#38142;&#25509;&#24674;&#22797;&#25216;&#26415;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#21152;&#24378;&#25991;&#26723;&#21644;&#32500;&#25252;&#23454;&#36341;&#65292;&#24320;&#21457;&#32773;&#20256;&#32479;&#19978;&#25163;&#21160;&#24314;&#31435;&#30456;&#20851;&#36719;&#20214;&#24037;&#20214;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290; &#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#24320;&#21457;&#32773;&#24120;&#24120;&#24573;&#35270;&#36825;&#31181;&#20570;&#27861;&#65292;&#23548;&#33268;&#37325;&#35201;&#20449;&#24687;&#20002;&#22833;&#12290; &#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33258;&#21160;&#38142;&#25509;&#24674;&#22797;&#25216;&#26415;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#22312;&#38543;&#26426;&#25286;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#65292;&#32780;&#23545;&#25968;&#25454;&#27844;&#38706;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#20851;&#27880;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#19981;&#20165;&#20445;&#30041;&#21644;&#25552;&#39640;&#29616;&#26377;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#22686;&#24378;&#20102;&#23427;&#20204;&#19982;&#29616;&#23454;&#29615;&#22659;&#30340;&#21305;&#37197;&#24230;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance documentation and maintenance practices, developers conventionally establish links between related software artifacts manually. Empirical research has revealed that developers frequently overlook this practice, resulting in significant information loss. To address this issue, automatic link recovery techniques have been proposed. However, these approaches primarily focused on improving prediction accuracy on randomly-split datasets, with limited attention given to the impact of data leakage and the generalizability of the predictive models. LinkFormer seeks to address these limitations. Our approach not only preserves and improves the accuracy of existing predictions but also enhances their alignment with real-world settings and their generalizability. First, to better utilize contextual information for prediction, we employ the Transformer architecture and fine-tune multiple pre-trained models on both textual and metadata information of issues and commits. Next, to gauge th
&lt;/p&gt;</description></item><item><title>GammaE&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23884;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;Gamma&#20998;&#24067;&#30340;&#32447;&#24615;&#29305;&#24615;&#21644;&#24378;&#36793;&#30028;&#25903;&#25345;&#26469;&#25429;&#25417;&#23454;&#20307;&#21644;&#26597;&#35810;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#21542;&#23450;&#21644;&#32852;&#21512;&#31639;&#31526;&#30340;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;FOL&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2210.15578</link><description>&lt;p&gt;
GammaE: &#22522;&#20110;Gamma&#20998;&#24067;&#30340;&#30693;&#35782;&#22270;&#35889;&#36923;&#36753;&#26597;&#35810;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs. (arXiv:2210.15578v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15578
&lt;/p&gt;
&lt;p&gt;
GammaE&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23884;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;Gamma&#20998;&#24067;&#30340;&#32447;&#24615;&#29305;&#24615;&#21644;&#24378;&#36793;&#30028;&#25903;&#25345;&#26469;&#25429;&#25417;&#23454;&#20307;&#21644;&#26597;&#35810;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#21542;&#23450;&#21644;&#32852;&#21512;&#31639;&#31526;&#30340;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;FOL&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#23884;&#20837;&#20197;&#36827;&#34892;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;KG&#20855;&#26377;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#26377;&#21069;&#36884;&#30340;&#24037;&#20316;&#23558;&#23454;&#20307;&#21644;&#26597;&#35810;&#25237;&#24433;&#21040;&#20960;&#20309;&#31354;&#38388;&#20013;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#31572;&#26696;&#12290;&#20294;&#26159;&#65292;&#24314;&#27169;&#21542;&#23450;&#21644;&#32852;&#21512;&#36816;&#31639;&#31526;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21542;&#23450;&#36816;&#31639;&#31526;&#27809;&#26377;&#20005;&#26684;&#30340;&#36793;&#30028;&#65292;&#36825;&#20250;&#29983;&#25104;&#37325;&#21472;&#30340;&#23884;&#20837;&#24182;&#23548;&#33268;&#33719;&#24471;&#27169;&#31946;&#30340;&#31572;&#26696;&#12290;&#21478;&#19968;&#20010;&#38480;&#21046;&#26159;&#24182;&#38598;&#36816;&#31639;&#31526;&#26159;&#38750;&#38381;&#21512;&#30340;&#65292;&#36825;&#21066;&#24369;&#20102;&#27169;&#22411;&#22788;&#29702;&#19968;&#31995;&#21015;&#24182;&#38598;&#36816;&#31639;&#31526;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#29575;&#23884;&#20837;&#27169;&#22411;&#65292;&#21363;Gamma Embeddings&#65288;GammaE&#65289;&#65292;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#21644;&#26597;&#35810;&#20197;&#22238;&#31572;KG&#19978;&#19981;&#21516;&#31867;&#22411;&#30340;FOL&#26597;&#35810;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;Gamma&#20998;&#24067;&#30340;&#32447;&#24615;&#29305;&#24615;&#21644;&#24378;&#36793;&#30028;&#25903;&#25345;&#26469;&#25429;&#25417;&#23454;&#20307;&#21644;&#26597;&#35810;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;GammaE&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21542;&#23450;&#21644;&#32852;&#21512;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model the negation and union operator. The negation operator has no strict boundaries, which generates overlapped embeddings and leads to obtaining ambiguous answers. An additional limitation is that the union operator is non-closure, which undermines the model to handle a series of union operators. To address these problems, we propose a novel probabilistic embedding model, namely Gamma Embeddings (GammaE), for encoding entities and queries to answer different types of FOL queries on KGs. We utilize the linear property and strong boundary support of the Gamma distribution to capture more features of entities and queries, which dramatically reduces model uncertainty. Furthermore, GammaE implements
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#24230;&#31227;&#21160;&#30340;&#36830;&#25509;&#36710;&#36742;&#19979;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#21033;&#29992;&#23616;&#37096;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#30340;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36890;&#36807;&#26435;&#37325;&#32452;&#21512;&#21644;&#23376;&#38598;&#36873;&#25321;&#26469;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#24182;&#26368;&#22823;&#21270;&#25104;&#21151;&#25509;&#25910;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15496</link><description>&lt;p&gt;
&#24102;&#36164;&#28304;&#32422;&#26463;&#30340;&#39640;&#24230;&#31227;&#21160;&#36830;&#25509;&#36710;&#36742;&#19979;&#30340;&#36710;&#32852;&#32593;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource Constrained Vehicular Edge Federated Learning with Highly Mobile Connected Vehicles. (arXiv:2210.15496v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#24230;&#31227;&#21160;&#30340;&#36830;&#25509;&#36710;&#36742;&#19979;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#21033;&#29992;&#23616;&#37096;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#30340;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36890;&#36807;&#26435;&#37325;&#32452;&#21512;&#21644;&#23376;&#38598;&#36873;&#25321;&#26469;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#24182;&#26368;&#22823;&#21270;&#25104;&#21151;&#25509;&#25910;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36710;&#32852;&#32593;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#65288;VEFL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#36793;&#32536;&#26381;&#21153;&#22120;&#21033;&#29992;&#39640;&#24230;&#31227;&#21160;&#30340;&#36830;&#25509;&#36710;&#36742;&#65288;CV&#65289;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#21644;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#26469;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#25910;&#25947;&#20998;&#26512;&#34920;&#26126;&#65292;VEFL&#35757;&#32451;&#25439;&#22833;&#21462;&#20915;&#20110;&#25104;&#21151;&#25509;&#25910;CV&#36890;&#36807;&#38388;&#27463;&#24615;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#65288;V2I&#65289;&#26080;&#32447;&#38142;&#36335;&#20256;&#36755;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#30001;&#20110;&#39640;&#24230;&#31227;&#21160;&#24615;&#65292;&#22312;&#20840;&#35774;&#22791;&#21442;&#19982;&#24773;&#20917;&#65288;FDPC&#65289;&#19979;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#26681;&#25454;CV&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#36887;&#30041;&#26102;&#38388;&#30340;&#21152;&#26435;&#32452;&#21512;&#32858;&#21512;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#22312;&#37096;&#20998;&#35774;&#22791;&#21442;&#19982;&#24773;&#20917;&#65288;PDPC&#65289;&#19979;&#36873;&#25321;CV&#30340;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#24310;&#36831;&#12289;&#33021;&#37327;&#21644;&#25104;&#26412;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#32852;&#21512;VEFL&#21644;&#26080;&#32447;&#25509;&#20837;&#25216;&#26415;&#65288;RAT&#65289;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#25509;&#25910;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#27010;&#29575;&#12290;&#32771;&#34385;&#21040;&#20248;&#21270;&#38382;&#39064;&#26159;NP-hard&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#24453;&#35299;&#20915;&#30340;&#23376;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a vehicular edge federated learning (VEFL) solution, where an edge server leverages highly mobile connected vehicles' (CVs') onboard central processing units (CPUs) and local datasets to train a global model. Convergence analysis reveals that the VEFL training loss depends on the successful receptions of the CVs' trained models over the intermittent vehicle-to-infrastructure (V2I) wireless links. Owing to high mobility, in the full device participation case (FDPC), the edge server aggregates client model parameters based on a weighted combination according to the CVs' dataset sizes and sojourn periods, while it selects a subset of CVs in the partial device participation case (PDPC). We then devise joint VEFL and radio access technology (RAT) parameters optimization problems under delay, energy and cost constraints to maximize the probability of successful reception of the locally trained models. Considering that the optimization problem is NP-hard, we decompose it i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#26657;&#20934;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#32467;&#26524;&#21457;&#29616;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#20855;&#26377;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#65292;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2210.12760</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A study of uncertainty quantification in overparametrized high-dimensional models. (arXiv:2210.12760v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#26657;&#20934;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#32467;&#26524;&#21457;&#29616;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#20855;&#26377;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#65292;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#21487;&#38752;&#21644;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#65292;&#26420;&#32032;&#30340;&#24230;&#37327;&#26041;&#27861;(&#22914;&#26368;&#21518;&#19968;&#23618;&#20998;&#25968;)&#24050;&#32463;&#34987;&#24191;&#20026;&#20154;&#30693;&#22320;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#28201;&#24230;&#32553;&#25918;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#36125;&#21494;&#26031;&#22788;&#29702;&#65292;&#20197;&#32531;&#35299;&#36807;&#24230;&#33258;&#20449;&#65292;&#36890;&#24120;&#36890;&#36807;&#25968;&#20540;&#35266;&#23519;&#25903;&#25345;&#23427;&#20204;&#20135;&#29983;&#26356;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#25968;&#23398;&#21487;&#22788;&#29702;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#24120;&#35265;&#19981;&#30830;&#23450;&#24230;&#37327;&#20043;&#38388;&#30340;&#23574;&#38160;&#27604;&#36739;&#65306;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25259;&#38706;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#19982;&#36807;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#30340;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#12290;&#36825;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#30340;&#26657;&#20934;&#26159;&#33391;&#22909;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#27604;&#36235;&#21183;&#20272;&#35745; (CTE) &#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#36890;&#29992;&#38544;&#21547;&#21333;&#35843;&#36235;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#26102;&#38388;&#25968;&#25454;&#65292;&#24182;&#36991;&#20813;&#20102;&#29992;&#20110;&#21333;&#35843;&#36235;&#21183;&#35782;&#21035;&#30340;&#26631;&#20934;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2210.09817</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#36890;&#29992;&#38544;&#21547;&#21333;&#35843;&#36235;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Universal hidden monotonic trend estimation with contrastive learning. (arXiv:2210.09817v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#27604;&#36235;&#21183;&#20272;&#35745; (CTE) &#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#36890;&#29992;&#38544;&#21547;&#21333;&#35843;&#36235;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#26102;&#38388;&#25968;&#25454;&#65292;&#24182;&#36991;&#20813;&#20102;&#29992;&#20110;&#21333;&#35843;&#36235;&#21183;&#35782;&#21035;&#30340;&#26631;&#20934;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#21333;&#35843;&#36235;&#21183;&#22240;&#32032;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982; Mann-Kendall &#27979;&#35797;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#36235;&#21183;&#20272;&#35745; (CTE)&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; CTE &#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#20219;&#20309;&#38544;&#34255;&#22312;&#26102;&#38388;&#25968;&#25454;&#20013;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#29992;&#20110;&#21333;&#35843;&#36235;&#21183;&#35782;&#21035;&#30340;&#26631;&#20934;&#20551;&#35774;&#12290;&#29305;&#21035;&#22320;&#65292;CTE &#21487;&#20197;&#23558;&#20219;&#20309;&#31867;&#22411;&#30340;&#26102;&#38388;&#25968;&#25454; (&#21521;&#37327;&#12289;&#22270;&#20687;&#12289;&#22270;&#12289;&#26102;&#38388;&#24207;&#21015;&#31561;) &#20316;&#20026;&#36755;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#21644;&#38382;&#39064;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340; CTE &#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe a universal method for extracting the underlying monotonic trend factor from time series data. We propose an approach related to the Mann-Kendall test, a standard monotonic trend detection method and call it contrastive trend estimation (CTE). We show that the CTE method identifies any hidden trend underlying temporal data while avoiding the standard assumptions used for monotonic trend identification. In particular, CTE can take any type of temporal data (vector, images, graphs, time series, etc.) as input. We finally illustrate the interest of our CTE method through several experiments on different types of data and problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35266;&#27979;&#31354;&#38388;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#39640;&#25928;&#23398;&#20064;&#65292;&#21487;&#20197;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2210.08065</link><description>&lt;p&gt;
Just Round&#65306;&#37327;&#21270;&#35266;&#27979;&#31354;&#38388;&#23454;&#29616;&#21160;&#24577;&#36816;&#21160;&#30340;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion. (arXiv:2210.08065v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35266;&#27979;&#31354;&#38388;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#39640;&#25928;&#23398;&#20064;&#65292;&#21487;&#20197;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26159;&#21512;&#25104;&#22797;&#26434;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#26368;&#24378;&#22823;&#24037;&#20855;&#20043;&#19968;&#12290;&#20294;&#35757;&#32451;DRL&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#65292;&#38656;&#35201;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#25165;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#23545;&#20110;&#19979;&#19968;&#20195;&#38656;&#35201;&#22312;&#36793;&#32536;&#19978;&#23398;&#20064;&#20197;&#36866;&#24212;&#20854;&#29615;&#22659;&#30340;&#29616;&#22330;&#26426;&#22120;&#20154;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#35266;&#27979;&#31354;&#38388;&#37327;&#21270;&#26469;&#24320;&#22987;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#27169;&#25311;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#21644;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#65292;&#22312;&#19981;&#24433;&#21709;&#23398;&#20064;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#29616;&#35266;&#27979;&#31354;&#38388;&#37327;&#21270;&#21487;&#20197;&#23558;&#25972;&#20307;&#20869;&#23384;&#25104;&#26412;&#38477;&#20302;&#22810;&#36798;4.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) is one of the most powerful tools for synthesizing complex robotic behaviors. But training DRL models is incredibly compute and memory intensive, requiring large training datasets and replay buffers to achieve performant results. This poses a challenge for the next generation of field robots that will need to learn on the edge to adapt to their environment. In this paper, we begin to address this issue through observation space quantization. We evaluate our approach using four simulated robot locomotion tasks and two state-of-the-art DRL algorithms, the on-policy Proximal Policy Optimization (PPO) and off-policy Soft Actor-Critic (SAC) and find that observation space quantization reduces overall memory costs by as much as 4.2x without impacting learning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GReaT&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#39640;&#24230;&#30495;&#23454;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#33410;&#20219;&#24847;&#23376;&#38598;&#29305;&#24449;&#24314;&#27169;&#34920;&#26684;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2210.06280</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#30495;&#23454;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models are Realistic Tabular Data Generators. (arXiv:2210.06280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GReaT&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#39640;&#24230;&#30495;&#23454;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#33410;&#20219;&#24847;&#23376;&#38598;&#29305;&#24449;&#24314;&#27169;&#34920;&#26684;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26368;&#21476;&#32769;&#21644;&#26368;&#26222;&#36941;&#30340;&#25968;&#25454;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#20855;&#26377;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#26679;&#26412;&#20173;&#28982;&#26159;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#35768;&#22810;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#65292;&#20294;&#36739;&#23569;&#30340;&#30740;&#31350;&#26041;&#21521;&#38024;&#23545;&#36817;&#26399;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#20204;&#20063;&#20855;&#26377;&#29983;&#25104;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GReaT&#65288;&#30495;&#23454;&#34920;&#26684;&#25968;&#25454;&#30340;&#29983;&#25104;&#65289;&#65292;&#23427;&#21033;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;LLM&#26469;&#37319;&#26679;&#21512;&#25104;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24863;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;GReaT&#21487;&#20197;&#36890;&#36807;&#23545;&#20219;&#24847;&#23376;&#38598;&#29305;&#24449;&#36827;&#34892;&#35843;&#33410;&#26469;&#24314;&#27169;&#34920;&#26684;&#25968;&#25454;&#20998;&#24067;&#65307;&#20854;&#20313;&#29305;&#24449;&#21017;&#26080;&#38656;&#39069;&#22806;&#24320;&#38144;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#37327;&#21270;&#25152;&#20135;&#29983;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data 
&lt;/p&gt;</description></item><item><title>Alt-Diff&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#38597;&#21487;&#27604;&#30697;&#38453;&#36827;&#34892;&#26114;&#36149;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24555;&#36895;&#21644;&#36882;&#24402;&#30340;&#26041;&#24335;&#24494;&#20998;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#38544;&#24335;&#24494;&#20998;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.01802</link><description>&lt;p&gt;
&#20248;&#21270;&#23618;&#30340;&#20132;&#26367;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Alternating Differentiation for Optimization Layers. (arXiv:2210.01802v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01802
&lt;/p&gt;
&lt;p&gt;
Alt-Diff&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#38597;&#21487;&#27604;&#30697;&#38453;&#36827;&#34892;&#26114;&#36149;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24555;&#36895;&#21644;&#36882;&#24402;&#30340;&#26041;&#24335;&#24494;&#20998;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#38544;&#24335;&#24494;&#20998;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20248;&#21270;&#38382;&#39064;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20248;&#21270;&#23618;&#20197;&#32534;&#30721;&#32422;&#26463;&#21644;&#24402;&#32435;&#20808;&#39564;&#30340;&#24819;&#27861;&#22312;&#36817;&#24180;&#26469;&#24050;&#32463;&#28145;&#20837;&#20154;&#24515;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#20197;&#19968;&#31181;&#38656;&#35201;&#22312;&#38597;&#21487;&#27604;&#30697;&#38453;&#19978;&#36827;&#34892;&#26114;&#36149;&#35745;&#31639;&#30340;&#26041;&#24335;&#38544;&#24335;&#24494;&#20998;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#19978;&#65292;&#36825;&#21487;&#33021;&#26159;&#24930;&#21644;&#20869;&#23384;&#23494;&#38598;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#26367;&#24494;&#20998;&#65288;Alt-Diff&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#24555;&#36895;&#19988;&#36882;&#24402;&#30340;&#26041;&#24335;&#24494;&#20998;&#20248;&#21270;&#38382;&#39064;&#65288;&#36825;&#37324;&#29305;&#21035;&#25351;&#24102;&#26377;&#22810;&#38754;&#20307;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65289;&#12290;Alt-Diff&#23558;&#24494;&#20998;&#36807;&#31243;&#20998;&#35299;&#20026;&#20027;&#38382;&#39064;&#26356;&#26032;&#21644;&#23545;&#20598;&#38382;&#39064;&#26356;&#26032;&#30340;&#20132;&#26367;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;Alt-Diff&#23588;&#20854;&#33021;&#22815;&#20943;&#23567;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#32500;&#24230;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20855;&#26377;&#22823;&#35268;&#27169;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38544;&#24335;&#24494;&#20998;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;Alt-Diff&#33719;&#24471;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
The idea of embedding optimization problems into deep neural networks as optimization layers to encode constraints and inductive priors has taken hold in recent years. Most existing methods focus on implicitly differentiating Karush-Kuhn-Tucker (KKT) conditions in a way that requires expensive computations on the Jacobian matrix, which can be slow and memory-intensive. In this paper, we developed a new framework, named Alternating Differentiation (Alt-Diff), that differentiates optimization problems (here, specifically in the form of convex optimization problems with polyhedral constraints) in a fast and recursive way. Alt-Diff decouples the differentiation procedure into a primal update and a dual update in an alternating way. Accordingly, Alt-Diff substantially decreases the dimensions of the Jacobian matrix especially for optimization with large-scale constraints and thus increases the computational speed of implicit differentiation. We show that the gradients obtained by Alt-Diff a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#37325;&#23614;&#25439;&#22833;&#24773;&#20917;&#19979;&#30340;PAC-Bayes&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#20026;&#19981;&#21516;&#30340;PAC-Bayesian&#26694;&#26550;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2210.00928</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#39532;&#27663;&#36807;&#31243;&#25512;&#23548;&#37325;&#23614;&#25439;&#22833;&#30340;PAC-Bayes&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales. (arXiv:2210.00928v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#37325;&#23614;&#25439;&#22833;&#24773;&#20917;&#19979;&#30340;PAC-Bayes&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#20026;&#19981;&#21516;&#30340;PAC-Bayesian&#26694;&#26550;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;PAC-Bayes&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#36731;&#23614;&#25439;&#22833;&#65288;&#20363;&#22914;&#20122;&#39640;&#26031;&#25110;&#20122;&#25351;&#25968;&#65289;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20294;&#20854;&#22312;&#37325;&#23614;&#25439;&#22833;&#24773;&#20917;&#19979;&#30340;&#25512;&#24191;&#20173;&#28982;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#22312;&#20551;&#23450;&#25439;&#22833;&#20989;&#25968;&#26377;&#30028;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#37325;&#23614;&#25439;&#22833;&#25552;&#20379;&#20102;PAC-Bayes&#27867;&#21270;&#30028;&#12290;&#22312;&#35813;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;\citet{kuzborskij2019efron}&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#21033;&#29992;&#36229;&#39532;&#27663;&#36807;&#31243;&#30340;&#39532;&#23572;&#31185;&#22827;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#26415;&#36890;&#36807;&#20026;&#26080;&#30028;&#38789;&#25552;&#20379;&#30028;&#38480;&#65292;&#20197;&#21450;&#20026;&#37325;&#23614;&#25439;&#22833;&#30340;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#23398;&#20064;&#25552;&#20379;&#30028;&#38480;&#65292;&#32479;&#19968;&#21644;&#25193;&#23637;&#20102;&#19981;&#21516;&#30340;PAC-Bayesian&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
While PAC-Bayes is now an established learning framework for light-tailed losses (\emph{e.g.}, subgaussian or subexponential), its extension to the case of heavy-tailed losses remains largely uncharted and has attracted a growing interest in recent years. We contribute PAC-Bayes generalisation bounds for heavy-tailed losses under the sole assumption of bounded variance of the loss function. Under that assumption, we extend previous results from \citet{kuzborskij2019efron}. Our key technical contribution is exploiting an extention of Markov's inequality for supermartingales. Our proof technique unifies and extends different PAC-Bayesian frameworks by providing bounds for unbounded martingales as well as bounds for batch and online learning with heavy-tailed losses.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20123;&#20989;&#25968;&#31867;&#30340;Natarajan&#32500;&#24230;&#19978;&#30028;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#25551;&#36848;&#26576;&#20123;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.07015</link><description>&lt;p&gt;
&#26576;&#20123;&#20989;&#25968;&#31867;&#30340;Natarajan&#32500;&#25968;&#30340;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Upper bounds on the Natarajan dimensions of some function classes. (arXiv:2209.07015v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20123;&#20989;&#25968;&#31867;&#30340;Natarajan&#32500;&#24230;&#19978;&#30028;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#25551;&#36848;&#26576;&#20123;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Natarajan&#32500;&#24230;&#26159;&#34920;&#24449;&#22810;&#31867;PAC&#21487;&#23398;&#20064;&#24615;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#23558;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#20174;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20123;&#20989;&#25968;&#31867;&#30340;Natarajan&#32500;&#24230;&#19978;&#30028;&#65292;&#21253;&#25324;&#65288;i&#65289;&#22810;&#31867;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20108;&#36827;&#21046;&#12289;&#32447;&#24615;&#21644;ReLU&#28608;&#27963;&#30340;&#22810;&#31867;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#33021;&#23545;&#25551;&#36848;&#26576;&#20123;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natarajan dimension is a fundamental tool for characterizing multi-class PAC learnability, generalizing the Vapnik-Chervonenkis (VC) dimension from binary to multi-class classification problems. This work establishes upper bounds on Natarajan dimensions for certain function classes, including (i) multi-class decision tree and random forests, and (ii) multi-class neural networks with binary, linear and ReLU activations. These results may be relevant for describing the performance of certain multi-class learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;GPU&#19978;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#21327;&#21516;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21327;&#21516;&#22810;&#20010;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#26174;&#30528;&#30340;&#25928;&#30410;&#12290;MIG&#25216;&#26415;&#36890;&#36807;&#23558;GPU&#20998;&#21306;&#65292;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#38656;&#35201;&#23436;&#25972;GPU&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#20855;&#26377;&#20248;&#21270;&#30340;&#25918;&#32622;&#31574;&#30053;&#30340;MIG&#19968;&#33324;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.06018</link><description>&lt;p&gt;
GPU&#19978;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#21327;&#21516;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Collocation on GPUs for Deep Learning Training. (arXiv:2209.06018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;GPU&#19978;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#21327;&#21516;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21327;&#21516;&#22810;&#20010;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#26174;&#30528;&#30340;&#25928;&#30410;&#12290;MIG&#25216;&#26415;&#36890;&#36807;&#23558;GPU&#20998;&#21306;&#65292;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#38656;&#35201;&#23436;&#25972;GPU&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#20855;&#26377;&#20248;&#21270;&#30340;&#25918;&#32622;&#31574;&#30053;&#30340;MIG&#19968;&#33324;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#20351;&#29992;GPU&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#27169;&#22411;&#35757;&#32451;&#37117;&#33021;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#24378;&#22823;&#30340;GPU&#12290;&#22810;&#23454;&#20363;GPU&#65288;MIG&#65289;&#26159;NVIDIA&#24341;&#20837;&#30340;&#19968;&#39033;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#21306;GPU&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#38656;&#35201;&#23436;&#25972;GPU&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21253;&#21547;&#21508;&#31181;&#22823;&#23567;&#21644;&#27169;&#22411;&#32452;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#19979;&#65292;MIG&#21551;&#29992;&#30340;A100 GPU&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;MIG&#30340;&#20248;&#21183;&#19982;&#26087;&#30340;GPU&#24037;&#20316;&#36127;&#36733;&#21327;&#21516;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65306;&#22312;&#21516;&#19968;GPU&#19978;&#30452;&#25509;&#25552;&#20132;&#22810;&#20010;&#36827;&#31243;&#21644;&#20351;&#29992;&#22810;&#36827;&#31243;&#26381;&#21153;&#65288;MPS&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21327;&#21516;&#22810;&#20010;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#26174;&#30528;&#30340;&#25928;&#30410;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;epoch&#26102;&#38388;&#22686;&#21152;&#65292;&#35757;&#32451;&#21534;&#21520;&#37327;&#20063;&#21487;&#33021;&#22686;&#21152;&#20102;&#22235;&#20493;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21516;&#26102;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#32858;&#21512;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#38656;&#27714;&#24517;&#39035;&#31526;&#21512;GPU&#30340;&#21487;&#29992;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;MIG&#25918;&#32622;&#31574;&#30053;&#22312;&#20998;&#37197;&#27169;&#22411;&#35757;&#32451;&#36164;&#28304;&#26102;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20855;&#26377;&#20248;&#21270;&#25918;&#32622;&#31574;&#30053;&#30340;MIG&#19968;&#33324;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning training is an expensive process that extensively uses GPUs, but not all model training saturates modern powerful GPUs. Multi-Instance GPU (MIG) is a new technology introduced by NVIDIA that can partition a GPU to better-fit workloads that do not require all the memory and compute resources of a full GPU. In this paper, we examine the performance of a MIG-enabled A100 GPU under deep learning workloads containing various sizes and combinations of models. We contrast the benefits of MIG to older workload collocation methods on GPUs: na\"ively submitting multiple processes on the same GPU and utilizing Multi-Process Service (MPS). Our results demonstrate that collocating multiple model training runs may yield significant benefits. In certain cases, it can lead up to four times training throughput despite increased epoch time. On the other hand, the aggregate memory footprint and compute needs of the models trained in parallel must fit the available memory and compute resourc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;Tetris&#65292;&#24182;&#25552;&#20986;&#20102;&#27010;&#24565;&#25552;&#31034;&#21644;&#38754;&#21521;&#33050;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#36755;&#20837;&#21253;&#25324;&#29992;&#25143;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.00068</link><description>&lt;p&gt;
&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#27010;&#24565;&#30693;&#35782;&#32435;&#20837;&#33050;&#26412;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Task-specific Concept Knowledge into Script Learning. (arXiv:2209.00068v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;Tetris&#65292;&#24182;&#25552;&#20986;&#20102;&#27010;&#24565;&#25552;&#31034;&#21644;&#38754;&#21521;&#33050;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#36755;&#20837;&#21253;&#25324;&#29992;&#25143;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Tetris&#30340;&#26032;&#20219;&#21153;&#65292;&#21517;&#20026;&#30446;&#26631;&#23548;&#21521;&#33050;&#26412;&#23436;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#26222;&#36941;&#30340;&#22330;&#26223;&#65292;&#36755;&#20837;&#19981;&#20165;&#21253;&#25324;&#30446;&#26631;&#65292;&#36824;&#21253;&#25324;&#29992;&#25143;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#21916;&#22909;&#21644;&#21382;&#21490;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#65306;&#27010;&#24565;&#25552;&#31034;&#21644;&#38754;&#21521;&#33050;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#27493;&#39588;&#37325;&#22797;&#21644;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#22522;&#20110;WikiHow&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#25968;&#25454;&#38598;&#12289;&#20179;&#24211;&#21644;&#27169;&#22411;&#23558;&#20844;&#24320;&#25552;&#20379;&#65292;&#20197;&#20419;&#36827;&#23545;&#36825;&#20010;&#26032;&#20219;&#21153;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Tetris, a new task of Goal-Oriented Script Completion. Unlike previous work, it considers a more realistic and general setting, where the input includes not only the goal but also additional user context, including preferences and history. To address this problem, we propose a novel approach, which uses two techniques to improve performance: (1) concept prompting, and (2) script-oriented contrastive learning that addresses step repetition and hallucination problems. On our WikiHow-based dataset, we find that both methods improve performance. The dataset, repository, and models will be publicly available to facilitate further research on this new task.
&lt;/p&gt;</description></item><item><title>DSL&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#23398;&#20064;&#20013;&#33258;&#21160;&#21457;&#29616;&#26377;&#24847;&#20041;&#31526;&#21495;&#35268;&#21017;&#30340;NeSy&#31995;&#32479;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.11561</link><description>&lt;p&gt;
&#28145;&#24230;&#31526;&#21495;&#23398;&#20064;&#65306;&#20174;&#24863;&#30693;&#20013;&#21457;&#29616;&#31526;&#21495;&#21644;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions. (arXiv:2208.11561v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11561
&lt;/p&gt;
&lt;p&gt;
DSL&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#23398;&#20064;&#20013;&#33258;&#21160;&#21457;&#29616;&#26377;&#24847;&#20041;&#31526;&#21495;&#35268;&#21017;&#30340;NeSy&#31995;&#32479;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#65288;NeSy&#65289;&#38598;&#25104;&#23558;&#31526;&#21495;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#38656;&#35201;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;NeSy&#31995;&#32479;&#20381;&#36182;&#20110;&#36923;&#36753;&#30693;&#35782;&#30340;&#36830;&#32493;&#25918;&#26494;&#65292;&#24182;&#19988;&#27169;&#22411;&#31649;&#36947;&#20869;&#19981;&#20570;&#20986;&#31163;&#25955;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#23450;&#32473;&#23450;&#31526;&#21495;&#35268;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31526;&#21495;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#19968;&#31181;NeSy&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;NeSy&#20989;&#25968;&#65292;&#21363;&#23558;&#36830;&#32493;&#25968;&#25454;&#26144;&#23556;&#21040;&#31163;&#25955;&#31526;&#21495;&#30340;&#65288;&#19968;&#32452;&#65289;&#24863;&#30693;&#20989;&#25968;&#30340;&#32452;&#21512;&#21644;&#31526;&#21495;&#20989;&#25968;&#12290;DSL&#21516;&#26102;&#23398;&#20064;&#24863;&#30693;&#21644;&#31526;&#21495;&#20989;&#25968;&#65292;&#20165;&#22312;&#23427;&#20204;&#30340;&#32452;&#21512;&#65288;NeSy&#20989;&#25968;&#65289;&#19978;&#36827;&#34892;&#22521;&#35757;&#12290;DSL&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#65292;&#23427;&#21487;&#20197;&#22312;&#21487;&#24494;&#30340;NN&#23398;&#20064;&#31649;&#36947;&#20869;&#21019;&#24314;&#20869;&#37096;&#65288;&#21487;&#35299;&#37322;&#30340;&#65289;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#24863;&#30693;&#36755;&#20837;&#12290;&#21019;&#24314;&#30340;&#31526;&#21495;&#26159;&#33258;&#21160;&#36873;&#25321;&#30340;&#65292;&#20197;&#29983;&#25104;&#26368;&#22909;&#35299;&#37322;&#25968;&#25454;&#30340;&#31526;&#21495;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;DSL&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23398;&#20064;&#31526;&#21495;&#35268;&#21017;&#21644;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems rely on continuous relaxation of logical knowledge, and no discrete decisions are made within the model pipeline. Furthermore, these methods assume that the symbolic rules are given. In this paper, we propose Deep Symbolic Learning (DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols. DSL learns simultaneously the perception and symbolic functions while being trained only on their composition (NeSy-function). The key novelty of DSL is that it can create internal (interpretable) symbolic representations and map them to perception inputs within a differentiable NN learning pipeline. The created symbols are automatically selected to generate symbolic functions that best explain the data. We pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#20363;&#23376;&#20998;&#23376;&#26469;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32473;&#23450;&#35774;&#35745;&#26465;&#20214;&#30340;&#20998;&#23376;&#12290;&#24182;&#19988;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#30446;&#26631;&#20989;&#25968;&#35757;&#32451;&#26816;&#32034;&#26426;&#21046;&#65292;&#26469;&#25552;&#39640;&#29983;&#25104;&#30340;&#20998;&#23376;&#30340;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11126</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#21487;&#25511;&#21270;&#21512;&#29289;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Controllable Molecule Generation. (arXiv:2208.11126v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#20363;&#23376;&#20998;&#23376;&#26469;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32473;&#23450;&#35774;&#35745;&#26465;&#20214;&#30340;&#20998;&#23376;&#12290;&#24182;&#19988;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#30446;&#26631;&#20989;&#25968;&#35757;&#32451;&#26816;&#32034;&#26426;&#21046;&#65292;&#26469;&#25552;&#39640;&#29983;&#25104;&#30340;&#20998;&#23376;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#21270;&#23398;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26032;&#20998;&#23376;&#24050;&#25104;&#20026;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;/&#24494;&#35843;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#36890;&#24120;&#19981;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#25511;&#21270;&#21512;&#29289;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#23567;&#32452;&#20363;&#23376;&#20998;&#23376;&#26469;&#24341;&#23548;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#28385;&#36275;&#32473;&#23450;&#35774;&#35745;&#26465;&#20214;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26816;&#32034;&#26426;&#21046;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#20998;&#23376;&#30340;&#26368;&#36817;&#37051;&#26469;&#35757;&#32451;&#24182;&#34701;&#21512;&#20363;&#23376;&#20998;&#23376;&#21644;&#36755;&#20837;&#20998;&#23376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#21160;&#24577;&#26356;&#26032;&#29983;&#25104;&#30340;&#20998;&#23376;&#21644;&#26816;&#32034;&#25968;&#25454;&#24211;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating new molecules with specified chemical and biological properties via generative models has emerged as a promising direction for drug discovery. However, existing methods require extensive training/fine-tuning with a large dataset, often unavailable in real-world generation tasks. In this work, we propose a new retrieval-based framework for controllable molecule generation. We use a small set of exemplar molecules, i.e., those that (partially) satisfy the design criteria, to steer the pre-trained generative model towards synthesizing molecules that satisfy the given design criteria. We design a retrieval mechanism that retrieves and fuses the exemplar molecules with the input molecule, which is trained by a new self-supervised objective that predicts the nearest neighbor of the input molecule. We also propose an iterative refinement process to dynamically update the generated molecules and retrieval database for better generalization. Our approach is agnostic to the choice of 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;DDD&#36712;&#36857;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26367;&#20195;&#32791;&#26102;&#30340;&#20301;&#38169;&#36816;&#21160;&#26102;&#38388;&#31215;&#20998;&#30340;DDD-GNN&#26694;&#26550;&#65292;&#26377;&#25928;&#21152;&#36895;&#20171;&#35266;&#23610;&#24230;&#19979;&#26230;&#20307;&#26448;&#26009;&#22609;&#24615;&#34892;&#20026;&#30340;&#35745;&#31639;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2208.03296</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#31163;&#25955;&#20301;&#38169;&#21160;&#21147;&#23398;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Accelerating discrete dislocation dynamics simulations with graph neural networks. (arXiv:2208.03296v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03296
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;DDD&#36712;&#36857;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26367;&#20195;&#32791;&#26102;&#30340;&#20301;&#38169;&#36816;&#21160;&#26102;&#38388;&#31215;&#20998;&#30340;DDD-GNN&#26694;&#26550;&#65292;&#26377;&#25928;&#21152;&#36895;&#20171;&#35266;&#23610;&#24230;&#19979;&#26230;&#20307;&#26448;&#26009;&#22609;&#24615;&#34892;&#20026;&#30340;&#35745;&#31639;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20301;&#38169;&#21160;&#21147;&#23398;&#65288;DDD&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#30740;&#31350;&#20171;&#35266;&#23610;&#24230;&#19979;&#26230;&#20307;&#26448;&#26009;&#22609;&#24615;&#34892;&#20026;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#23558;&#20301;&#38169;&#32447;&#30340;&#36816;&#21160;&#19982;&#23439;&#35266;&#21709;&#24212;&#30456;&#36830;&#12290;&#28982;&#32780;&#65292;DDD&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#38480;&#21046;&#20854;&#36866;&#29992;&#33539;&#22260;&#30340;&#29942;&#39048;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;DDD-GNN&#26694;&#26550;&#65292;&#20854;&#20013;&#32791;&#26102;&#30340;&#20301;&#38169;&#36816;&#21160;&#26102;&#38388;&#31215;&#20998;&#23436;&#20840;&#30001;&#22522;&#20110;DDD&#36712;&#36857;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#26367;&#20195;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19968;&#31181;&#31616;&#21333;&#20294;&#30456;&#20851;&#30340;&#20301;&#38169;&#32447;&#22312;&#38556;&#30861;&#29289;&#38453;&#21015;&#19978;&#28369;&#21160;&#30340;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DDD-GNN&#27169;&#22411;&#22312;&#19968;&#23450;&#30340;&#24212;&#21464;&#36895;&#29575;&#21644;&#38556;&#30861;&#29289;&#23494;&#24230;&#33539;&#22260;&#20869;&#31283;&#23450;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#37325;&#29616;&#20102;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;DDD&#27169;&#25311;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#22312;&#26102;&#38388;&#31215;&#20998;&#26399;&#38388;&#26126;&#30830;&#35745;&#31639;&#33410;&#28857;&#21147;&#25110;&#20301;&#38169;&#36801;&#31227;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32435;&#20837;&#20171;&#35266;&#23610;&#24230;&#19979;&#21147;&#23398;&#34892;&#20026;&#30340;&#24314;&#27169;&#21644;&#20223;&#30495;&#25171;&#24320;&#20102;&#26032;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete dislocation dynamics (DDD) is a widely employed computational method to study plasticity at the mesoscale that connects the motion of dislocation lines to the macroscopic response of crystalline materials. However, the computational cost of DDD simulations remains a bottleneck that limits its range of applicability. Here, we introduce a new DDD-GNN framework in which the expensive time-integration of dislocation motion is entirely substituted by a graph neural network (GNN) model trained on DDD trajectories. As a first application, we demonstrate the feasibility and potential of our method on a simple yet relevant model of a dislocation line gliding through an array of obstacles. We show that the DDD-GNN model is stable and reproduces very well unseen ground-truth DDD simulation responses for a range of straining rates and obstacle densities, without the need to explicitly compute nodal forces or dislocation mobilities during time-integration. Our approach opens new promising 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.02814</link><description>&lt;p&gt;
&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Risk Control. (arXiv:2208.02814v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#25512;&#24191;&#33267;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;&#35813;&#31639;&#27861;&#23558;&#20998;&#35010;&#31526;&#21512;&#24615;&#39044;&#27979;&#21450;&#20854;&#35206;&#30422;&#20445;&#35777;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#31867;&#20284;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#22312;$\mathcal{O}(1/n)$&#22240;&#23376;&#20869;&#20445;&#25345;&#32039;&#23494;&#24615;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.12261</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20849;&#24773;&#24515;&#29702;&#30340;&#26381;&#21153;&#12290;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#32531;&#35299;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20851;&#31995;&#24314;&#27169;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#21462;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#36328;&#27169;&#24577;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;MMGCN&#65289;&#30452;&#25509;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#20887;&#20313;&#20449;&#24687;&#65292;&#19988;&#21487;&#33021;&#20002;&#22833;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#65288;GraphCFC&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#21644;&#20114;&#21160;&#20449;&#24687;&#12290;GraphCFC&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23376;&#31354;&#38388;&#25552;&#21462;&#22120;&#21644;&#25104;&#23545;&#36328;&#27169;&#24577;&#34917;&#20805;&#65288;PairCC&#65289;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;OOD&#25506;&#27979;&#22120;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#20869;&#32447;&#21644;&#22806;&#32447;&#30340;&#21512;&#25104;&#31354;&#38388;&#20197;&#21450;&#22686;&#24378;&#31867;&#22411;&#23545;OOD&#25506;&#27979;&#22120;&#30340;&#26657;&#20934;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20339;&#21327;&#35758;&#26159;&#21512;&#25104;&#28508;&#31354;&#38388;&#20869;&#32447;&#21644;&#22810;&#26679;&#21270;&#30340;&#20687;&#32032;&#31354;&#38388;&#22806;&#32447;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;OOD&#26816;&#27979;&#30340;AUROC $15&#65285;-35&#65285;$&#12290;</title><link>http://arxiv.org/abs/2207.05286</link><description>&lt;p&gt;
&#20102;&#35299;&#24744;&#30340;&#31354;&#38388;&#65306;&#20869;&#32447;&#21644;&#22806;&#32447;&#26500;&#24314;&#29992;&#20110;&#26657;&#20934;&#21307;&#23398;OOD&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Know Your Space: Inlier and Outlier Construction for Calibrating Medical OOD Detectors. (arXiv:2207.05286v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;OOD&#25506;&#27979;&#22120;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#20869;&#32447;&#21644;&#22806;&#32447;&#30340;&#21512;&#25104;&#31354;&#38388;&#20197;&#21450;&#22686;&#24378;&#31867;&#22411;&#23545;OOD&#25506;&#27979;&#22120;&#30340;&#26657;&#20934;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20339;&#21327;&#35758;&#26159;&#21512;&#25104;&#28508;&#31354;&#38388;&#20869;&#32447;&#21644;&#22810;&#26679;&#21270;&#30340;&#20687;&#32032;&#31354;&#38388;&#22806;&#32447;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;OOD&#26816;&#27979;&#30340;AUROC $15&#65285;-35&#65285;$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#20135;&#29983;&#33391;&#22909;&#26657;&#20934;&#30340;&#21307;&#23398;&#22270;&#20687;OOD&#25506;&#27979;&#22120;&#65292;&#20197;&#20415;&#23433;&#20840;&#37096;&#32626;&#12290;&#30001;&#20110;&#31579;&#36873;&#20986;&#21512;&#36866;&#30340;&#26657;&#20934;&#25968;&#25454;&#38598;&#36739;&#20026;&#22256;&#38590;&#65292;&#22240;&#27492;&#21512;&#25104;&#22686;&#24378;&#25216;&#26415;&#22312;&#20869;&#32447;/&#22806;&#32447;&#29305;&#23450;&#26041;&#38754;&#24050;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#20869;&#32447;&#21644;&#22806;&#32447;&#30340;&#21512;&#25104;&#31354;&#38388;&#20197;&#21450;&#22686;&#24378;&#31867;&#22411;&#26041;&#38754;&#36824;&#23384;&#22312;&#30528;&#19968;&#20123;&#38656;&#35201;&#27880;&#24847;&#30340;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;OOD&#25506;&#27979;&#22120;&#30340;&#26657;&#20934;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27969;&#34892;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;OOD&#25506;&#27979;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#21327;&#35758;&#26159;&#21512;&#25104;&#28508;&#31354;&#38388;&#20869;&#32447;&#21644;&#22810;&#26679;&#21270;&#30340;&#20687;&#32032;&#31354;&#38388;&#22806;&#32447;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#21307;&#23398;&#25104;&#20687;&#22522;&#20934;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24320;&#25918;&#24335;&#35782;&#21035;&#35774;&#32622;&#20013;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;OOD&#26816;&#27979;&#30340;AUROC&#21487;&#30456;&#23545;&#25552;&#39640;$15&#65285;-35&#65285;$
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of producing well-calibrated out-of-distribution (OOD) detectors, in order to enable safe deployment of medical image classifiers. Motivated by the difficulty of curating suitable calibration datasets, synthetic augmentations have become highly prevalent for inlier/outlier specification. While there have been rapid advances in data augmentation techniques, this paper makes a striking finding that the space in which the inliers and outliers are synthesized, in addition to the type of augmentation, plays a critical role in calibrating OOD detectors. Using the popular energy-based OOD detection framework, we find that the optimal protocol is to synthesize latent-space inliers along with diverse pixel-space outliers. Based on empirical studies with multiple medical imaging benchmarks, we demonstrate that our approach consistently leads to superior OOD detection ($15\% - 35\%$ in AUROC) over the state-of-the-art in a variety of open-set recognition settings.
&lt;/p&gt;</description></item><item><title>SALSA&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#32479;&#35745;&#23494;&#30721;&#20998;&#26512;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#65292;&#21487;&#20197;&#25915;&#20987;&#23567;&#21040;&#20013;&#31561;&#22823;&#23567;&#12289;&#20855;&#26377;&#31232;&#30095;&#20108;&#36827;&#21046;&#31192;&#23494;&#30340;LWE&#23454;&#20363;&#65292;&#21487;&#33021;&#20250;&#25193;&#23637;&#21040;&#23454;&#38469;&#30340;LWE&#23494;&#30721;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2207.04785</link><description>&lt;p&gt;
SALSA&#65306;&#29992;Transformers&#25915;&#20987;&#26684;&#23494;&#30721;
&lt;/p&gt;
&lt;p&gt;
SALSA: Attacking Lattice Cryptography with Transformers. (arXiv:2207.04785v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04785
&lt;/p&gt;
&lt;p&gt;
SALSA&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#32479;&#35745;&#23494;&#30721;&#20998;&#26512;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#65292;&#21487;&#20197;&#25915;&#20987;&#23567;&#21040;&#20013;&#31561;&#22823;&#23567;&#12289;&#20855;&#26377;&#31232;&#30095;&#20108;&#36827;&#21046;&#31192;&#23494;&#30340;LWE&#23454;&#20363;&#65292;&#21487;&#33021;&#20250;&#25193;&#23637;&#21040;&#23454;&#38469;&#30340;LWE&#23494;&#30721;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#20844;&#38053;&#23494;&#30721;&#31995;&#32479;&#23558;&#20250;&#38754;&#20020;&#20840;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#8220;&#37327;&#23376;&#25239;&#24615;&#8221;&#23494;&#30721;&#31995;&#32479;&#38656;&#27714;&#26497;&#39640;&#65292;&#22522;&#20110;Learn With Errors&#65288;LWE&#65289;&#38382;&#39064;&#30340;&#26684;&#23494;&#30721;&#31995;&#32479;&#24050;&#25104;&#20026;&#26631;&#20934;&#21270;&#30340;&#24378;&#26377;&#21147;&#31454;&#20105;&#32773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;Transformer&#25191;&#34892;&#27169;&#25968;&#31639;&#26415;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#23494;&#30721;&#20998;&#26512;&#25216;&#26415;&#23558;&#21322;&#35757;&#32451;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#25552;&#20986;SALSA&#65306;&#19968;&#31181;&#29992;&#20110;&#25915;&#20987;&#22522;&#20110;LWE&#30340;&#23494;&#30721;&#26041;&#26696;&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#12290;SALSA&#21487;&#20197;&#23436;&#20840;&#24674;&#22797;&#23567;&#21040;&#20013;&#31561;&#22823;&#23567;&#30340;&#20855;&#26377;&#31232;&#30095;&#20108;&#36827;&#21046;&#31192;&#23494;&#30340;LWE&#23454;&#20363;&#30340;&#23494;&#38053;&#65292;&#24182;&#21487;&#33021;&#25193;&#23637;&#21040;&#25915;&#20987;&#23454;&#38469;&#30340;&#22522;&#20110;LWE&#30340;&#23494;&#30721;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently deployed public-key cryptosystems will be vulnerable to attacks by full-scale quantum computers. Consequently, "quantum resistant" cryptosystems are in high demand, and lattice-based cryptosystems, based on a hard problem known as Learning With Errors (LWE), have emerged as strong contenders for standardization. In this work, we train transformers to perform modular arithmetic and combine half-trained models with statistical cryptanalysis techniques to propose SALSA: a machine learning attack on LWE-based cryptographic schemes. SALSA can fully recover secrets for small-to-mid size LWE instances with sparse binary secrets, and may scale to attack real-world LWE-based cryptosystems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03578</link><description>&lt;p&gt;
&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#32423;&#21035;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;IR&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#21644;IR&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#36991;&#20813;&#20197;&#24448;&#26041;&#27861;&#23384;&#22312;&#30340;&#24120;&#35265;&#38169;&#35823;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00713</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning
&lt;/p&gt;
&lt;p&gt;
q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#25506;&#32034;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;Q-learning&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23567;q&#20989;&#25968;&#8221;&#20316;&#20026;&#22823;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q&#20989;&#25968;&#30340;q-learning&#29702;&#35770;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#31361;&#30772;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38598;&#22823;&#23567;&#19982;&#27169;&#22411;&#35823;&#24046;&#24130;&#24459;&#30340;&#23610;&#24230;&#30028;&#38480;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2206.14486</link><description>&lt;p&gt;
&#36229;&#36234;&#31070;&#32463;&#23610;&#24230;&#23450;&#24459;&#65306;&#36890;&#36807;&#25968;&#25454;&#20462;&#21098;&#25171;&#36133;&#24130;&#24459;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Beyond neural scaling laws: beating power law scaling via data pruning. (arXiv:2206.14486v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#31361;&#30772;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38598;&#22823;&#23567;&#19982;&#27169;&#22411;&#35823;&#24046;&#24130;&#24459;&#30340;&#23610;&#24230;&#30028;&#38480;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36941;&#23384;&#22312;&#30340;&#31070;&#32463;&#23610;&#24230;&#23450;&#24459;&#20197;&#35757;&#32451;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#35268;&#27169;&#25110;&#20004;&#32773;&#30340;&#24130;&#20026;&#27169;&#22411;&#35823;&#24046;&#19979;&#38477;&#30340;&#39537;&#21160;&#21147;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#20165;&#36890;&#36807;&#23610;&#24230;&#26469;&#23454;&#29616;&#36825;&#20123;&#25913;&#36827;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#25968;&#25454;&#38598;&#22823;&#23567;&#19982;&#35823;&#24046;&#27604;&#20363;&#30340;&#23610;&#24230;&#65292;&#24182;&#23637;&#31034;&#29702;&#35770;&#19978;&#25105;&#20204;&#22914;&#20309;&#31361;&#30772;&#24130;&#24459;&#23610;&#24230;&#65292;&#24182;&#22312;pruning&#31639;&#27861;&#26465;&#20214;&#19979;&#28508;&#22312;&#22320;&#29978;&#33267;&#33021;&#23558;&#20854;&#38477;&#33267;&#25351;&#25968;&#23610;&#24230;&#12290;&#25105;&#20204;&#25509;&#30528;&#22312;CIFAR-10&#12289;SVHN&#21644;ImageNet&#30340;ResNet&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#35266;&#23519;&#21040;&#23454;&#36341;&#20013;&#20248;&#20110;&#24130;&#24459;&#23610;&#24230;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#23547;&#25214;&#20248;&#36136;pruning&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#23545;ImageNet&#19978;&#30340;&#21313;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#65292;&#28982;&#21518;&#36816;&#34892;&#26465;&#20214;&#26799;&#24230;&#26356;&#26032;&#26469;&#20943;&#23569;&#19978;&#23618;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#19988;&#25910;&#25947;&#24615;&#20445;&#35777;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2206.08868</link><description>&lt;p&gt;
&#24102;&#32422;&#26463;&#19979;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#26465;&#20214;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem. (arXiv:2206.08868v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#65292;&#28982;&#21518;&#36816;&#34892;&#26465;&#20214;&#26799;&#24230;&#26356;&#26032;&#26469;&#20943;&#23569;&#19978;&#23618;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#19988;&#25910;&#25947;&#24615;&#20445;&#35777;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#19968;&#31867;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#8212;&#8212;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#21478;&#19968;&#20010;&#20984;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#38598;&#19978;&#26368;&#23567;&#21270;&#24179;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#24050;&#32463;&#21457;&#23637;&#20986;&#20102;&#20960;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#35201;&#20040;&#26159;&#19978;&#23618;&#30446;&#26631;&#30340;&#28176;&#36817;&#24615;&#65292;&#35201;&#20040;&#26159;&#25910;&#25947;&#36895;&#29575;&#32531;&#24930;&#19988;&#20122;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20999;&#21106;&#24179;&#38754;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#65292;&#28982;&#21518;&#36816;&#34892;&#26465;&#20214;&#26799;&#24230;&#26356;&#26032;&#26469;&#20943;&#23569;&#19978;&#23618;&#30446;&#26631;&#20989;&#25968;&#12290;&#24403;&#19978;&#23618;&#30446;&#26631;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;${\mathcal{O}}(\max\{1/\epsilon_f,1/\epsilon_g\})$&#27425;&#36845;&#20195;&#25165;&#33021;&#25214;&#21040;&#19968;&#20010;&#23545;&#20110;&#19978;&#23618; &#21644;&#19979;&#23618;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;$\epsilon_f$&#21644;$\epsilon_g$&#26368;&#20248;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a class of bilevel optimization problems, also known as simple bilevel optimization, where we minimize a smooth objective function over the optimal solution set of another convex constrained optimization problem. Several iterative methods have been developed for tackling this class of problems. Alas, their convergence guarantees are either asymptotic for the upper-level objective, or the convergence rates are slow and sub-optimal. To address this issue, in this paper, we introduce a novel bilevel optimization method that locally approximates the solution set of the lower-level problem via a cutting plane, and then runs a conditional gradient update to decrease the upper-level objective. When the upper-level objective is convex, we show that our method requires ${\mathcal{O}}(\max\{1/\epsilon_f,1/\epsilon_g\})$ iterations to find a solution that is $\epsilon_f$-optimal for the upper-level objective and $\epsilon_g$-optimal for the lower-level objective. Moreover,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#38543;&#26426;&#34920;&#31034;&#38598;&#21512;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#36724;&#24182;&#34892;&#20999;&#21106;&#26469;&#25191;&#34892;&#25968;&#25454;&#20998;&#21306;&#65292;&#20197;&#35299;&#20915;&#23396;&#31435;&#26862;&#26519;&#31639;&#27861;&#19981;&#33021;&#25104;&#21151;&#26816;&#27979;&#39640;&#32500;/&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#38590;&#20197;&#38548;&#31163;&#30340;&#22256;&#38590;&#24322;&#24120;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.06602</link><description>&lt;p&gt;
&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Isolation Forest for Anomaly Detection. (arXiv:2206.06602v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#38543;&#26426;&#34920;&#31034;&#38598;&#21512;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#36724;&#24182;&#34892;&#20999;&#21106;&#26469;&#25191;&#34892;&#25968;&#25454;&#20998;&#21306;&#65292;&#20197;&#35299;&#20915;&#23396;&#31435;&#26862;&#26519;&#31639;&#27861;&#19981;&#33021;&#25104;&#21151;&#26816;&#27979;&#39640;&#32500;/&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#38590;&#20197;&#38548;&#31163;&#30340;&#22256;&#38590;&#24322;&#24120;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23396;&#31435;&#26862;&#26519;&#65288;iForest&#65289;&#30001;&#20110;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26222;&#36866;&#26377;&#25928;&#24615;&#21644;&#24378;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#32780;&#36880;&#28176;&#25104;&#20026;&#21487;&#33021;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#32447;&#24615;&#36724;&#24182;&#34892;&#38548;&#31163;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#65288;i&#65289;&#26816;&#27979;&#38590;&#20197;&#22312;&#39640;&#32500;/&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#31354;&#38388;&#20013;&#38548;&#31163;&#30340;&#22256;&#38590;&#24322;&#24120;&#30340;&#22833;&#36133;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#33261;&#21517;&#26157;&#30528;&#30340;&#31639;&#27861;&#20559;&#24046;&#65292;&#23558;&#39044;&#26399;&#36739;&#20302;&#30340;&#24322;&#24120;&#24471;&#20998;&#20998;&#37197;&#32473;&#24037;&#20214;&#21306;&#22495;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#12290;&#24341;&#20837;&#20102;&#20960;&#20010;iForest&#25193;&#23637;&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#20173;&#28982;&#20351;&#29992;&#27973;&#23618;&#30340;&#32447;&#24615;&#25968;&#25454;&#20998;&#21306;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#38548;&#31163;&#30495;&#27491;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#26041;&#26696;&#65292;&#21033;&#29992;&#38543;&#24847;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#38543;&#26426;&#34920;&#31034;&#38598;&#21512;&#20013;&#65292;&#38543;&#21518;&#24212;&#29992;&#38543;&#26426;&#36724;&#24182;&#34892;&#20999;&#21106;&#26469;&#25191;&#34892;&#25968;&#25454;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isolation forest (iForest) has been emerging as arguably the most popular anomaly detector in recent years due to its general effectiveness across different benchmarks and strong scalability. Nevertheless, its linear axis-parallel isolation method often leads to (i) failure in detecting hard anomalies that are difficult to isolate in high-dimensional/non-linear-separable data space, and (ii) notorious algorithmic bias that assigns unexpectedly lower anomaly scores to artefact regions. These issues contribute to high false negative errors. Several iForest extensions are introduced, but they essentially still employ shallow, linear data partition, restricting their power in isolating true anomalies. Therefore, this paper proposes deep isolation forest. We introduce a new representation scheme that utilises casually initialised neural networks to map original data into random representation ensembles, where random axis-parallel cuts are subsequently applied to perform the data partition. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACMP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#65292;&#20811;&#26381;&#20102;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.05437</link><description>&lt;p&gt;
ACMP: Allen-Cahn&#20449;&#24687;&#20256;&#36882;&#29992;&#20110;&#24102;&#26377;&#29289;&#36136;&#30456;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ACMP: Allen-Cahn Message Passing for Graph Neural Networks with Particle Phase Transition. (arXiv:2206.05437v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACMP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#65292;&#20811;&#26381;&#20102;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#26159;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#29305;&#24449;&#25552;&#21462;&#21333;&#20803;&#65292;&#32771;&#34385;&#20174;&#19968;&#23618;&#21040;&#19979;&#19968;&#23618;&#30340;&#32593;&#32476;&#20256;&#25773;&#20013;&#30340;&#30456;&#37051;&#33410;&#28857;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#26469;&#24314;&#27169;&#36825;&#31181;&#36807;&#31243;&#65292;&#24182;&#22312;&#30456;&#21464;&#24314;&#27169;&#20013;&#24341;&#20837;Allen-Cahn&#21147;&#12290;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#26159;&#19968;&#31181;&#21453;&#24212;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#31890;&#23376;&#20998;&#31163;&#32780;&#19981;&#20250;&#25193;&#25955;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#31181;Allen-Cahn&#20449;&#24687;&#20256;&#36882;(ACMP)&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#31890;&#23376;&#31995;&#32479;&#35299;&#30340;&#25968;&#20540;&#36845;&#20195;&#26500;&#25104;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#12290;ACMP&#20855;&#26377;&#31616;&#21333;&#30340;&#23454;&#29616;&#21644;&#31070;&#32463;ODE&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19978;&#35777;&#26126;&#30340;Dirichlet&#33021;&#37327;&#20005;&#26684;&#27491;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#27169;&#22411;&#30340;GNN&#65292;&#36991;&#20813;&#20102;&#24120;&#35265;&#30340;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20351;&#29992;ACMP&#30340;GNN&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#23454;&#38469;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node class
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.00759</link><description>&lt;p&gt;
Merlin-Arthur&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Formal Interpretability with Merlin-Arthur Classifiers. (arXiv:2206.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#26159;&#20687;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#22797;&#26434;&#26234;&#33021;&#20307;&#20063;&#33021;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;&#36825;&#20123;&#20445;&#35777;&#21253;&#25324;&#23545;&#27492;&#20998;&#31867;&#22120;&#36873;&#25321;&#30340;&#29305;&#24449;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#19978;&#19979;&#30028;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21463;&#20132;&#20114;&#24335;&#35777;&#26126;&#31995;&#32479;&#20013; Merlin-Arthur &#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#24182;&#20197;&#21487;&#27979;&#37327;&#30340;&#25351;&#26631;&#65288;&#22914;&#22768;&#38899;&#21644;&#23436;&#25972;&#24615;&#65289;&#34920;&#36798;&#20102;&#36825;&#20123;&#32422;&#26463;&#12290;&#19982;&#29616;&#26377;&#30340;&#20132;&#20114;&#24335;&#35774;&#32622;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#30340;&#30456;&#23545;&#24378;&#24230;&#20197;&#21450;&#26032;&#30340;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#26469;&#25429;&#25417;&#20351;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#22256;&#38590;&#30340;&#31934;&#30830;&#30456;&#20851;&#24615;&#31867;&#22411;&#12290; &#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#23454;&#39564;&#21487;&#39564;&#35777;&#39640;&#20114;&#20449;&#24687;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new type of multi-agent interactive classifier that provides provable interpretability guarantees even for complex agents such as neural networks. These guarantees consist of bounds on the mutual information of the features selected by this classifier. Our results are inspired by the Merlin-Arthur protocol from Interactive Proof Systems and express these bounds in terms of measurable metrics such as soundness and completeness. Compared to existing interactive setups we do not rely on optimal agents or on the assumption that features are distributed independently. Instead, we use the relative strength of the agents as well as the new concept of Asymmetric Feature Correlation which captures the precise kind of correlations that make interpretability guarantees difficult. %relates the information carried by sets of features to one of the individual features. We test our results through numerical experiments on two small-scale datasets where high mutual information can be veri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;DRL&#30340;&#23398;&#20064;&#34920;&#31034;&#24212;&#35813;&#28385;&#36275;&#19968;&#20010;&#26377;&#21033;&#30340;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;PEER&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#26174;&#24335;&#27491;&#21017;&#21270;&#26469;&#32500;&#25345;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.14557</link><description>&lt;p&gt;
&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#21333;&#27491;&#21017;&#21270;&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning. (arXiv:2205.14557v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;DRL&#30340;&#23398;&#20064;&#34920;&#31034;&#24212;&#35813;&#28385;&#36275;&#19968;&#20010;&#26377;&#21033;&#30340;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;PEER&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#26174;&#24335;&#27491;&#21017;&#21270;&#26469;&#32500;&#25345;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25215;&#35834;&#20195;&#29702;&#33021;&#22815;&#20174;&#39640;&#32500;&#20449;&#24687;&#20013;&#23398;&#20064;&#21040;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#32780;&#34920;&#31034;&#23398;&#20064;&#21017;&#33021;&#22815;&#28040;&#38500;&#19981;&#30456;&#20851;&#21644;&#20887;&#20313;&#30340;&#20449;&#24687;&#24182;&#20445;&#30041;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;Q&#32593;&#32476;&#21450;&#20854;&#30446;&#26631;Q&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#22312;&#29702;&#35770;&#19978;&#24212;&#35813;&#28385;&#36275;&#19968;&#20010;&#26377;&#21033;&#30340;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#20856;&#22411;&#30340;DRL&#35774;&#32622;&#20013;&#20004;&#20010;&#30456;&#37051;&#26102;&#38388;&#27493;&#38271;&#30340;&#20215;&#20540;&#20989;&#25968;&#30340;&#34920;&#31034;&#30456;&#20284;&#24230;&#23384;&#22312;&#19968;&#20010;&#19978;&#30028;&#12290;&#20294;&#26159;&#65292;&#36890;&#36807;&#35828;&#26126;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#20064;&#21040;&#30340;DRL&#20195;&#29702;&#21487;&#33021;&#36829;&#21453;&#36825;&#20010;&#23646;&#24615;&#65292;&#24182;&#23548;&#33268;&#27425;&#20248;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#34920;&#31034;&#31616;&#21333;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#35780;&#20272;"(PEER)&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#26174;&#24335;&#27491;&#21017;&#21270;&#26469;&#32500;&#25345;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes irrelevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the $Q$-network and its target $Q$-network should, in theory, satisfy a favorable distinguishable representation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called Policy Evaluation with Easy Regularization on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regularization on internal representations. And we provide the convergence rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;TFLEX&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#24314;&#27169;&#25152;&#26377;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#36816;&#31639;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#21521;&#37327;&#36923;&#36753;&#20197;&#22788;&#29702;&#19977;&#20010;&#39069;&#22806;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;</title><link>http://arxiv.org/abs/2205.14307</link><description>&lt;p&gt;
TFLEX: &#26102;&#38388;&#29305;&#24449;&#36923;&#36753;&#23884;&#20837;&#26694;&#26550;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph. (arXiv:2205.14307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;TFLEX&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#24314;&#27169;&#25152;&#26377;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#36816;&#31639;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#21521;&#37327;&#36923;&#36753;&#20197;&#22788;&#29702;&#19977;&#20010;&#39069;&#22806;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#65292;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#21457;&#25381;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#22797;&#26434;&#26597;&#35810;&#23884;&#20837;&#65288;CQE&#65289;&#26041;&#27861;&#20391;&#37325;&#20110;&#38745;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#32780;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;TKG&#19978;&#30340;&#25512;&#29702;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;1.&#26597;&#35810;&#24212;&#35813;&#22238;&#31572;&#23454;&#20307;&#25110;&#26102;&#38388;&#25139;&#65307;2.&#36816;&#31639;&#31526;&#24212;&#35813;&#21516;&#26102;&#32771;&#34385;&#23454;&#20307;&#38598;&#19978;&#30340;&#38598;&#21512;&#36923;&#36753;&#21644;&#26102;&#38388;&#25139;&#38598;&#19978;&#30340;&#26102;&#38388;&#36923;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;TKG&#19978;&#30340;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#19977;&#20010;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;TFLEX&#30340;&#26102;&#38388;CQE&#65292;&#29992;&#20110;&#22238;&#31572;&#26102;&#38388;&#22797;&#26434;&#26597;&#35810;&#12290;&#25105;&#20204;&#21033;&#29992;&#21521;&#37327;&#36923;&#36753;&#35745;&#31639;Temporal Feature-Logic&#23884;&#20837;&#30340;&#36923;&#36753;&#37096;&#20998;&#65292;&#20174;&#32780;&#33258;&#28982;&#22320;&#24314;&#27169;&#23454;&#20307;&#38598;&#19978;&#30340;&#25152;&#26377;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#36816;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25193;&#23637;&#26102;&#38388;&#25139;&#38598;&#19978;&#30340;&#21521;&#37327;&#36923;&#36753;&#65292;&#20197;&#22788;&#29702;&#19977;&#20010;&#39069;&#22806;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#65288;After&#65292;Before&#21644;Between&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-hop logical reasoning over knowledge graph (KG) plays a fundamental role in many artificial intelligence tasks. Recent complex query embedding (CQE) methods for reasoning focus on static KGs, while temporal knowledge graphs (TKGs) have not been fully explored. Reasoning over TKGs has two challenges: 1. The query should answer entities or timestamps; 2. The operators should consider both set logic on entity set and temporal logic on timestamp set. To bridge this gap, we define the multi-hop logical reasoning problem on TKGs. With generated three datasets, we propose the first temporal CQE named Temporal Feature-Logic Embedding framework (TFLEX) to answer the temporal complex queries. We utilize vector logic to compute the logic part of Temporal Feature-Logic embeddings, thus naturally modeling all First-Order Logic (FOL) operations on entity set. In addition, our framework extends vector logic on timestamp set to cope with three extra temporal operators (After, Before and Between)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#21040;&#24102;&#26435;&#22270;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#37327;&#23376;&#30005;&#36335;&#65288;ansatz&#65289;&#65292;&#24182;&#22312;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#20248;&#24322;&#65292;&#20805;&#20998;&#35828;&#26126;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;ansatz&#26159;&#25104;&#21151;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2205.06109</link><description>&lt;p&gt;
&#24102;&#26435;&#22270;&#23398;&#20064;&#30340;&#31561;&#21464;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Equivariant quantum circuits for learning on weighted graphs. (arXiv:2205.06109v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#21040;&#24102;&#26435;&#22270;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#37327;&#23376;&#30005;&#36335;&#65288;ansatz&#65289;&#65292;&#24182;&#22312;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#20248;&#24322;&#65292;&#20805;&#20998;&#35828;&#26126;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;ansatz&#26159;&#25104;&#21151;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#26159;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#23454;&#29616;&#20248;&#21183;&#30340;&#39046;&#20808;&#20505;&#36873;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24403;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;ansatz &#26159;&#20915;&#23450;&#31639;&#27861;&#30340;&#21487;&#35757;&#32451;&#24615;&#21644;&#24615;&#33021;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#20013;&#65292;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#32467;&#26500;&#30340;ansatz&#30340;&#25991;&#29486;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#24102;&#26435;&#22270;&#20219;&#21153;&#30340;ansatz &#65292;&#35813;ansatz&#23562;&#37325;&#37325;&#35201;&#30340;&#22270;&#24418;&#23545;&#31216;&#24615;&#65292;&#21363;&#33410;&#28857;&#32622;&#25442;&#19979;&#30340;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#31181;ansatz&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#20013;&#30740;&#31350;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#32467;&#26524;&#35777;&#23454;&#20102;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340; ansatzs&#26159;QML&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms are the leading candidate for advantage on near-term quantum hardware. When training a parametrized quantum circuit in this setting to solve a specific problem, the choice of ansatz is one of the most important factors that determines the trainability and performance of the algorithm. In quantum machine learning (QML), however, the literature on ansatzes that are motivated by the training data structure is scarce. In this work, we introduce an ansatz for learning tasks on weighted graphs that respects an important graph symmetry, namely equivariance under node permutations. We evaluate the performance of this ansatz on a complex learning task, namely neural combinatorial optimization, where a machine learning model is used to learn a heuristic for a combinatorial optimization problem. We analytically and numerically study the performance of our model, and our results strengthen the notion that symmetry-preserving ansatzes are a key to success in QML.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;13&#31181;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;153&#20010;&#38647;&#36798;&#32593;&#32476;&#37197;&#32622;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#31639;&#27861;&#27604;&#20154;&#31867;&#19987;&#23478;&#35201;&#26356;&#22909;&#12290;&#20294;&#23427;&#20204;&#30340;&#25490;&#21517;&#21462;&#20915;&#20110;&#39044;&#31639;&#21644;&#20301;&#32622;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2205.03670</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#38647;&#36798;&#32593;&#32476;&#37197;&#32622;&#31639;&#27861;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Automated Algorithm Selection for Radar Network Configuration. (arXiv:2205.03670v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03670
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;13&#31181;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;153&#20010;&#38647;&#36798;&#32593;&#32476;&#37197;&#32622;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#31639;&#27861;&#27604;&#20154;&#31867;&#19987;&#23478;&#35201;&#26356;&#22909;&#12290;&#20294;&#23427;&#20204;&#30340;&#25490;&#21517;&#21462;&#20915;&#20110;&#39044;&#31639;&#21644;&#20301;&#32622;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38647;&#36798;&#32593;&#32476;&#30340;&#37197;&#32622;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#20351;&#29992;&#27169;&#25311;&#22120;&#25163;&#21160;&#23436;&#25104;&#12290;&#19981;&#21516;&#25968;&#37327;&#21644;&#31867;&#22411;&#30340;&#38647;&#36798;&#20197;&#21450;&#38647;&#36798;&#24212;&#28085;&#30422;&#30340;&#19981;&#21516;&#20301;&#32622;&#32473;&#38647;&#36798;&#37197;&#32622;&#38382;&#39064;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#23454;&#20363;&#12290;&#20934;&#30830;&#24314;&#27169;&#36825;&#20123;&#23454;&#20363;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#37197;&#32622;&#36136;&#37327;&#21462;&#20915;&#20110;&#22823;&#37327;&#21442;&#25968;&#12289;&#20869;&#37096;&#38647;&#36798;&#22788;&#29702;&#20197;&#21450;&#38647;&#36798;&#38656;&#35201;&#25918;&#32622;&#30340;&#22320;&#24418;&#12290;&#32463;&#20856;&#30340;&#20248;&#21270;&#31639;&#27861;&#22240;&#27492;&#19981;&#33021;&#24212;&#29992;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20381;&#38752;&#8220;&#35797;&#38169;&#8221;&#40657;&#30418;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;13&#31181;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;153&#20010;&#38647;&#36798;&#32593;&#32476;&#37197;&#32622;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#34920;&#29616;&#27604;&#20154;&#31867;&#19987;&#23478;&#35201;&#22909;&#24471;&#22810;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25490;&#21517;&#21462;&#20915;&#20110;&#21487;&#35780;&#20272;&#37197;&#32622;&#30340;&#39044;&#31639;&#21644;&#25152;&#22788;&#20301;&#32622;&#30340;&#39640;&#31243;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The configuration of radar networks is a complex problem that is often performed manually by experts with the help of a simulator. Different numbers and types of radars as well as different locations that the radars shall cover give rise to different instances of the radar configuration problem. The exact modeling of these instances is complex, as the quality of the configurations depends on a large number of parameters, on internal radar processing, and on the terrains on which the radars need to be placed. Classic optimization algorithms can therefore not be applied to this problem, and we rely on "trial-and-error" black-box approaches.  In this paper, we study the performances of 13 black-box optimization algorithms on 153 radar network configuration problem instances. The algorithms perform considerably better than human experts. Their ranking, however, depends on the budget of configurations that can be evaluated and on the elevation profile of the location. We therefore also inve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#27744;&#26089;&#26399;&#36864;&#21270;&#25968;&#25454;&#39044;&#27979;&#30005;&#27744;&#24490;&#29615;&#23551;&#21629;&#33539;&#22260;&#30340;&#26041;&#27861;&#65292;&#20854;QRF&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#28857;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.12420</link><description>&lt;p&gt;
&#21033;&#29992;&#30005;&#27744;&#26089;&#26399;&#36864;&#21270;&#25968;&#25454;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#30005;&#27744;&#24490;&#29615;&#23551;&#21629;&#33539;&#22260;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Battery Cycle Life Range Prediction Using Early Degradation Data at Cell Level. (arXiv:2204.12420v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#27744;&#26089;&#26399;&#36864;&#21270;&#25968;&#25454;&#39044;&#27979;&#30005;&#27744;&#24490;&#29615;&#23551;&#21629;&#33539;&#22260;&#30340;&#26041;&#27861;&#65292;&#20854;QRF&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#28857;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26089;&#26399;&#36864;&#21270;&#25968;&#25454;&#39044;&#27979;&#30005;&#27744;&#24490;&#29615;&#23551;&#21629;&#22312;&#30005;&#27744;&#20135;&#21697;&#29983;&#21629;&#21608;&#26399;&#20013;&#26377;&#24456;&#22810;&#28508;&#22312;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#31649;&#29702;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#30340;&#30005;&#27744;&#26411;&#22788;&#29702;&#65292;&#24182;&#38477;&#20302;&#32463;&#27982;&#21644;&#25216;&#26415;&#39118;&#38505;&#65292;&#38656;&#35201;&#39044;&#27979;&#20855;&#26377;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#24490;&#29615;&#23551;&#21629;&#33539;&#22260;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#35813;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#36825;&#37324;&#24341;&#20837;&#20102;&#19968;&#31181;Quantile Regression Forest (QRF)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#20551;&#35774;&#20219;&#20309;&#29305;&#23450;&#30340;&#24490;&#29615;&#23551;&#21629;&#20998;&#24067;&#65292;&#22312;&#39640;&#31934;&#24230;&#30340;&#28857;&#39044;&#27979;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#24490;&#29615;&#23551;&#21629;&#33539;&#22260;&#30340;&#39044;&#27979;&#65292;&#24182;&#25506;&#31350;&#20102;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Battery cycle life prediction using early degradation data has many potential applications throughout the battery product life cycle. For that reason, various data-driven methods have been proposed for point prediction of battery cycle life with minimum knowledge of the battery degradation mechanisms. However, managing the rapidly increasing amounts of batteries at end-of-life with lower economic and technical risk requires prediction of cycle life with quantified uncertainty, which is still lacking. The interpretability (i.e., the reason for high prediction accuracy) of these advanced data-driven methods is also worthy of investigation. Here, a Quantile Regression Forest (QRF) model, having the advantage of not assuming any specific distribution of cycle life, is introduced to make cycle life range prediction with uncertainty quantified as the width of the prediction interval, in addition to point predictions with high accuracy. The hyperparameters of the QRF model are optimized with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#23884;&#20837;BART&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22238;&#24402;&#26641;&#30340;&#26411;&#31471;&#33410;&#28857;&#32423;&#21035;&#19978;&#21253;&#21547;&#38543;&#26426;&#25928;&#24212;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#38656;&#35201;&#25351;&#23450;&#38543;&#26426;&#25928;&#24212;&#32467;&#26500;&#30340;&#38656;&#35201;&#12290;&#27169;&#22411;&#22312;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#25552;&#20379;&#38543;&#26426;&#25928;&#24212;&#26041;&#24046;&#30340;&#19968;&#33268;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2204.07207</link><description>&lt;p&gt;
&#20998;&#23618;&#23884;&#20837;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Embedded Bayesian Additive Regression Trees. (arXiv:2204.07207v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#23884;&#20837;BART&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22238;&#24402;&#26641;&#30340;&#26411;&#31471;&#33410;&#28857;&#32423;&#21035;&#19978;&#21253;&#21547;&#38543;&#26426;&#25928;&#24212;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#38656;&#35201;&#25351;&#23450;&#38543;&#26426;&#25928;&#24212;&#32467;&#26500;&#30340;&#38656;&#35201;&#12290;&#27169;&#22411;&#22312;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#25552;&#20379;&#38543;&#26426;&#25928;&#24212;&#26041;&#24046;&#30340;&#19968;&#33268;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;&#65288;BART&#65289;&#25193;&#23637;&#65292;&#31216;&#20026;&#20998;&#23618;&#23884;&#20837;BART (HE-BART)&#12290;&#35813;&#27169;&#22411;&#20801;&#35768;&#22312;&#19968;&#32452;&#22238;&#24402;&#26641;&#30340;&#26411;&#31471;&#33410;&#28857;&#32423;&#21035;&#19978;&#21253;&#21547;&#38543;&#26426;&#25928;&#24212;&#65292;&#20351;HE-BART&#25104;&#20026;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#30340;&#38750;&#21442;&#25968;&#26367;&#20195;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#22312;&#27169;&#22411;&#20013;&#25351;&#23450;&#38543;&#26426;&#25928;&#24212;&#32467;&#26500;&#30340;&#38656;&#35201;&#65292;&#24182;&#20445;&#25345;&#26631;&#20934;BART&#30340;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#24615;&#36136;&#12290;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#30340;&#25193;&#23637;&#22312;&#35768;&#22810;&#26631;&#20934;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#31034;&#20363;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#20102;&#26356;&#20248;&#36234;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21516;&#26102;&#20173;&#28982;&#25552;&#20379;&#20102;&#38543;&#26426;&#25928;&#24212;&#26041;&#24046;&#30340;&#19968;&#33268;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#30340;&#26410;&#26469;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#23427;&#22312;&#26356;&#22823;&#12289;&#26356;&#39640;&#32423;&#30340;&#25968;&#25454;&#38598;&#21644;&#32467;&#26500;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple yet powerful extension of Bayesian Additive Regression Trees which we name Hierarchical Embedded BART (HE-BART). The model allows for random effects to be included at the terminal node level of a set of regression trees, making HE-BART a non-parametric alternative to mixed effects models which avoids the need for the user to specify the structure of the random effects in the model, whilst maintaining the prediction and uncertainty calibration properties of standard BART. Using simulated and real-world examples, we demonstrate that this new extension yields superior predictions for many of the standard mixed effects models' example data sets, and yet still provides consistent estimates of the random effect variances. In a future version of this paper, we outline its use in larger, more advanced data sets and structures.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#24335;&#36719;&#25552;&#31034;&#23454;&#29616;&#20102;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;&#23646;&#24615;-&#23545;&#35937;&#32452;&#21512;&#65292;&#36229;&#36807;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#23450;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#22312;&#26354;&#32447;&#19979;&#38754;&#31215;&#19978;&#24179;&#22343;&#39640;10.9&#65285;&#12290;</title><link>http://arxiv.org/abs/2204.03574</link><description>&lt;p&gt;
&#23398;&#20064;&#32452;&#21512;&#36719;&#25552;&#31034;&#20197;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03574
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#24335;&#36719;&#25552;&#31034;&#23454;&#29616;&#20102;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;&#23646;&#24615;-&#23545;&#35937;&#32452;&#21512;&#65292;&#36229;&#36807;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#23450;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#22312;&#26354;&#32447;&#19979;&#38754;&#31215;&#19978;&#24179;&#22343;&#39640;10.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#36719;&#25552;&#31034;&#65288;CSP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#23558;CSP&#24320;&#21457;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20063;&#23601;&#26159;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;&#23646;&#24615;-&#23545;&#35937;&#32452;&#21512;&#65288;&#20363;&#22914;&#32769;&#29483;&#21644;&#23567;&#32769;&#34382;&#65289;&#12290;VLM&#20855;&#26377;&#28789;&#27963;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#34920;&#31034;&#20219;&#24847;&#31867;&#21035;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#32452;&#21512;&#38646;&#26679;&#26412;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#22914;&#29305;&#23450;&#20219;&#21153;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;CSP&#23558;&#23450;&#20041;&#31867;&#21035;&#30340;&#23646;&#24615;&#21644;&#23545;&#35937;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#35789;&#27719;&#26631;&#35760;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#35789;&#27719;&#34920;&#34987;&#35843;&#25972;&#20026;&#35782;&#21035;&#20197;&#22810;&#31181;&#26041;&#24335;&#32452;&#25104;&#20196;&#29260;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#32769;&#29483;&#21644;&#30333;&#29483;&#65289;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#23558;&#25152;&#23398;&#30340;&#23646;&#24615;-&#23545;&#35937;&#35789;&#27719;&#34920;&#20197;&#26032;&#30340;&#32452;&#21512;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#35782;&#21035;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CSP&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;CLIP&#24179;&#22343;&#39640;10.9&#20010;&#30334;&#20998;&#28857;&#30340;AUC&#65288;&#26354;&#32447;&#19979;&#38754;&#31215;&#25351;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30340;&#28436;&#21464;&#12289;&#25913;&#36827;&#21644;&#24212;&#29992;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12289;&#30452;&#35266;&#30340;&#36830;&#25509;&#12289;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#36890;&#29992;&#36817;&#20284;&#33021;&#21147;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2203.11316</link><description>&lt;p&gt;
&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;&#65306;&#26368;&#26032;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Random vector functional link network: recent developments, applications, and future directions. (arXiv:2203.11316v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30340;&#28436;&#21464;&#12289;&#25913;&#36827;&#21644;&#24212;&#29992;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12289;&#30452;&#35266;&#30340;&#36830;&#25509;&#12289;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#36890;&#29992;&#36817;&#20284;&#33021;&#21147;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#32858;&#31867;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;BP&#30340;&#36845;&#20195;&#35757;&#32451;&#26041;&#27861;&#34987;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#26159;&#36825;&#20250;&#23548;&#33268;&#20986;&#29616;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#38382;&#39064;&#12289;&#23545;&#23398;&#20064;&#29575;&#25935;&#24863;&#20197;&#21450;&#25910;&#25947;&#32531;&#24930;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#12290;RVFL&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#29305;&#28857;&#65292;&#22914;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12289;&#30452;&#35266;&#30340;&#36830;&#25509;&#12289;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#36890;&#29992;&#36817;&#20284;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RVFL&#27169;&#22411;&#28436;&#21464;&#30340;&#31532;&#19968;&#20010;&#20840;&#38754;&#32508;&#36848;&#25991;&#29486;&#65292;&#21487;&#20316;&#20026;&#26032;&#25163;&#21644;&#23454;&#36341;&#32773;&#30340;&#24191;&#27867;&#24635;&#32467;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27973;&#23618;RVFL&#12289;&#38598;&#25104;RVFL&#12289;&#28145;&#23618;RVFL&#21644;&#38598;&#25104;&#28145;&#23618;RVFL&#27169;&#22411;&#12290;RVFL&#27169;&#22411;&#30340;&#21464;&#21270;&#12289;&#25913;&#36827;&#21644;&#24212;&#29992;&#24471;&#21040;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been successfully employed in various domains such as classification, regression and clustering, etc. Generally, the back propagation (BP) based iterative approaches are used to train the neural networks, however, it results in the issues of local minima, sensitivity to learning rate and slow convergence. To overcome these issues, randomization based neural networks such as random vector functional link (RVFL) network have been proposed. RVFL model has several characteristics such as fast training speed, direct links, simple architecture, and universal approximation capability, that make it a viable randomized neural network. This article presents the first comprehensive review of the evolution of RVFL model, which can serve as the extensive summary for the beginners as well as practitioners. We discuss the shallow RVFLs, ensemble RVFLs, deep RVFLs and ensemble deep RVFL models. The variations, improvements and applications of RVFL models are discussed in detail. M
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#23384;&#22312;&#27169;&#22411;&#35268;&#26684;&#19981;&#20934;&#30830;&#21644;&#24322;&#24120;&#20540;&#24773;&#20917;&#19979;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#33258;&#30001;&#33021;&#37327;&#20934;&#21017;&#65292;&#36890;&#36807;&#23558;&#24191;&#20041;&#23545;&#25968;&#24471;&#20998;&#20989;&#25968;&#19982;PAC$^m$&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.01859</link><description>&lt;p&gt;
&#40065;&#26834;PAC$^m$: &#22312;&#27169;&#22411;&#35268;&#26684;&#19981;&#20934;&#30830;&#21644;&#23384;&#22312;&#24322;&#24120;&#20540;&#24773;&#20917;&#19979;&#35757;&#32451;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers. (arXiv:2203.01859v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01859
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23384;&#22312;&#27169;&#22411;&#35268;&#26684;&#19981;&#20934;&#30830;&#21644;&#24322;&#24120;&#20540;&#24773;&#20917;&#19979;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#33258;&#30001;&#33021;&#37327;&#20934;&#21017;&#65292;&#36890;&#36807;&#23558;&#24191;&#20041;&#23545;&#25968;&#24471;&#20998;&#20989;&#25968;&#19982;PAC$^m$&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#22312;&#27169;&#22411;&#35268;&#26684;&#19981;&#20934;&#30830;&#21644;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#24050;&#30693;&#23384;&#22312;&#27867;&#21270;&#33021;&#21147;&#30340;&#19981;&#36275;&#12290;PAC-Bayes&#29702;&#35770;&#35777;&#26126;&#20102;&#36125;&#21494;&#26031;&#23398;&#20064;&#25152;&#26368;&#23567;&#21270;&#30340;&#33258;&#30001;&#33021;&#37327;&#20934;&#21017;&#26159;&#22312;&#20551;&#35774;&#26410;&#34987;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#37319;&#26679;&#20998;&#24067;&#19979;&#65292;&#23545;Gibbs&#39044;&#27979;&#22120;&#65288;&#21363;&#20174;&#21518;&#39564;&#38543;&#26426;&#25277;&#21462;&#30340;&#21333;&#20010;&#27169;&#22411;&#65289;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#35813;&#35266;&#28857;&#25552;&#20379;&#20102;&#36125;&#21494;&#26031;&#23398;&#20064;&#22312;&#27169;&#22411;&#35268;&#26684;&#19981;&#20934;&#30830;&#19988;&#38656;&#35201;&#38598;&#25104;&#65292;&#20197;&#21450;&#25968;&#25454;&#21463;&#21040;&#24322;&#24120;&#20540;&#24433;&#21709;&#26102;&#30340;&#23616;&#38480;&#24615;&#30340;&#35777;&#26126;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;&#25512;&#23548;&#20986;&#20102;PAC-Bayes&#19978;&#30028; - &#31216;&#20026;PAC$^m$ - &#24341;&#20837;&#20102;&#33258;&#30001;&#33021;&#37327;&#24230;&#37327;&#65292;&#21487;&#32771;&#34385;&#38598;&#21512;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#33719;&#24471;&#22312;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#33258;&#30001;&#33021;&#37327;&#20934;&#21017;&#65292;&#23558;&#24191;&#20041;&#23545;&#25968;&#24471;&#20998;&#20989;&#25968;&#19982;PAC$^m$&#38598;&#25104;&#19978;&#30028;&#30456;&#32467;&#21512;&#12290;&#24314;&#35758;&#30340;&#33258;&#30001;&#33021;&#37327;&#35757;&#32451;...&#65288;&#25688;&#35201;&#26410;&#23436;&#65292;&#35814;&#24773;&#35831;&#26597;&#30475;&#21407;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;
Standard Bayesian learning is known to have suboptimal generalization capabilities under model misspecification and in the presence of outliers. PAC-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data is affected by outliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy training 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StratDef&#30340;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#26041;&#27861;&#30340;&#25112;&#30053;&#38450;&#24481;&#31995;&#32479;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25514;&#26045;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;StratDef&#21160;&#24577;&#22320;&#21644;&#31574;&#30053;&#22320;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.07568</link><description>&lt;p&gt;
StratDef: &#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#30340;&#25112;&#30053;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
StratDef: Strategic Defense Against Adversarial Attacks in ML-based Malware Detection. (arXiv:2202.07568v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StratDef&#30340;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#26041;&#27861;&#30340;&#25112;&#30053;&#38450;&#24481;&#31995;&#32479;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25514;&#26045;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;StratDef&#21160;&#24577;&#22320;&#21644;&#31574;&#30053;&#22320;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#22823;&#22810;&#38598;&#20013;&#22312;&#22270;&#20687;&#35782;&#21035;&#39046;&#22495;&#12290;&#23613;&#31649;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#24456;&#39640;&#65292;&#20294;&#23427;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#32780;&#19988;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#19968;&#20123;&#26041;&#27861;&#19978;&#65292;&#32780;&#27809;&#26377;&#20855;&#20307;&#30340;&#24212;&#29992;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StratDef&#30340;&#25112;&#30053;&#38450;&#24481;&#31995;&#32479;&#65292;&#23427;&#22522;&#20110;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#28041;&#21450;&#27169;&#22411;&#31995;&#32479;&#21270;&#26500;&#24314;&#12289;&#36873;&#25321;&#21644;&#25112;&#30053;&#20351;&#29992;&#30340;&#25361;&#25112;&#65292;&#20197;&#26368;&#22823;&#21270;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#65292;StratDef&#21160;&#24577;&#22320;&#21644;&#31574;&#30053;&#22320;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20026;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25514;&#26045;&#39318;&#27425;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#23041;&#32961;&#27169;&#22411;&#25506;&#32034;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#32773;&#30340;&#30693;&#35782;&#27700;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, most research towards defenses against adversarial attacks on machine learning models has been in the image recognition domain. The malware detection domain has received less attention despite its importance. Moreover, most work exploring these defenses has focused on several methods but with no strategy when applying them. In this paper, we introduce StratDef, which is a strategic defense system based on a moving target defense approach. We overcome challenges related to the systematic construction, selection, and strategic use of models to maximize adversarial robustness. StratDef dynamically and strategically chooses the best models to increase the uncertainty for the attacker while minimizing critical aspects in the adversarial ML domain, like attack transferability. We provide the first comprehensive evaluation of defenses against adversarial attacks on machine learning for malware detection, where our threat model explores different levels of threat, attacker know
&lt;/p&gt;</description></item><item><title>&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.00665</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25674;&#20248;&#21270;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on amortized optimization. (arXiv:2202.00665v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#32463;&#24120;&#22312;&#21453;&#22797;&#35299;&#20915;&#30456;&#21516;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#20998;&#25674;&#20248;&#21270;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#20123;&#35774;&#32622;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#30456;&#20284;&#38382;&#39064;&#23454;&#20363;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21464;&#20998;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#27604;&#19981;&#20351;&#29992;&#20998;&#25674;&#30340;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#27425;&#25945;&#31243;&#20171;&#32461;&#20102;&#36825;&#20123;&#36827;&#27493;&#32972;&#21518;&#30340;&#20998;&#25674;&#20248;&#21270;&#22522;&#30784;&#65292;&#24182;&#27010;&#36848;&#20102;&#23427;&#20204;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25945;&#31243;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/facebookresearch/amortized-optimization-tutorial&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes times faster than traditional optimization methods that do not use amortization. This tutorial presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. The source code for this tutorial is available at https://github.com/facebookresearch/amortized-optimization-tutorial.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#25216;&#26415;&#65292;&#35745;&#31639;&#26426;&#31243;&#24207;&#36890;&#36807;&#23581;&#35797;&#12289;&#24471;&#21040;&#21453;&#39304;&#21644;&#20877;&#27425;&#23581;&#35797;&#26469;&#33258;&#25105;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#39046;&#22495;&#27604;&#26368;&#22909;&#30340;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2201.02135</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#19968;&#26412;&#25945;&#26448;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning, a textbook. (arXiv:2201.02135v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02135
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#25216;&#26415;&#65292;&#35745;&#31639;&#26426;&#31243;&#24207;&#36890;&#36807;&#23581;&#35797;&#12289;&#24471;&#21040;&#21453;&#39304;&#21644;&#20877;&#27425;&#23581;&#35797;&#26469;&#33258;&#25105;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#39046;&#22495;&#27604;&#26368;&#22909;&#30340;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#28216;&#25103;&#29609;&#27861;&#12289;&#20998;&#23376;&#37325;&#32452;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#12290;&#22312;&#25152;&#26377;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#35745;&#31639;&#26426;&#31243;&#24207;&#24050;&#32463;&#23398;&#20250;&#20102;&#33258;&#25105;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#12290;&#23427;&#20204;&#24050;&#32463;&#23398;&#20250;&#20102;&#39134;&#34892;&#27169;&#22411;&#30452;&#21319;&#26426;&#21644;&#36827;&#34892;&#20687;&#29615;&#21644;&#32763;&#28378;&#36825;&#26679;&#30340;&#29305;&#25216;&#21160;&#20316;&#12290;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#29978;&#33267;&#27604;&#26368;&#22909;&#30340;&#20154;&#31867;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#20363;&#22914; Atari&#12289;&#22260;&#26827;&#12289;&#25169;&#20811;&#21644;&#26143;&#38469;&#20105;&#38712;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22797;&#26434;&#29615;&#22659;&#30340;&#26041;&#24335;&#35753;&#25105;&#20204;&#24819;&#36215;&#20102;&#23401;&#23376;&#20204;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#36890;&#36807;&#23581;&#35797;&#12289;&#24471;&#21040;&#21453;&#39304;&#21644;&#20877;&#27425;&#23581;&#35797;&#26469;&#20805;&#28385;&#20048;&#36259;&#22320;&#23398;&#20064;&#12290;&#35745;&#31639;&#26426;&#20284;&#20046;&#30495;&#27491;&#20855;&#22791;&#20102;&#20154;&#31867;&#23398;&#20064;&#30340;&#26041;&#38754;&#65292;&#36825;&#35302;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#26790;&#24819;&#30340;&#26680;&#24515;&#12290;&#30740;&#31350;&#25104;&#21151;&#24341;&#36215;&#20102;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#20851;&#27880;&#65292;&#23398;&#26657;&#24320;&#22987;&#24320;&#35774;&#30456;&#20851;&#35838;&#31243;&#12290;&#26412;&#20070;&#30340;&#30446;&#30340;&#23601;&#26159;&#25552;&#20379;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning has gathered much attention recently. Impressive results were achieved in activities as diverse as autonomous driving, game playing, molecular recombination, and robotics. In all these fields, computer programs have taught themselves to solve difficult problems. They have learned to fly model helicopters and perform aerobatic manoeuvers such as loops and rolls. In some applications they have even become better than the best humans, such as in Atari, Go, poker and StarCraft. The way in which deep reinforcement learning explores complex environments reminds us of how children learn, by playfully trying out things, getting feedback, and trying again. The computer seems to truly possess aspects of human learning; this goes to the heart of the dream of artificial intelligence. The successes in research have not gone unnoticed by educators, and universities have started to offer courses on the subject. The aim of this book is to provide a comprehensive overview of
&lt;/p&gt;</description></item><item><title>GREED&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#21644;&#39044;&#27979;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#21644;SED&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#30830;&#20445;&#36317;&#31163;&#20445;&#25345;&#24230;&#37327;&#23646;&#24615;&#24182;&#20351;SED&#30340;&#39640;&#25928;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#65292;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2112.13143</link><description>&lt;p&gt;
GREED: &#19968;&#31181;&#23398;&#20064;&#22270;&#36317;&#31163;&#20989;&#25968;&#30340;&#31070;&#32463;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GREED: A Neural Framework for Learning Graph Distance Functions. (arXiv:2112.13143v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13143
&lt;/p&gt;
&lt;p&gt;
GREED&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#21644;&#39044;&#27979;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#21644;SED&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#30830;&#20445;&#36317;&#31163;&#20445;&#25345;&#24230;&#37327;&#23646;&#24615;&#24182;&#20351;SED&#30340;&#39640;&#25928;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#65292;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22270;&#30340;&#36317;&#31163;&#20989;&#25968;&#20013;&#65292;&#22270;&#21644;&#23376;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#21644;SED&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#21644;&#26368;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#24230;&#37327;&#20043;&#19968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#31934;&#30830;&#35745;&#31639;&#37117;&#26159;NP&#38590;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#35745;&#31639;&#38590;&#28857;&#65292;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#21644;&#39044;&#27979;&#32534;&#36753;&#36317;&#31163;&#30340;&#31070;&#32463;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#22312;&#21462;&#24471;&#30456;&#24403;&#36827;&#23637;&#30340;&#21516;&#26102;&#65292;&#23384;&#22312;&#38656;&#35201;&#35299;&#20915;&#30340;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#36817;&#20284;&#36317;&#31163;&#20989;&#25968;&#30340;&#21151;&#25928;&#19981;&#20165;&#22312;&#20110;&#20854;&#36924;&#36817;&#31934;&#24230;&#65292;&#36824;&#22312;&#20110;&#20854;&#23646;&#24615;&#30340;&#20445;&#25345;&#12290;&#34429;&#28982;GED&#26159;&#19968;&#20010;&#24230;&#37327;&#65292;&#20294;&#20854;&#31070;&#32463;&#36924;&#36817;&#24182;&#19981;&#25552;&#20379;&#36825;&#26679;&#30340;&#20445;&#35777;&#12290;&#36825;&#19968;&#28857;&#31105;&#27490;&#20102;&#23427;&#20204;&#22312;&#20381;&#36182;&#20110;&#24230;&#37327;&#36317;&#31163;&#20989;&#25968;&#30340;&#39640;&#38454;&#20219;&#21153;&#65288;&#22914;&#32858;&#31867;&#25110;&#32034;&#24341;&#65289;&#20013;&#30340;&#20351;&#29992;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;SED&#26159;&#19981;&#23545;&#31216;&#30340;&#65292;&#22240;&#27492;&#19968;&#20123;&#29616;&#26377;&#30340;GED&#26694;&#26550;&#26080;&#27861;&#25193;&#23637;&#21040;SED&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;GREED&#30340;&#26032;&#22411;&#36830;&#38145;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#39044;&#27979;GED&#21644;SED&#36317;&#31163;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#30830;&#20445;&#23398;&#20064;&#30340;&#36317;&#31163;&#20445;&#25345;&#24230;&#37327;&#23646;&#24615;&#65292;&#24182;&#20351;SED&#30340;&#39640;&#25928;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;GREED&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among various distance functions for graphs, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called GREED, which thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;</title><link>http://arxiv.org/abs/2112.01360</link><description>&lt;p&gt;
&#36335;&#29992;&#25143;&#26816;&#27979;&#30340;&#27010;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Approach for Road-Users Detection. (arXiv:2112.01360v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#24847;&#21619;&#30528;&#23545;&#35821;&#20041;&#23545;&#35937;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#36890;&#24120;&#26159;&#22478;&#24066;&#39550;&#39542;&#29615;&#22659;&#30340;&#29305;&#33394;&#65292;&#22914;&#34892;&#20154;&#21644;&#36710;&#36742;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#24102;&#26377;&#36807;&#20110;&#33258;&#20449;&#30340;&#24471;&#20998;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#20851;&#38190;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#39046;&#22495;&#65292;&#36825;&#26159;&#38750;&#24120;&#19981;&#24076;&#26395;&#30475;&#21040;&#30340;&#65292;&#22240;&#20026;&#28041;&#21450;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#20013;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#65292;&#21521;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#28155;&#21152;&#36825;&#31181;&#27010;&#29575;&#23618;&#12290;&#24314;&#35758;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;Sigmoid&#25110;Softmax&#39044;&#27979;&#23618;&#65292;&#36825;&#20123;&#23618;&#36890;&#24120;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#30495;&#38451;&#24615;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;2D-KITTI&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#20351;&#29992;&#20102;YOLOV4&#21644;S&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection in autonomous driving applications implies that the detection and tracking of semantic objects are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and S
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#27969;&#25968;&#25454;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#24230;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;&#23567;&#25209;&#37327;SG&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#37327;SG&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#36845;&#20195;&#24179;&#22343;&#20540;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#21152;&#36895;&#25910;&#25947;&#30340;&#26041;&#27861;&#21644;&#21516;&#26102;&#25552;&#20379;&#26041;&#24046;&#20943;&#23569;&#21644;&#21152;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2109.07117</link><description>&lt;p&gt;
&#27969;&#25968;&#25454;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Streaming Data. (arXiv:2109.07117v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#27969;&#25968;&#25454;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#24230;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;&#23567;&#25209;&#37327;SG&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#37327;SG&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#36845;&#20195;&#24179;&#22343;&#20540;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#21152;&#36895;&#25910;&#25947;&#30340;&#26041;&#27861;&#21644;&#21516;&#26102;&#25552;&#20379;&#26041;&#24046;&#20943;&#23569;&#21644;&#21152;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27969;&#24335;&#26694;&#26550;&#26469;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;/&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#27969;&#24335;&#26694;&#26550;&#31867;&#20284;&#20110;&#20351;&#29992;&#36880;&#27493;&#21040;&#36798;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#27425;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#24230;&#65307;&#36825;&#21253;&#25324;&#33879;&#21517;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SG&#65289;&#31639;&#27861;&#65288;&#20063;&#31216;&#20026;Robbins-Monro&#31639;&#27861;&#65289;&#65292;&#23567;&#25209;&#37327;SG&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#37327;SG&#31639;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#36845;&#20195;&#24179;&#22343;&#20540;&#65288;&#20063;&#31216;&#20026;Polyak-Ruppert&#24179;&#22343;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;i&#65289;&#22914;&#20309;&#36890;&#36807;&#26681;&#25454;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#27425;&#26469;&#36873;&#25321;&#23398;&#20064;&#36895;&#29575;&#26469;&#21152;&#36895;&#25910;&#25947;&#65307;ii&#65289;Polyak-Ruppert&#24179;&#22343;&#20540;&#22312;&#36798;&#21040;Cramer-Rao&#19979;&#30028;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#65307;iii&#65289;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#27425;&#19982;Polyak-Ruppert&#24179;&#22343;&#20540;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#26041;&#24046;&#20943;&#23569;&#21644;&#21152;&#36895;&#25910;&#25947;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#65288;&#22914;&#22312;&#32447;&#65292;&#39034;&#24207;&#21644;&#22823;&#35268;&#27169;&#65289;&#37117;&#26159;&#26377;&#21033;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a streaming framework for analyzing stochastic approximation/optimization problems. This streaming framework is analogous to solving optimization problems using time-varying mini-batches that arrive sequentially. We provide non-asymptotic convergence rates of various gradient-based algorithms; this includes the famous Stochastic Gradient (SG) descent (a.k.a. Robbins-Monro algorithm), mini-batch SG and time-varying mini-batch SG algorithms, as well as their iterated averages (a.k.a. Polyak-Ruppert averaging). We show i) how to accelerate convergence by choosing the learning rate according to the time-varying mini-batches, ii) that Polyak-Ruppert averaging achieves optimal convergence in terms of attaining the Cramer-Rao lower bound, and iii) how time-varying mini-batches together with Polyak-Ruppert averaging can provide variance reduction and accelerate convergence simultaneously, which is advantageous for many learning problems, such as online, sequential, and large-scale
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#8212;&#8212;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#28040;&#38500;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#26377;&#38480;&#23485;&#24230;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#20445;&#30041;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#30340;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.13097</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#29702;&#35770;&#32473;&#20986;&#20102;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods. (arXiv:2108.13097v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.13097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#8212;&#8212;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#28040;&#38500;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#26377;&#38480;&#23485;&#24230;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#20445;&#30041;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#30340;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#22522;&#20110;&#23427;&#20204;&#36328;&#22810;&#20010;&#23618;&#27425;&#23545;&#36755;&#20837;&#36827;&#34892;&#21464;&#25442;&#20197;&#24314;&#31435;&#33391;&#22909;&#30340;&#39640;&#32423;&#34920;&#31034;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24120;&#35268;&#30340;&#29702;&#35770;&#26041;&#27861;&#65288;&#27491;&#24335;&#20026;NNGPs&#65289;&#28041;&#21450;&#26080;&#38480;&#23485;&#38480;&#21046;&#28040;&#38500;&#20102;&#34920;&#31034;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38480;&#23485;&#38480;&#21046;&#8212;&#8212;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#65292;&#23427;&#23637;&#29616;&#20102;&#22312;&#26377;&#38480;&#23485;&#24230;&#27169;&#22411;&#20013;&#38236;&#20687;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#19968;&#20123;&#26631;&#20934;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#30340;&#31616;&#21333;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#36125;&#21494;&#26031;&#34920;&#31034;&#23398;&#20064;&#26497;&#38480;&#19979;&#30340;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#65288;DGPs&#65289;&#20855;&#26377;&#30830;&#20999;&#30340;&#22810;&#20803;&#39640;&#26031;&#21518;&#39564;&#20998;&#24067;&#65292;&#21518;&#39564;&#21327;&#26041;&#24046;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#19968;&#31181;&#21487;&#35299;&#37322;&#30446;&#26631;&#24471;&#21040;&#65292;&#35813;&#30446;&#26631;&#32467;&#21512;&#20102;&#22686;&#24378;&#24615;&#33021;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#19968;&#31995;&#21015;&#30340;KL-&#25955;&#24230;&#65292;&#20351;&#24471;&#21518;&#39564;&#20998;&#24067;&#25509;&#36817;&#20808;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a log-likelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We
&lt;/p&gt;</description></item><item><title>MRCpy&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;0-1&#25439;&#22833;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#25552;&#20379;&#20102;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2108.01952</link><description>&lt;p&gt;
MRCpy&#65306;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.01952
&lt;/p&gt;
&lt;p&gt;
MRCpy&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;0-1&#25439;&#22833;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#25552;&#20379;&#20102;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29616;&#26377;&#30340;&#30417;&#30563;&#20998;&#31867;&#24211;&#37117;&#26159;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20351;&#29992;&#20195;&#29702;&#25439;&#22833;&#25216;&#26415;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;MRCpy&#24211;&#65292;&#35813;&#24211;&#23454;&#29616;&#20102;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#65288;MRC&#65289;&#65292;&#24182;&#21487;&#21033;&#29992;0-1&#25439;&#22833;&#12290;&#36825;&#31181;&#25216;&#26415;&#20135;&#29983;&#20102;&#35768;&#22810;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;MRCpy&#20026;&#19981;&#21516;&#21464;&#37327;&#30340;MRC&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#24182;&#36981;&#24490;&#27969;&#34892;Python&#24211;&#30340;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;MRCpy&#36824;&#25552;&#20379;&#20102;&#23454;&#29616;&#19968;&#20123;&#27969;&#34892;&#25216;&#26415;&#30340;&#21151;&#33021;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#30475;&#20316;&#26159;MRC&#65292;&#20363;&#22914;L1&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#65292;0-1&#23545;&#25239;&#24615;&#21644;&#26368;&#22823;&#29109;&#26426;&#12290;&#27492;&#22806;&#65292;MRCpy&#36824;&#23454;&#29616;&#20102;&#26368;&#36817;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#22914;&#20613;&#37324;&#21494;&#65292;ReLU&#21644;&#38408;&#20540;&#29305;&#24449;&#12290;&#35813;&#24211;&#37319;&#29992;&#38754;&#21521;&#23545;&#35937;&#30340;&#26041;&#27861;&#35774;&#35745;&#65292;&#26041;&#20415;&#21327;&#20316;&#32773;&#21644;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing libraries for supervised classification implement techniques that are based on empirical risk minimization and utilize surrogate losses. We present MRCpy library that implements minimax risk classifiers (MRCs) that are based on robust risk minimization and can utilize 0-1-loss. Such techniques give rise to a manifold of classification methods that can provide tight bounds on the expected loss. MRCpy provides a unified interface for different variants of MRCs and follows the standards of popular Python libraries. The presented library also provides implementation for popular techniques that can be seen as MRCs such as L1-regularized logistic regression, zero-one adversarial, and maximum entropy machines. In addition, MRCpy implements recent feature mappings such as Fourier, ReLU, and threshold features. The library is designed with an object-oriented approach that facilitates collaborators and users.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2107.08574</link><description>&lt;p&gt;
&#19968;&#31181;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#30340;&#35843;&#21046;&#23618;
&lt;/p&gt;
&lt;p&gt;
A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32570;&#22833;&#21644;&#36136;&#37327;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#24320;&#21457;&#32773;&#36890;&#24120;&#21482;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#31934;&#24515;&#31579;&#36873;&#20986;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20250;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#21033;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#12290;&#36825;&#21463;&#21551;&#21457;&#20110;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#35843;&#21046;&#65292;&#30382;&#36136;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#30340;&#21487;&#38752;&#24615;&#21644;&#20854;&#20182;&#25968;&#25454;&#30340;&#23384;&#22312;&#31243;&#24230;&#19978;&#19979;&#35843;&#33410;&#36755;&#20837;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#21487;&#38752;&#24615;&#24471;&#20998;&#20316;&#20026;&#35843;&#21046;&#20449;&#21495;&#65292;&#21457;&#29616;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#65288;&#21253;&#25324;&#39069;&#22806;&#30340;&#32570;&#22833;&#25968;&#25454;&#65289;&#26356;&#21152;&#40065;&#26834;&#12290;&#36825;&#20123;&#27169;&#22411;&#20248;&#20110;&#25554;&#34917;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23436;&#20840;&#36339;&#36807;&#25554;&#34917;&#36807;&#31243;&#33410;&#30465;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputatio
&lt;/p&gt;</description></item><item><title>Auto-NBA&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#25628;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32806;&#21512;&#30340;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2106.06575</link><description>&lt;p&gt;
Auto-NBA: &#38024;&#23545;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32852;&#21512;&#31354;&#38388;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators. (arXiv:2106.06575v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06575
&lt;/p&gt;
&lt;p&gt;
Auto-NBA&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#25628;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32806;&#21512;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#21516;&#26102;&#20248;&#21270;&#21644;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#38656;&#35201;&#32852;&#21512;&#32771;&#34385;&#19977;&#20010;&#19981;&#21516;&#21364;&#39640;&#24230;&#32806;&#21512;&#30340;&#26041;&#38754;&#65306;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#25628;&#32034;&#38754;&#20020;&#30340;&#25361;&#25112;&#23578;&#26410;&#34987;&#23436;&#20840;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#21253;&#25324;&#65288;1&#65289;&#26159;&#21542;&#25193;&#22823;&#30001;&#20110;&#24040;&#22823;&#32852;&#21512;&#31354;&#38388;&#32780;&#23548;&#33268;&#30340;&#20869;&#23384;&#28040;&#32791;&#65292;&#36824;&#26159;&#37319;&#29992;&#27425;&#20248;&#35774;&#35745;&#65292;&#65288;2&#65289;&#21152;&#36895;&#22120;&#35774;&#35745;&#31354;&#38388;&#30340;&#31163;&#25955;&#24615;&#19982;&#32593;&#32476;&#21644;&#27604;&#29305;&#23485;&#24230;&#35774;&#35745;&#31354;&#38388;&#30456;&#32806;&#21512;&#19988;&#19981;&#21516;&#65292;&#20197;&#21450;&#65288;3&#65289;&#32593;&#32476;-&#21152;&#36895;&#22120;&#32852;&#21512;&#25628;&#32034;&#20013;&#30340;&#8220;&#40481;&#29983;&#34507;&#34507;&#29983;&#40481;&#8221;&#38382;&#39064;&#65306;&#21363;&#32852;&#21512;&#25628;&#32034;&#38656;&#35201;&#35745;&#31639;&#25805;&#20316;&#30340;&#30828;&#20214;&#25104;&#26412;&#65292;&#28982;&#32780;&#22312;&#25628;&#32034;&#26399;&#38388;&#65292;&#23578;&#19981;&#30693;&#36947;&#25972;&#20010;&#32593;&#32476;&#30340;&#26368;&#20339;&#21152;&#36895;&#22120;&#65292;&#22240;&#27492;&#26080;&#27861;&#35745;&#31639;&#36825;&#20123;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Auto-NBA&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#38024;&#23545;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32852;&#21512;&#31354;&#38388;&#30340;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
While maximizing deep neural networks' (DNNs') acceleration efficiency requires a joint search/design of three different yet highly coupled aspects, including the networks, bitwidths, and accelerators, the challenges associated with such a joint search have not yet been fully understood and addressed. The key challenges include (1) the dilemma of whether to explode the memory consumption due to the huge joint space or achieve sub-optimal designs, (2) the discrete nature of the accelerator design space that is coupled yet different from that of the networks and bitwidths, and (3) the chicken and egg problem associated with network-accelerator co-search, i.e., co-search requires operation-wise hardware cost, which is lacking during search as the optimal accelerator depending on the whole network is still unknown during search. To tackle these daunting challenges towards optimal and fast development of DNN accelerators, we propose a framework dubbed Auto-NBA to enable jointly searching fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20803;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#40065;&#26834;&#27169;&#22411;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#36817;&#20284;&#27491;&#30830;&#22320;&#23398;&#20064;GMMs&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#33021;&#22815;&#36817;&#20284;&#30830;&#23450;&#25311;&#21512;&#20998;&#24067;&#25152;&#38656;&#30340;&#26368;&#23569;&#32452;&#20214;&#25968;&#12290;</title><link>http://arxiv.org/abs/2106.02774</link><description>&lt;p&gt;
GMM&#30340;&#40065;&#26834;&#27169;&#22411;&#36873;&#25321;&#21644;&#36817;&#20284;&#27491;&#30830;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Model Selection and Nearly-Proper Learning for GMMs. (arXiv:2106.02774v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20803;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#40065;&#26834;&#27169;&#22411;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#36817;&#20284;&#27491;&#30830;&#22320;&#23398;&#20064;GMMs&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#33021;&#22815;&#36817;&#20284;&#30830;&#23450;&#25311;&#21512;&#20998;&#24067;&#25152;&#38656;&#30340;&#26368;&#23569;&#32452;&#20214;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#36890;&#24120;&#20551;&#23450;&#25968;&#25454;&#26159;&#20174;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#20294;&#26159;&#22914;&#26524;&#20107;&#20808;&#19981;&#30693;&#36947;&#32452;&#20998;&#25968;&#20250;&#21457;&#29983;&#20160;&#20040;&#21602;&#65311;&#20272;&#35745;&#32452;&#20998;&#25968;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#36523;&#19978;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23601;&#31639;&#26159;&#27809;&#26377;&#26377;&#25928;&#31639;&#27861;&#65292;&#26356;&#19981;&#29992;&#35828;&#33021;&#23481;&#24525;&#23545;&#25239;&#24615;&#25200;&#21160;&#20102;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20803;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#40065;&#26834;&#27169;&#22411;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20250;&#20174;&#19968;&#20010;&#19982;$k$&#20010;&#32452;&#20998;&#30340;GMM $\epsilon$ -close&#30340;&#20998;&#24067;&#20013;&#20135;&#29983;$\textsf{poly}(k / \epsilon)$&#20010;&#26679;&#26412;&#65292;&#29992;$\textsf{poly}(k / \epsilon)$&#26102;&#38388;&#26500;&#24314;&#19968;&#20010;&#26377;$\widetilde{O}(k)$&#20010;&#32452;&#20214;&#30340;GMM&#65292;&#21487;&#22312;$\widetilde {O} (\epsilon)$&#20869;&#36817;&#20284;&#34920;&#31034;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33021;&#22815;&#36817;&#20284;&#30830;&#23450;&#25311;&#21512;&#20998;&#24067;&#25152;&#38656;&#30340;&#26368;&#23569;&#32452;&#20214;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#21807;&#19968;&#24050;&#30693;&#30340;&#26377;&#25928;&#31639;&#27861;&#38656;&#35201;&#33267;&#23569; $O(k \log \log n)$ &#20010;&#32452;&#20214;&#25165;&#33021;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#36825;&#24050;&#36817;&#20046;&#36798;&#21040;&#20102;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#27491;&#30830;&#30340;&#65292;&#21363;&#65292;&#20854;&#20855;&#26377;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20165;&#20855;&#26377;&#23545;&#25968;&#22240;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#23545;&#25239;&#25200;&#21160;&#19979;&#40065;&#26834;&#22320;&#23398;&#20064;GMMs&#12290;
&lt;/p&gt;
&lt;p&gt;
In learning theory, a standard assumption is that the data is generated from a finite mixture model. But what happens when the number of components is not known in advance? The problem of estimating the number of components, also called model selection, is important in its own right but there are essentially no known efficient algorithms with provable guarantees let alone ones that can tolerate adversarial corruptions. In this work, we study the problem of robust model selection for univariate Gaussian mixture models (GMMs). Given $\textsf{poly}(k/\epsilon)$ samples from a distribution that is $\epsilon$-close in TV distance to a GMM with $k$ components, we can construct a GMM with $\widetilde{O}(k)$ components that approximates the distribution to within $\widetilde{O}(\epsilon)$ in $\textsf{poly}(k/\epsilon)$ time. Thus we are able to approximately determine the minimum number of components needed to fit the distribution within a logarithmic factor. Prior to our work, the only known 
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#26080;&#27169;&#22411;&#22320;&#37325;&#26032;&#35774;&#35745;&#38024;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#23454;&#39564;&#27425;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#20004;&#27493;&#35774;&#35745;&#26041;&#27861;&#65292;&#20808;&#35774;&#35745;&#32447;&#24615;&#25511;&#21046;&#24459;&#36798;&#21040;&#21021;&#27493;&#25511;&#21046;&#65292;&#20877;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2103.03808</link><description>&lt;p&gt;
&#20004;&#27493;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#38750;&#32447;&#24615;&#26368;&#20248;&#35843;&#33410;&#22120;&#26080;&#27169;&#22411;&#37325;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Two-step reinforcement learning for model-free redesign of nonlinear optimal regulator. (arXiv:2103.03808v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03808
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#26080;&#27169;&#22411;&#22320;&#37325;&#26032;&#35774;&#35745;&#38024;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#23454;&#39564;&#27425;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#20004;&#27493;&#35774;&#35745;&#26041;&#27861;&#65292;&#20808;&#35774;&#35745;&#32447;&#24615;&#25511;&#21046;&#24459;&#36798;&#21040;&#21021;&#27493;&#25511;&#21046;&#65292;&#20877;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#25511;&#21046;&#24212;&#29992;&#20013;&#65292;&#38381;&#29615;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;&#30528;&#24037;&#21378;&#29305;&#24615;&#30340;&#25913;&#21464;&#32780;&#38543;&#26102;&#38388;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#37325;&#26032;&#35774;&#35745;&#25511;&#21046;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#31995;&#32479;&#24314;&#27169;&#36807;&#31243;&#65292;&#36825;&#23545;&#20110;&#38381;&#29615;&#31995;&#32479;&#26469;&#35828;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#26368;&#20248;&#25511;&#21046;&#22120;&#21487;&#20197;&#20165;&#22522;&#20110;&#38381;&#29615;&#31995;&#32479;&#27979;&#37327;&#25968;&#25454;&#37325;&#26032;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;RL&#30340;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#25511;&#21046;&#19981;&#33391;&#30340;&#31995;&#32479;&#36827;&#34892;&#30456;&#24403;&#25968;&#37327;&#30340;&#35797;&#38169;&#23454;&#39564;&#65292;&#36825;&#21487;&#33021;&#20250;&#21152;&#36895;&#24037;&#21378;&#30340;&#30952;&#25439;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20004;&#27493;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#25552;&#39640;&#20102;RL&#22312;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#35843;&#33410;&#22120;&#37325;&#26032;&#35774;&#35745;&#38382;&#39064;&#20013;&#30340;&#26242;&#24577;&#23398;&#20064;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#25511;&#21046;&#24459;&#65292;&#20197;&#23454;&#29616;&#19968;&#23450;&#31243;&#24230;&#30340;&#25511;&#21046;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In many practical control applications, the performance level of a closed-loop system degrades over time due to the change of plant characteristics. Thus, there is a strong need for redesigning a controller without going through the system modeling process, which is often difficult for closed-loop systems. Reinforcement learning (RL) is one of the promising approaches that enable model-free redesign of optimal controllers for nonlinear dynamical systems based only on the measurement of the closed-loop system. However, the learning process of RL usually requires a considerable number of trial-and-error experiments using the poorly controlled system that may accumulate wear on the plant. To overcome this limitation, we propose a model-free two-step design approach that improves the transient learning performance of RL in an optimal regulator redesign problem for unknown nonlinear systems. Specifically, we first design a linear control law that attains some degree of control performance i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#35748;&#20026;&#36825;&#31867;&#39034;&#24207;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#24182;&#33021;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2101.09855</link><description>&lt;p&gt;
&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#30340;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Weak Signal Asymptotics for Sequentially Randomized Experiments. (arXiv:2101.09855v5 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.09855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#35748;&#20026;&#36825;&#31867;&#39034;&#24207;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#24182;&#33021;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#21253;&#25324;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#23454;&#39564;&#12290;&#22312;&#19968;&#20010;$n$&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35753;&#19981;&#21516;&#21160;&#20316;&#30340;&#24179;&#22343;&#22870;&#21169;&#38388;&#38553;&#25353;&#29031;$1/\sqrt{n}$&#30340;&#27604;&#20363;&#32553;&#25918;&#65292;&#20197;&#20445;&#25345;&#23398;&#20064;&#20219;&#21153;&#30340;&#38590;&#24230;&#38543;&#30528;$n$&#30340;&#22686;&#38271;&#32780;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#20854;&#20013;&#65292;&#33218;&#36873;&#25321;&#30340;&#27010;&#29575;&#20250;&#38543;&#30528;&#29366;&#24577;&#30340;&#21464;&#21270;&#32780;&#25345;&#32493;&#21464;&#21270;&#65292;&#24182;&#22312;&#28385;&#36275;&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;&#25193;&#25955;&#26497;&#38480;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#31934;&#32454;&#30340;&#12289;&#29305;&#23450;&#20110;&#23454;&#20363;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#65292;&#21253;&#25324;&#27748;&#26222;&#26862;&#37319;&#26679;&#65288;&#20294;&#19981;&#21253;&#25324;&#19981;&#28385;&#36275;&#25105;&#20204;&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;UCB&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#39034;&#24207;&#23454;&#39564;&#30340;&#34920;&#29616;&#37117;&#33021;&#22312;&#38271;&#26102;&#38388;&#20869;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the lens of weak signal asymptotics to study a class of sequentially randomized experiments, including those that arise in solving multi-armed bandit problems. In an experiment with $n$ time steps, we let the mean reward gaps between actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the learning task as $n$ grows. In this regime, we show that the sample paths of a class of sequentially randomized experiments -- adapted to this scaling regime and with arm selection probabilities that vary continuously with state -- converge weakly to a diffusion limit, given as the solution to a stochastic differential equation. The diffusion limit enables us to derive refined, instance-specific characterization of stochastic dynamics, and to obtain several insights on the regret and belief evolution of a number of sequential experiments including Thompson sampling (but not UCB, which does not satisfy our continuity assumption). We show that all sequential experiments wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#21442;&#29031;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#32467;&#35770;&#19982;&#29983;&#29289;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#35299;&#37322;&#65292;&#21253;&#25324;&#28436;&#21270;&#21644;&#20154;&#33041;&#30340;&#21151;&#33021;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2001.11825</link><description>&lt;p&gt;
&#36882;&#24402;&#12289;&#28436;&#21270;&#21644;&#33258;&#25105;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Recursion, evolution and conscious self. (arXiv:2001.11825v4 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.11825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#21442;&#29031;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#32467;&#35770;&#19982;&#29983;&#29289;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#35299;&#37322;&#65292;&#21253;&#25324;&#28436;&#21270;&#21644;&#20154;&#33041;&#30340;&#21151;&#33021;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#23398;&#20064;&#29702;&#35770;&#65292;&#23427;&#22823;&#33268;&#26159;&#33258;&#21160;&#30340;&#65292;&#21363;&#20165;&#38656;&#35201;&#26368;&#23569;&#37327;&#30340;&#21021;&#22987;&#32534;&#31243;&#65292;&#24182;&#19988;&#22522;&#20110;&#33258;&#25105;&#21442;&#29031;&#30340;&#28508;&#22312;&#35745;&#31639;&#29616;&#35937;&#65288;&#21363;&#31639;&#27861;&#20855;&#26377;&#20854;&#31243;&#24207;&#20316;&#20026;&#36755;&#20837;&#30340;&#28508;&#22312;&#33021;&#21147;&#65289;&#12290;&#32467;&#35770;&#19982;&#29983;&#29289;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#35299;&#37322;&#65292;&#21253;&#25324;&#28436;&#21270;&#65288;&#19982;&#36798;&#23572;&#25991;&#20027;&#20041;&#30456;&#32467;&#21512;&#65289;&#20197;&#21450;&#20154;&#33041;&#30340;&#21151;&#33021;&#21644;&#23398;&#20064;&#33021;&#21147;&#65288;&#26368;&#37325;&#35201;&#30340;&#26159;&#65289;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#33258;&#24049;&#36523;&#19978;&#24863;&#30693;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and study a learning theory which is roughly automatic, that is, it does not require but a minimum of initial programming, and is based on the potential computational phenomenon of self-reference, (i.e. the potential ability of an algorithm to have its program as an input).  The conclusions agree with scientific findings in both biology and neuroscience and provide a plethora of explanations both (in conjunction with Darwinism) about evolution, as well as for the functionality and learning capabilities of human brain, (most importantly), as we perceive them in ourselves.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#27169;&#25311;&#22120;&#30340;&#26032;&#25216;&#26415;&#29992;&#20110;&#20998;&#26512;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#21306;&#20998;&#22909;&#30340;&#37319;&#26679;&#31574;&#30053;&#21644;&#22351;&#37319;&#26679;&#31574;&#30053;&#30340;&#38590;&#24230;&#19978;&#12290;&#22312;&#32431;&#25506;&#32034;&#22330;&#26223;&#30340;&#32467;&#26500;&#21270;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#24212;&#29992;&#20102;&#35813;&#25216;&#26415;&#65292;&#23637;&#31034;&#20102;&#26377;&#20013;&#31561;&#32622;&#20449;&#24230;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#25991;&#29486;&#20013;&#22312; $\delta \to 0$ &#26102;&#24471;&#21040;&#30340;&#28176;&#36817;&#22797;&#26434;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#23454;&#36136;&#24615;&#24046;&#24322;&#65292;&#24182;&#19988;&#36824;&#35777;&#26126;&#20102;&#20316;&#20026;&#39030;&#37096;-k&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#23454;&#20363;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/1702.05186</link><description>&lt;p&gt;
&#27169;&#25311;&#22120;&#65306;&#29702;&#35299;&#22312;&#20013;&#31561;&#32622;&#20449;&#24230;&#26465;&#20214;&#19979;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime. (arXiv:1702.05186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1702.05186
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#27169;&#25311;&#22120;&#30340;&#26032;&#25216;&#26415;&#29992;&#20110;&#20998;&#26512;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#21306;&#20998;&#22909;&#30340;&#37319;&#26679;&#31574;&#30053;&#21644;&#22351;&#37319;&#26679;&#31574;&#30053;&#30340;&#38590;&#24230;&#19978;&#12290;&#22312;&#32431;&#25506;&#32034;&#22330;&#26223;&#30340;&#32467;&#26500;&#21270;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#24212;&#29992;&#20102;&#35813;&#25216;&#26415;&#65292;&#23637;&#31034;&#20102;&#26377;&#20013;&#31561;&#32622;&#20449;&#24230;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#25991;&#29486;&#20013;&#22312; $\delta \to 0$ &#26102;&#24471;&#21040;&#30340;&#28176;&#36817;&#22797;&#26434;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#23454;&#36136;&#24615;&#24046;&#24322;&#65292;&#24182;&#19988;&#36824;&#35777;&#26126;&#20102;&#20316;&#20026;&#39030;&#37096;-k&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#23454;&#20363;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;&#27169;&#25311;&#22120;&#8221;&#65292;&#29992;&#20110;&#20998;&#26512;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#32771;&#34385;&#20219;&#20309;&#22266;&#23450;&#37319;&#26679;&#31574;&#30053;&#21487;&#20197;&#25910;&#38598;&#22810;&#23569;&#20449;&#24687;&#65292;&#32780;&#26159;&#32771;&#34385;&#22312;&#32473;&#23450;&#30340;&#26377;&#38480;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#20869;&#65292;&#21306;&#20998;&#22909;&#30340;&#37319;&#26679;&#31574;&#30053;&#21644;&#22351;&#30340;&#37319;&#26679;&#31574;&#30053;&#26377;&#22810;&#38590;&#12290;&#36825;&#31181;&#35270;&#35282;&#30340;&#25913;&#21464;&#20351;&#25105;&#20204;&#33021;&#22815;&#21305;&#37197;Fano&#21644;&#21464;&#37327;&#27979;&#37327;&#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#32780;&#19981;&#20250;&#38519;&#20837;&#20219;&#20309;&#19968;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#20013;&#12290;&#20026;&#20102;&#20855;&#20307;&#35828;&#26126;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#24212;&#29992;&#21040;&#20102;&#19968;&#20010;&#22266;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#32431;&#25506;&#32034;&#22330;&#26223;&#20013;&#30340;&#32467;&#26500;&#21270;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22343;&#20540;&#38480;&#21046;&#19979;&#65292;&#26377;&#20013;&#31561;&#32622;&#20449;&#24230;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#25991;&#29486;&#20013;&#22312; $\delta \to 0$ &#26102;&#24471;&#21040;&#30340;&#28176;&#36817;&#22797;&#26434;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20316;&#20026;&#39030;&#37096;-k&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#23454;&#20363;&#30340;&#19979;&#30028;&#65292;&#20854;&#21253;&#25324;&#36866;&#24403;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel technique for analyzing adaptive sampling called the {\em Simulator}. Our approach differs from the existing methods by considering not how much information could be gathered by any fixed sampling strategy, but how difficult it is to distinguish a good sampling strategy from a bad one given the limited amount of data collected up to any given time. This change of perspective allows us to match the strength of both Fano and change-of-measure techniques, without succumbing to the limitations of either method. For concreteness, we apply our techniques to a structured multi-arm bandit problem in the fixed-confidence pure exploration setting, where we show that the constraints on the means imply a substantial gap between the moderate-confidence sample complexity, and the asymptotic sample complexity as $\delta \to 0$ found in the literature. We also prove the first instance-based lower bounds for the top-k problem which incorporate the appropriate log-factors. Moreover, o
&lt;/p&gt;</description></item></channel></rss>