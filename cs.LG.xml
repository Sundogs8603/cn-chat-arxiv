<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01376</link><description>&lt;p&gt;
LoTR: &#20302;&#24352;&#37327;&#31209;&#26435;&#37325;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoTR: Low Tensor Rank Weight Adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01376
&lt;/p&gt;
&lt;p&gt;
LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24605;&#24819;&#25512;&#24191;&#21644;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;Transformer&#26550;&#26500;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;LoRA&#31867;&#26041;&#27861;&#26159;&#22522;&#20110;&#26799;&#24230;&#26356;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#20197;&#24352;&#37327;&#20998;&#35299;&#30340;&#24418;&#24335;&#34920;&#31034;&#21442;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#27599;&#20010;&#23618;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#37117;&#30001;&#19977;&#20010;&#30697;&#38453;&#30340;&#20056;&#31215;&#26500;&#25104;&#65292;&#32780;&#24352;&#37327;&#32467;&#26500;&#26159;&#30001;&#36825;&#20010;&#20056;&#31215;&#30340;&#24038;&#21491;&#20056;&#23376;&#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#24341;&#36215;&#30340;&#12290;&#36890;&#36807;&#23545;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#23618;&#21516;&#26102;&#21387;&#32553;&#65292;LoTR&#33021;&#22815;&#27604;LoRA&#22312;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#23618;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26680;&#24515;&#24352;&#37327;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#26435;&#37325;&#32500;&#24230;&#65292;&#21487;&#20197;&#20219;&#24847;&#32553;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#24120;&#24265;&#20215;&#21644;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01327</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#20013;&#30340;&#30417;&#30563;&#31639;&#27861;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Supervised Algorithmic Fairness in Distribution Shifts: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#38754;&#23545;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#65292;&#22914;&#20309;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#22240;&#21508;&#31181;&#22240;&#32032;&#32780;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#31181;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#23545;&#29305;&#23450;&#36890;&#36807;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#26469;&#34920;&#24449;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20102;&#24635;&#32467;&#65292;&#24182;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#36825;&#20123;&#21464;&#21270;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#25991;&#29486;&#20013;&#31361;&#20986;&#20102;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20221;&#35843;&#26597;&#21015;&#20986;&#20102;&#29992;&#20110;&#23454;&#35777;&#30740;&#31350;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#25968;&#30028;&#38480;&#65292;&#22686;&#24378;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#25351;&#25968;&#35889;&#34928;&#20943;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38750;&#24179;&#20961;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#24449;&#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#30340;&#26680;&#22238;&#24402;&#22120;&#21017;&#20855;&#26377;&#28798;&#38590;&#24615;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01297</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35889;&#34920;&#24449;&#26680;&#23725;&#22238;&#24402;&#30340;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01297
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#25968;&#30028;&#38480;&#65292;&#22686;&#24378;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#25351;&#25968;&#35889;&#34928;&#20943;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38750;&#24179;&#20961;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#24449;&#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#30340;&#26680;&#22238;&#24402;&#22120;&#21017;&#20855;&#26377;&#28798;&#38590;&#24615;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#26680;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#30340;&#26032;&#30028;&#38480;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#22686;&#24378;&#20102;&#22312;&#22266;&#23450;&#36755;&#20837;&#32500;&#24230;&#30340;&#36807;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#26680;&#23725;&#22238;&#24402;&#30340;&#29616;&#26377;&#38750;&#28176;&#36817;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#20855;&#26377;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#30028;&#38480;&#65307;&#23545;&#20110;&#25351;&#25968;&#34928;&#20943;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#38750;&#24179;&#20961;&#21644;&#26032;&#39062;&#30340;&#12290;&#25105;&#20204;&#23545;&#36807;&#25311;&#21512;&#30340;&#32467;&#35770;&#26159;&#21452;&#37325;&#30340;&#65306;(i) &#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#24517;&#39035;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#24456;&#22909;&#30340;&#27867;&#21270;&#65307;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#25152;&#35859;&#30340;&#28201;&#21644;&#36807;&#25311;&#21512;&#65307;(ii) &#22914;&#26524;&#20219;&#20309;&#26680;&#23725;&#22238;&#24402;&#22120;&#30340;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#65292;&#21017;&#20854;&#27867;&#21270;&#24046;&#65292;&#21363;&#34920;&#29616;&#20986;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#36825;&#22686;&#21152;&#20102;&#26680;&#23725;&#22238;&#24402;&#22120;&#34920;&#29616;&#20986;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#21487;&#29992;&#29305;&#24449;&#35889;&#34928;&#20943;&#27425;&#22810;&#39033;&#24335;&#30340;&#26497;&#31471;&#24773;&#20917;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#21512;&#20102;&#26032;&#30340;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;(RMT)&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) te
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#36793;&#38469;&#25233;&#21046;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#38477;&#20302;&#36807;&#21435;&#20219;&#21153;&#30340;&#36951;&#24536;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01262</link><description>&lt;p&gt;
&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65306;&#36890;&#36807;&#27010;&#29575;&#32553;&#25918;&#36827;&#34892;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascaded Scaling Classifier: class incremental learning with probability scaling
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#36793;&#38469;&#25233;&#21046;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#38477;&#20302;&#36807;&#21435;&#20219;&#21153;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#33719;&#21462;&#26032;&#30693;&#35782;&#24182;&#23558;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#20165;&#26377;&#36731;&#24494;&#30340;&#36951;&#24536;&#12290;&#21516;&#26679;&#30340;&#33021;&#21147;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#36830;&#32493;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#24433;&#21709;&#21040;&#36807;&#21435;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#36951;&#24536;&#21487;&#20197;&#36890;&#36807;&#22238;&#25918;&#23384;&#20648;&#30340;&#36807;&#21435;&#20219;&#21153;&#26679;&#26412;&#26469;&#32531;&#35299;&#65292;&#20294;&#26159;&#23545;&#20110;&#38271;&#24207;&#21015;&#20219;&#21153;&#21487;&#33021;&#38656;&#35201;&#36739;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#65307;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#20445;&#23384;&#26679;&#26412;&#30340;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#37327;&#20998;&#31867;&#22120;&#65292;&#20998;&#21035;&#31216;&#20026;&#36793;&#38469;&#25233;&#21046;&#21644;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#12290;&#21069;&#32773;&#32467;&#21512;&#20102;&#36719;&#32422;&#26463;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#36807;&#21435;&#23398;&#20064;&#30340;&#30693;&#35782;&#21516;&#26102;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#12290;&#21518;&#32773;&#26159;&#19968;&#31181;&#24102;&#26377;&#38376;&#25511;&#30340;&#22686;&#37327;&#20998;&#31867;&#22120;&#65292;&#24110;&#21161;&#27169;&#22411;&#20462;&#25913;&#36807;&#21435;&#30340;&#39044;&#27979;&#32780;&#19981;&#30452;&#25509;&#24178;&#25200;&#23427;&#20204;&#12290;&#36825;&#26159;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03312</link><description>&lt;p&gt;
&#28145;&#24230;&#34917;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Depth Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#22312;&#19968;&#20123;&#65288;&#28304;&#65289;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#30446;&#26631;&#27979;&#35797;&#25968;&#25454;&#26102;&#65292;&#24120;&#24120;&#20250;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#24046;&#36317;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22914;&#39046;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#21487;&#33021;&#38656;&#35201;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#28304;&#25968;&#25454;&#65288;&#36890;&#24120;&#19981;&#21487;&#29992;&#65289;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;&#26080;&#28304;DA&#65292;&#21017;&#38656;&#35201;&#22810;&#27425;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#65292;&#21363;&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#31232;&#30095;&#28145;&#24230;&#22270;&#25512;&#26029;&#20986;&#23494;&#38598;&#28145;&#24230;&#22270;&#30340;&#20219;&#21153;&#65292;&#20197;&#22312;&#19968;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#27599;&#31181;&#25968;&#25454;&#27169;&#24577;&#20013;&#30340;&#39046;&#22495;&#36716;&#31227;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#31232;&#30095;&#28145;&#24230;&#27169;&#24577;&#23637;&#29616;&#20986;&#27604;&#22270;&#20687;&#26356;&#23567;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#28304;&#39046;&#22495;&#20013;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22359;&#65292;&#23427;&#20445;&#30041;&#20102;&#20174;&#20165;&#32534;&#30721;&#31232;&#30095;&#28145;&#24230;&#29305;&#24449;&#21040;&#32534;&#30721;&#22270;&#20687;&#21644;&#31232;&#30095;&#28145;&#24230;&#30340;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#23884;&#20837;&#27169;&#22359;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
&lt;/p&gt;</description></item><item><title>HASSOD&#26159;&#19968;&#31181;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03311</link><description>&lt;p&gt;
HASSOD&#65306;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HASSOD: Hierarchical Adaptive Self-Supervised Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03311
&lt;/p&gt;
&lt;p&gt;
HASSOD&#26159;&#19968;&#31181;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#26102;&#27809;&#26377;&#26126;&#30830;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20197;&#21450;&#29702;&#35299;&#29289;&#20307;&#30340;&#37096;&#20998;&#25972;&#20307;&#32452;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#21463;&#21040;&#36825;&#20004;&#20010;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;(HASSOD)&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#29289;&#20307;&#24182;&#29702;&#35299;&#20854;&#32452;&#25104;&#12290;HASSOD&#37319;&#29992;&#20998;&#23618;&#33258;&#36866;&#24212;&#32858;&#31867;&#31574;&#30053;&#65292;&#26681;&#25454;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;&#34920;&#31034;&#23558;&#21306;&#22495;&#20998;&#32452;&#20026;&#23545;&#35937;&#25513;&#30721;&#65292;&#33258;&#36866;&#24212;&#30830;&#23450;&#27599;&#20010;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;HASSOD&#36890;&#36807;&#20998;&#26512;&#25513;&#30721;&#20043;&#38388;&#30340;&#35206;&#30422;&#20851;&#31995;&#21644;&#26500;&#24314;&#26641;&#32467;&#26500;&#26469;&#35782;&#21035;&#23545;&#35937;&#30340;&#23618;&#27425;&#32423;&#21035;&#12290;&#36825;&#31181;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#24182;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#20302;&#25928;&#30340;&#22810;&#36718;&#33258;&#25105;&#35757;&#32451;&#36807;&#31243;&#65292;&#32780;&#37319;&#21462;&#20102;&#19968;&#31181;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#19978;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead ad
&lt;/p&gt;</description></item><item><title>AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.03309</link><description>&lt;p&gt;
AONeuS: &#19968;&#31181;&#29992;&#20110;&#22768;&#20809;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03309
&lt;/p&gt;
&lt;p&gt;
AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#24863;&#30693;&#21644;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#28041;&#21450;&#24314;&#31569;&#12289;&#23433;&#20840;&#12289;&#28023;&#27915;&#32771;&#21476;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#12290;&#24694;&#21155;&#30340;&#25805;&#20316;&#26465;&#20214;&#12289;&#33030;&#24369;&#30340;&#29615;&#22659;&#21644;&#26377;&#38480;&#30340;&#23548;&#33322;&#25511;&#21046;&#36890;&#24120;&#23548;&#33268;&#27700;&#19979;&#33322;&#34892;&#22120;&#38480;&#21046;&#20854;&#36816;&#21160;&#33539;&#22260;&#21644;&#27979;&#37327;&#22522;&#32447;&#12290;&#22312;&#19977;&#32500;&#22330;&#26223;&#37325;&#24314;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30693;&#36947;&#36739;&#23567;&#30340;&#22522;&#32447;&#20250;&#22686;&#21152;&#37325;&#24314;&#38590;&#24230;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65288;AONeuS&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#19982;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#36827;&#34892;&#34701;&#21512;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20123;&#20114;&#34917;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#22312;&#21463;&#38480;&#22522;&#32447;&#19978;&#25429;&#33719;&#30340;&#27979;&#37327;&#20013;&#37325;&#24314;&#20986;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.03305</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#35821;&#20041;&#26377;&#24847;&#20041;&#21644;&#39640;&#25928;&#30340;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#19981;&#23547;&#24120;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20363;&#22914;&#23431;&#33322;&#21592;&#39569;&#22312;&#26376;&#29699;&#19978;&#30340;&#39532;&#65292;&#24182;&#19988;&#26377;&#27491;&#30830;&#30340;&#38452;&#24433;&#12290;&#36825;&#20123;&#36755;&#20986;&#34920;&#26126;&#20102;&#27169;&#22411;&#20855;&#26377;&#32452;&#21512;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#27169;&#22411;&#26159;&#22914;&#20309;&#20570;&#21040;&#36825;&#19968;&#28857;&#30340;&#21602;&#65311;&#25105;&#20204;&#22312;&#26465;&#20214;DDPMs&#19978;&#36827;&#34892;&#20102;&#25511;&#21046;&#23454;&#39564;&#65292;&#23398;&#20064;&#29983;&#25104;&#20197;&#25351;&#23450;&#30340;$x$&#21644;$y$&#20301;&#32622;&#20026;&#20013;&#24515;&#30340;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20135;&#29983;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#34920;&#31034;&#23545;&#20110;&#23454;&#29616;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32463;&#21382;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28508;&#22312;&#34920;&#31034;&#38454;&#27573;&#65306;(A&#38454;&#27573;)&#27809;&#26377;&#28508;&#22312;&#32467;&#26500;&#65292;(B&#38454;&#27573;)&#19968;&#20010;&#28151;&#20081;&#29366;&#24577;&#30340;2D&#27969;&#24418;&#65292;&#20197;&#21450;(C&#38454;&#27573;)&#19968;&#20010;&#26377;&#24207;&#30340;2D&#27969;&#24418;&#12290;&#23545;&#24212;&#20110;&#36825;&#20123;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#65306;1&#65289;&#29983;&#25104;&#22810;&#20010;&#20984;&#36215;&#65292;2&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#65292;&#20294;$x$&#21644;$y$&#20301;&#32622;&#19981;&#20934;&#30830;&#65292;3&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#19988;&#20301;&#32622;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35206;&#30422;&#21644;&#35843;&#33410;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#32499;&#32034;&#25193;&#23637;&#26102;&#38656;&#35201;&#20445;&#25345;&#32531;&#20914;&#21306;&#26469;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03303</link><description>&lt;p&gt;
&#27809;&#20851;&#31995;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25351;&#20196;&#35206;&#30422;&#21644;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Nevermind: Instruction Override and Moderation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03303
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35206;&#30422;&#21644;&#35843;&#33410;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#32499;&#32034;&#25193;&#23637;&#26102;&#38656;&#35201;&#20445;&#25345;&#32531;&#20914;&#21306;&#26469;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;&#26368;&#27969;&#34892;&#30340;&#19987;&#26377;&#27169;&#22411;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35299;&#20915;&#22312;&#20914;&#31361;&#24773;&#20917;&#19979;&#30340;&#26126;&#30830;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#65292;&#20363;&#22914;&#35206;&#30422;&#12290;&#36825;&#20123;&#21253;&#25324;&#27169;&#22411;&#22312;&#20854;&#26435;&#37325;&#20013;&#35206;&#30422;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#35206;&#30422;&#65288;&#25110;&#35843;&#33410;&#65289;&#25552;&#31034;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#36827;&#34892;&#23436;&#20840;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#25913;&#36827;&#25351;&#20196;&#36981;&#24490;&#30340;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616; - &#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#36981;&#24490;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#38750;&#24120;&#26381;&#20174;&#65292;&#29978;&#33267;&#26377;&#20123;&#36807;&#24230;&#12290;&#24403;&#36890;&#36807;&#32499;&#32034;&#25193;&#23637;&#26469;&#25193;&#23637;&#21040;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#26102;&#65292;&#38656;&#35201;&#20445;&#25345;&#19982;&#22256;&#24785;&#36793;&#32536;&#30340;&#26174;&#33879;&#32531;&#20914;&#21306;&#65292;&#20197;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25351;&#20196;&#36981;&#24490;&#30340;&#25913;&#21892;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#25351;&#20196;&#35206;&#30422;/&#36234;&#29425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja
&lt;/p&gt;</description></item><item><title>Swin-UMamba&#26159;&#19968;&#31181;&#20197;Mamba&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;UNet&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;Swin-UMamba&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#36739;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#26356;&#23569;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#23041;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03302</link><description>&lt;p&gt;
Swin-UMamba&#65306;&#20197;Mamba&#20026;&#22522;&#30784;&#30340;&#20855;&#26377;ImageNet&#39044;&#35757;&#32451;&#30340;UNet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03302
&lt;/p&gt;
&lt;p&gt;
Swin-UMamba&#26159;&#19968;&#31181;&#20197;Mamba&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;UNet&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;Swin-UMamba&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#36739;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#26356;&#23569;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#23041;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#38656;&#35201;&#25972;&#21512;&#20174;&#23616;&#37096;&#29305;&#24449;&#21040;&#20840;&#23616;&#20381;&#36182;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#24314;&#27169;&#38271;&#36317;&#31163;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21463;&#21040;&#20854;&#23616;&#37096;&#24863;&#21463;&#37326;&#30340;&#38480;&#21046;&#65292;&#32780;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21463;&#21040;&#39640;&#20108;&#27425;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;&#22240;&#20854;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#27969;&#34892;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#26356;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#26356;&#23569;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;&#22823;&#22810;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23041;&#21147;&#65292;&#32780;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#39640;&#25928;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;Swin-UMamba&#65292;&#19987;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed speci
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>Ginger&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26354;&#29575;&#36817;&#20284;&#26041;&#27861;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#23427;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#26469;&#36870;&#21521;&#35745;&#31639;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#30697;&#38453;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39640;&#20869;&#23384;&#21644;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03295</link><description>&lt;p&gt;
Ginger: &#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#26354;&#29575;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03295
&lt;/p&gt;
&lt;p&gt;
Ginger&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26354;&#29575;&#36817;&#20284;&#26041;&#27861;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#23427;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#26469;&#36870;&#21521;&#35745;&#31639;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#30697;&#38453;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39640;&#20869;&#23384;&#21644;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#27861;&#65292;&#30001;&#20110;&#21033;&#29992;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#26354;&#29575;&#20449;&#24687;&#21644;&#39044;&#22788;&#29702;&#30697;&#38453;&#65292;&#34987;&#35748;&#20026;&#26356;&#21152;&#24378;&#22823;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#35825;&#20154;&#30340;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#19981;&#26131;&#24212;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#35745;&#31639;&#30697;&#38453;&#30340;&#36870;&#25152;&#38656;&#30340;&#20108;&#27425;&#20869;&#23384;&#21644;&#19977;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#20351;&#29992;&#20808;&#36827;&#30340;&#30828;&#20214;&#20063;&#19981;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ginger&#65292;&#19968;&#31181;&#29992;&#20110;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#30697;&#38453;&#36870;&#30340;&#29305;&#24449;&#20998;&#35299;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20855;&#26377;&#39640;&#25928;&#30340;&#32447;&#24615;&#20869;&#23384;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30452;&#25509;&#32500;&#25252;&#26465;&#20214;&#30697;&#38453;&#30340;&#36870;&#65292;&#20197;&#20351;&#36817;&#20284;&#26356;&#21152;&#20934;&#30830;&#65292;&#32780;&#19981;&#26159;&#36817;&#20284;&#26465;&#20214;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Ginger&#22312;&#38750;&#20984;&#30446;&#26631;&#19978;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03293</link><description>&lt;p&gt;
Flora: &#20302;&#31209;&#36866;&#37197;&#22120;&#26159;&#24708;&#24708;&#30340;&#26799;&#24230;&#21387;&#32553;&#22120;
&lt;/p&gt;
&lt;p&gt;
Flora: Low-Rank Adapters Are Secretly Gradient Compressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#30340;&#26174;&#30528;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#26469;&#23384;&#20648;&#35757;&#32451;&#30340;&#20248;&#21270;&#29366;&#24577;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#26469;&#36890;&#36807;&#35757;&#32451;&#26356;&#23569;&#30340;&#21442;&#25968;&#26469;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;LoRA&#23558;&#25972;&#20307;&#26435;&#37325;&#26356;&#26032;&#30697;&#38453;&#38480;&#21046;&#20026;&#20302;&#31209;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#30830;&#23450;&#23427;&#21487;&#20197;&#36817;&#20284;&#20026;&#38543;&#26426;&#25237;&#24433;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Flora&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20139;&#21463;&#20248;&#21270;&#29366;&#24577;&#30340;&#27425;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03292</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#20316;&#20026;&#40657;&#30418;&#20113;&#26381;&#21153;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#65292;&#26080;&#27861;&#35775;&#38382;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#23601;&#24341;&#21457;&#20102;&#38646;&#26679;&#26412;&#31163;&#32676;&#25968;&#25454;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#27979;&#19981;&#23646;&#20110;&#20998;&#31867;&#22120;&#26631;&#31614;&#38598;&#20294;&#34987;&#38169;&#35823;&#22320;&#24402;&#31867;&#20026;&#20837;&#22495;&#65288;ID&#65289;&#23545;&#35937;&#30340;OOD&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;RONIN&#20351;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#29992;&#20462;&#22797;&#26367;&#25442;&#25481;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#12290;RONIN&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#36755;&#20837;&#23545;&#35937;&#25509;&#36817;&#20837;&#22495;&#22495;&#12290;&#32467;&#26524;&#26159;&#65292;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;ID&#24773;&#20917;&#19979;&#38750;&#24120;&#25509;&#36817;&#21407;&#22987;&#23545;&#35937;&#65292;&#22312;OOD&#24773;&#20917;&#19979;&#21017;&#30456;&#24046;&#36739;&#36828;&#65292;&#20351;&#24471;RONIN&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RONIN&#22312;&#38646;&#26679;&#26412;&#21644;&#38750;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.
&lt;/p&gt;</description></item><item><title>InstanceDiffusion&#36890;&#36807;&#28155;&#21152;&#23454;&#20363;&#32423;&#25511;&#21046;&#65292;&#20351;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20301;&#32622;&#26465;&#20214;&#19979;&#36229;&#36807;&#20102;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03290</link><description>&lt;p&gt;
InstanceDiffusion&#65306;&#22270;&#20687;&#29983;&#25104;&#30340;&#23454;&#20363;&#32423;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
InstanceDiffusion: Instance-level Control for Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03290
&lt;/p&gt;
&lt;p&gt;
InstanceDiffusion&#36890;&#36807;&#28155;&#21152;&#23454;&#20363;&#32423;&#25511;&#21046;&#65292;&#20351;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20301;&#32622;&#26465;&#20214;&#19979;&#36229;&#36807;&#20102;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#20294;&#19981;&#33021;&#23545;&#22270;&#20687;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;InstanceDiffusion&#65292;&#23558;&#31934;&#30830;&#30340;&#23454;&#20363;&#32423;&#25511;&#21046;&#28155;&#21152;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;InstanceDiffusion&#25903;&#25345;&#27599;&#20010;&#23454;&#20363;&#30340;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#26465;&#20214;&#65292;&#24182;&#20801;&#35768;&#20197;&#31616;&#21333;&#30340;&#21333;&#20010;&#28857;&#12289;&#28034;&#40486;&#12289;&#36793;&#30028;&#26694;&#25110;&#22797;&#26434;&#30340;&#23454;&#20363;&#20998;&#21106;&#25513;&#30721;&#21450;&#20854;&#32452;&#21512;&#26041;&#24335;&#25351;&#23450;&#23454;&#20363;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#30340;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#23454;&#20363;&#32423;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;UniFusion&#22359;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23454;&#20363;&#32423;&#26465;&#20214;&#65292;ScaleU&#22359;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#65292;Multi-instance&#37319;&#26679;&#22120;&#25552;&#39640;&#20102;&#22810;&#23454;&#20363;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;InstanceDiffusion&#22312;&#27599;&#20010;&#20301;&#32622;&#26465;&#20214;&#19979;&#26126;&#26174;&#36229;&#36807;&#20102;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;COCO&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;box&#36755;&#20837;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;20.4%AP50 box&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#36827;&#34892;&#21069;&#30651;&#30340;&#33258;&#21160;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RTL&#20195;&#30721;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#32534;&#35793;&#22833;&#36133;&#21644;PPA&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.03289</link><description>&lt;p&gt;
&#35753;&#27599;&#19968;&#27493;&#37117;&#26377;&#20215;&#20540;&#65306;&#20351;&#29992;MCTS&#30340;LLM&#22522;&#30784;&#39640;&#36136;&#37327;RTL&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#36827;&#34892;&#21069;&#30651;&#30340;&#33258;&#21160;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RTL&#20195;&#30721;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#32534;&#35793;&#22833;&#36133;&#21644;PPA&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#38754;&#20020;&#32534;&#35793;&#22833;&#36133;&#21644;&#20122;&#26368;&#20248;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;(PPA)&#25928;&#29575;&#31561;&#25361;&#25112;&#12290;&#36825;&#26159;&#30001;&#20110;&#20256;&#32479;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#32570;&#20047;&#23545;PPA&#30340;&#24847;&#35782;&#25152;&#33268;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26469;&#36827;&#34892;&#21069;&#30651;&#65292;&#24341;&#23548;&#21464;&#25442;&#22120;&#29983;&#25104;&#21487;&#32534;&#35793;&#30340;&#12289;&#21151;&#33021;&#27491;&#30830;&#30340;&#12289;PPA&#20248;&#21270;&#30340;&#20195;&#30721;&#12290;&#22312;RTL&#20195;&#30721;&#38598;&#19978;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#20165;&#20351;&#29992;&#25552;&#31034;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#19968;&#33268;&#22320;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#26420;&#32032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;PPA&#19981;&#25935;&#24863;&#30340;&#32570;&#28857;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;LLM&#29983;&#25104;&#30340;&#26368;&#22823;&#35774;&#35745;&#65288;16&#20301;&#21152;&#27861;&#22120;&#65289;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#22312;&#38754;&#31215;&#24310;&#36831;&#20056;&#31215;&#19978;&#23454;&#29616;31.8%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Lennard-Jones&#23618;&#65288;LJL&#65289;&#26469;&#22343;&#34913;2D&#21644;3D&#28857;&#20113;&#30340;&#23494;&#24230;&#65292;&#36890;&#36807;&#31995;&#32479;&#37325;&#26032;&#25490;&#21015;&#28857;&#24182;&#27169;&#25311;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#36798;&#21040;&#36817;&#20284;&#22343;&#21248;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#28857;&#20113;&#29983;&#25104;&#21644;&#25913;&#21892;&#28857;&#20998;&#24067;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03287</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#24067;&#24402;&#19968;&#21270;&#30340;Lennard-Jones&#23618;
&lt;/p&gt;
&lt;p&gt;
A Lennard-Jones Layer for Distribution Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03287
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Lennard-Jones&#23618;&#65288;LJL&#65289;&#26469;&#22343;&#34913;2D&#21644;3D&#28857;&#20113;&#30340;&#23494;&#24230;&#65292;&#36890;&#36807;&#31995;&#32479;&#37325;&#26032;&#25490;&#21015;&#28857;&#24182;&#27169;&#25311;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#36798;&#21040;&#36817;&#20284;&#22343;&#21248;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#28857;&#20113;&#29983;&#25104;&#21644;&#25913;&#21892;&#28857;&#20998;&#24067;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Lennard-Jones&#23618;&#65288;LJL&#65289;&#26469;&#36890;&#36807;&#31995;&#32479;&#37325;&#26032;&#25490;&#21015;&#28857;&#32780;&#19981;&#30772;&#22351;&#23427;&#20204;&#30340;&#25972;&#20307;&#32467;&#26500;&#65288;&#20998;&#24067;&#24402;&#19968;&#21270;&#65289;&#26469;&#22343;&#34913;2D&#21644;3D&#28857;&#20113;&#30340;&#23494;&#24230;&#12290;LJL&#27169;&#25311;&#20102;&#27599;&#20010;&#28857;&#22312;&#32473;&#23450;&#26102;&#38388;&#20869;&#32771;&#34385;&#20854;&#26368;&#36817;&#37051;&#30340;&#25490;&#26021;&#21644;&#24369;&#21560;&#24341;&#30456;&#20114;&#20316;&#29992;&#30340;&#32791;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#65292;&#23558;&#28857;&#25512;&#21040;&#19968;&#20010;&#21183;&#33021;&#35895;&#20013;&#65292;&#36798;&#21040;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#31283;&#23450;&#37197;&#32622;&#65292;&#36817;&#20284;&#20110;&#22343;&#21248;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;LJL&#24212;&#29992;&#20110;&#23558;&#38543;&#26426;&#29983;&#25104;&#30340;&#28857;&#20113;&#37325;&#26032;&#20998;&#24067;&#20026;&#38543;&#26426;&#22343;&#21248;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;LJL&#36824;&#23884;&#20837;&#21040;&#28857;&#20113;&#32593;&#32476;&#30340;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#30340;&#21518;&#26399;&#28155;&#21152;&#23427;&#20204;&#12290;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#35780;&#20272;&#20102;&#21033;&#29992;LJL&#25913;&#21892;3D&#28857;&#20113;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;LJL&#24212;&#29992;&#20110;&#25913;&#21892;&#22522;&#20110;&#24471;&#20998;&#30340;3D&#28857;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization). LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time. This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process. We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution. Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process. The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively. Finally, we apply LJLs to improve the point distribution of a score-based 3D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03286</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#19968;&#33268;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Training-Free Consistent Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#36896;&#24615;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#19979;&#19968;&#33268;&#22320;&#25551;&#32472;&#30456;&#21516;&#30340;&#20027;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#25945;&#25480;&#23427;&#25551;&#36848;&#29305;&#23450;&#29992;&#25143;&#25552;&#20379;&#20027;&#39064;&#30340;&#26032;&#35789;&#27719;&#25110;&#32773;&#20026;&#27169;&#22411;&#28155;&#21152;&#22270;&#20687;&#26465;&#20214;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#38024;&#23545;&#27599;&#20010;&#20027;&#39064;&#36827;&#34892;&#28459;&#38271;&#30340;&#20248;&#21270;&#25110;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#21644;&#25551;&#32472;&#22810;&#20010;&#20027;&#39064;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#35757;&#32451;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#20027;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20027;&#39064;&#39537;&#21160;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20197;&#20419;&#36827;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31574;&#30053;&#20197;&#40723;&#21169;&#24067;&#23616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#27169;&#22411;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#22791;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03284</link><description>&lt;p&gt;
&#20132;&#26131;&#65292;&#36824;&#26159;&#19981;&#20132;&#26131;&#65288;&#25110;&#32773;&#35841;&#30693;&#36947;&#65289;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#27169;&#22411;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#22791;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23545;&#35805;&#32773;&#32771;&#34385;&#20182;&#20154;&#30340;&#19981;&#30830;&#23450;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#24773;&#32490;&#12290;&#20294;&#21363;&#20351;&#26159;&#26368;&#20339;&#30340;&#20154;&#31867;&#23545;&#35805;&#32773;&#20063;&#26080;&#27861;&#23436;&#32654;&#22320;&#39044;&#27979;&#23545;&#35805;&#30340;&#36712;&#36857;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22810;&#22909;&#22320;&#34920;&#31034;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;FortUne Dial&#65292;&#36825;&#26159;&#8220;&#23545;&#35805;&#39044;&#27979;&#8221;&#20219;&#21153;&#30340;&#25193;&#23637;&#65306;&#35780;&#20272;&#19981;&#20165;&#20165;&#20197;&#20934;&#30830;&#24230;&#20026;&#26631;&#20934;&#65292;&#36824;&#37319;&#29992;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#25935;&#24863;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#20351;&#20010;&#21035;&#23454;&#20363;&#21487;&#20197;&#25918;&#24323;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#34920;&#31034;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#31181;&#26041;&#24335;&#65288;&#20869;&#37096;&#20351;&#29992;&#20998;&#25968;&#21644;&#30452;&#25509;&#20351;&#29992;&#20196;&#29260;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#20004;&#31181;&#34920;&#31034;&#30340;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23545;&#20843;&#20010;&#22256;&#38590;&#30340;&#35848;&#21028;&#35821;&#26009;&#24211;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24494;&#35843;&#31574;&#30053;&#65288;&#19968;&#31181;&#20256;&#32479;&#30340;&#30417;&#30563;&#31574;&#30053;&#21644;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65289;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#26657;&#20934;&#24471;&#19978;&#19982;&#20854;&#23610;&#23544;&#30456;&#24403;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.03282</link><description>&lt;p&gt;
&#19968;&#20010;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#22312;RLHF&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Partially Observed Reward-States in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#30740;&#31350;&#22240;&#20854;&#22312;LLMs&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#21050;&#28608;&#30340;&#21453;&#24212;&#24050;&#30693;&#20381;&#36182;&#20110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#8220;&#20869;&#37096;&#29366;&#24577;&#8221;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#20013;&#38388;&#21453;&#39304;&#65292;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#23558;RLHF&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PORRL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;RLHF&#20013;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#30340;&#20154;&#31867;&#21453;&#39304; - &#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#21040;PORRL&#30340;&#32553;&#20943;&#12290;&#23545;&#20110;&#22522;&#25968;&#21453;&#39304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30340;&#32479;&#35745;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#23454;&#20363;&#21270;&#20026;POR-UCRL&#21644;POR-UCBVI&#12290;&#23545;&#20110;&#20915;&#26007;&#21453;&#39304;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#25968;&#21453;&#39304;&#32553;&#20943;&#19981;&#33021;&#36798;&#21040;&#20122;&#32447;&#24615;&#30340;&#20915;&#26007;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.03271</link><description>&lt;p&gt;
&#24819;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#35299;&#20915;&#20219;&#21153;&#25152;&#38656;&#30340;&#20449;&#24687;&#19981;&#26159;&#21021;&#22987;&#32473;&#23450;&#30340;&#65292;&#32780;&#38656;&#35201;&#36890;&#36807;&#35810;&#38382;&#21518;&#32493;&#38382;&#39064;&#26469;&#20027;&#21160;&#23547;&#27714;&#65288;&#20363;&#22914;&#65292;&#21307;&#29983;&#21521;&#24739;&#32773;&#35810;&#38382;&#30151;&#29366;&#30340;&#26356;&#22810;&#32454;&#33410;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24605;&#24819;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;UoT&#65289;&#65292;&#19968;&#31181;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20027;&#21160;&#25552;&#38382;&#20449;&#24687;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UoT&#32467;&#21512;&#20102;1&#65289;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20223;&#30495;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#21487;&#33021;&#30340;&#26410;&#26469;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20854;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;2&#65289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#28608;&#21169;&#27169;&#22411;&#23547;&#27714;&#20449;&#24687;&#65307;3&#65289;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#30340;&#26041;&#24335;&#36873;&#25321;&#26368;&#20339;&#30340;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#25925;&#38556;&#25490;&#38500;&#21644;'20&#30340;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#21019;&#24314;&#36866;&#29992;&#20110;MQTT&#21327;&#35758;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#20998;&#31867;&#25915;&#20987;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03270</link><description>&lt;p&gt;
&#29992;&#20110;&#26816;&#27979;MQTT-IoT&#21327;&#35758;&#25915;&#20987;&#30340;&#22810;&#31867;&#20998;&#31867;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT Protocol
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#21019;&#24314;&#36866;&#29992;&#20110;MQTT&#21327;&#35758;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#20998;&#31867;&#25915;&#20987;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#22823;&#37327;&#23384;&#22312;&#65292;&#31995;&#32479;&#24517;&#39035;&#20351;&#29992;&#21508;&#31181;&#25216;&#26415;&#21644;&#21327;&#35758;&#12290;&#36825;&#24847;&#21619;&#30528;&#29289;&#32852;&#32593;&#32593;&#32476;&#27604;&#20256;&#32479;&#32593;&#32476;&#26356;&#21152;&#24322;&#26500;&#12290;&#36825;&#32473;&#32593;&#32476;&#23433;&#20840;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#20445;&#25252;&#36825;&#20123;&#31995;&#32479;&#21644;&#35774;&#22791;&#65292;&#36825;&#20123;&#35774;&#22791;&#38656;&#35201;&#25345;&#32493;&#36830;&#25509;&#21040;&#20114;&#32852;&#32593;&#12290;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#29992;&#20110;&#20445;&#25252;&#29289;&#32852;&#32593;&#31995;&#32479;&#20813;&#21463;&#32593;&#32476;&#23618;&#38754;&#30340;&#21508;&#31181;&#24322;&#24120;&#21644;&#25915;&#20987;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#21019;&#24314;&#20998;&#31867;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#21253;&#21547;MQTT&#21327;&#35758;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#21463;&#25915;&#20987;&#24103;&#30340;&#25968;&#25454;&#38598;&#26469;&#20026;IDS&#25552;&#20379;&#36755;&#20837;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#20998;&#31867;&#26041;&#27861;&#65292;&#38598;&#25104;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#20855;&#26377;&#38750;&#24120;&#28385;&#24847;&#32467;&#26524;&#30340;&#24490;&#29615;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large number of sensors and actuators that make up the Internet of Things obliges these systems to use diverse technologies and protocols. This means that IoT networks are more heterogeneous than traditional networks. This gives rise to new challenges in cybersecurity to protect these systems and devices which are characterized by being connected continuously to the Internet. Intrusion detection systems (IDS) are used to protect IoT systems from the various anomalies and attacks at the network level. Intrusion Detection Systems (IDS) can be improved through machine learning techniques. Our work focuses on creating classification models that can feed an IDS using a dataset containing frames under attacks of an IoT system that uses the MQTT protocol. We have addressed two types of method for classifying the attacks, ensemble methods and deep learning models, more specifically recurrent networks with very satisfactory results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ISPA&#65288;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#30830;&#12289;&#31616;&#27905;&#12289;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21160;&#29289;&#22768;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#21160;&#29289;&#22768;&#38899;&#34920;&#31034;&#20026;&#25991;&#26412;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#35821;&#35328;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#21644;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03269</link><description>&lt;p&gt;
ISPA: &#29992;&#20110;&#36716;&#24405;&#21160;&#29289;&#22768;&#38899;&#30340;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ISPA&#65288;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#30830;&#12289;&#31616;&#27905;&#12289;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21160;&#29289;&#22768;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#21160;&#29289;&#22768;&#38899;&#34920;&#31034;&#20026;&#25991;&#26412;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#35821;&#35328;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#21644;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29983;&#29289;&#22768;&#23398;&#20381;&#36182;&#35889;&#22270;&#21644;&#36830;&#32493;&#30340;&#27599;&#24103;&#38899;&#39057;&#34920;&#31034;&#26469;&#20998;&#26512;&#21160;&#29289;&#22768;&#38899;&#65292;&#24182;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#12289;&#19982;&#35821;&#35328;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#36716;&#24405;&#20154;&#31867;&#35821;&#38899;&#22768;&#38899;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ISPA&#65288;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#30830;&#12289;&#31616;&#27905;&#12289;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21160;&#29289;&#22768;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#22768;&#23398;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#36716;&#24405;&#21644;&#20998;&#31867;&#21160;&#29289;&#22768;&#38899;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#19982;&#20351;&#29992;&#36830;&#32493;&#12289;&#31264;&#23494;&#38899;&#39057;&#34920;&#31034;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#21160;&#29289;&#22768;&#38899;&#34920;&#31034;&#20026;&#25991;&#26412;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#35821;&#35328;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#21644;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, bioacoustics has relied on spectrograms and continuous, per-frame audio representations for the analysis of animal sounds, also serving as input to machine learning models. Meanwhile, the International Phonetic Alphabet (IPA) system has provided an interpretable, language-independent method for transcribing human speech sounds. In this paper, we introduce ISPA (Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system designed for transcribing animal sounds into text. We compare acoustics-based and feature-based methods for transcribing and classifying animal sounds, demonstrating their comparable performance with baseline methods utilizing continuous, dense audio representations. By representing animal sounds with text, we effectively treat them as a "foreign language," and we show that established human language ML paradigms and models, such as language models, can be successfully applied to improve performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03268</link><description>&lt;p&gt;
&#20174;&#25512;&#29702;&#36335;&#24452;&#32858;&#21512;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#35757;&#32451;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#20851;&#31995;&#22914;&#20309;&#20419;&#20351;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#22312;&#39044;&#35757;&#32451;&#26102;&#36890;&#36807;&#32858;&#21512;&#38388;&#25509;&#30340;&#25512;&#29702;&#36335;&#24452;&#26469;&#24471;&#20986;&#26032;&#32467;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35270;&#35282;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#31561;&#20851;&#38190;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#36335;&#24452;&#24418;&#24335;&#21270;&#20026;&#22312;&#30693;&#35782;/&#25512;&#29702;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#12290;&#23545;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#24067;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20851;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#27010;&#29575;&#30340;&#21152;&#26435;&#21644;&#26159;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21512;&#29702;&#26041;&#24335;&#12290;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#35757;&#32451;&#23545;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
&lt;/p&gt;</description></item><item><title>MobilityGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#27169;&#22411;&#30340;&#22686;&#24378;&#22411;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#36716;&#25442;&#20026;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#22320;&#29702;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#37325;&#21147;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#36947;&#36335;&#36830;&#25509;&#30697;&#38453;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#30340;&#22320;&#29702;&#31354;&#38388;&#31227;&#21160;&#25968;&#25454;&#30340;&#35821;&#20041;&#30495;&#23454;&#24615;&#21644;&#21508;&#20010;&#29305;&#24449;&#30340;&#20445;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.03264</link><description>&lt;p&gt;
MobilityGPT: &#22522;&#20110;GPT&#27169;&#22411;&#30340;&#22686;&#24378;&#22411;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MobilityGPT: Enhanced Human Mobility Modeling with a GPT model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03264
&lt;/p&gt;
&lt;p&gt;
MobilityGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#27169;&#22411;&#30340;&#22686;&#24378;&#22411;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#36716;&#25442;&#20026;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#22320;&#29702;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#37325;&#21147;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#36947;&#36335;&#36830;&#25509;&#30697;&#38453;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#30340;&#22320;&#29702;&#31354;&#38388;&#31227;&#21160;&#25968;&#25454;&#30340;&#35821;&#20041;&#30495;&#23454;&#24615;&#21644;&#21508;&#20010;&#29305;&#24449;&#30340;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#29305;&#24449;&#21644;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#22320;&#29702;&#31354;&#38388;&#31227;&#21160;&#25968;&#25454;&#22312;&#35821;&#20041;&#19978;&#26159;&#30495;&#23454;&#30340;&#65292;&#21253;&#25324;&#19968;&#33268;&#30340;&#20301;&#32622;&#24207;&#21015;&#20197;&#21450;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#30340;&#29305;&#24449;&#65292;&#22914;&#23545;&#22320;&#29702;&#38480;&#21046;&#30340;&#32422;&#26463;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#65292;&#21033;&#29992;&#20102;Generative Pre-trained Transformer (GPT)&#12290;&#20026;&#20102;&#30830;&#20445;&#20854;&#21487;&#25511;&#30340;&#29983;&#25104;&#26469;&#32531;&#35299;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;MobilityGPT&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#37325;&#21147;&#30340;&#37319;&#26679;&#26041;&#27861;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;transformer&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#36947;&#36335;&#36830;&#25509;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#32422;&#26463;&#65292;&#35813;&#30697;&#38453;&#32473;&#20986;&#20102;&#36712;&#36857;&#29983;&#25104;&#20013;&#24207;&#21015;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#20174;&#32780;&#20445;&#25345;&#29983;&#25104;&#30340;&#36712;&#36857;&#22312;&#22320;&#29702;&#33539;&#22260;&#20869;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#30340;&#25913;&#36827;&#30446;&#26631;&#20989;&#25968;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement L
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03256</link><description>&lt;p&gt;
&#23398;&#20064;Predict-then-Optimize&#26694;&#26550;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Best-in-Class Policies for the Predict-then-Optimize Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03256
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#31216;&#20026;Perturbation Gradient&#65288;PG&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#12290;&#36825;&#20123;&#25439;&#22833;&#30452;&#25509;&#36817;&#20284;&#20102;&#19979;&#28216;&#20915;&#31574;&#25439;&#22833;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#26367;&#20195;&#25439;&#22833;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#30340;&#36817;&#20284;&#35823;&#24046;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#28040;&#22833;&#12290;&#36825;&#24847;&#21619;&#30528;&#20248;&#21270;&#25105;&#20204;&#30340;&#26367;&#20195;&#25439;&#22833;&#21487;&#20197;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#24471;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#35823;&#35774;&#32622;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#24403;&#22522;&#30784;&#27169;&#22411;&#35823;&#35774;&#32622;&#19988;&#22122;&#22768;&#19981;&#26159;&#20013;&#24515;&#23545;&#31216;&#26102;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#26696;&#12290;&#37492;&#20110;&#22312;&#23454;&#36341;&#20013;&#35823;&#35774;&#32622;&#24456;&#24120;&#35265;--&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#21487;&#33021;&#26356;&#21916;&#27426;&#19968;&#20010;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;--PG&#25439;&#22833;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#29702;&#35770;&#19978;&#26377;&#20381;&#25454;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#20915;&#31574;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21387;&#32553;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#29702;&#35770;&#27867;&#21270;&#20445;&#35777;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03254</link><description>&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21644;&#27867;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Minimum Description Length and Generalization Guarantees for Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21387;&#32553;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#29702;&#35770;&#27867;&#21270;&#20445;&#35777;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#39640;&#25928;&#30340;&#32479;&#35745;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#25214;&#21040;&#19981;&#20165;&#22312;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#32780;&#19988;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#20063;&#34920;&#29616;&#33391;&#22909;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#23613;&#31649;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#24341;&#21457;&#20102;&#35768;&#22810;&#20852;&#36259;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#65307;&#23545;&#20110;&#29702;&#35770;&#19978;&#30340;&#27867;&#21270;&#20445;&#35777;&#20960;&#20046;&#27809;&#26377;&#20160;&#20040;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#21387;&#32553;&#24615;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#26631;&#31614;&#25110;&#28508;&#22312;&#21464;&#37327;&#65288;&#34920;&#31034;&#24418;&#24335;&#65289;&#30340;"&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;"&#65288;MDL&#65289;&#26469;&#25512;&#23548;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#19982;&#36890;&#24120;&#34987;&#35748;&#20026;&#21453;&#26144;&#31639;&#27861;&#27867;&#21270;&#33021;&#21147;&#30340;&#32534;&#30721;&#22120;&#36755;&#20837;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26032;&#30028;&#38480;&#28041;&#21450;&#34920;&#31034;&#65288;&#25110;&#26631;&#31614;&#65289;&#20998;&#24067;&#20043;&#38388;&#30340;"&#22810;&#23383;&#27597;"&#30456;&#23545;&#29109;&#65292;&#22312;&#30456;&#20851;&#25991;&#29486;&#20013;&#23545;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#21453;&#26144;&#36824;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees.   In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the "Minimum Description Length" (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the "multi-letter" relative entropy between the distribution of the representations (or labels) of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#24341;&#21457;&#37197;&#23545;&#27604;&#36739;&#26469;&#20844;&#24179;&#20027;&#21160;&#25490;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32676;&#32452;&#35823;&#24046;&#30340;&#33539;&#25968;&#26469;&#23454;&#29616;&#23545;&#19981;&#21516;&#20844;&#24179;&#27010;&#24565;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.03252</link><description>&lt;p&gt;
&#26469;&#33258;&#37197;&#23545;&#20559;&#22909;&#30340;&#20844;&#24179;&#20027;&#21160;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Fair Active Ranking from Pairwise Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#24341;&#21457;&#37197;&#23545;&#27604;&#36739;&#26469;&#20844;&#24179;&#20027;&#21160;&#25490;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32676;&#32452;&#35823;&#24046;&#30340;&#33539;&#25968;&#26469;&#23454;&#29616;&#23545;&#19981;&#21516;&#20844;&#24179;&#27010;&#24565;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#24341;&#21457;&#37197;&#23545;&#27604;&#36739;&#26469;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#19988;&#20844;&#24179;&#65288;PACF&#65289;&#23545;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#23646;&#20110;&#19981;&#30456;&#20132;&#32676;&#32452;&#30340;$n$&#20010;&#39033;&#30446;&#38598;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;&#25105;&#20204;&#25552;&#20986;&#30340;&#20844;&#24179;&#30446;&#26631;&#20989;&#25968;&#25214;&#21040;&#19968;&#20010;$(\epsilon, \delta)$-PACF-Ranking&#12290;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;Oracle&#65292;&#23545;&#20110;&#27599;&#20010;&#26597;&#35810;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#36873;&#25321;&#19968;&#23545;&#39033;&#30446;&#65292;&#24182;&#20174;Oracle&#25509;&#25910;&#21040;&#38543;&#26426;&#30340;&#33719;&#32988;&#32773;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#35201;&#27714;&#26368;&#23567;&#21270;&#32676;&#32452;&#35823;&#24046;&#30340;$\ell_q$&#33539;&#25968;&#65292;&#20854;&#20013;&#32676;&#32452;&#35823;&#24046;&#26159;&#35813;&#32676;&#32452;&#20013;&#25152;&#26377;&#39033;&#30446;&#35823;&#24046;&#30340;$\ell_p$&#33539;&#25968;&#65292;&#23545;&#20110;$p, q \geq 1$&#12290;&#36825;&#25193;&#23637;&#20102;Saha&#65286;Gopalan&#65288;2019&#65289;&#25552;&#20986;&#30340;$\epsilon$-Best-Ranking&#30446;&#26631;&#20989;&#25968;&#12290;&#36890;&#36807;&#37319;&#29992;&#25105;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25506;&#32034;&#24179;&#31561;&#25110;&#27604;&#20363;&#35823;&#24046;&#31561;&#22522;&#26412;&#20844;&#24179;&#27010;&#24565;&#30340;&#28789;&#27963;&#24615;&#12290;&#35843;&#25972;&#21442;&#25968;$p$&#21644;$q$&#21487;&#20197;&#25353;&#38656;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\epsilon, \delta)$-PACF-Ranking according to a fair objective function that we propose. We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle. Our proposed objective function asks to minimize the $\ell_q$ norm of the error of the groups, where the error of a group is the $\ell_p$ norm of the error of all the items within that group, for $p, q \geq 1$. This generalizes the objective function of $\epsilon$-Best-Ranking, proposed by Saha &amp; Gopalan (2019).   By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework. Adjusting parameters $p$ and $q$ allows tailoring to specific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;CLIP&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#65292;&#20351;&#24471;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#28145;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03251</link><description>&lt;p&gt;
CLIP&#21487;&#20197;&#29702;&#35299;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
CLIP Can Understand Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;CLIP&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#65292;&#20351;&#24471;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#28145;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23558;CLIP&#25512;&#24191;&#21040;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;CLIP&#22312;&#22270;&#20687;&#22359;&#21644;&#19982;&#28145;&#24230;&#30456;&#20851;&#30340;&#25552;&#31034;&#20043;&#38388;&#24471;&#21040;&#36866;&#24403;&#30456;&#20284;&#24615;&#26159;&#20302;&#25928;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36866;&#24212;CLIP&#29992;&#20110;&#26377;&#24847;&#20041;&#30340;&#23494;&#38598;&#39044;&#27979;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#20854;&#21407;&#22987;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#12290;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#19968;&#20010;&#21517;&#20026;mirror&#30340;&#23567;&#22411;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#20316;&#20026;&#20854;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#38745;&#24577;&#25552;&#31034;&#65292;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;NYU Depth v2&#21644;KITTI&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#20960;&#20010;&#20808;&#21069;&#30340;&#20165;&#35270;&#35273;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#32988;&#36807;&#20102;&#27599;&#20010;&#22522;&#20110;CLIP&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;&#20851;&#20110;&#26102;&#38388;&#28145;&#24230;&#19968;&#33268;&#24615;&#21644;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;CLIP&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26102;&#28382;&#30740;&#31350;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#65292;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SSO&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#33021;&#22815;&#20248;&#21270;&#25216;&#33021;&#38598;&#65292;&#24182;&#23454;&#29616;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.03244</link><description>&lt;p&gt;
&#25216;&#33021;&#38598;&#20248;&#21270;&#65306;&#36890;&#36807;&#21487;&#36716;&#31227;&#25216;&#33021;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#65292;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SSO&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#33021;&#22815;&#20248;&#21270;&#25216;&#33021;&#38598;&#65292;&#24182;&#23454;&#29616;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#26469;&#19981;&#26029;&#25913;&#36827;LLM&#28436;&#21592;&#30340;&#34920;&#29616;&#24182;&#19981;&#31616;&#21333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26469;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;LLM&#28436;&#21592;&#30340;&#24615;&#33021;&#12290;SSO&#36890;&#36807;&#25552;&#21462;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#24182;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#26469;&#26500;&#24314;&#25216;&#33021;&#12290;&#36825;&#20123;&#25216;&#33021;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;SSO&#36890;&#36807;&#20462;&#21098;&#19981;&#20877;&#20135;&#29983;&#39640;&#22870;&#21169;&#30340;&#25216;&#33021;&#26469;&#36827;&#19968;&#27493;&#23436;&#21892;&#25216;&#33021;&#38598;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#35270;&#39057;&#28216;&#25103;NetHack&#21644;&#25991;&#26412;&#29615;&#22659;ScienceWorld&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;SSO&#20248;&#21270;&#25216;&#33021;&#38598;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;NetHack&#20219;&#21153;&#20013;&#65292;SSO&#30340;&#24615;&#33021;&#36229;&#36807;&#22522;&#20934;&#26041;&#27861;40%&#65292;&#24182;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#26032;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-
&lt;/p&gt;</description></item><item><title>PINN-BO&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30693;&#35782;&#30340;&#40657;&#31665;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03243</link><description>&lt;p&gt;
PINN-BO:&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#31665;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PINN-BO: A Black-box Optimization Algorithm using Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03243
&lt;/p&gt;
&lt;p&gt;
PINN-BO&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30693;&#35782;&#30340;&#40657;&#31665;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#31665;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#22024;&#26434;&#19988;&#26114;&#36149;&#30340;&#40657;&#31665;&#20989;&#25968;&#20013;&#21457;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#21147;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#20559;&#24494;&#20998;&#26041;&#31243;&#24120;&#24120;&#26159;&#38416;&#26126;&#25511;&#21046;&#40657;&#31665;&#20989;&#25968;&#30340;&#22522;&#26412;&#21407;&#29702;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PINN-BO&#65292;&#19968;&#31181;&#37319;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#31665;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#30693;&#35782;&#19982;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;NTK&#29702;&#35770;&#30340;&#36827;&#23637;&#20998;&#26512;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#29702;&#35770;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#22312;&#40657;&#31665;&#20989;&#25968;&#35780;&#20272;&#20013;&#20351;&#29992;PDE&#21644;PINN-BO&#21487;&#20197;&#24471;&#21040;&#26356;&#32039;&#30340;&#21518;&#24724;&#30028;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#20248;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box optimization is a powerful approach for discovering global optima in noisy and expensive black-box functions, a problem widely encountered in real-world scenarios. Recently, there has been a growing interest in leveraging domain knowledge to enhance the efficacy of machine learning methods. Partial Differential Equations (PDEs) often provide an effective means for elucidating the fundamental principles governing the black-box functions. In this paper, we propose PINN-BO, a black-box optimization algorithm employing Physics-Informed Neural Networks that integrates the knowledge from Partial Differential Equations (PDEs) to improve the sample efficiency of the optimization. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory and prove that the use of the PDE alongside the black-box function evaluations, PINN-BO leads to a tighter regret bound. We perform several experiments on a variety of optimization tasks and show that o
&lt;/p&gt;</description></item><item><title>FROSTER&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#24335;&#35789;&#27719;&#21160;&#20316;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20923;&#32467;&#30340;CLIP&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#22312;&#20445;&#25345;CLIP&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#26377;&#25928;&#36866;&#24212;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.03241</link><description>&lt;p&gt;
FROSTER: &#20923;&#32467;&#30340;CLIP&#26159;&#29992;&#20110;&#24320;&#25918;&#24335;&#35789;&#27719;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#21147;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03241
&lt;/p&gt;
&lt;p&gt;
FROSTER&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#24335;&#35789;&#27719;&#21160;&#20316;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20923;&#32467;&#30340;CLIP&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#22312;&#20445;&#25345;CLIP&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#26377;&#25928;&#36866;&#24212;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FROSTER&#65292;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#24335;&#35789;&#27719;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#25928;&#26694;&#26550;&#12290;CLIP&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#24471;&#30410;&#20110;&#20854;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#30340;&#39044;&#35757;&#32451;&#25152;&#23637;&#29616;&#20986;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;CLIP&#30452;&#25509;&#24212;&#29992;&#20110;&#24320;&#25918;&#24335;&#35789;&#27719;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;CLIP&#30340;&#39044;&#35757;&#32451;&#20013;&#32570;&#23569;&#26102;&#38388;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23545;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;CLIP&#30340;&#24494;&#35843;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#38459;&#30861;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#22788;&#29702;&#26410;&#30693;&#21160;&#20316;&#26102;&#32467;&#26524;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveAnno3D&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#36817;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03235</link><description>&lt;p&gt;
ActiveAnno3D - &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveAnno3D&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#36817;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31579;&#36873;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#65292;&#21019;&#24314;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#38590;&#39064;&#20381;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ActiveAnno3D&#65292;&#19968;&#20010;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#36830;&#32493;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#22312;&#35745;&#31639;&#38656;&#27714;&#21644;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;nuScenes&#21644;TUM Traffic Intersection&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20351;&#29992;BEVFusion&#21644;PV-RCNN&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20165;&#20351;&#29992;TUM Traffic Intersection&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#35757;&#32451;&#25968;&#25454;&#65288;77.25 mAP&#30456;&#27604;&#20110;83.50 mAP&#65289;&#26102;&#65292;&#20351;&#29992;PV-RCNN&#21644;&#22522;&#20110;&#29109;&#30340;&#26597;&#35810;&#31574;&#30053;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;BEVFusion&#21017;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#33719;&#24471;&#20102;64.31&#30340;mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21521;&#37327;&#22330;&#20844;&#24335;&#26368;&#23567;&#21270;&#26631;&#20934;&#27969;&#30340;&#25439;&#22833;&#65292;&#24182;&#22312;&#35757;&#32451;&#21521;&#37327;&#22330;&#27169;&#22411;&#26102;&#23637;&#31034;&#20102;&#26356;&#23567;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03232</link><description>&lt;p&gt;
&#26234;&#33021;&#27969;&#21305;&#37197;&#65306;&#20851;&#20110;&#27969;&#21305;&#37197;&#31639;&#27861;&#30340;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Smart Flow Matching: On The Theory of Flow Matching Algorithms with Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21521;&#37327;&#22330;&#20844;&#24335;&#26368;&#23567;&#21270;&#26631;&#20934;&#27969;&#30340;&#25439;&#22833;&#65292;&#24182;&#22312;&#35757;&#32451;&#21521;&#37327;&#22330;&#27169;&#22411;&#26102;&#23637;&#31034;&#20102;&#26356;&#23567;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26631;&#20934;&#27969;&#30340;&#25439;&#22833;&#30340;&#21521;&#37327;&#22330;&#12290;&#35813;&#20844;&#24335;&#22312;&#32473;&#23450;&#30340;&#20998;&#24067;&#961;&#8320;&#21644;&#26410;&#30693;&#30340;&#20998;&#24067;&#961;&#8321;&#20043;&#38388;&#36827;&#34892;&#20998;&#26512;&#24615;&#20381;&#36182;&#12290;&#22522;&#20110;&#36825;&#20010;&#20844;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#20197;&#26465;&#20214;&#27969;&#21305;&#37197;&#30340;&#26041;&#24335;&#35757;&#32451;&#21521;&#37327;&#22330;&#27169;&#22411;&#12290;&#19982;&#26631;&#20934;&#30340;&#26465;&#20214;&#27969;&#21305;&#37197;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25439;&#22833;&#22312;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#26041;&#27861;&#35780;&#20272;&#26102;&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#27169;&#22411;&#21644;&#22823;&#32500;&#24230;&#34920;&#26684;&#25968;&#25454;&#27169;&#22411;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the exact formula for the vector field that minimizes the loss for the standard flow. This formula depends analytically on a given distribution \rho_0 and an unknown one \rho_1. Based on the presented formula, a new loss and algorithm for training a vector field model in the style of Conditional Flow Matching are provided. Our loss, in comparison to the standard Conditional Flow Matching approach, exhibits smaller variance when evaluated through Monte Carlo sampling methods. Numerical experiments on synthetic models and models on tabular data of large dimensions demonstrate better learning results with the use of the presented algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;A/B&#27979;&#35797;&#20013;&#25913;&#36827;&#23545;&#26410;&#26469;&#29992;&#25143;&#27963;&#21160;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#39044;&#27979;&#20010;&#20307;&#34987;&#20171;&#20837;&#30340;&#36895;&#29575;&#65292;&#24182;&#25552;&#20379;&#21452;&#37325;&#39044;&#27979;&#33021;&#21147;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#31383;&#21475;&#20013;&#26032;&#23458;&#25143;&#25968;&#37327;&#21644;&#34987;&#35266;&#23519;&#27425;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.03231</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;A/B&#27979;&#35797;&#20013;&#25913;&#36827;&#23545;&#26410;&#26469;&#29992;&#25143;&#27963;&#21160;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved prediction of future user activity in online A/B testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;A/B&#27979;&#35797;&#20013;&#25913;&#36827;&#23545;&#26410;&#26469;&#29992;&#25143;&#27963;&#21160;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#39044;&#27979;&#20010;&#20307;&#34987;&#20171;&#20837;&#30340;&#36895;&#29575;&#65292;&#24182;&#25552;&#20379;&#21452;&#37325;&#39044;&#27979;&#33021;&#21147;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#31383;&#21475;&#20013;&#26032;&#23458;&#25143;&#25968;&#37327;&#21644;&#34987;&#35266;&#23519;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#38543;&#26426;&#23454;&#39564;&#25110;A/B&#27979;&#35797;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#21442;&#19982;&#29575;&#21313;&#20998;&#37325;&#35201;&#12290;&#36825;&#20123;&#39044;&#27979;&#19981;&#20165;&#25351;&#23548;&#23454;&#39564;&#32773;&#20248;&#21270;&#23454;&#39564;&#30340;&#25345;&#32493;&#26102;&#38388;&#65292;&#36824;&#25552;&#39640;&#20102;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;A/B&#27979;&#35797;&#20013;&#39044;&#27979;&#20010;&#20307;&#34987;&#20171;&#20837;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21452;&#37325;&#39044;&#27979;&#33021;&#21147;&#65306;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#31383;&#21475;&#20013;&#30340;&#26032;&#23458;&#25143;&#25968;&#37327;&#65292;&#20197;&#21450;&#19982;&#29616;&#26377;&#30340;&#21487;&#29992;&#26041;&#27861;&#19981;&#21516;&#65292;&#20182;&#20204;&#23558;&#34987;&#35266;&#23519;&#30340;&#27425;&#25968;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29992;&#20110;&#24418;&#25104;&#23545;&#26410;&#26469;&#29992;&#25143;&#27963;&#21160;&#39044;&#27979;&#30340;&#25152;&#38656;&#25968;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#38381;&#21512;&#34920;&#36798;&#24335;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20351;&#29992;&#25968;&#20540;&#31639;&#27861;&#22914;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#12290;&#22312;&#20840;&#38754;&#38416;&#36848;&#25105;&#20204;&#30340;&#27169;&#22411;&#20043;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online randomized experiments or A/B tests, accurate predictions of participant inclusion rates are of paramount importance. These predictions not only guide experimenters in optimizing the experiment's duration but also enhance the precision of treatment effect estimates. In this paper we present a novel, straightforward, and scalable Bayesian nonparametric approach for predicting the rate at which individuals will be exposed to interventions within the realm of online A/B testing. Our approach stands out by offering dual prediction capabilities: it forecasts both the quantity of new customers expected in future time windows and, unlike available alternative methods, the number of times they will be observed. We derive closed-form expressions for the posterior distributions of the quantities needed to form predictions about future user activity, thereby bypassing the need for numerical algorithms such as Markov chain Monte Carlo. After a comprehensive exposition of our model, we te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;CT&#30340;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#20013;&#30340;&#35299;&#21078;&#20998;&#21106;&#25552;&#20379;&#20102;&#38024;&#23545;3D U-shaped&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03230</link><description>&lt;p&gt;
&#22522;&#20110;CT&#30340;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#30340;&#35299;&#21078;&#20998;&#21106;&#65306;&#38024;&#23545;3D U-shaped&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;CT&#30340;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#20013;&#30340;&#35299;&#21078;&#20998;&#21106;&#25552;&#20379;&#20102;&#38024;&#23545;3D U-shaped&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#24739;&#32773;&#29305;&#23450;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#21644;&#20223;&#30495;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#38656;&#35201;&#20174;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#20013;&#39640;&#25928;&#12289;&#31283;&#20581;&#22320;&#21019;&#24314;&#25968;&#23383;&#35299;&#21078;&#27169;&#22411;&#12290;&#28145;&#24230;&#23398;&#20064;(DL)&#29616;&#22312;&#26159;&#21508;&#31181;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#32780;U-shaped DL&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33258;2D UNet&#20197;&#26469;&#23601;&#19968;&#30452;&#22914;&#27492;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32593;&#32476;&#37197;&#32622;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;U-shaped&#27169;&#22411;&#30340;&#21464;&#20307;&#12290;&#20511;&#21161;&#26368;&#36817;&#22823;&#22411;&#22810;&#26631;&#31614;&#25968;&#25454;&#24211;&#30340;&#21457;&#23637;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#31995;&#32479;&#22522;&#20934;&#30740;&#31350;&#21487;&#20197;&#20026;&#20020;&#24202;&#37096;&#32626;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20294;&#27492;&#31867;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;3D U-shaped&#27169;&#22411;(3DUNet&#12289;STUNet&#12289;AttentionUNet&#12289;SwinUNETR&#12289;FocalSegNet&#21644;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#22235;&#20010;&#21464;&#20307;&#30340;3D SwinUnet)&#30340;&#31532;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#22522;&#20110;CT&#30340;&#33016;&#37096;&#35299;&#21078;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent rising interests in patient-specific thoracic surgical planning and simulation require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms. Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet. To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations. Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare. We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic su
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03226</link><description>&lt;p&gt;
FuseMoE&#65306;&#29992;&#20110;&#28789;&#27963;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#19987;&#23478;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23427;&#20204;&#38754;&#20020;&#22788;&#29702;&#22810;&#31181;&#27169;&#24577;&#30340;&#21452;&#37325;&#25361;&#25112;&#65292;&#36825;&#20123;&#27169;&#24577;&#32463;&#24120;&#22240;&#32570;&#22833;&#20803;&#32032;&#32780;&#19981;&#23436;&#25972;&#65292;&#20197;&#21450;&#25910;&#38598;&#26679;&#26412;&#30340;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;&#25104;&#21151;&#21033;&#29992;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#65292;&#21516;&#26102;&#20811;&#26381;&#39640;&#36136;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#65292;&#26159;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;``FuseMoE''&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#21019;&#26032;&#38376;&#25511;&#20989;&#25968;&#30340;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#12290;FuseMoE&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#38376;&#25511;&#20989;&#25968;&#26377;&#21161;&#20110;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;FuseMoE&#30340;&#23454;&#38469;&#23454;&#29992;&#24615;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#21333;&#27425;GD&#30456;&#27604;&#65292;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#23454;&#29616;&#32593;&#32476;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#30340;&#37325;&#21472;&#65292;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#24191;&#27867;&#20989;&#25968;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.03220</link><description>&lt;p&gt;
&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#22312;&#20004;&#23618;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#22909;&#22788;&#65306;&#25171;&#30772;&#20449;&#24687;&#21644;&#36339;&#36291;&#25351;&#25968;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#21333;&#27425;GD&#30456;&#27604;&#65292;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#23454;&#29616;&#32593;&#32476;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#30340;&#37325;&#21472;&#65292;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#24191;&#27867;&#20989;&#25968;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#20851;&#27880;&#37325;&#22797;&#22810;&#27425;&#20351;&#29992;&#25209;&#27425;&#30340;&#22810;&#27425;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#65292;&#24182;&#23637;&#31034;&#23427;&#19982;&#21333;&#27425;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#26174;&#33879;&#25913;&#21464;&#20102;&#23545;&#20110;&#21738;&#20123;&#20989;&#25968;&#26159;&#21487;&#23398;&#20064;&#30340;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#26377;&#38480;&#27493;&#38271;&#30340;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#20449;&#24687;&#25351;&#25968;&#65288;Ben Arous&#31561;&#20154;&#65292;2021&#65289;&#21644;&#36339;&#36291;&#25351;&#25968;&#65288;Abbe&#31561;&#20154;&#65292;2023&#65289;&#25152;&#32473;&#20986;&#30340;&#26799;&#24230;&#27969;&#21644;&#21333;&#27425;GD&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#65292;&#32593;&#32476;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#36798;&#25104;&#37325;&#21472;&#65292;&#21363;&#20351;&#20989;&#25968;&#19981;&#28385;&#36275;&#38454;&#26799;&#24615;&#36136;&#65288;Abbe&#31561;&#20154;&#65292;2021&#65289;&#12290;&#25105;&#20204;&#23545;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#65288;&#24191;&#27867;&#30340;&#65289;&#20989;&#25968;&#31867;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#21160;&#24577;&#30340;&#38381;&#24335;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamica
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03214</link><description>&lt;p&gt;
&#26377;&#26426;&#25110;&#25193;&#25955;&#65306;&#25105;&#20204;&#33021;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22270;&#20687;&#30340;&#20986;&#29616;&#23436;&#20840;&#39072;&#35206;&#20102;&#33402;&#26415;&#30028;&#12290;&#20174;&#20154;&#31867;&#33402;&#26415;&#20013;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#22686;&#21152;&#12290;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#32773;&#27450;&#35784;&#37027;&#20123;&#25903;&#20184;&#39640;&#20215;&#36141;&#20080;&#20154;&#31867;&#33402;&#26415;&#21697;&#30340;&#20010;&#20154;&#21644;&#31105;&#27490;&#20351;&#29992;AI&#22270;&#20687;&#30340;&#20844;&#21496;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#28508;&#22312;&#27169;&#22411;&#23849;&#28291;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#32773;&#26469;&#35828;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#22270;&#20687;&#30340;&#26041;&#27861;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#20197;&#21450;&#36890;&#36807;&#19987;&#19994;&#33402;&#26415;&#23478;&#21033;&#29992;&#20182;&#20204;&#23545;&#33402;&#26415;&#25216;&#24039;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;7&#31181;&#39118;&#26684;&#30340;&#30495;&#23454;&#20154;&#31867;&#33402;&#26415;&#65292;&#20174;5&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19982;&#20043;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;8&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#20248;&#21270;&#34203;&#23450;&#35860;&#26725;&#30340;&#21442;&#25968;&#21270;&#26469;&#24674;&#22797;&#36755;&#36816;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.03207</link><description>&lt;p&gt;
&#20809;&#23398;&#19982;&#20248;&#21270;&#30340;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Light and Optimal Schr\"odinger Bridge Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#20248;&#21270;&#34203;&#23450;&#35860;&#26725;&#30340;&#21442;&#25968;&#21270;&#26469;&#24674;&#22797;&#36755;&#36816;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;(SB)&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#25193;&#23637;&#65292;&#20063;&#19982;&#29109;&#26368;&#20248;&#36755;&#36816;(EOT)&#30456;&#20114;&#20851;&#32852;&#12290;&#26368;&#36817;&#30340;SB&#27714;&#35299;&#22120;&#21033;&#29992;&#20102;&#26222;&#36941;&#30340;&#26725;&#21305;&#37197;&#31243;&#24207;&#12290;&#36825;&#20123;&#31243;&#24207;&#26088;&#22312;&#22312;&#21482;&#26377;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#36755;&#36816;&#35745;&#21010;&#30340;&#24773;&#20917;&#19979;&#65292;&#24674;&#22797;&#19968;&#31181;&#38543;&#26426;&#36807;&#31243;&#26469;&#36755;&#36816;&#36136;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#32473;&#23450;EOT&#35745;&#21010;&#65292;&#36825;&#20123;&#31243;&#24207;&#21487;&#20197;&#34987;&#36866;&#24212;&#29992;&#20110;&#35299;&#20915;SB&#12290;&#36825;&#20010;&#20107;&#23454;&#34987;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24191;&#27867;&#21033;&#29992;&#65292;&#24418;&#25104;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;SB&#27714;&#35299;&#22120;&#12290;&#20851;&#38190;&#23601;&#26159;&#24674;&#22797;EOT&#35745;&#21010;&#65306;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#20351;&#29992;&#21551;&#21457;&#24335;&#30340;&#36817;&#20284;&#26041;&#27861;(&#22914;&#23567;&#25209;&#37327;&#36755;&#36816;)&#25110;&#32773;&#24314;&#31435;&#36845;&#20195;&#21305;&#37197;&#31243;&#24207;&#65292;&#36825;&#26679;&#35774;&#35745;&#19978;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32047;&#31215;&#20102;&#35823;&#24046;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;SB&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;\textbf{&#20248;&#21270;&#30340;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;}&#12290;&#23427;&#21033;&#29992;&#20102;&#34203;&#23450;&#35860;&#26725;&#30340;&#26368;&#20248;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Schr\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal parameterization of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#22823;&#35268;&#27169;MIMO&#22522;&#31449;&#30340;&#30561;&#30496;&#27169;&#24335;&#21644;&#22825;&#32447;&#20999;&#25442;&#65292;&#23454;&#29616;&#22312;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#33021;&#37327;&#30340;&#33410;&#30465;&#65292;&#21516;&#26102;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#22522;&#32447;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03204</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#33410;&#30465;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#22823;&#35268;&#27169;MIMO&#22522;&#31449;&#30340;&#30561;&#30496;&#27169;&#24335;&#21644;&#22825;&#32447;&#20999;&#25442;&#65292;&#23454;&#29616;&#22312;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#33021;&#37327;&#30340;&#33410;&#30465;&#65292;&#21516;&#26102;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#22522;&#32447;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#32423;&#39640;&#32423;&#30561;&#30496;&#27169;&#24335;&#65288;ASM&#65289;&#21644;&#22522;&#31449;&#30340;&#22825;&#32447;&#20999;&#25442;&#36827;&#34892;&#20915;&#31574;&#65292;&#26469;&#26368;&#23567;&#21270;&#22810;&#20010;&#22823;&#35268;&#27169;MIMO&#65288;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65289;&#22522;&#31449;&#22312;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#30340;&#24635;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#12290;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DEC-POMDP&#65289;&#65292;&#20197;&#23454;&#29616;&#20010;&#20307;&#22522;&#31449;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#23567;&#21306;&#38388;&#24178;&#25200;&#26159;&#24517;&#35201;&#30340;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#26469;&#23398;&#20064;&#21327;&#21516;&#22522;&#31449;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#20102;&#25552;&#39640;&#20854;&#21487;&#20280;&#32553;&#24615;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MAPPO-neighbor&#31574;&#30053;&#30340;&#20462;&#25913;&#29256;&#26412;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#30340;MAPPO&#26234;&#33021;&#20307;&#30456;&#27604;&#22522;&#32447;&#31574;&#30053;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#33258;&#21160;&#30561;&#30496;&#27169;&#24335;1&#65288;&#31526;&#21495;&#32423;&#20241;&#30496;&#65289;&#31639;&#27861;&#30456;&#27604;&#65292;MAPPO-neighbor&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;</title><link>https://arxiv.org/abs/2402.03201</link><description>&lt;p&gt;
&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#36827;&#34892;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guidance with Spherical Gaussian Constraint for Conditional Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#21487;&#24494;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#25351;&#23548;&#26469;&#22788;&#29702;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#26679;&#26412;&#36136;&#37327;&#19978;&#20570;&#20986;&#22949;&#21327;&#65292;&#24182;&#38656;&#35201;&#36739;&#23567;&#30340;&#24341;&#23548;&#27493;&#38271;&#65292;&#23548;&#33268;&#37319;&#26679;&#36807;&#31243;&#21464;&#38271;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22312;&#24341;&#23548;&#25439;&#22833;&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#27969;&#24418;&#20559;&#31163;&#30340;&#26681;&#26412;&#38382;&#39064;&#25152;&#22312;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#25439;&#22833;&#24341;&#23548;&#30340;&#20272;&#35745;&#35823;&#24046;&#30340;&#29305;&#23450;&#19979;&#30028;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27969;&#24418;&#20559;&#31163;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#65288;DSG&#65289;&#30340;&#25193;&#25955;&#65292;&#20174;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#38598;&#20013;&#29616;&#35937;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;DSG&#36890;&#36807;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#24341;&#23548;&#27493;&#39588;&#32422;&#26463;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form 
&lt;/p&gt;</description></item><item><title>&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03191</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#65292;&#32858;&#31867;&#21644;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Isotropy, Clusters, and Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03191
&lt;/p&gt;
&lt;p&gt;
&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23884;&#20837;&#31354;&#38388;&#26159;&#21542;&#22343;&#21248;&#21033;&#29992;&#25152;&#26377;&#32500;&#24230;&#65288;&#21363;&#26159;&#21542;&#20855;&#26377;&#21516;&#24615;&#36136;&#65289;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#35752;&#35770;&#12290;&#26377;&#35777;&#25454;&#25903;&#25345;&#21644;&#21453;&#23545;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#26045;&#21516;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#21516;&#24615;&#36136;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#35201;&#27714;&#19982;&#32858;&#31867;&#30340;&#23384;&#22312;&#19981;&#20860;&#23481;&#65292;&#36825;&#20063;&#23545;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#20107;&#23454;&#65292;&#24182;&#29992;&#23427;&#26469;&#38416;&#26126;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#21516;&#19968;&#20010;&#30406;&#22320;&#20869;&#30340;&#8220;&#30456;&#36830;&#8221;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#33976;&#39311;&#23558;&#20854;&#20182;&#30406;&#22320;&#30340;&#30693;&#35782;&#38544;&#21547;&#32435;&#20837;&#21040;&#21516;&#19968;&#20010;&#30406;&#22320;&#20869;&#65292;&#24357;&#34917;&#20102;&#36830;&#25509;&#22686;&#21152;&#23545;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#37325;&#26032;&#21457;&#29616;&#20102;&#65288;&#22810;&#30406;&#22320;&#65289;&#28145;&#24230;&#38598;&#25104;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#25512;&#27979;&#65292;&#22312;&#27809;&#26377;&#20174;&#20854;&#20182;&#30406;&#22320;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#26377;&#25928;&#21033;&#29992;&#20854;&#20182;&#30406;&#22320;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.03187</link><description>&lt;p&gt;
&#21333;&#19968;&#30406;&#22320;&#30340;&#22909;&#22788;&#26377;&#22810;&#22823;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good is a Single Basin?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#21516;&#19968;&#20010;&#30406;&#22320;&#20869;&#30340;&#8220;&#30456;&#36830;&#8221;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#33976;&#39311;&#23558;&#20854;&#20182;&#30406;&#22320;&#30340;&#30693;&#35782;&#38544;&#21547;&#32435;&#20837;&#21040;&#21516;&#19968;&#20010;&#30406;&#22320;&#20869;&#65292;&#24357;&#34917;&#20102;&#36830;&#25509;&#22686;&#21152;&#23545;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#37325;&#26032;&#21457;&#29616;&#20102;&#65288;&#22810;&#30406;&#22320;&#65289;&#28145;&#24230;&#38598;&#25104;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#25512;&#27979;&#65292;&#22312;&#27809;&#26377;&#20174;&#20854;&#20182;&#30406;&#22320;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#26377;&#25928;&#21033;&#29992;&#20854;&#20182;&#30406;&#22320;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25439;&#22833;&#26223;&#35266;&#30340;&#22810;&#27169;&#24577;&#24615;&#24120;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#38598;&#25104;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21508;&#31181;&#8220;&#30456;&#36830;&#8221;&#30340;&#38598;&#25104;&#26469;&#25506;&#31350;&#36825;&#31181;&#20449;&#24565;&#65292;&#36825;&#20123;&#38598;&#25104;&#34987;&#38480;&#21046;&#22312;&#21516;&#19968;&#20010;&#30406;&#22320;&#20869;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22686;&#21152;&#36830;&#25509;&#30830;&#23454;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#36890;&#36807;&#33976;&#39311;&#30340;&#26041;&#24335;&#38544;&#21547;&#22320;&#23558;&#20854;&#20182;&#30406;&#22320;&#30340;&#30693;&#35782;&#32435;&#20837;&#21040;&#21516;&#19968;&#20010;&#30406;&#22320;&#20013;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#36890;&#36807;&#20877;&#21457;&#29616;&#65288;&#22810;&#30406;&#22320;&#65289;&#28145;&#24230;&#38598;&#25104;&#26469;&#20943;&#36731;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#27979;&#65292;&#34429;&#28982;&#20219;&#20309;&#32473;&#23450;&#30406;&#22320;&#20013;&#33267;&#23569;&#37096;&#20998;&#23384;&#22312;&#30528;&#39069;&#22806;&#30406;&#22320;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#27809;&#26377;&#20174;&#20854;&#20182;&#30406;&#22320;&#20013;&#23398;&#20064;&#23427;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#26377;&#25928;&#21033;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-modal nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles. In this work, we probe this belief by constructing various "connected" ensembles which are restricted to lie in the same basin. Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance. However, when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles within a single basin. Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#20803;&#23398;&#20064;&#26694;&#26550;SeMPL&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#23398;&#20064;&#21644;&#39044;&#27979;&#32473;&#23450;&#36719;&#20214;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;SeMPL&#36890;&#36807;&#20381;&#27425;&#39034;&#24207;&#35757;&#32451;&#20803;&#29615;&#22659;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#26032;&#29615;&#22659;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.03183</link><description>&lt;p&gt;
&#29992;&#39034;&#24207;&#20803;&#23398;&#20064;&#39044;&#27979;&#22810;&#20010;&#29615;&#22659;&#19979;&#30340;&#37197;&#32622;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#20803;&#23398;&#20064;&#26694;&#26550;SeMPL&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#23398;&#20064;&#21644;&#39044;&#27979;&#32473;&#23450;&#36719;&#20214;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;SeMPL&#36890;&#36807;&#20381;&#27425;&#39034;&#24207;&#35757;&#32451;&#20803;&#29615;&#22659;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#26032;&#29615;&#22659;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#39044;&#27979;&#32473;&#23450;&#36719;&#20214;&#37197;&#32622;&#30340;&#24615;&#33021;&#23545;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#27963;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#22312;&#21333;&#20010;&#29615;&#22659;&#19979;&#26500;&#24314;&#24615;&#33021;&#27169;&#22411;&#25110;&#26410;&#33021;&#27491;&#30830;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#29615;&#22659;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#26032;&#29615;&#22659;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#22810;&#20010;&#29615;&#22659;&#19979;&#30340;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;SeMPL - &#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#19981;&#21516;(meta)&#29615;&#22659;&#20013;&#27979;&#37327;&#30340;&#37197;&#32622;&#20013;&#23398;&#20064;&#20849;&#21516;&#30340;&#29702;&#35299;&#65292;&#24182;&#23558;&#23427;&#20204;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#30446;&#26631;&#29615;&#22659;&#20013;&#12290;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#19982;&#24120;&#35265;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65288;&#22914;MAML&#21644;MetaSGD&#65289;&#24182;&#34892;&#35757;&#32451;&#20803;&#29615;&#22659;&#19981;&#21516;&#65292;&#25105;&#20204;&#20381;&#27425;&#39034;&#24207;&#35757;&#32451;&#23427;&#20204;&#12290;&#35757;&#32451;&#39034;&#24207;&#33258;&#28982;&#22320;&#20801;&#35768;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating t
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#29616;&#26377;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;LLMs&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20419;&#36827;&#21508;&#31181;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03182</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Empowering Time Series Analysis with Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#29616;&#26377;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;LLMs&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20419;&#36827;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26469;&#35828;&#65292;&#20174;&#22836;&#24320;&#22987;&#23436;&#20840;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#36890;&#29992;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#31181;&#31867;&#24222;&#22823;&#65292;&#20197;&#21450;&#23548;&#33268;&#27010;&#24565;&#28418;&#31227;&#30340;&#38750;&#31283;&#24577;&#65292;&#38459;&#30861;&#20102;&#36830;&#32493;&#27169;&#22411;&#36866;&#24212;&#21644;&#37325;&#26032;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;LLMs&#21487;&#20197;&#21033;&#29992;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20419;&#36827;&#21508;&#31181;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#27010;&#36848;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38416;&#36848;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#21160;&#26426;&#65292;&#20197;&#21450;LLMs&#30340;&#31616;&#35201;&#22522;&#30784;&#30693;&#35782;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22522;&#20110;LLMs&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#19968;&#33324;&#27969;&#31243;&#65292;&#24182;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#20064;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;800&#20010;&#21442;&#25968;&#21644;900&#27425;&#20056;&#27861;&#26469;&#23454;&#29616;&#20302;&#35299;&#30721;&#22797;&#26434;&#24230;&#12290;&#35813;&#32534;&#30721;&#22120;&#22312;&#21387;&#32553;&#35270;&#39057;&#26102;&#33021;&#22815;&#21033;&#29992;&#26102;&#38388;&#20887;&#20313;&#65292;&#24182;&#22312;&#25509;&#36817;AVC&#30340;&#36895;&#29575;&#22833;&#30495;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36807;&#25311;&#21512;&#32534;&#35299;&#30721;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03179</link><description>&lt;p&gt;
Cool-chic&#35270;&#39057;&#65306;&#36890;&#36807;800&#20010;&#21442;&#25968;&#23398;&#20064;&#35270;&#39057;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Cool-chic video: Learned video coding with 800 parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03179
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#20064;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;800&#20010;&#21442;&#25968;&#21644;900&#27425;&#20056;&#27861;&#26469;&#23454;&#29616;&#20302;&#35299;&#30721;&#22797;&#26434;&#24230;&#12290;&#35813;&#32534;&#30721;&#22120;&#22312;&#21387;&#32553;&#35270;&#39057;&#26102;&#33021;&#22815;&#21033;&#29992;&#26102;&#38388;&#20887;&#20313;&#65292;&#24182;&#22312;&#25509;&#36817;AVC&#30340;&#36895;&#29575;&#22833;&#30495;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36807;&#25311;&#21512;&#32534;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36890;&#36807;&#23398;&#20064;&#30340;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#65292;&#27599;&#20010;&#35299;&#30721;&#20687;&#32032;&#26377;900&#27425;&#20056;&#27861;&#65292;&#24635;&#20849;&#26377;800&#20010;&#21442;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#35299;&#30721;&#22797;&#26434;&#24230;&#26368;&#20302;&#30340;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#20043;&#19968;&#12290;&#23427;&#22522;&#20110;&#36807;&#25311;&#21512;&#30340;&#22270;&#29255;&#32534;&#35299;&#30721;&#22120;Cool-chic&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#26102;&#22495;&#32534;&#30721;&#27169;&#22359;&#26469;&#24378;&#21270;&#35270;&#39057;&#30340;&#26102;&#38388;&#20887;&#20313;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#21387;&#32553;&#35270;&#39057;&#20197;&#23454;&#29616;&#20302;&#24310;&#36831;&#21644;&#38543;&#26426;&#35775;&#38382;&#37197;&#32622;&#65292;&#24182;&#22312;&#25509;&#36817;AVC&#30340;&#36895;&#29575;&#22833;&#30495;&#26465;&#20214;&#19979;&#20248;&#20110;&#20854;&#20182;&#36807;&#25311;&#21512;&#32534;&#35299;&#30721;&#22120;&#65292;&#22914;FFNeRV&#12290;&#35813;&#31995;&#32479;&#26159;&#24320;&#28304;&#30340;&#65306;orange-opensource.github.io/Cool-Chic&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall. To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity. It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies. The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV. The system is made open-source: orange-opensource.github.io/Cool-Chic.
&lt;/p&gt;</description></item><item><title>CIDAR&#26159;&#31532;&#19968;&#20010;&#30001;&#20154;&#24037;&#35780;&#23457;&#23545;&#40784;&#25991;&#21270;&#30340;&#38463;&#25289;&#20271;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#29616;&#26377;&#25351;&#20196;&#25968;&#25454;&#38598;&#23545;&#35199;&#26041;&#25991;&#21270;&#30340;&#22266;&#26377;&#20559;&#35265;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#20016;&#23500;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38463;&#25289;&#20271;&#25991;&#21270;&#23545;&#40784;&#30340;&#30740;&#31350;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03177</link><description>&lt;p&gt;
CIDAR: &#38463;&#25289;&#20271;&#25991;&#30340;&#25991;&#21270;&#30456;&#20851;&#25351;&#20196;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CIDAR: Culturally Relevant Instruction Dataset For Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03177
&lt;/p&gt;
&lt;p&gt;
CIDAR&#26159;&#31532;&#19968;&#20010;&#30001;&#20154;&#24037;&#35780;&#23457;&#23545;&#40784;&#25991;&#21270;&#30340;&#38463;&#25289;&#20271;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#29616;&#26377;&#25351;&#20196;&#25968;&#25454;&#38598;&#23545;&#35199;&#26041;&#25991;&#21270;&#30340;&#22266;&#26377;&#20559;&#35265;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#20016;&#23500;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38463;&#25289;&#20271;&#25991;&#21270;&#23545;&#40784;&#30340;&#30740;&#31350;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#25104;&#20026;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#20027;&#35201;&#38754;&#21521;&#33521;&#35821;&#25110;&#32773;&#26469;&#28304;&#20110;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23545;&#35199;&#26041;&#25991;&#21270;&#30340;&#22266;&#26377;&#20559;&#35265;&#12290;&#36825;&#31181;&#20559;&#35265;&#23545;&#38463;&#25289;&#20271;&#25991;&#31561;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#35821;&#35328;&#32467;&#26500;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#38463;&#25289;&#20271;&#25991;&#21453;&#26144;&#20102;&#38463;&#25289;&#20271;&#22320;&#21306;&#22810;&#26679;&#25991;&#21270;&#30340;&#29420;&#29305;&#35821;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;CIDAR&#65292;&#21363;&#31532;&#19968;&#20010;&#30001;&#20154;&#24037;&#35780;&#23457;&#23545;&#40784;&#25991;&#21270;&#30340;&#38463;&#25289;&#20271;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65288;https://hf.co/datasets/arbml/CIDAR&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;CIDAR&#21253;&#21547;&#20102;&#20195;&#34920;&#38463;&#25289;&#20271;&#22320;&#21306;&#30340;10000&#20010;&#25351;&#20196;&#19982;&#36755;&#20986;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#20854;&#20182;&#27169;&#22411;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#35843;&#20248;&#32467;&#26524;&#65292;&#35752;&#35770;&#20102;CIDAR&#30340;&#25991;&#21270;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;CIDAR&#21487;&#20197;&#24110;&#21161;&#20016;&#23500;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38463;&#25289;&#20271;&#25991;&#21270;&#23545;&#40784;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#22312;...&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;KernelPCA&#21644;K-means Clustering&#22312;BERTopic&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;</title><link>https://arxiv.org/abs/2402.03176</link><description>&lt;p&gt;
&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Topic Modelling Approaches in the Banking Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;KernelPCA&#21644;K-means Clustering&#22312;BERTopic&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#33258;&#21160;&#25552;&#21462;&#20027;&#39064;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#26381;&#21153;&#34892;&#19994;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#30417;&#25511;&#23458;&#25143;&#35752;&#35770;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;Latent Dirichlet Allocation&#65292;LDA&#65289;&#22312;&#20027;&#39064;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26080;&#27861;&#23545;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#24314;&#27169;&#65292;&#23427;&#20204;&#30340;&#32467;&#26524;&#24182;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;BERTopic&#26550;&#26500;&#20013;&#20351;&#29992;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;Kernel Principal Component Analysis&#65292;KernelPCA&#65289;&#21644;K-means&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23612;&#26085;&#21033;&#20122;&#38134;&#34892;&#23458;&#25143;&#30340;&#25512;&#25991;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;BERTopic&#26550;&#26500;&#20013;&#20351;&#29992;KernelPCA&#21644;K-means&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#20854;&#20013;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03175</link><description>&lt;p&gt;
The Matrix: &#19968;&#20010;&#29992;&#20110;LLMs&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Matrix: A Bayesian learning model for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;LLM&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20197;&#27492;&#21407;&#21017;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#30001;&#20808;&#39564;&#21644;&#22810;&#39033;&#24335;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#34920;&#31034;&#30340;&#29702;&#24819;&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;LLMs&#22914;&#20309;&#36924;&#36817;&#35813;&#30697;&#38453;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23884;&#20837;&#21644;&#22810;&#39033;&#24335;&#20998;&#24067;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;Dirichlet&#36924;&#36817;&#23450;&#29702;&#26469;&#36924;&#36817;&#20219;&#20309;&#20808;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#22914;&#20309;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#21407;&#29702;&#19968;&#33268;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#20855;&#20307;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#26356;&#22823;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35270;&#20026;&#38656;&#35201;&#26356;&#26032;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20107;&#20214;&#35302;&#21457;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#21160;&#21147;&#23398;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20849;&#35782;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#27010;&#29575;&#20445;&#35777;&#30340;&#39044;&#27979;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.03174</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#20107;&#20214;&#35302;&#21457;&#22312;&#32447;&#23398;&#20064;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20107;&#20214;&#35302;&#21457;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#21160;&#21147;&#23398;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20849;&#35782;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#27010;&#29575;&#20445;&#35777;&#30340;&#39044;&#27979;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#20849;&#35782;&#25511;&#21046;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#30693;&#21160;&#21147;&#23398;&#19979;&#31649;&#29702;&#20849;&#35782;&#25511;&#21046;&#20173;&#28982;&#26159;&#25511;&#21046;&#35774;&#35745;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#21644;&#29615;&#22659;&#25200;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#25511;&#21046;&#27861;&#21017;&#65292;&#36890;&#36807;&#36741;&#21161;&#21160;&#21147;&#23398;&#26469;&#22686;&#24378;&#12290;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#34917;&#20607;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#25104;&#20998;&#12290;&#20026;&#20102;&#25345;&#32493;&#25552;&#39640;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#21644;&#20998;&#24067;&#24335;&#20107;&#20214;&#35302;&#21457;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#27010;&#29575;&#20445;&#35777;&#39044;&#27979;&#35823;&#24046;&#36793;&#30028;&#30340;Lyapunov&#29702;&#35770;&#30830;&#20445;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#20998;&#24067;&#24335;&#25511;&#21046;&#22120;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consensus control in multi-agent systems has received significant attention and practical implementation across various domains. However, managing consensus control under unknown dynamics remains a significant challenge for control design due to system uncertainties and environmental disturbances. This paper presents a novel learning-based distributed control law, augmented by an auxiliary dynamics. Gaussian processes are harnessed to compensate for the unknown components of the multi-agent system. For continuous enhancement in predictive performance of Gaussian process model, a data-efficient online learning strategy with a decentralized event-triggered mechanism is proposed. Furthermore, the control performance of the proposed approach is ensured via the Lyapunov theory, based on a probabilistic guarantee for prediction error bounds. To demonstrate the efficacy of the proposed learning-based controller, a comparative analysis is conducted, contrasting it with both conventional distri
&lt;/p&gt;</description></item><item><title>&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23545;&#39532;&#26684;&#37324;&#24067;&#24773;&#24863;&#20998;&#26512;&#22120;&#36896;&#25104;&#20102;&#20005;&#37325;&#24433;&#21709;&#65292;&#23558;&#20854;&#24615;&#33021;&#20174;F1&#24471;&#20998;0.95&#38477;&#20302;&#21040;0.33&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#24378;&#35843;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#30340;&#36947;&#24503;&#21644;&#36131;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.03171</link><description>&lt;p&gt;
&#39532;&#26684;&#37324;&#24067;&#24773;&#24863;&#20998;&#26512;&#22120;&#19978;&#30340;&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Homograph Attacks on Maghreb Sentiment Analyzers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03171
&lt;/p&gt;
&lt;p&gt;
&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23545;&#39532;&#26684;&#37324;&#24067;&#24773;&#24863;&#20998;&#26512;&#22120;&#36896;&#25104;&#20102;&#20005;&#37325;&#24433;&#21709;&#65292;&#23558;&#20854;&#24615;&#33021;&#20174;F1&#24471;&#20998;0.95&#38477;&#20302;&#21040;0.33&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#24378;&#35843;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#30340;&#36947;&#24503;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23545;&#39532;&#26684;&#37324;&#24067;&#21271;&#38750;&#22269;&#23478;&#19981;&#21516;&#38463;&#25289;&#20271;&#26041;&#35328;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#24403;&#25968;&#25454;&#20197;&#8220;&#38463;&#25289;&#20271;&#23383;&#27597;&#25340;&#38899;&#8221;&#20070;&#20889;&#26102;&#65292;&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23548;&#33268;&#21464;&#21387;&#22120;&#20998;&#31867;&#24615;&#33021;&#20174;F1&#24471;&#20998;0.95&#19979;&#38477;&#21040;0.33&#65292;&#20943;&#23569;&#20102;65.3%&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20984;&#26174;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#30340;&#36947;&#24503;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#26032;&#25552;&#20986;&#30340;&#36873;&#25321;&#24615;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;Mamba&#20855;&#26377;&#19982;transformers&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#23545;&#20110;&#28041;&#21450;&#36739;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;ICL&#20219;&#21153;&#65292;Mamba&#21487;&#20197;&#25104;&#20026;transformers&#30340;&#39640;&#25928;&#26367;&#20195;&#21697;&#12290;</title><link>https://arxiv.org/abs/2402.03170</link><description>&lt;p&gt;
Mamba&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Mamba Capable of In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#26032;&#25552;&#20986;&#30340;&#36873;&#25321;&#24615;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;Mamba&#20855;&#26377;&#19982;transformers&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#23545;&#20110;&#28041;&#21450;&#36739;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;ICL&#20219;&#21153;&#65292;Mamba&#21487;&#20197;&#25104;&#20026;transformers&#30340;&#39640;&#25928;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#26032;&#25552;&#20986;&#30340;&#36873;&#25321;&#24615;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;Mamba&#20855;&#26377;&#19982;transformers&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#31616;&#21333;&#20989;&#25968;&#36924;&#36817;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;Mamba&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20004;&#31867;&#20219;&#21153;&#20013;&#65292;Mamba&#22312;ICL&#26041;&#38754;&#30340;&#24615;&#33021;&#19982;transformer&#27169;&#22411;&#30456;&#21305;&#37197;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#31867;&#20284;transformers&#65292;Mamba&#20284;&#20046;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#20854;&#20869;&#37096;&#34920;&#31034;&#26469;&#35299;&#20915;ICL&#38382;&#39064;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#28041;&#21450;&#36739;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;ICL&#20219;&#21153;&#65292;Mamba&#21487;&#20197;&#25104;&#20026;transformers&#30340;&#39640;&#25928;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL. Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;&#65292;&#22312;&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#30340;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#22823;&#32500;&#35889;&#34892;&#20026;&#21644;&#20449;&#22122;&#27604;&#20934;&#30830;&#39044;&#27979;&#20102;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.03169</link><description>&lt;p&gt;
&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#30340;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;&#65292;&#22312;&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#30340;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#22823;&#32500;&#35889;&#34892;&#20026;&#21644;&#20449;&#22122;&#27604;&#20934;&#30830;&#39044;&#27979;&#20102;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#35745;&#31639;&#38408;&#20540;&#38468;&#36817;&#30340;&#19968;&#33324;&#23574;&#23792;&#24352;&#37327;&#27169;&#22411;&#65292;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#20272;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35748;&#35782;&#12290;&#20381;&#38752;&#22823;&#22411;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#25968;&#25454;&#24352;&#37327;&#30340;&#23637;&#24320;&#30340;&#22823;&#32500;&#35889;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#20915;&#23450;&#20027;&#35201;&#20449;&#21495;&#26041;&#21521;&#21487;&#26816;&#27979;&#24615;&#30340;&#30456;&#20851;&#20449;&#22122;&#27604;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#22312;&#38750;&#24179;&#20961;&#21306;&#22495;&#30340;&#25130;&#26029;&#22810;&#32447;&#24615;&#22855;&#24322;&#20540;&#20998;&#35299;(MLSVD)&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;&#36825;&#19968;&#28857;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20316;&#20026;&#26356;&#39640;&#38454;&#27491;&#20132;&#36845;&#20195;(HOOI)&#26041;&#26696;&#30340;&#21021;&#22987;&#21270;&#65292;&#20854;&#25910;&#25947;&#21040;&#26368;&#20339;&#20302;&#22810;&#32447;&#24615;&#31209;&#36924;&#36817;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#22312;&#22823;&#32500;&#26497;&#38480;&#19979;&#25910;&#25947;&#21069;&#30340;&#36845;&#20195;&#27425;&#25968;&#36235;&#20110;1&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime. This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03167</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;: &#26080;&#29615;&#31639;&#27861;&#26356;&#26032;&#21644;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21452;&#32423;&#20248;&#21270;&#65288;SBO&#65289;&#22312;&#22788;&#29702;&#23884;&#22871;&#32467;&#26500;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;SBO&#65292;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#20316;&#20026;&#26377;&#25928;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#19982;&#30452;&#25509;&#30456;&#37051;&#33410;&#28857;&#36827;&#34892;&#36890;&#20449;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#22686;&#24378;&#31639;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#31639;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#26114;&#36149;&#30340;&#20869;&#37096;&#24490;&#29615;&#26356;&#26032;&#21644;&#23545;&#32593;&#32476;&#25299;&#25169;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#23884;&#22871;&#21452;&#32423;&#31639;&#27861;&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#65288;D-SOBA&#65289;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;&#65292;&#39318;&#27425;&#28548;&#28165;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#37327;&#21270;&#36807;&#31243;&#65292;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19978;&#20855;&#26377;&#25913;&#36827;&#65292;&#21487;&#25193;&#23637;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.03158</link><description>&lt;p&gt;
&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimal and Near-Optimal Adaptive Vector Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#37327;&#21270;&#36807;&#31243;&#65292;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19978;&#20855;&#26377;&#25913;&#36827;&#65292;&#21487;&#25193;&#23637;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#22522;&#26412;&#20248;&#21270;&#65292;&#21253;&#25324;&#21387;&#32553;&#26799;&#24230;&#12289;&#27169;&#22411;&#26435;&#37325;&#21644;&#28608;&#27963;&#20197;&#21450;&#25968;&#25454;&#38598;&#12290;&#26368;&#20934;&#30830;&#30340;&#37327;&#21270;&#24418;&#24335;&#26159;&#8220;&#33258;&#36866;&#24212;&#8221;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#23545;&#20110;&#32473;&#23450;&#36755;&#20837;&#30340;&#35823;&#24046;&#26469;&#20248;&#21270;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#65288;AVQ&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#28176;&#36817;&#25913;&#36827;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#25214;&#21040;&#26368;&#20248;&#35299;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#65292;&#20197;&#22788;&#29702;&#22823;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#33021;&#20250;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26356;&#24191;&#27867;&#22320;&#20351;&#29992;AVQ&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.
&lt;/p&gt;</description></item><item><title>DogSurf&#26159;&#19968;&#31181;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#35270;&#21147;&#21463;&#25439;&#30340;&#20154;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23548;&#33322;&#65292;&#22312;&#34920;&#38754;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;99.925%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03156</link><description>&lt;p&gt;
DogSurf: &#22235;&#36275;&#26426;&#22120;&#20154;&#21487;&#35782;&#21035;&#34920;&#38754;&#30340;GRU&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30450;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03156
&lt;/p&gt;
&lt;p&gt;
DogSurf&#26159;&#19968;&#31181;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#35270;&#21147;&#21463;&#25439;&#30340;&#20154;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23548;&#33322;&#65292;&#22312;&#34920;&#38754;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;99.925%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22235;&#36275;&#26426;&#22120;&#20154;&#26469;&#24110;&#21161;&#35270;&#21147;&#21463;&#25439;&#30340;&#20154;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35753;&#22235;&#36275;&#26426;&#22120;&#20154;&#26816;&#27979;&#21040;&#28369;&#28316;&#30340;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#22768;&#38899;&#21644;&#35302;&#35273;&#21453;&#39304;&#21578;&#30693;&#29992;&#25143;&#20309;&#26102;&#20572;&#19979;&#12290;&#38024;&#23545;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#22810;&#31867;&#34920;&#38754;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#29575;&#20026;99.925%&#30340;&#26368;&#20808;&#36827;&#30340;GRU&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#22312;Unitree Go1 Edu&#26426;&#22120;&#20154;&#19978;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24050;&#22312;&#20844;&#20849;&#39046;&#22495;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DogSurf - a newapproach of using quadruped robots to help visually impaired people navigate in real world. The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop. A state-of-the-art GRU-based neural network architecture with mean accuracy of 99.925% was proposed for the task of multiclass surface classification for quadruped robots. A dataset was collected on a Unitree Go1 Edu robot. The dataset and code have been posted to the public domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#29289;&#29702;&#21512;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#25968;&#20540;&#35299;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#31995;&#21015;&#21442;&#25968;&#19979;&#30340;&#36895;&#24230;&#21644;&#21387;&#21147;&#20989;&#25968;&#30340;&#25554;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03153</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#21512;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Learning solutions of parametric Navier-Stokes with physics-informed neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#29289;&#29702;&#21512;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#25968;&#20540;&#35299;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#31995;&#21015;&#21442;&#25968;&#19979;&#30340;&#36895;&#24230;&#21644;&#21387;&#21147;&#20989;&#25968;&#30340;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#29289;&#29702;&#21512;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26469;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#65288;NSE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21487;&#34892;&#30340;&#20248;&#21270;&#38382;&#39064;&#35774;&#32622;&#20013;&#32469;&#36807;&#20102;PINNs&#22312;&#25910;&#25947;&#21040;&#39640;&#24230;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;PDEs&#65288;&#22914;NSE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#20316;&#20026;PINNs&#30340;&#36755;&#20837;&#65292;&#19982;&#26102;&#31354;&#22352;&#26631;&#19968;&#36215;&#35757;&#32451;PINNs&#65292;&#20351;&#29992;&#21442;&#25968;&#30340;&#23454;&#20363;&#29983;&#25104;&#30340;&#25968;&#20540;&#35299;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;PINNs&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#30340;&#20108;&#32500;&#22278;&#26609;&#27969;&#38382;&#39064;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26088;&#22312;&#23398;&#20064;&#22312;&#19968;&#23450;&#33539;&#22260;&#30340;&#38647;&#35834;&#25968;&#20316;&#20026;&#24863;&#20852;&#36259;&#21442;&#25968;&#26102;&#30340;&#36895;&#24230;&#21644;&#21387;&#21147;&#20989;&#25968;&#12290;&#20351;&#29992;&#26469;&#33258;&#29983;&#25104;&#30340;&#25968;&#20540;&#27169;&#25311;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#23545;&#19968;&#31995;&#21015;&#21442;&#25968;&#36827;&#34892;&#35299;&#20915;&#26041;&#26696;&#20989;&#25968;&#30340;&#25554;&#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;PINNs&#19982;&#26080;&#32422;&#26463;&#30340;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#36825;&#20010;&#38382;&#39064;&#35774;&#32622;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30740;&#31350;&#32771;&#34385;PDE&#30340;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE). Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE. We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters. We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest. Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters. Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#32452;&#32455;&#65292;&#20854;&#36890;&#36807;&#20998;&#35010;&#12289;&#32858;&#21512;&#12289;&#35843;&#21046;&#12289;&#21152;&#26435;&#21644;&#27714;&#21644;&#31561;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03149</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#32452;&#32455;&#65292;&#20854;&#36890;&#36807;&#20998;&#35010;&#12289;&#32858;&#21512;&#12289;&#35843;&#21046;&#12289;&#21152;&#26435;&#21644;&#27714;&#21644;&#31561;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#65288;MRR&#65289;&#30340;&#27169;&#25311;&#20809;&#23398;&#26550;&#26500;&#65292;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21152;&#36895;&#36890;&#29992;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#20026;&#20102;&#23454;&#29616;GEMM&#21151;&#33021;&#65292;&#36825;&#20123;&#22522;&#20110;MRR&#30340;&#26550;&#26500;&#19968;&#33324;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#65306;&#65288;i&#65289;&#23558;&#22810;&#20010;&#20809;&#20449;&#21495;&#20998;&#35010;&#65288;&#22797;&#21046;&#65289;&#20197;&#36798;&#21040;&#26576;&#31181;&#22810;&#20998;&#25903;&#65292;&#65288;ii&#65289;&#23558;&#22810;&#20010;&#20809;&#20449;&#21495;&#32858;&#21512;&#65288;&#22797;&#29992;&#65289;&#20197;&#36798;&#21040;&#26576;&#31181;&#22810;&#36755;&#20837;&#65292;&#65288;iii&#65289;&#35843;&#21046;&#20809;&#20449;&#21495;&#20197;&#23558;&#36755;&#20837;&#20540;&#21360;&#32622;&#20110;&#27169;&#25311;&#20449;&#21495;&#24133;&#24230;&#19978;&#65292;&#65288;iv&#65289;&#23545;&#35843;&#21046;&#30340;&#20809;&#20449;&#21495;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#23454;&#29616;&#27169;&#25311;&#36755;&#20837;&#26435;&#37325;&#30456;&#20056;&#65292;&#65288;v&#65289;&#23545;&#20809;&#20449;&#21495;&#36827;&#34892;&#27714;&#21644;&#12290;MRR&#22522;&#20110;&#30340;GEMM&#21152;&#36895;&#22120;&#20197;&#20219;&#24847;&#39034;&#24207;&#25191;&#34892;&#21069;&#22235;&#31181;&#20449;&#21495;&#25805;&#20316;&#65292;&#24573;&#30053;&#20102;&#36825;&#20123;&#25805;&#20316;&#39034;&#24207;&#23545;&#20854;&#24615;&#33021;&#30340;&#21487;&#33021;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#21152;&#36895;&#22120;&#32452;&#32455;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#27493;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21152;&#26435;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#22312;&#19981;&#21516;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#26469;&#35757;&#32451;&#21333;&#27493;&#27169;&#22411;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26377;&#25928;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.03146</link><description>&lt;p&gt;
&#29992;&#20110;&#31283;&#20581;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#27493;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#27493;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21152;&#26435;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#22312;&#19981;&#21516;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#26469;&#35757;&#32451;&#21333;&#27493;&#27169;&#22411;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26377;&#25928;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#20381;&#36182;&#20110;&#20174;&#25968;&#25454;&#23398;&#21040;&#30340;&#21333;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#27169;&#25311;&#36712;&#36857;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#21333;&#27493;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#30446;&#26631;&#26469;&#35757;&#32451;&#21333;&#27493;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#21516;&#30340;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#21152;&#26435;&#24179;&#22343;&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#25968;&#25454;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#29992;&#65288;&#35266;&#27979;&#19978;&#30340;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#65289;&#65292;&#32780;&#36825;&#31181;&#24773;&#20917;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#20026;&#20102;&#25903;&#25345;&#22810;&#27493;&#25439;&#22833;&#20989;&#25968;&#65292;&#39318;&#20808;&#25105;&#20204;&#22312;&#20004;&#31181;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#23427;&#30340;&#24615;&#36136;&#65306;i&#65289;&#19968;&#32500;&#32447;&#24615;&#31995;&#32479;&#65292;&#21644;ii&#65289;&#20004;&#21442;&#25968;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#29615;&#22659;&#25110;&#25968;&#25454;&#38598;&#65289;&#20013;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26410;&#26469;&#39044;&#27979;&#20013;&#30340;&#24179;&#22343;R2&#24471;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. Finally,
&lt;/p&gt;</description></item><item><title>SafEDMD&#26159;&#19968;&#31181;&#22522;&#20110;EDMD&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#21644;&#35748;&#35777;&#23548;&#21521;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#36827;&#34892;&#35748;&#35777;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03145</link><description>&lt;p&gt;
SafEDMD&#65306;&#19968;&#31181;&#19987;&#20026;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#32780;&#35774;&#35745;&#30340;&#35748;&#35777;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03145
&lt;/p&gt;
&lt;p&gt;
SafEDMD&#26159;&#19968;&#31181;&#22522;&#20110;EDMD&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#21644;&#35748;&#35777;&#23548;&#21521;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#36827;&#34892;&#35748;&#35777;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#21160;&#24577;&#25511;&#21046;&#31995;&#32479;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20854;&#20013;&#31639;&#23376;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;EDMD&#65289;&#21551;&#21457;&#24335;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#21644;&#35748;&#35777;&#23548;&#21521;&#30340;EDMD&#65288;SafEDMD&#65289;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;EDMD&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#23427;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35777;&#20070;&#65292;&#20174;&#32780;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#29983;&#25104;&#21487;&#38752;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#20026;&#20102;&#30830;&#20445;SafEDMD&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#27604;&#20363;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#22312;&#21407;&#28857;&#22788;&#28040;&#22833;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#36827;&#34892;&#35748;&#35777;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#35828;&#26126;&#20102;&#25152;&#24320;&#21457;&#30340;&#26426;&#21046;&#65292;&#24182;&#24378;&#35843;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion. To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming. We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#21442;&#25968;&#30340;&#21098;&#26525;&#31639;&#27861;KEN&#65292;&#23427;&#33021;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#24133;&#33410;&#30465;&#20869;&#23384;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#23454;&#29616;&#20102;&#23545;transformer&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03142</link><description>&lt;p&gt;
"&#23569;&#21363;&#26159;&#22810;&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#31616;&#21333;&#38750;&#21442;&#25968;&#21098;&#26525;&#31639;&#27861;"
&lt;/p&gt;
&lt;p&gt;
Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#21442;&#25968;&#30340;&#21098;&#26525;&#31639;&#27861;KEN&#65292;&#23427;&#33021;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#24133;&#33410;&#30465;&#20869;&#23384;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#23454;&#29616;&#20102;&#23545;transformer&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#21098;&#26525;&#31639;&#27861;&#36890;&#24120;&#23384;&#22312;&#26550;&#26500;&#29305;&#24322;&#24615;&#12289;&#36807;&#24230;&#22797;&#26434;&#21644;&#20381;&#36182;&#22797;&#26434;&#35745;&#31639;&#31561;&#38480;&#21046;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#30340;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#31639;&#27861;KEN&#12290;KEN&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26377;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#23558;&#20854;&#20182;&#21442;&#25968;&#24674;&#22797;&#21040;&#39044;&#35757;&#32451;&#29366;&#24577;&#65292;&#20174;&#32780;&#26500;&#24314;&#20248;&#21270;&#21518;&#30340;transformer&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21482;&#23384;&#20648;&#20248;&#21270;&#21518;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;&#23545;&#19971;&#20010;transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03141</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#30701;&#26102;&#24310;&#20219;&#21153;&#25552;&#21319;&#38271;&#26102;&#24310;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#24773;&#26223;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24310;&#36831;&#24773;&#26223;&#26159;&#25351;&#35266;&#23519;&#21644;&#20132;&#20114;&#23384;&#22312;&#24310;&#36831;&#30340;&#24120;&#35265;&#23454;&#38469;&#24773;&#20917;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#29366;&#24577;&#22686;&#24378;&#25216;&#26415;&#22312;&#24310;&#36831;&#27493;&#39588;&#20013;&#21487;&#33021;&#20250;&#20986;&#29616;&#29366;&#24577;&#31354;&#38388;&#25193;&#22823;&#25110;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Auxiliary-Delayed Reinforcement Learning&#65288;AD-RL&#65289;&#65292;&#21033;&#29992;&#19968;&#20010;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AD-RL&#22312;&#30701;&#26102;&#24310;&#20219;&#21153;&#20013;&#23398;&#20064;&#20540;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#38271;&#26102;&#24310;&#20219;&#21153;&#20013;&#30340;&#33258;&#20030;&#21644;&#31574;&#30053;&#25913;&#36827;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#19982;&#30452;&#25509;&#22312;&#21407;&#22987;&#38271;&#26102;&#24310;&#20219;&#21153;&#19978;&#23398;&#20064;&#30456;&#27604;&#65292;&#36825;&#26679;&#20570;&#21487;&#20197;&#22823;&#22823;&#20943;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficienc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#32972;&#26223;&#20449;&#24687;&#34701;&#20837;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36229;&#38598;&#30340;&#19981;&#21464;&#37327;&#32479;&#35745;&#37327;&#32435;&#20837;&#25152;&#20851;&#27880;&#30340;&#23376;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#23450;&#36229;&#32423;&#23376;&#38598;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.03139</link><description>&lt;p&gt;
&#25552;&#21319;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#65306;&#23558;&#32972;&#26223;&#20449;&#24687;&#34701;&#20837;&#21040;&#38598;&#21512;&#34920;&#31034;&#20013;
&lt;/p&gt;
&lt;p&gt;
Enhancing Neural Subset Selection: Integrating Background Information into Set Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03139
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#32972;&#26223;&#20449;&#24687;&#34701;&#20837;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36229;&#38598;&#30340;&#19981;&#21464;&#37327;&#32479;&#35745;&#37327;&#32435;&#20837;&#25152;&#20851;&#27880;&#30340;&#23376;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#23450;&#36229;&#32423;&#23376;&#38598;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#65292;&#22914;AI&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#21270;&#21512;&#29289;&#36873;&#25321;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26500;&#24314;&#27169;&#22411;&#65292;&#25429;&#25417;&#25928;&#29992;&#20989;&#25968;&#20540;&#19982;&#20854;&#30456;&#24212;&#36229;&#38598;&#20013;&#23376;&#38598;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#38598;&#21512;&#20989;&#25968;&#26102;&#24448;&#24448;&#24573;&#35270;&#20102;&#36229;&#38598;&#20013;&#21253;&#21547;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#35770;&#30340;&#35266;&#28857;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#30446;&#26631;&#20540;&#22312;&#36755;&#20837;&#38598;&#21512;&#21644;&#23376;&#38598;&#30340;&#26465;&#20214;&#19979;&#26102;&#65292;&#23558;&#36229;&#38598;&#30340;&#19981;&#21464;&#37327;&#32479;&#35745;&#37327;&#32435;&#20837;&#25152;&#20851;&#27880;&#30340;&#23376;&#38598;&#26159;&#26377;&#25928;&#23398;&#20064;&#30340;&#20851;&#38190;&#12290;&#36825;&#30830;&#20445;&#36755;&#20986;&#20540;&#23545;&#20110;&#23376;&#38598;&#21450;&#20854;&#30456;&#24212;&#30340;&#36229;&#38598;&#30340;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#30340;&#36229;&#32423;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific super
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03138</link><description>&lt;p&gt;
Just Cluster It: &#19968;&#31181;&#20351;&#29992;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#39640;&#32500;&#31354;&#38388;&#25506;&#32034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#24449;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#35270;&#20026;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#20351;&#29992;&#32858;&#31867;&#26469;&#36827;&#34892;&#25506;&#32034;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#65292;&#19982;&#20108;&#32500;&#29615;&#22659;&#30456;&#27604;&#65292;&#29366;&#24577;&#36716;&#25442;&#20013;&#30340;&#20687;&#32032;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#19981;&#37027;&#20040;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#36827;&#34892;&#21608;&#26399;&#24615;&#21644;&#20840;&#23616;&#32858;&#31867;&#26469;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#21363;&#20272;&#35745;&#20266;&#35745;&#25968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#29305;&#24449;&#20063;&#21487;&#20197;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#32858;&#31867;&#20197;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#28982;&#32780;&#24403;&#36825;&#20123;&#29305;&#24449;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#26102;&#65292;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#30001;&#20110;&#20854;&#39044;&#35757;&#32451;&#30340;&#24402;&#32435;&#20559;&#24046;&#22312;&#34920;&#31034;&#20013;&#26356;&#21152;&#26377;&#25928;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20026;&#38598;&#25104;&#39044;&#35757;&#32451;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Hinglish&#24773;&#24863;&#20998;&#31867;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#35821;&#35328;&#36873;&#25321;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23588;&#20854;&#26159;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#23384;&#22312;&#26102;&#12290;&#36825;&#23545;&#20110;&#24773;&#24863;&#20998;&#31867;&#30340;&#35299;&#37322;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03137</link><description>&lt;p&gt;
&#31038;&#20250;&#35821;&#35328;&#23398;&#20449;&#24687;&#30340;&#35299;&#37322;&#24615;: &#20197;Hinglish&#24773;&#24863;&#20998;&#31867;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03137
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Hinglish&#24773;&#24863;&#20998;&#31867;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#35821;&#35328;&#36873;&#25321;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23588;&#20854;&#26159;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#23384;&#22312;&#26102;&#12290;&#36825;&#23545;&#20110;&#24773;&#24863;&#20998;&#31867;&#30340;&#35299;&#37322;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#35821;&#35328;&#34920;&#36798;&#20855;&#26377;&#22266;&#26377;&#30340;&#29305;&#27530;&#24615;&#21644;&#20027;&#35266;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#20013;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#21542;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#36866;&#24212;&#19981;&#21516;&#35821;&#35328;&#38388;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#20173;&#28982;&#26377;&#24453;&#32771;&#39564;&#12290;&#31038;&#20250;&#35821;&#35328;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;Hinglish&#35828;&#35805;&#32773;&#22312;&#34920;&#36798;&#28040;&#26497;&#24773;&#32490;&#26102;&#36716;&#29992;&#21360;&#22320;&#35821;&#65292;&#22312;&#34920;&#36798;&#31215;&#26497;&#24773;&#32490;&#26102;&#36716;&#29992;&#33521;&#35821;&#12290;&#20026;&#20102;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#23398;&#20064;&#36825;&#20123;&#20851;&#32852;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;3&#20010;PLMs&#22312;&#19968;&#20010;Hinglish&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35821;&#35328;&#23545;&#24773;&#24863;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;LIME&#21644;&#22522;&#20110;&#35789;&#20803;&#30340;&#35821;&#35328;ID&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#30830;&#23454;&#23398;&#20064;&#21040;&#20102;&#35821;&#35328;&#36873;&#25321;&#21644;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#24403;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23384;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#21487;&#20197;&#22686;&#24378;&#36825;&#31181;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#32467;&#35770;&#65292;&#20351;&#29992;&#31038;&#20250;&#35821;&#35328;&#23398;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#31867;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also concl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03131</link><description>&lt;p&gt;
&#29992;&#20110;&#36328;&#35821;&#35328;&#26631;&#31614;&#25237;&#24433;&#30340;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Constrained Decoding for Cross-lingual Label Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#23545;&#21333;&#35789;&#21644;&#30701;&#35821;&#36827;&#34892;&#32454;&#31890;&#24230;&#39044;&#27979;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#36828;&#36828;&#33853;&#21518;&#20110;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#21033;&#29992;&#32763;&#35793;&#21644;&#26631;&#31614;&#25237;&#24433;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#20307;&#26469;&#35828;(1)&#23558;&#21487;&#29992;&#30340;&#20197;&#21450;&#24102;&#26377;&#40644;&#37329;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;(&#20363;&#22914;&#33521;&#35821;)&#32763;&#35793;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#21644;/&#25110;(2)&#23558;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#27979;&#35797;&#25968;&#25454;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#30340;&#36328;&#24230;&#32423;&#21035;&#26631;&#31614;&#25237;&#23556;&#22238;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;&#30001;&#20110;&#22312;&#36755;&#20837;&#21040;&#32763;&#35793;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#27880;&#20837;&#20102;&#39069;&#22806;&#26631;&#35760;&#65292;&#23548;&#33268;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we e
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#22312;&#38750;&#20984;&#21644;&#20984;&#35774;&#32622;&#19979;&#37117;&#33021;&#21462;&#24471;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25351;&#20986;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03126</link><description>&lt;p&gt;
&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#33258;&#30001;&#24230;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Free is Parameter-Free Stochastic Optimization?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03126
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#22312;&#38750;&#20984;&#21644;&#20984;&#35774;&#32622;&#19979;&#37117;&#33021;&#21462;&#24471;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25351;&#20986;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#21487;&#20197;&#23384;&#22312;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65306;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20248;&#35843;&#21442;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#30495;&#23454;&#38382;&#39064;&#21442;&#25968;&#26377;&#24456;&#22810;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#26041;&#27861;&#21482;&#33021;&#34987;&#35270;&#20026;&#8220;&#37096;&#20998;&#8221;&#26080;&#21442;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#30495;&#23454;&#38382;&#39064;&#21442;&#25968;&#26377;&#19968;&#20123;&#38750;&#24179;&#20961;&#30340;&#30693;&#35782;&#65292;&#27604;&#22914;&#38543;&#26426;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#12289;&#21040;&#26368;&#23567;&#20540;&#30340;&#36317;&#31163;&#30340;&#19978;&#30028;&#31561;&#12290;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#26356;&#22797;&#26434;&#30340;&#20808;&#36827;&#31639;&#27861;&#12290;&#22312;&#20855;&#26377;&#22122;&#22768;&#20989;&#25968;&#20540;&#30340;&#20984;&#35774;&#32622;&#19979;&#65292;&#22312;&#36739;&#23567;&#30340;&#22122;&#22768;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#38543;&#26426;&#26799;&#24230;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#20351;&#24471;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20013;&#28040;&#38500;&#30828;&#26631;&#31614;&#32422;&#26463;&#65292;&#32771;&#34385;&#21040;&#26631;&#31614;&#24179;&#28369;&#21644;mixup&#25216;&#26415;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#24674;&#22797;&#22686;&#24378;&#26631;&#31614;&#21644;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#20026;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#25552;&#20379;&#20102;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.03124</link><description>&lt;p&gt;
&#22312;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20013;&#28040;&#38500;&#30828;&#26631;&#31614;&#32422;&#26463;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20013;&#28040;&#38500;&#30828;&#26631;&#31614;&#32422;&#26463;&#65292;&#32771;&#34385;&#21040;&#26631;&#31614;&#24179;&#28369;&#21644;mixup&#25216;&#26415;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#24674;&#22797;&#22686;&#24378;&#26631;&#31614;&#21644;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#20026;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#25552;&#20379;&#20102;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26088;&#22312;&#20174;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#26292;&#38706;&#30340;&#20013;&#38388;&#26799;&#24230;&#20013;&#37325;&#26500;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#25915;&#20987;&#25104;&#21151;&#65292;&#20294;&#20197;&#24448;&#30340;&#25152;&#26377;&#26041;&#27861;&#65292;&#20174;&#37325;&#26500;&#21333;&#20010;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#25918;&#23485;&#21040;&#25209;&#22788;&#29702;&#32423;&#21035;&#30340;&#21333;&#22270;&#20687;&#38480;&#21046;&#65292;&#37117;&#21482;&#22312;&#30828;&#26631;&#31614;&#32422;&#26463;&#19979;&#36827;&#34892;&#27979;&#35797;&#12290;&#21363;&#20351;&#23545;&#20110;&#21333;&#22270;&#20687;&#37325;&#24314;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#30340;&#31639;&#27861;&#26469;&#24674;&#22797;&#22686;&#24378;&#30340;&#36719;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20174;&#25193;&#22823;&#25209;&#37327;&#22823;&#23567;&#36716;&#21521;&#30740;&#31350;&#30828;&#26631;&#31614;&#32422;&#26463;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26631;&#31614;&#24179;&#28369;&#21644;mixup&#25216;&#26415;&#30340;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21516;&#26102;&#20174;&#21333;&#36755;&#20837;&#26799;&#24230;&#20013;&#24674;&#22797;&#30495;&#23454;&#30340;&#22686;&#24378;&#26631;&#31614;&#21644;&#26368;&#21518;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#20026;&#20219;&#20309;&#22522;&#20110;&#20998;&#26512;&#30340;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#25552;&#20379;&#20102;&#24517;&#35201;&#26465;&#20214;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.03119</link><description>&lt;p&gt;
&#22909;&#30340;&#25945;&#24072;&#35299;&#37322;: &#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Good Teachers Explain: Explanation-Enhanced Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#25104;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30693;&#36947;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#25945;&#24072;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#24050;&#32463;&#21457;&#29616;&#23398;&#29983;&#27169;&#22411;&#36890;&#24120;&#19981;&#20250;&#23398;&#21040;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#30456;&#20284;&#23646;&#24615;&#65292;&#22914;&#22522;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#24120;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#22240;&#20026;&#36825;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#8220;&#27491;&#30830;&#30340;&#29305;&#24449;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#32463;&#20856;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#21450;&#25945;&#24072;&#21644;&#23398;&#29983;&#25152;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#30456;&#20284;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#31616;&#21333;&#19988;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#8221;&#65288;e$^2$KD&#65289;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#22987;&#32456;&#25552;&#20379;&#20102;&#22823;&#24133;&#24230;&#30340;&#22686;&#30410;&#65292;&#65288;2&#65289;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21019;&#24314;&#25925;&#20107;&#21270;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#34892;&#20026;&#27169;&#24335;&#65292;&#20351;&#24471;&#25925;&#20107;&#26495;&#33021;&#22815;&#36866;&#24212;&#21160;&#24577;&#21040;&#36798;&#25110;&#36873;&#25321;&#30340;&#25968;&#25454;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.03116</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25925;&#20107;&#21270;&#21487;&#35270;&#21270;&#29305;&#24449;&#34892;&#20026;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Feature-Action Design Patterns for Storytelling Visualizations with Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03116
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21019;&#24314;&#25925;&#20107;&#21270;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#34892;&#20026;&#27169;&#24335;&#65292;&#20351;&#24471;&#25925;&#20107;&#26495;&#33021;&#22815;&#36866;&#24212;&#21160;&#24577;&#21040;&#36798;&#25110;&#36873;&#25321;&#30340;&#25968;&#25454;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21019;&#24314;&#25925;&#20107;&#21270;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#29616;&#22914;&#20170;&#65292;&#35768;&#22810;&#20010;&#20154;&#20915;&#31574;&#37117;&#20381;&#36182;&#20110;&#23450;&#26399;&#35775;&#38382;&#21160;&#24577;&#25968;&#25454;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#25152;&#35265;&#12290;&#22240;&#27492;&#65292;&#20026;&#36873;&#25321;&#26576;&#20010;&#29305;&#23450;&#24773;&#22659;&#19979;&#20010;&#20154;&#36873;&#23450;&#30340;&#21160;&#24577;&#25968;&#25454;&#26500;&#24314;&#25925;&#20107;&#21270;&#21487;&#35270;&#21270;&#26159;&#21487;&#21462;&#30340;&#12290;&#30001;&#20110;&#38656;&#35201;&#35762;&#36848;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25925;&#20107;&#65292;&#22522;&#20110;&#24050;&#30693;&#25968;&#25454;&#30340;&#39044;&#23450;&#20041;&#25925;&#20107;&#26495;&#19981;&#33021;&#36731;&#26494;&#22320;&#36866;&#24212;&#21160;&#24577;&#25968;&#25454;&#65292;&#20063;&#26080;&#27861;&#25193;&#23637;&#21040;&#35768;&#22810;&#19981;&#21516;&#30340;&#20010;&#20154;&#21644;&#24773;&#22659;&#12290;&#26368;&#21021;&#21463;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#20256;&#36798;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38656;&#27714;&#30340;&#28608;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#26041;&#27861;&#65292;&#29992;&#20110;&#20803;&#21019;&#20316;&#25925;&#20107;&#65292;&#23427;&#21487;&#20197;&#35774;&#35745;&#21253;&#25324;&#29305;&#24449;&#34892;&#20026;&#27169;&#24335;&#30340;&#25925;&#20107;&#26495;&#65292;&#20197;&#39044;&#27979;&#21160;&#24577;&#21040;&#36798;&#25110;&#36873;&#25321;&#30340;&#25968;&#25454;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#38500;&#20102;&#28041;&#21450;COVID-19&#25968;&#25454;&#30340;&#20803;&#25925;&#20107;&#26495;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#36827;&#23637;&#30340;&#25925;&#20107;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to create storytelling visualization with time series data. Many personal decisions nowadays rely on access to dynamic data regularly, as we have seen during the COVID-19 pandemic. It is thus desirable to construct storytelling visualization for dynamic data that is selected by an individual for a specific context. Because of the need to tell data-dependent stories, predefined storyboards based on known data cannot accommodate dynamic data easily nor scale up to many different individuals and contexts. Motivated initially by the need to communicate time series data during the COVID-19 pandemic, we developed a novel computer-assisted method for meta-authoring of stories, which enables the design of storyboards that include feature-action patterns in anticipation of potential features that may appear in dynamically arrived or selected data. In addition to meta-storyboards involving COVID-19 data, we also present storyboards for telling stories about progress in a mach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#12289;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#24418;&#25104;&#39640;&#24230;&#31616;&#32422;&#19988;&#20934;&#30830;&#24615;&#21487;&#23218;&#32654;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#37322;&#29983;&#29289;&#29616;&#35937;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03115</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discovering interpretable models of scientific image data with deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#12289;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#24418;&#25104;&#39640;&#24230;&#31616;&#32422;&#19988;&#20934;&#30830;&#24615;&#21487;&#23218;&#32654;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#37322;&#29983;&#29289;&#29616;&#35937;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#32473;&#23450;&#19968;&#20123;&#22797;&#26434;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#65292;&#24773;&#20917;&#19979;&#25214;&#21040;&#21487;&#35299;&#37322;&#30340;&#12289;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#33258;&#28982;&#29616;&#35937;&#27169;&#22411;&#65311;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#31185;&#23398;&#35265;&#35299;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#12289;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#22238;&#24402;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#24418;&#25104;&#21487;&#35299;&#37322;&#30340;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#26174;&#24494;&#38236;&#35266;&#23519;&#32454;&#32990;&#29366;&#24577;&#20998;&#31867;&#30340;&#27979;&#35797;&#38382;&#39064;&#26469;&#35777;&#26126;&#23427;&#20204;&#19982;&#29983;&#29289;&#25104;&#20687;&#39046;&#22495;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#39640;&#24230;&#31616;&#32422;&#30340;&#27169;&#22411;&#65292;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;&#40657;&#30418;&#22522;&#20934;&#27169;&#22411;&#30340;&#32422;98%&#65292;&#20294;&#22797;&#26434;&#24230;&#20165;&#20026;&#20854;&#19968;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#21487;&#35299;&#37322;&#27169;&#22411;&#22312;&#35299;&#37322;&#28508;&#22312;&#29983;&#29289;&#29616;&#35937;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images? Can we use such models to derive scientific insight from the data? In this paper, we propose some methods for achieving this. In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data. We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data. We find that such methods can produce highly parsimonious models that achieve $\sim98\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity. We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#21644;&#35299;&#37322;&#20598;&#27694;&#21270;&#21512;&#29289;&#30340;&#32418;&#22806;&#20809;&#35889;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38598;&#20013;&#20851;&#27880;&#19982;&#21151;&#33021;&#22242;&#37051;&#36817;&#30340;&#21270;&#23398;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20809;&#35889;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#25581;&#31034;&#32418;&#22806;&#20809;&#35889;&#29305;&#24449;&#19982;&#20998;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.03112</link><description>&lt;p&gt;
&#21033;&#29992;&#20855;&#26377;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20598;&#27694;&#22522;&#22242;&#30340;&#32418;&#22806;&#20809;&#35889;
&lt;/p&gt;
&lt;p&gt;
Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#21644;&#35299;&#37322;&#20598;&#27694;&#21270;&#21512;&#29289;&#30340;&#32418;&#22806;&#20809;&#35889;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38598;&#20013;&#20851;&#27880;&#19982;&#21151;&#33021;&#22242;&#37051;&#36817;&#30340;&#21270;&#23398;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20809;&#35889;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#25581;&#31034;&#32418;&#22806;&#20809;&#35889;&#29305;&#24449;&#19982;&#20998;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#65288;IR&#65289;&#20809;&#35889;&#26159;&#21270;&#23398;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#36890;&#36807;&#25391;&#21160;&#21644;&#36716;&#21160;&#36291;&#36801;&#38416;&#26126;&#20998;&#23376;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#29420;&#29305;&#30340;&#25391;&#21160;&#21644;&#36716;&#21160;&#27169;&#24335;&#25152;&#34920;&#24449;&#30340;&#22797;&#26434;&#20998;&#23376;&#29305;&#24449;&#32473;&#20998;&#26512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#20598;&#27694;&#21270;&#21512;&#29289;&#65292;&#20197;&#25552;&#21319;&#32418;&#22806;&#20809;&#35889;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#38598;&#20013;&#20851;&#27880;&#37051;&#36817;&#21151;&#33021;&#22242;&#38468;&#36817;&#30340;&#21270;&#23398;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20809;&#35889;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25581;&#31034;&#20102;&#32418;&#22806;&#20809;&#35889;&#29305;&#24449;&#19982;&#20998;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#36884;&#24452;&#26469;&#35299;&#26512;&#22797;&#26434;&#30340;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions. However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges. Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds. Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions. This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03110</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Latent Auto-Regressive Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#38750;&#24179;&#31283;&#22870;&#21169;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#33218;&#30340;&#24179;&#22343;&#22870;&#21169;&#38543;&#26102;&#38388;&#21464;&#21270;&#26159;&#30001;&#19968;&#20123;&#26410;&#30693;&#30340;&#28508;&#22312;&#33258;&#22238;&#24402;(AR)&#29366;&#24577;&#30340;&#39034;&#24207;k&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#29615;&#22659;&#31216;&#20026;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#12290;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#30340;&#19981;&#21516;&#24418;&#24335;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#37117;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#20581;&#24247;&#25110;&#25945;&#32946;&#31561;&#26032;&#20852;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#36825;&#37324;&#32570;&#20047;&#23545;&#29615;&#22659;&#30340;&#26426;&#21046;&#24314;&#27169;&#12290;&#22914;&#26524;AR&#39034;&#24207;k&#24050;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#34920;&#29616;&#20986;O(k&#8730;T)&#30340;&#36951;&#25022;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;k&#34987;&#38169;&#35823;&#22320;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20063;&#32988;&#36807;&#26631;&#20934;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#31574;&#30053;&#23450;&#20041;&#23616;&#37096;&#25628;&#32034;&#21306;&#22495;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20110;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03104</link><description>&lt;p&gt;
&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#31574;&#30053;&#23450;&#20041;&#23616;&#37096;&#25628;&#32034;&#21306;&#22495;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20110;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23547;&#25214;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#23558;BO&#24212;&#29992;&#20110;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#23558;&#25628;&#32034;&#22495;&#21010;&#20998;&#25104;&#21253;&#21547;&#20840;&#23616;&#26368;&#20248;&#35299;&#21487;&#33021;&#24615;&#36739;&#39640;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#28982;&#21518;&#22312;&#36825;&#20123;&#21306;&#22495;&#20869;&#20351;&#29992;BO&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#65288;CMA&#65289;&#31574;&#30053;&#23450;&#20041;&#23616;&#37096;&#21306;&#22495;&#30340;&#26032;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;CMA&#26469;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20272;&#35745;&#25968;&#25454;&#28857;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#27010;&#29575;&#30340;&#25628;&#32034;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20010;&#25628;&#32034;&#20998;&#24067;&#65292;&#25105;&#20204;&#23450;&#20041;&#30001;&#39640;&#27010;&#29575;&#25968;&#25454;&#28857;&#32452;&#25104;&#30340;&#23616;&#37096;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#19968;&#20010;&#20803;&#31639;&#27861;&#65292;&#21487;&#20197;&#25972;&#21512;&#29616;&#26377;&#30340;&#40657;&#30418;BO&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21644;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#26469;&#25913;&#36827;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03099</link><description>&lt;p&gt;
&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#65306;&#29992;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#22686;&#24378;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21644;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#26469;&#25913;&#36827;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#21644;&#25991;&#26412;&#20219;&#21153;&#25351;&#20196;&#30340;&#22266;&#26377;&#27495;&#20041;&#65292;&#25552;&#31034;&#24037;&#31243;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#20010;&#21253;&#21547;&#19978;&#27425;&#35797;&#39564;&#32467;&#26524;&#30340;&#20803;&#25552;&#31034;&#24182;&#25552;&#20986;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#33258;&#21160;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#26159;&#22256;&#38590;&#19988;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#20351;&#29992;&#26657;&#20934;&#36807;&#31243;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#19982;&#29992;&#25143;&#24847;&#22270;&#30456;&#31526;&#30340;&#25552;&#31034;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#31995;&#32479;&#32852;&#21512;&#29983;&#25104;&#36793;&#30028;&#29992;&#20363;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#36741;&#21161;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#21644;&#21512;&#27861;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#19981;&#20165;&#20855;&#26377;&#26356;&#22909;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#32780;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#25915;&#20987;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03095</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#25239;&#24615;&#25200;&#21160;&#65306;&#36890;&#36807;&#21512;&#27861;&#35821;&#20041;&#30340;&#27969;&#24418;&#36741;&#21161;&#23545;&#25239;&#24615;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#36741;&#21161;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#21644;&#21512;&#27861;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#19981;&#20165;&#20855;&#26377;&#26356;&#22909;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#32780;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#25915;&#20987;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#24694;&#24847;&#24494;&#23567;&#25200;&#21160;&#25805;&#32437;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30456;&#24403;&#33030;&#24369;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#36807;&#26368;&#23567;&#21270;&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#20960;&#20309;&#36317;&#31163;&#26469;&#30830;&#20445;&#20854;&#35270;&#35273;&#19978;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#20294;&#26159;&#36825;&#31181;&#20960;&#20309;&#36317;&#31163;&#30340;&#32422;&#26463;&#23548;&#33268;&#20102;&#26377;&#38480;&#30340;&#25915;&#20987;&#21487;&#36801;&#31227;&#24615;&#12289;&#36739;&#24046;&#30340;&#35270;&#35273;&#36136;&#37327;&#21644;&#20154;&#31867;&#19981;&#21487;&#23519;&#35273;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30417;&#30563;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#21644;&#21512;&#27861;&#35821;&#20041;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20854;&#20013;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#27969;&#24418;&#65292;&#20854;&#20013;&#21253;&#21547;&#36830;&#32493;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#38750;&#23545;&#25239;&#24615;&#26679;&#26412;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21512;&#27861;&#36807;&#28193;&#12290;&#22312;MNIST&#21644;&#24037;&#19994;&#32570;&#38519;&#25968;&#25454;&#38598;&#19978;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19981;&#20165;&#20855;&#26377;&#26356;&#22909;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25915;&#20987;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks were significantly vulnerable to adversarial examples manipulated by malicious tiny perturbations. Although most conventional adversarial attacks ensured the visual imperceptibility between adversarial examples and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability. In this paper, we proposed a supervised semantic-transformation generative model to generate adversarial examples with real and legitimate semantics, wherein an unrestricted adversarial manifold containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to adversarial ones. Comprehensive experiments on MNIST and industrial defect datasets showed that our adversarial examples not only exhibited better visual quality but also achieved superior attack transferability a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03094</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;CD-FSOD&#65289;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#26816;&#27979;&#26032;&#39046;&#22495;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#24320;&#38598;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;DE-ViT&#65289;&#22312;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#21644;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#21035;&#65292;&#25105;&#20204;&#33258;&#28982;&#20250;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#36825;&#31181;&#24320;&#38598;&#26816;&#27979;&#26041;&#27861;&#33021;&#21542;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;CD-FSOD&#65311;2&#65289;&#22914;&#26524;&#19981;&#33021;&#65292;&#22914;&#20309;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#26102;&#22686;&#24378;&#24320;&#38598;&#26041;&#27861;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#34913;&#37327;&#39046;&#22495;&#24046;&#24322;&#30340;&#25351;&#26631;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#39046;&#22495;&#24230;&#37327;&#20540;&#30340;&#26032;&#30340;CD-FSOD&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;CD-FSOD&#19978;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20598;&#25289;&#26684;&#26391;&#26085;&#23398;&#20064;&#65288;DLL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38181;&#23545;&#20598;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#21270;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38181;&#20248;&#21270;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#22312;&#20248;&#21270;&#38382;&#39064;&#19978;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;0.5%&#12290;</title><link>https://arxiv.org/abs/2402.03086</link><description>&lt;p&gt;
&#23545;&#20598;&#25289;&#26684;&#26391;&#26085;&#23398;&#20064;&#29992;&#20110;&#38181;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Dual Lagrangian Learning for Conic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20598;&#25289;&#26684;&#26391;&#26085;&#23398;&#20064;&#65288;DLL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38181;&#23545;&#20598;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#21270;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38181;&#20248;&#21270;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#22312;&#20248;&#21270;&#38382;&#39064;&#19978;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;0.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20598;&#25289;&#26684;&#26391;&#26085;&#23398;&#20064;&#65288;DLL&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#38181;&#23545;&#20598;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#31034;&#33021;&#21147;&#30340;&#21407;&#29702;&#24615;&#23398;&#20064;&#26041;&#27861;&#12290;DLL&#21033;&#29992;&#38181;&#23545;&#20598;&#25552;&#20379;&#23545;&#20598;&#21487;&#34892;&#35299;&#65292;&#24182;&#22240;&#27492;&#23545;&#21442;&#25968;&#21270;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38181;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#30028;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#24494;&#20998;&#38181;&#25237;&#24433;&#23618;&#65292;&#19968;&#20010;&#31995;&#32479;&#30340;&#23545;&#20598;&#23436;&#25104;&#36807;&#31243;&#20197;&#21450;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;DLL&#30340;&#26377;&#25928;&#24615;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#20248;&#21270;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;DLL&#21487;&#20197;&#22312;&#20248;&#21270;&#24615;&#33021;&#30340;0.5%&#20043;&#20869;&#25552;&#20379;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology that combines conic duality theory with the represen- tation power of ML models. DLL leverages conic duality to provide dual-feasible solutions, and therefore valid Lagrangian dual bounds, for para- metric linear and nonlinear conic optimization problems. The paper introduces differentiable conic projection layers, a systematic dual com- pletion procedure, and a self-supervised learning framework. The effectiveness of DLL is demon- strated on linear and nonlinear parametric opti- mization problems for which DLL provides valid dual bounds within 0.5% of optimality.
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#36848;&#35770;&#25991;&#23545;&#36817;&#26399;&#35270;&#35273;&#25991;&#26412;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#12289;&#22810;&#35282;&#24230;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#22270;&#20687;&#22686;&#24378;&#12289;&#24674;&#22797;&#12289;&#25805;&#20316;&#31561;&#26041;&#38754;&#12290;&#36890;&#36807;&#28145;&#20837;&#35752;&#35770;&#29305;&#23450;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#22914;&#32467;&#26500;&#12289;&#31508;&#21010;&#12289;&#35821;&#20041;&#12289;&#39118;&#26684;&#21644;&#31354;&#38388;&#20869;&#23481;&#31561;&#65292;&#25581;&#31034;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#29420;&#29305;&#25991;&#26412;&#29305;&#24449;&#22312;&#35270;&#35273;&#25991;&#26412;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03082</link><description>&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#36935;&#35265;&#20302;&#23618;&#27425;&#35270;&#35273;&#65306;&#35270;&#35273;&#25991;&#26412;&#22788;&#29702;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03082
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#36848;&#35770;&#25991;&#23545;&#36817;&#26399;&#35270;&#35273;&#25991;&#26412;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#12289;&#22810;&#35282;&#24230;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#22270;&#20687;&#22686;&#24378;&#12289;&#24674;&#22797;&#12289;&#25805;&#20316;&#31561;&#26041;&#38754;&#12290;&#36890;&#36807;&#28145;&#20837;&#35752;&#35770;&#29305;&#23450;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#22914;&#32467;&#26500;&#12289;&#31508;&#21010;&#12289;&#35821;&#20041;&#12289;&#39118;&#26684;&#21644;&#31354;&#38388;&#20869;&#23481;&#31561;&#65292;&#25581;&#31034;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#29420;&#29305;&#25991;&#26412;&#29305;&#24449;&#22312;&#35270;&#35273;&#25991;&#26412;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#26159;&#25991;&#26723;&#21644;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#38500;&#20102;&#35270;&#35273;&#25991;&#26412;&#30340;&#26816;&#27979;&#21644;&#35782;&#21035;&#65292;&#35270;&#35273;&#25991;&#26412;&#22788;&#29702;&#39046;&#22495;&#20063;&#32463;&#21382;&#20102;&#30740;&#31350;&#30340;&#28608;&#22686;&#65292;&#36825;&#24471;&#30410;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25991;&#26412;&#19982;&#26222;&#36890;&#23545;&#35937;&#20855;&#26377;&#19981;&#21516;&#30340;&#29420;&#29305;&#23646;&#24615;&#21644;&#29305;&#24449;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#35266;&#23519;&#21040;&#65292;&#22312;&#35270;&#35273;&#25991;&#26412;&#22788;&#29702;&#20013;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#29420;&#29305;&#30340;&#25991;&#26412;&#29305;&#24449;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#22810;&#26041;&#38754;&#30340;&#23545;&#26368;&#36817;&#36827;&#23637;&#36827;&#34892;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#27425;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20174;&#25991;&#26412;&#22270;&#20687;&#22686;&#24378;&#21644;&#24674;&#22797;&#21040;&#25991;&#26412;&#22270;&#20687;&#25805;&#20316;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#28982;&#21518;&#26159;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#29305;&#23450;&#30340;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#65292;&#22914;&#32467;&#26500;&#12289;&#31508;&#21010;&#12289;&#35821;&#20041;&#12289;&#39118;&#26684;&#21644;&#31354;&#38388;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain. Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models. However, challenges persist due to the unique properties and features that distinguish text from general objects. Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field. Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms. Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial conte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#24182;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#29992;&#25143;&#30340;&#29366;&#24577;&#25277;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.03081</link><description>&lt;p&gt;
&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference-Conditioned Language-Guided Abstraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#24182;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#29992;&#25143;&#30340;&#29366;&#24577;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#26159;&#20154;&#20204;&#25945;&#23548;&#26426;&#22120;&#20154;&#30340;&#24120;&#29992;&#26041;&#24335;&#65292;&#20294;&#23427;&#23481;&#26131;&#20986;&#29616;&#35823;&#23548;&#24615;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#26500;&#24314;&#29366;&#24577;&#25277;&#35937;&#65292;&#21363;&#21253;&#21547;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#20197;&#36827;&#34892;&#26356;&#36890;&#29992;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25277;&#35937;&#20063;&#21462;&#20915;&#20110;&#29992;&#25143;&#22312;&#20219;&#21153;&#20013;&#20851;&#27880;&#30340;&#20559;&#22909;&#65292;&#36825;&#21487;&#33021;&#24456;&#38590;&#29992;&#35821;&#35328;&#25551;&#36848;&#25110;&#20165;&#20165;&#36890;&#36807;&#35821;&#35328;&#35814;&#23613;&#35828;&#26126;&#12290;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25277;&#35937;&#26469;&#25429;&#25417;&#36825;&#20123;&#28508;&#22312;&#20559;&#22909;&#21602;&#65311;&#25105;&#20204;&#35266;&#23519;&#21040;&#20154;&#31867;&#34892;&#20026;&#21453;&#26144;&#20102;&#20182;&#20204;&#30340;&#19990;&#30028;&#35266;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#21578;&#35785;&#25105;&#20204;&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#20559;&#22909;&#23384;&#22312;&#24046;&#24322;&#65292;&#21363;&#20182;&#20204;&#30340;&#29366;&#24577;&#25277;&#35937;&#20063;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30452;&#25509;&#26597;&#35810;&#36825;&#20123;&#20559;&#22909;&#65292;&#32473;&#23450;&#24050;&#21457;&#29983;&#34892;&#20026;&#21464;&#21270;&#30340;&#30693;&#35782;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;LM&#26377;&#20004;&#31181;&#26041;&#24335;&#65306;&#39318;&#20808;&#65292;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#26597;&#35810;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#39034;&#24207;&#20132;&#20114;&#30340;&#24773;&#26223;&#12290;&#35770;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#21457;&#36865;&#32773;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#24635;&#32467;&#35201;&#28857;&#26159;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#21457;&#36865;&#32773;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03077</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#65306;&#20174;&#38646;&#24320;&#22987;&#23398;&#20250;&#35828;&#26381;
&lt;/p&gt;
&lt;p&gt;
Markov Persuasion Processes: Learning to Persuade from Scratch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#39034;&#24207;&#20132;&#20114;&#30340;&#24773;&#26223;&#12290;&#35770;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#21457;&#36865;&#32773;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#24635;&#32467;&#35201;&#28857;&#26159;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#21457;&#36865;&#32773;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#35828;&#26381;&#20013;&#65292;&#19968;&#20010;&#28040;&#24687;&#28789;&#36890;&#30340;&#21457;&#36865;&#32773;&#21487;&#20197;&#31574;&#30053;&#24615;&#22320;&#21521;&#25509;&#25910;&#32773;&#36879;&#38706;&#20449;&#24687;&#65292;&#20197;&#35828;&#26381;&#20182;&#20204;&#37319;&#21462;&#26399;&#26395;&#30340;&#34892;&#21160;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#39034;&#24207;&#20132;&#20114;&#30340;&#24773;&#22659;&#20013;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#65288;MPPs&#65289;&#26469;&#25429;&#25417;&#22312;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#65292;&#21457;&#36865;&#32773;&#38754;&#23545;&#19968;&#31995;&#21015;&#30701;&#35270;&#25509;&#25910;&#32773;&#30340;&#39034;&#24207;&#24773;&#26223;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#30340;MPPs&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#20805;&#20998;&#36816;&#20316;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#20551;&#35774;&#21457;&#36865;&#32773;&#30693;&#36947;&#25509;&#25910;&#32773;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22788;&#29702;&#21457;&#36865;&#32773;&#23545;&#29615;&#22659;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#30340;MPPs&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#36865;&#32773;&#30340;&#37096;&#20998;&#21453;&#39304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#24724;&#24680;&#19982;&#26368;&#20339;&#20449;&#24687;&#25259;&#38706;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#27425;&#32447;&#24615;&#22686;&#38271;&#65292;&#23601;&#20687;&#23398;&#20064;&#36807;&#31243;&#20013;&#32047;&#35745;&#30340;&#35828;&#26381;&#21147;&#25439;&#22833;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions. Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially. Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment. The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards. We fix such issues by addressing MPPs where the sender has no knowledge about the environment. We design a learning algorithm for the sender, working with partial feedback. We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26469;&#25506;&#32034;&#20154;&#31867;&#33719;&#21462;&#22810;&#20010;&#20840;&#26032;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24773;&#22659;&#32447;&#32034;&#22312;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03072</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#25277;&#35937;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Learning to Abstract Visuomotor Mappings using Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26469;&#25506;&#32034;&#20154;&#31867;&#33719;&#21462;&#22810;&#20010;&#20840;&#26032;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24773;&#22659;&#32447;&#32034;&#22312;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#33719;&#24471;&#20840;&#26032;&#25216;&#33021;&#30340;&#22810;&#20010;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#26684;&#23376;&#23548;&#33322;&#33539;&#24335;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19981;&#21516;"&#26684;&#23376;&#19990;&#30028;"&#23454;&#26045;&#30340;&#24773;&#22659;&#32447;&#32034;&#26159;&#21542;&#33021;&#22815;&#20351;&#21442;&#19982;&#32773;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20004;&#20010;&#19981;&#21516;&#30340;&#20851;&#38190;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#25552;&#20379;&#24773;&#22659;&#20449;&#24687;&#26102;&#65292;&#20219;&#21153;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;&#23545;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#26469;&#35828;&#20063;&#26159;&#22914;&#27492;&#65292;&#23427;&#20204;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#26159;&#21542;&#25509;&#25910;&#24773;&#22659;&#20449;&#24687;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#20154;&#31867;&#34920;&#29616;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#32447;&#32034;&#20801;&#35768;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#26102;&#24418;&#25104;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#65292;&#32780;&#23427;&#20204;&#30340;&#32570;&#22833;&#21017;&#26356;&#20542;&#21521;&#20110;&#20849;&#20139;&#19968;&#20010;&#34920;&#31034;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#31574;&#30053;&#37117;&#21487;&#20197;&#23398;&#20064;&#22810;&#20010;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#24773;&#22659;&#32447;&#32034;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigated the human capacity to acquire multiple visuomotor mappings for de novo skills. Using a grid navigation paradigm, we tested whether contextual cues implemented as different "grid worlds", allow participants to learn two distinct key-mappings more efficiently. Our results indicate that when contextual information is provided, task performance is significantly better. The same held true for meta-reinforcement learning agents that differed in whether or not they receive contextual information when performing the task. We evaluated their accuracy in predicting human performance in the task and analyzed their internal representations. The results indicate that contextual cues allow the formation of separate representations in space and time when using different visuomotor mappings, whereas the absence of them favors sharing one representation. While both strategies can allow learning of multiple visuomotor mappings, we showed contextual cues provide a computational advantage 
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03055</link><description>&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#23398;&#20064;&#20197;PAC-Bayes&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03055
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;PAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#32531;&#35299;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#12290;PAC&#36890;&#36807;&#23558;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#26080;&#32541;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28436;&#21592;&#35757;&#32451;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;PAC&#31639;&#27861;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;Probably Approximately Correct-Bayesian&#65288;PAC-Bayes&#65289;&#20998;&#26512;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#25512;&#26029;&#35780;&#35770;&#23478;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#23545;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#30340;&#34701;&#20837;&#20351;PAC&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#35843;&#25972;&#20854;&#25506;&#32034;&#31574;&#30053;&#65292;&#25351;&#23548;&#28436;&#21592;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#20013;&#30340;&#22266;&#23450;&#25110;&#39044;&#23450;&#30340;&#25506;&#32034;&#26041;&#26696;&#30456;&#27604;&#65292;PAC&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;PAC-Bayes&#20998;&#26512;&#24341;&#23548;&#30340;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26159;&#21521;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26356;&#20855;&#33258;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#31574;&#30053;&#36808;&#20986;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#39532;&#26469;&#35199;&#20122;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;Llama2&#21644;Mistral&#27169;&#22411;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;RAG&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.03053</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#39532;&#26469;&#35199;&#20122;&#23884;&#20837;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#39532;&#26469;&#35199;&#20122;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;Llama2&#21644;Mistral&#27169;&#22411;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;RAG&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;Llama2&#21644;Mistral&#20004;&#31181;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#24182;&#22312;&#28041;&#21450;&#36127;&#27491;&#32452;&#23545;&#30340;&#23884;&#20837;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#19987;&#38376;&#38024;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#26816;&#32034;&#36741;&#21161;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#27169;&#22411;&#12290;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;6&#20159;&#21442;&#25968;Llama2&#27169;&#22411;&#22312;b.cari.com.my&#12289;c.cari.com.my&#12289;&#39532;&#26469;&#35199;&#20122;&#26032;&#38395;&#21644;&#39532;&#26469;&#35199;&#20122;Twitter&#27979;&#35797;&#38598;&#30340;&#25152;&#26377;recall@k&#25351;&#26631;&#19978;&#37117;&#20248;&#20110;OpenAI&#30340;text-embedding-ada-002&#12290;&#22312;RAG&#27169;&#22411;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39532;&#26469;&#35199;&#20122;&#29615;&#22659;&#20013;&#19982;OpenAI&#30340;text-embedding-ada-002&#30456;&#31454;&#20105;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;20&#20159;&#21442;&#25968;Llama2&#27169;&#22411;&#22312;&#8220;Melayu&#8221;&#20851;&#38190;&#35789;&#30740;&#31350;&#35770;&#25991;&#25968;&#25454;&#38598;&#30340;Recall@5&#12289;Recall@10&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#34920;&#29616;&#65292;&#24182;&#22312;lom.agc.gov.my&#25968;&#25454;&#38598;&#30340;Recall@3&#12289;Recall@5&#21644;Recall@10&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#24494;&#35843;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;RAG&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All 
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#20999;&#25442;&#36890;&#20449;&#25299;&#25169;&#19979;&#37096;&#20998;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;Euler-Lagrange&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#36319;&#36394;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#21327;&#20316;&#31639;&#27861;&#26694;&#26550;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#25429;&#25417;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31283;&#23450;&#24615;&#20998;&#26512;&#30830;&#20445;&#20102;&#26377;&#30028;&#30340;&#36319;&#36394;&#35823;&#24046;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03048</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#21327;&#20316;&#23398;&#20064;&#26469;&#22788;&#29702;&#20999;&#25442;&#25299;&#25169;&#19979;&#30340;Euler-Lagrange&#31995;&#32479;&#36319;&#36394;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#20999;&#25442;&#36890;&#20449;&#25299;&#25169;&#19979;&#37096;&#20998;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;Euler-Lagrange&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#36319;&#36394;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#21327;&#20316;&#31639;&#27861;&#26694;&#26550;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#25429;&#25417;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31283;&#23450;&#24615;&#20998;&#26512;&#30830;&#20445;&#20102;&#26377;&#30028;&#30340;&#36319;&#36394;&#35823;&#24046;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#20999;&#25442;&#36890;&#20449;&#25299;&#25169;&#19979;&#37096;&#20998;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;Euler-Lagrange&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#36319;&#36394;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#30456;&#20851;&#24615;&#24863;&#30693;&#21327;&#20316;&#31639;&#27861;&#26694;&#26550;&#65292;&#33021;&#22815;&#28789;&#27963;&#25429;&#25417;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#20854;&#26174;&#33879;&#29305;&#28857;&#26159;&#36890;&#36807;&#35268;&#36991;&#35745;&#31639;&#23494;&#38598;&#30340;&#21518;&#39564;&#26041;&#24046;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#32858;&#21512;&#26435;&#37325;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#20998;&#24067;&#24335;&#25511;&#21046;&#31639;&#27861;&#30830;&#20445;&#20102;&#26377;&#30028;&#30340;&#36319;&#36394;&#35823;&#24046;&#27010;&#29575;&#12290;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#21327;&#35758;&#22312;&#26377;&#25928;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#21151;&#25928;&#65292;&#20351;&#20854;&#25104;&#20026;&#35299;&#20915;&#20855;&#26377;&#19981;&#30830;&#23450;&#21160;&#21147;&#23398;&#21644;&#21160;&#24577;&#36890;&#20449;&#32467;&#26500;&#29305;&#24449;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#40065;&#26834;&#36319;&#36394;&#25511;&#21046;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an innovative learning-based approach to tackle the tracking control problem of Euler-Lagrange multi-agent systems with partially unknown dynamics operating under switching communication topologies. The approach leverages a correlation-aware cooperative algorithm framework built upon Gaussian process regression, which adeptly captures inter-agent correlations for uncertainty predictions. A standout feature is its exceptional efficiency in deriving the aggregation weights achieved by circumventing the computationally intensive posterior variance calculations. Through Lyapunov stability analysis, the distributed control law ensures bounded tracking errors with high probability. Simulation experiments validate the protocol's efficacy in effectively managing complex scenarios, establishing it as a promising solution for robust tracking control in multi-agent systems characterized by uncertain dynamics and dynamic communication structures.
&lt;/p&gt;</description></item><item><title>PFDM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20813;&#35299;&#26512;&#22120;&#34394;&#25311;&#35797;&#31359;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#30446;&#26631;&#20154;&#29289;&#36523;&#19978;&#8220;&#31359;&#8221;&#19978;&#26381;&#35013;&#65292;&#26080;&#38656;&#20934;&#30830;&#30340;&#20998;&#21106;&#25513;&#30721;&#65292;&#24182;&#36890;&#36807;&#26381;&#35013;&#34701;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#36798;&#21040;&#39640;&#20445;&#30495;&#24230;&#30340;&#35797;&#31359;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03047</link><description>&lt;p&gt;
PFDM: &#36879;&#36807;&#25193;&#25955;&#27169;&#22411;&#30340;&#20813;&#35299;&#26512;&#22120;&#34394;&#25311;&#35797;&#31359;
&lt;/p&gt;
&lt;p&gt;
PFDM: Parser-Free Virtual Try-on via Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03047
&lt;/p&gt;
&lt;p&gt;
PFDM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20813;&#35299;&#26512;&#22120;&#34394;&#25311;&#35797;&#31359;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#30446;&#26631;&#20154;&#29289;&#36523;&#19978;&#8220;&#31359;&#8221;&#19978;&#26381;&#35013;&#65292;&#26080;&#38656;&#20934;&#30830;&#30340;&#20998;&#21106;&#25513;&#30721;&#65292;&#24182;&#36890;&#36807;&#26381;&#35013;&#34701;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#36798;&#21040;&#39640;&#20445;&#30495;&#24230;&#30340;&#35797;&#31359;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#35797;&#31359;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#32447;&#21644;&#23454;&#20307;&#36141;&#29289;&#20013;&#30340;&#26381;&#35013;&#36141;&#29289;&#20307;&#39564;&#65292;&#24341;&#36215;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#30340;&#35797;&#31359;&#25928;&#26524;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#20998;&#21106;&#25513;&#30721;&#65292;&#36825;&#20123;&#25513;&#30721;&#36890;&#24120;&#26159;&#30001;&#20934;&#30830;&#26080;&#35823;&#30340;&#35299;&#26512;&#22120;&#25110;&#25163;&#21160;&#26631;&#27880;&#29983;&#25104;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;PFDM&#65289;&#30340;&#20813;&#35299;&#26512;&#22120;&#34394;&#25311;&#35797;&#31359;&#26041;&#27861;&#12290;&#32473;&#23450;&#20004;&#20010;&#22270;&#20687;&#65292;PFDM&#21487;&#20197;&#36890;&#36807;&#38544;&#24335;&#21464;&#24418;&#26080;&#32541;&#22320;&#23558;&#26381;&#35013;&#8220;&#31359;&#8221;&#22312;&#30446;&#26631;&#20154;&#29289;&#36523;&#19978;&#65292;&#32780;&#26080;&#38656;&#20854;&#20182;&#20449;&#24687;&#12290;&#20026;&#20102;&#26377;&#25928;&#23398;&#20064;&#35813;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#35768;&#22810;&#20266;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#22312;&#20154;&#29289;&#19978;&#31359;&#19978;&#21508;&#31181;&#26381;&#35013;&#26500;&#24314;&#26679;&#21697;&#23545;&#12290;&#22312;&#22823;&#35268;&#27169;&#25193;&#23637;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#26381;&#35013;&#34701;&#21512;&#27880;&#24847;&#21147;&#65288;GFA&#65289;&#26426;&#21046;&#34701;&#21512;&#20154;&#29289;&#21644;&#26381;&#35013;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PFDM&#33021;&#25104;&#21151;&#22788;&#29702;&#22797;&#26434;&#24773;&#20917;&#12289;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can "wear" garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and out
&lt;/p&gt;</description></item><item><title>Open RL Benchmark&#26159;&#19968;&#32452;&#36319;&#36394;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#30340;&#25968;&#25454;&#65292;&#24182;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;RL&#24211;&#21644;&#21442;&#32771;&#23454;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#34913;&#37327;RL&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20063;&#21487;&#20379;&#31038;&#21306;&#36827;&#34892;&#19979;&#36733;&#12289;&#20351;&#29992;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.03046</link><description>&lt;p&gt;
&#24320;&#25918;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65306;&#20840;&#38754;&#36319;&#36394;&#30340;&#23454;&#39564;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03046
&lt;/p&gt;
&lt;p&gt;
Open RL Benchmark&#26159;&#19968;&#32452;&#36319;&#36394;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#30340;&#25968;&#25454;&#65292;&#24182;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;RL&#24211;&#21644;&#21442;&#32771;&#23454;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#34913;&#37327;RL&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20063;&#21487;&#20379;&#31038;&#21306;&#36827;&#34892;&#19979;&#36733;&#12289;&#20351;&#29992;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35770;&#25991;&#20013;&#65292;&#23398;&#20064;&#26354;&#32447;&#26159;&#34913;&#37327;RL&#31639;&#27861;&#26377;&#25928;&#24615;&#30340;&#26377;&#29992;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26354;&#32447;&#30340;&#23436;&#25972;&#21407;&#22987;&#25968;&#25454;&#24456;&#23569;&#20844;&#24320;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#29616;&#23454;&#39564;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#32452;&#23436;&#20840;&#36319;&#36394;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#65292;&#21253;&#25324;&#36890;&#24120;&#30340;&#25968;&#25454;&#22914;&#19968;&#27425;&#24615;&#22238;&#25253;&#65292;&#20197;&#21450;&#25152;&#26377;&#29305;&#23450;&#31639;&#27861;&#21644;&#31995;&#32479;&#25351;&#26631;&#12290;&#24320;&#25918;&#24335;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#26159;&#31038;&#21306;&#39537;&#21160;&#30340;&#65306;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#19979;&#36733;&#12289;&#20351;&#29992;&#21644;&#36129;&#29486;&#25968;&#25454;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24050;&#32463;&#36319;&#36394;&#20102;&#36229;&#36807;25000&#27425;&#36816;&#34892;&#65292;&#32047;&#35745;&#26102;&#38388;&#36229;&#36807;8&#24180;&#12290;&#24320;&#25918;&#24335;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#24211;&#21644;&#21442;&#32771;&#23454;&#29616;&#12290;&#25105;&#20204;&#29305;&#21035;&#27880;&#37325;&#30830;&#20445;&#27599;&#20010;&#23454;&#39564;&#30340;&#31934;&#30830;&#21487;&#20877;&#29616;&#24615;&#65292;&#19981;&#20165;&#25552;&#20379;&#23436;&#25972;&#30340;&#21442;&#25968;&#65292;&#36824;&#25552;&#20379;&#29992;&#20110;&#29983;&#25104;&#23454;&#39564;&#30340;&#20381;&#36182;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. Open RL Benchmark covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, O
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#22312;&#25991;&#26412;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#30456;&#20284;&#24230;&#24046;&#24322;&#21644;&#29420;&#29305;&#24615;&#65288;SIDU&#65289;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26412;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#39044;&#27979;&#20851;&#38190;&#30340;&#26377;&#19978;&#19979;&#25991;&#24847;&#20041;&#30340;&#25991;&#26412;&#20803;&#32032;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#19977;&#23618;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;XAI&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03043</link><description>&lt;p&gt;
SIDU-TXT: &#19968;&#31181;&#20855;&#26377;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#30340;NLP&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#22312;&#25991;&#26412;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#30456;&#20284;&#24230;&#24046;&#24322;&#21644;&#29420;&#29305;&#24615;&#65288;SIDU&#65289;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26412;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#39044;&#27979;&#20851;&#38190;&#30340;&#26377;&#19978;&#19979;&#25991;&#24847;&#20041;&#30340;&#25991;&#26412;&#20803;&#32032;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#19977;&#23618;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;XAI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26377;&#21161;&#20110;&#35299;&#35835;&#8220;&#40657;&#30418;&#8221;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#20027;&#35201;&#29992;&#20110;&#22270;&#20687;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#20294;&#35299;&#37322;&#24615;&#22312;&#25991;&#26412;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;XAI&#26041;&#27861;&#22312;&#25991;&#26412;&#39046;&#22495;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#34987;&#35748;&#20026;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#20013;&#23450;&#20301;&#25972;&#20010;&#26174;&#33879;&#21306;&#22495;&#33021;&#21147;&#20248;&#36234;&#30340;XAI&#26041;&#27861;&#8212;&#8212;&#30456;&#20284;&#24230;&#24046;&#24322;&#21644;&#29420;&#29305;&#24615;&#65288;SIDU&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#25968;&#25454;&#12290;&#25193;&#23637;&#26041;&#27861;SIDU-TXT&#21033;&#29992;&#26469;&#33258;&#8220;&#40657;&#30418;&#8221;&#27169;&#22411;&#30340;&#29305;&#24449;&#28608;&#27963;&#22270;&#29983;&#25104;&#28909;&#21147;&#22270;&#65292;&#20197;&#35789;&#20026;&#21333;&#20301;&#25552;&#20379;&#35299;&#37322;&#65292;&#31361;&#20986;&#26174;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#26377;&#19978;&#19979;&#25991;&#24847;&#20041;&#30340;&#25991;&#26412;&#20803;&#32032;&#12290;&#37492;&#20110;&#36824;&#27809;&#26377;&#32479;&#19968;&#30340;XAI&#35780;&#20272;&#26631;&#20934;&#65292;&#26412;&#30740;&#31350;&#24212;&#29992;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#19977;&#23618;&#35780;&#20272;&#26694;&#26550;&#65306;&#21151;&#33021;&#22522;&#30784;&#12289;&#20154;&#31867;&#22522;&#30784;&#21644;&#24212;&#29992;&#22522;&#30784;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aids in deciphering 'black-box' models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded,
&lt;/p&gt;</description></item><item><title>&#8220;InteractiveVideo&#8221;&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#31934;&#30830;&#12289;&#26377;&#25928;&#22320;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#35270;&#39057;&#30340;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#28789;&#27963;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.03040</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#35270;&#39057;&#65306;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#21487;&#25511;&#35270;&#39057;&#29983;&#25104;&#19982;&#22810;&#27169;&#24577;&#25351;&#20196;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03040
&lt;/p&gt;
&lt;p&gt;
&#8220;InteractiveVideo&#8221;&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#31934;&#30830;&#12289;&#26377;&#25928;&#22320;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#35270;&#39057;&#30340;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#28789;&#27963;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20132;&#20114;&#24335;&#35270;&#39057;&#8221;&#30340;&#29992;&#25143;&#20013;&#24515;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#22270;&#20687;&#25110;&#25991;&#26412;&#30340;&#29983;&#25104;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35774;&#35745;&#29992;&#20110;&#21160;&#24577;&#20132;&#20114;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#21508;&#31181;&#30452;&#35266;&#30340;&#26426;&#21046;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#25552;&#31034;&#12289;&#32472;&#30011;&#12289;&#25302;&#25918;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#22810;&#27169;&#24577;&#25351;&#20196;&#26426;&#21046;&#65292;&#26088;&#22312;&#23558;&#29992;&#25143;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#26080;&#32541;&#38598;&#25104;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#36755;&#20837;&#21644;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#21512;&#20316;&#21644;&#21709;&#24212;&#24335;&#20132;&#20114;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#31934;&#30830;&#32780;&#26377;&#25928;&#30340;&#29992;&#25143;&#25351;&#20196;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#32467;&#26524;&#30340;&#36845;&#20195;&#21644;&#31934;&#32454;&#21270;&#35843;&#25972;&#12290;&#36890;&#36807;&#8220;&#20132;&#20114;&#24335;&#35270;&#39057;&#8221;&#65292;&#29992;&#25143;&#21487;&#20197;&#28789;&#27963;&#22320;&#31934;&#24515;&#35843;&#25972;&#35270;&#39057;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21253;&#25324;&#32472;&#30011;&#21442;&#32771;&#22270;&#20687;&#12289;&#32534;&#36753;&#35821;&#20041;&#21644;&#35843;&#25972;&#35270;&#39057;&#21160;&#20316;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03038</link><description>&lt;p&gt;
&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Combination of Sample Selection Strategies for Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#22914;&#20803;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#38480;&#26679;&#26412;&#25968;&#37327;&#23545;&#25972;&#20307;&#25104;&#21151;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#21313;&#20998;&#26126;&#30830;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#21482;&#34987;&#22312;&#20856;&#22411;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;8&#20010;&#22270;&#20687;&#21644;6&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;5&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#24443;&#24213;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;&#20010;&#20307;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#20114;&#34917;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20010;&#20307;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#21450;&#26368;&#36817;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25903;&#25345;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#32467;&#26500;&#21551;&#21457;&#30340;&#20989;&#25968;SDE&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#23454;&#29616;&#23545;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#36817;&#20284;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20943;&#36731;&#25351;&#25968;&#32423;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03028</link><description>&lt;p&gt;
&#21463;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#32467;&#26500;&#21551;&#21457;&#30340;&#20989;&#25968;SDE&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Functional SDE approximation inspired by a deep operator network architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#32467;&#26500;&#21551;&#21457;&#30340;&#20989;&#25968;SDE&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#23454;&#29616;&#23545;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#36817;&#20284;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20943;&#36731;&#25351;&#25968;&#32423;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#35299;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#32467;&#26500;&#28789;&#24863;&#26469;&#33258;&#20110;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;DeepONets&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#22522;&#20110;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#31639;&#23376;&#23398;&#20064;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#20013;&#34920;&#31034;&#30340;&#38477;&#32500;&#22522;&#30784;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#38543;&#26426;&#36807;&#31243;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#65288;PCE&#65289;&#65292;&#24182;&#23558;&#30456;&#24212;&#30340;&#26550;&#26500;&#31216;&#20026;SDEONet&#12290;&#22312;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#39046;&#22495;&#20013;&#65292;PCE&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;SDE&#20013;&#24182;&#38750;&#22914;&#27492;&#65292;&#20256;&#32479;&#30340;&#37319;&#26679;&#26041;&#27861;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;&#21151;&#33021;&#24615;&#26041;&#27861;&#24456;&#23569;&#35265;&#12290;&#25130;&#26029;&#30340;PCE&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#38543;&#30528;&#26368;&#22823;&#22810;&#39033;&#24335;&#38454;&#25968;&#21644;&#22522;&#20989;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20998;&#37327;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25152;&#25552;&#20986;&#30340;SDEONet&#32467;&#26500;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26469;&#20943;&#36731;&#25351;&#25968;&#32423;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel approach to approximate solutions of Stochastic Differential Equations (SDEs) by Deep Neural Networks is derived and analysed. The architecture is inspired by the notion of Deep Operator Networks (DeepONets), which is based on operator learning in function spaces in terms of a reduced basis also represented in the network. In our setting, we make use of a polynomial chaos expansion (PCE) of stochastic processes and call the corresponding architecture SDEONet. The PCE has been used extensively in the area of uncertainty quantification (UQ) with parametric partial differential equations. This however is not the case with SDE, where classical sampling methods dominate and functional approaches are seen rarely. A main challenge with truncated PCEs occurs due to the drastic growth of the number of components with respect to the maximum polynomial degree and the number of basis elements. The proposed SDEONet architecture aims to alleviate the issue of exponential complexity by learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20256;&#25773;&#35270;&#35282;&#20998;&#26512;&#20102;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03025</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#29702;&#35299;&#21644;&#24341;&#23548;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20256;&#25773;&#35270;&#35282;&#20998;&#26512;&#20102;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#26159;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#35782;&#21035;&#31561;&#20215;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22522;&#20110;&#32858;&#21512;&#30340;&#24369;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#25773;&#35270;&#35282;&#26469;&#20998;&#26512;&#24369;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#65292;&#24182;&#35299;&#37322;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23454;&#36136;&#19978;&#26159;&#23547;&#25214;&#29992;&#20110;&#23545;&#23454;&#20307;&#30456;&#20284;&#24230;&#36827;&#34892;&#20256;&#25773;&#30340;&#25805;&#20316;&#31526;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#23613;&#31649;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#23384;&#22312;&#32467;&#26500;&#24322;&#36136;&#24615;&#65292;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#65292;&#36825;&#26159;&#23454;&#20307;&#23545;&#40784;&#30340;&#26680;&#24515;&#21069;&#25552;&#65292;&#20294;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#36328;&#30693;&#35782;&#22270;&#35889;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;PipEA&#65292;&#23454;&#29616;&#20102;&#25928;&#26524;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03021</link><description>&lt;p&gt;
&#25968;&#25454;&#35825;&#23548;&#30340;&#22810;&#23610;&#24230;&#25439;&#22833;&#21644;&#39640;&#25928;&#22810;&#36895;&#29575;&#26799;&#24230;&#19979;&#38477;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Data-induced multiscale losses and efficient multirate gradient descent schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#23610;&#24230;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#12290;&#22914;&#26524;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20855;&#26377;&#23610;&#24230;&#30340;&#26174;&#33879;&#21464;&#21270;&#65292;&#21017;&#20854;&#34987;&#31216;&#20026;&#22810;&#23610;&#24230;&#25968;&#25454;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#25439;&#22833;&#26223;&#35266;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65292;&#21253;&#25324;&#20854;&#26799;&#24230;&#21644;&#26469;&#33258;&#25968;&#25454;&#30340;&#28023;&#26862;&#30697;&#38453;&#12290;&#30456;&#24212;&#22320;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#21463;&#31185;&#23398;&#35745;&#31639;&#20013;&#20351;&#29992;&#30340;&#22810;&#23610;&#24230;&#31639;&#27861;&#30340;&#21551;&#21457;&#12290;&#36825;&#31181;&#26041;&#27861;&#35797;&#22270;&#36229;&#36234;&#32463;&#39564;&#24615;&#23398;&#20064;&#29575;&#36873;&#25321;&#65292;&#25552;&#20379;&#19968;&#31181;&#26356;&#31995;&#32479;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#21518;&#26399;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning. A dataset is multiscale if its distribution shows large variations in scale across different directions. This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data. Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing. This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages.
&lt;/p&gt;</description></item><item><title>Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03019</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#30340;Taylor&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Taylor Videos for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03019
&lt;/p&gt;
&lt;p&gt;
Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#21160;&#20316;&#26159;&#21160;&#20316;&#35782;&#21035;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21160;&#20316;(i)&#27809;&#26377;&#26126;&#30830;&#30340;&#24418;&#24335;&#65292;(ii)&#25317;&#26377;&#35832;&#22914;&#20301;&#31227;&#12289;&#36895;&#24230;&#21644;&#21152;&#36895;&#24230;&#31561;&#21508;&#31181;&#27010;&#24565;&#65292;(iii)&#36890;&#24120;&#20250;&#21463;&#21040;&#19981;&#31283;&#23450;&#20687;&#32032;&#24341;&#36215;&#30340;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Taylor&#35270;&#39057;&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#23427;&#31361;&#20986;&#26174;&#31034;&#20102;&#27599;&#20010;&#24103;&#20013;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#25381;&#25163;&#65289;&#34987;&#31216;&#20026;Taylor&#24103;&#12290;Taylor&#35270;&#39057;&#30340;&#21629;&#21517;&#26469;&#28304;&#20110;Taylor&#32423;&#25968;&#65292;&#23427;&#20351;&#29992;&#37325;&#35201;&#30340;&#39033;&#26469;&#36817;&#20284;&#32473;&#23450;&#28857;&#19978;&#30340;&#20989;&#25968;&#12290;&#22312;&#35270;&#39057;&#30340;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#26088;&#22312;&#20174;&#35270;&#39057;&#26102;&#38388;&#22359;&#20013;&#25552;&#21462;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#22359;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24103;&#12289;&#24046;&#20998;&#24103;&#21644;&#39640;&#38454;&#24046;&#20998;&#24103;&#36827;&#34892;Taylor&#23637;&#24320;&#65292;&#20197;&#36817;&#20284;&#35745;&#31639;&#36215;&#22987;&#24103;&#19978;&#30340;&#36825;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Taylor&#32423;&#25968;&#20013;&#39640;&#38454;&#39033;&#30340;&#27714;&#21644;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03017</link><description>&lt;p&gt;
&#21521;&#32511;&#33394;&#19988;&#31867;&#20154;&#30340;&#20154;&#24037;&#26234;&#33021;&#36808;&#36827;&#65306;&#24403;&#20195;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#21644;&#35745;&#31639;&#30340;&#26114;&#36149;&#24615;&#20351;&#20854;&#22312;&#35768;&#22810;&#25968;&#25454;&#21463;&#38480;&#30340;&#30495;&#23454;&#24212;&#29992;&#20013;&#19981;&#23454;&#29992;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#23545;&#26032;&#23398;&#20064;&#20219;&#21153;&#30340;&#24555;&#36895;&#36866;&#24212;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#27491;&#24335;&#23450;&#20041;&#20102;FSL&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#19982;&#19981;&#21516;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#25193;&#23637;&#20102;&#20197;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#32463;&#20856;&#21644;&#26032;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#22609;&#36896;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#12289;&#31361;&#20986;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#20030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#37051;&#23621;&#30340;&#21487;&#20449;&#24230;&#26377;&#36873;&#25321;&#24615;&#22320;&#35831;&#27714;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20010;&#20307;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#26041;&#24046;&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03014</link><description>&lt;p&gt;
&#20449;&#20219;&#35841;&#65311;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#36873;&#20030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Whom to Trust? Elective Learning for Distributed Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#20030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#37051;&#23621;&#30340;&#21487;&#20449;&#24230;&#26377;&#36873;&#25321;&#24615;&#22320;&#35831;&#27714;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20010;&#20307;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#26041;&#24046;&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#26469;&#22686;&#24378;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#20030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;&#20808;&#39564;&#30340;&#36873;&#20030;&#24335;&#20998;&#24067;&#24335;GP&#65288;Pri-GP&#65289;&#65292;&#23427;&#36171;&#20104;&#20102;&#26234;&#33021;&#20307;&#22522;&#20110;&#20449;&#20219;&#24230;&#26377;&#36873;&#25321;&#22320;&#21521;&#37051;&#23621;&#26234;&#33021;&#20307;&#35831;&#27714;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;Pri-GP&#26377;&#25928;&#25552;&#39640;&#20102;&#20010;&#20307;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#20307;&#20808;&#39564;&#30693;&#35782;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#23427;&#28040;&#38500;&#20102;&#22312;&#20998;&#24067;&#24335;GP&#20013;&#30830;&#23450;&#32858;&#21512;&#26435;&#37325;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#26041;&#24046;&#35745;&#31639;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Pri-GP&#26694;&#26550;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#39044;&#27979;&#35823;&#24046;&#36793;&#30028;&#65292;&#30830;&#20445;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.
&lt;/p&gt;</description></item><item><title>&#36755;&#20986;&#25200;&#21160;&#23545;&#20108;&#20803;&#32447;&#24615;&#20998;&#31867;&#20013;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#22312;&#20010;&#20307;&#20844;&#24179;&#24615;&#26041;&#38754;&#26159;&#26377;&#30028;&#30340;&#20294;&#19982;&#27169;&#22411;&#32500;&#24230;&#25104;&#27491;&#27604;&#65292;&#22312;&#32676;&#20307;&#20844;&#24179;&#24615;&#26041;&#38754;&#21017;&#30001;&#35282;&#36793;&#36317;&#30340;&#20998;&#24067;&#20915;&#23450;&#12290;</title><link>https://arxiv.org/abs/2402.03011</link><description>&lt;p&gt;
&#20851;&#20110;&#36755;&#20986;&#25200;&#21160;&#23545;&#20108;&#20803;&#32447;&#24615;&#20998;&#31867;&#20013;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Output Perturbation on Fairness in Binary Linear Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03011
&lt;/p&gt;
&lt;p&gt;
&#36755;&#20986;&#25200;&#21160;&#23545;&#20108;&#20803;&#32447;&#24615;&#20998;&#31867;&#20013;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#22312;&#20010;&#20307;&#20844;&#24179;&#24615;&#26041;&#38754;&#26159;&#26377;&#30028;&#30340;&#20294;&#19982;&#27169;&#22411;&#32500;&#24230;&#25104;&#27491;&#27604;&#65292;&#22312;&#32676;&#20307;&#20844;&#24179;&#24615;&#26041;&#38754;&#21017;&#30001;&#35282;&#36793;&#36317;&#30340;&#20998;&#24067;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#22914;&#20309;&#19982;&#20108;&#20803;&#32447;&#24615;&#20998;&#31867;&#20013;&#30340;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#30456;&#20114;&#20316;&#29992;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#36755;&#20986;&#25200;&#21160;&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#32463;&#20856;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#25200;&#21160;&#27169;&#22411;&#30456;&#23545;&#20110;&#21407;&#22987;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#30340;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#27700;&#24179;&#30340;&#39640;&#27010;&#29575;&#36793;&#30028;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36755;&#20986;&#25200;&#21160;&#23545;&#20844;&#24179;&#24615;&#27700;&#24179;&#30340;&#24433;&#21709;&#26159;&#26377;&#30028;&#30340;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#30340;&#32500;&#24230;&#22686;&#38271;&#32780;&#22686;&#22823;&#12290;&#23545;&#20110;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24433;&#21709;&#30001;&#25152;&#35859;&#30340;&#35282;&#36793;&#36317;&#65288;&#21363;&#38750;&#31169;&#26377;&#27169;&#22411;&#30340;&#26377;&#31526;&#21495;&#36793;&#36317;&#20056;&#20197;&#27599;&#20010;&#31034;&#20363;&#30340;&#33539;&#25968;&#65289;&#30340;&#20998;&#24067;&#26469;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We theoretically study how differential privacy interacts with both individual and group fairness in binary linear classification. More precisely, we focus on the output perturbation mechanism, a classic approach in privacy-preserving machine learning. We derive high-probability bounds on the level of individual and group fairness that the perturbed models can achieve compared to the original model. Hence, for individual fairness, we prove that the impact of output perturbation on the level of fairness is bounded but grows with the dimension of the model. For group fairness, we show that this impact is determined by the distribution of so-called angular margins, that is signed margins of the non-private model re-scaled by the norm of each example.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03008</link><description>&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusive Gibbs Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03008
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#28151;&#21512;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#65288;DiGS&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#37319;&#26679;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;DiGS&#38598;&#25104;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21033;&#29992;&#39640;&#26031;&#21367;&#31215;&#21019;&#24314;&#19968;&#20010;&#36741;&#21161;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#36830;&#25509;&#23396;&#31435;&#30340;&#27169;&#24577;&#65292;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#20174;&#20004;&#20010;&#31354;&#38388;&#20013;&#20132;&#26367;&#25277;&#21462;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37319;&#26679;&#22810;&#27169;&#24577;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#24182;&#34892;&#28201;&#24230;&#27861;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21463;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#24433;&#21709;&#30340;&#26114;&#36149;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#65292;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#25512;&#24191;&#21040;&#21253;&#21547;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#21442;&#25968;&#30340;&#31995;&#32479;&#20248;&#21270;&#20013;&#65292;&#36890;&#36807;&#22312;&#25152;&#26377;&#21464;&#37327;&#19978;&#25311;&#21512;&#20840;&#23616;&#20195;&#29702;&#27169;&#22411;&#65292;&#20294;&#21482;&#22312;&#23545;&#19981;&#21487;&#25511;&#21464;&#37327;&#30340;&#27979;&#37327;&#26465;&#20214;&#19979;&#20248;&#21270;&#21487;&#25511;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.03006</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#21463;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#24433;&#21709;&#30340;&#26114;&#36149;&#23454;&#39564;&#21644;&#27169;&#25311;&#30340;&#23454;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21463;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#24433;&#21709;&#30340;&#26114;&#36149;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#65292;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#25512;&#24191;&#21040;&#21253;&#21547;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#21442;&#25968;&#30340;&#31995;&#32479;&#20248;&#21270;&#20013;&#65292;&#36890;&#36807;&#22312;&#25152;&#26377;&#21464;&#37327;&#19978;&#25311;&#21512;&#20840;&#23616;&#20195;&#29702;&#27169;&#22411;&#65292;&#20294;&#21482;&#22312;&#23545;&#19981;&#21487;&#25511;&#21464;&#37327;&#30340;&#27979;&#37327;&#26465;&#20214;&#19979;&#20248;&#21270;&#21487;&#25511;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#31243;&#23454;&#39564;&#36890;&#24120;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#65292;&#21487;&#20197;&#23558;&#21442;&#25968;&#35774;&#32622;&#20026;&#20219;&#20309;&#25152;&#38656;&#20540;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#36890;&#24120;&#20551;&#35774;&#30456;&#21516;&#26465;&#20214;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#35768;&#22810;&#23454;&#39564;&#21463;&#19981;&#21487;&#25511;&#21046;&#30340;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#28201;&#24230;&#12289;&#28287;&#24230;&#21644;&#39118;&#36895;&#65289;&#30340;&#24433;&#21709;&#12290;&#22312;&#20248;&#21270;&#36825;&#20123;&#23454;&#39564;&#26102;&#65292;&#24212;&#35813;&#37325;&#28857;&#20851;&#27880;&#22312;&#32473;&#23450;&#19981;&#21487;&#25511;&#21464;&#37327;&#26465;&#20214;&#19979;&#25214;&#21040;&#26368;&#20248;&#20540;&#12290;&#26412;&#25991;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#25512;&#24191;&#21040;&#22312;&#21253;&#21547;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#21442;&#25968;&#30340;&#21464;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#31995;&#32479;&#20248;&#21270;&#12290;&#35813;&#25512;&#24191;&#36890;&#36807;&#22312;&#25152;&#26377;&#21487;&#25511;&#21644;&#29615;&#22659;&#21464;&#37327;&#19978;&#25311;&#21512;&#20840;&#23616;&#20195;&#29702;&#27169;&#22411;&#65292;&#20294;&#21482;&#22312;&#23545;&#19981;&#21487;&#25511;&#21464;&#37327;&#30340;&#27979;&#37327;&#26465;&#20214;&#19979;&#20248;&#21270;&#21487;&#25511;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#21512;&#25104;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#30740;&#31350;&#20102;&#22122;&#22768;&#27700;&#24179;&#12289;&#29615;&#22659;&#21442;&#25968;&#25968;&#37327;&#21644;&#21442;&#25968;&#27874;&#21160;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experiments in engineering are typically conducted in controlled environments where parameters can be set to any desired value. This assumes that the same applies in a real-world setting -- an assumption that is often incorrect as many experiments are influenced by uncontrollable environmental conditions such as temperature, humidity and wind speed. When optimising such experiments, the focus should lie on finding optimal values conditionally on these uncontrollable variables. This article extends Bayesian optimisation to the optimisation of systems in changing environments that include controllable and uncontrollable parameters. The extension fits a global surrogate model over all controllable and environmental variables but optimises only the controllable parameters conditional on measurements of the uncontrollable variables. The method is validated on two synthetic test functions and the effects of the noise level, the number of the environmental parameters, the parameter fluctuatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#21644;&#36741;&#21161;&#26799;&#24230;&#22312;&#35757;&#32451;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#27491;&#20132;&#25237;&#24433;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;EMA&#65288;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65289;&#21487;&#20197;&#25913;&#36827;&#26799;&#24230;&#25163;&#26415;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#31649;&#36947;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02998</link><description>&lt;p&gt;
&#23567;&#24515;&#20351;&#29992;&#25163;&#26415;&#20992;&#65306;&#20351;&#29992;EMA&#25913;&#36827;&#26799;&#24230;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Careful with that Scalpel: Improving Gradient Surgery with an EMA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#21644;&#36741;&#21161;&#26799;&#24230;&#22312;&#35757;&#32451;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#27491;&#20132;&#25237;&#24433;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;EMA&#65288;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65289;&#21487;&#20197;&#25913;&#36827;&#26799;&#24230;&#25163;&#26415;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#31649;&#36947;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#31649;&#36947;&#20013;&#65292;&#38500;&#20102;&#26368;&#23567;&#21270;&#21333;&#19968;&#30340;&#35757;&#32451;&#25439;&#22833;&#22806;&#65292;&#36824;&#20381;&#36182;&#20110;&#36741;&#21161;&#30446;&#26631;&#26469;&#37327;&#21270;&#21644;&#40723;&#21169;&#27169;&#22411;&#30340;&#21487;&#21462;&#23646;&#24615;&#65288;&#20363;&#22914;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#40065;&#26834;&#24615;&#65292;&#19982;&#20808;&#21069;&#30340;&#19968;&#33268;&#24615;&#65289;&#12290;&#34429;&#28982;&#23558;&#36741;&#21161;&#25439;&#22833;&#19982;&#35757;&#32451;&#25439;&#22833;&#30456;&#21152;&#20316;&#20026;&#27491;&#21017;&#21270;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26799;&#24230;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#30456;&#21152;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65307;&#36825;&#34987;&#31216;&#20026;&#26799;&#24230;&#25163;&#26415;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#30475;&#20316;&#26159;&#19968;&#20010;&#32422;&#26463;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#36741;&#21161;&#30446;&#26631;&#22312;&#35757;&#32451;&#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#38598;&#21512;&#20013;&#34987;&#26368;&#23567;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#21452;&#23618;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#21442;&#25968;&#26356;&#26032;&#26041;&#21521;&#65292;&#23427;&#23558;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#21644;&#36741;&#21161;&#26799;&#24230;&#22312;&#35757;&#32451;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#27491;&#20132;&#25237;&#24433;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#26799;&#24230;&#26469;&#33258;&#23567;&#25209;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#26469;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior). Although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery. We cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss. To solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient. In a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#20219;&#21153;&#25110;&#39046;&#22495;&#30693;&#35782;&#26469;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33719;&#24471;&#30340;&#25991;&#26412;&#34920;&#31034;&#36890;&#24120;&#20248;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#32780;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#35299;&#37322;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.02996</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text-Guided Image Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02996
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#20219;&#21153;&#25110;&#39046;&#22495;&#30693;&#35782;&#26469;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33719;&#24471;&#30340;&#25991;&#26412;&#34920;&#31034;&#36890;&#24120;&#20248;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#32780;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#35299;&#37322;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32858;&#31867;&#23558;&#19968;&#32452;&#22270;&#20687;&#20998;&#25104;&#26377;&#24847;&#20041;&#30340;&#32452;&#65292;&#36890;&#24120;&#36890;&#36807;&#20154;&#24037;&#32473;&#20986;&#30340;&#27880;&#37322;&#36827;&#34892;&#35299;&#37322;&#12290;&#36825;&#20123;&#27880;&#37322;&#36890;&#24120;&#20197;&#25991;&#26412;&#24418;&#24335;&#23384;&#22312;&#65292;&#24341;&#21457;&#20102;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#32858;&#31867;&#30340;&#25277;&#35937;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#24573;&#35270;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;VQA&#27169;&#22411;&#26469;&#27880;&#20837;&#20219;&#21153;&#25110;&#39046;&#22495;&#30693;&#35782;&#29992;&#20110;&#32858;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#22270;&#20687;&#32858;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;&#25991;&#26412;&#34920;&#31034;&#36890;&#24120;&#20248;&#20110;&#22270;&#20687;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#35299;&#37322;&#27604;&#30456;&#24212;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#26356;&#22909;&#22320;&#25551;&#36848;&#20102;&#32858;&#31867;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02992</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Decoding-time Realignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#20943;&#23569;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#21644;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#22312;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#21644;&#40723;&#21169;&#20445;&#25345;&#19982;&#26410;&#23545;&#40784;&#27169;&#22411;&#25509;&#36817;&#30340;&#25509;&#36817;&#24615;&#35268;&#21017;&#39033;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#26435;&#34913;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#35268;&#21017;&#21270;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#65306;&#35268;&#21017;&#21270;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#22870;&#21169;&#27450;&#39575;&#32780;&#38477;&#20302;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#36807;&#24230;&#35268;&#21017;&#21270;&#21017;&#38459;&#30861;&#23545;&#40784;&#12290;&#20256;&#32479;&#26041;&#27861;&#25214;&#21040;&#26368;&#20339;&#35268;&#21017;&#21270;&#27700;&#24179;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#35268;&#21017;&#21270;&#24378;&#24230;&#37325;&#26032;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#12290;DeRa&#21487;&#20197;&#23545;&#23545;&#40784;&#27169;&#22411;&#30340;&#31243;&#24230;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
&lt;/p&gt;</description></item><item><title>DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02989</link><description>&lt;p&gt;
DexDiffuser: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;
&lt;/p&gt;
&lt;p&gt;
DexDiffuser: Generating Dexterous Grasps with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02989
&lt;/p&gt;
&lt;p&gt;
DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DexDiffuser&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28789;&#24039;&#25235;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#37096;&#20998;&#29289;&#20307;&#28857;&#20113;&#19978;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#25235;&#21462;&#23039;&#21183;&#12290;DexDiffuser&#21253;&#25324;&#26465;&#20214;&#25193;&#25955;&#22411;&#25235;&#21462;&#37319;&#26679;&#22120;DexSampler&#21644;&#28789;&#24039;&#25235;&#21462;&#35780;&#20272;&#22120;DexEvaluator&#12290;DexSampler&#36890;&#36807;&#23545;&#38543;&#26426;&#25235;&#21462;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#65292;&#29983;&#25104;&#19982;&#29289;&#20307;&#28857;&#20113;&#26465;&#20214;&#30456;&#20851;&#30340;&#39640;&#36136;&#37327;&#25235;&#21462;&#23039;&#21183;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#25235;&#21462;&#20248;&#21270;&#31574;&#30053;&#65306;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#25193;&#25955;(Evaluator-Guided Diffusion&#65292;EGD)&#21644;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#37319;&#26679;&#20248;&#21270;(Evaluator-based Sampling Refinement&#65292;ESR)&#12290;&#25105;&#20204;&#22312;&#34394;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;Allegro Hand&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;DexDiffuser&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22810;&#25351;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;FFHNet&#65292;&#24179;&#22343;&#25235;&#21462;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;21.71-22.20%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp success rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24847;&#35782;&#30340;&#25439;&#22833;&#20989;&#25968;&#21464;&#20307;&#65292;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#20272;&#35745;&#30340;&#27599;&#20010;&#34892;&#20154;&#30340;&#20851;&#38190;&#24615;&#35780;&#20998;&#65292;&#20197;&#22686;&#24378;&#23545;&#20851;&#38190;&#34892;&#20154;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02986</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#29992;&#20110;&#34892;&#20154;&#26816;&#27979;&#30340;&#23433;&#20840;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Safety-Adapted Loss for Pedestrian Detection in Automated Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24847;&#35782;&#30340;&#25439;&#22833;&#20989;&#25968;&#21464;&#20307;&#65292;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#20272;&#35745;&#30340;&#27599;&#20010;&#34892;&#20154;&#30340;&#20851;&#38190;&#24615;&#35780;&#20998;&#65292;&#20197;&#22686;&#24378;&#23545;&#20851;&#38190;&#34892;&#20154;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#20013;&#65292;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#21361;&#21450;&#34892;&#20154;&#21644;&#20854;&#20182;&#26131;&#21463;&#20260;&#23475;&#30340;&#36947;&#36335;&#29992;&#25143;&#65288;VRU&#65289;&#12290;&#30001;&#20110;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#23433;&#20840;&#25351;&#26631;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35782;&#21035;&#23433;&#20840;&#20851;&#38190;&#30340;VRU&#65292;&#24182;&#23558;&#39118;&#38505;&#21453;&#39304;&#32473;&#29289;&#20307;&#26816;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#36807;&#31243;&#20013;&#24182;&#27809;&#26377;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;DNN&#20250;&#23545;&#25152;&#26377;&#35823;&#26816;&#36827;&#34892;&#30456;&#21516;&#30340;&#24809;&#32602;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#20851;&#38190;&#25925;&#38556;&#26696;&#20363;&#65288;&#21363;&#20551;&#38452;&#24615;&#65289;&#30340;&#21457;&#29983;&#65292;&#21487;&#33021;&#38656;&#35201;&#19968;&#31181;&#23433;&#20840;&#24847;&#35782;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#23545;&#20851;&#38190;&#34892;&#20154;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23433;&#20840;&#24847;&#35782;&#25439;&#22833;&#21464;&#20998;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;&#27599;&#20010;&#34892;&#20154;&#30340;&#20851;&#38190;&#24615;&#35780;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22522;&#20110;&#21487;&#36798;&#24615;&#38598;&#30340;&#30896;&#25758;&#26102;&#38388;&#65288;TTC-RSB&#65289;&#24230;&#37327;&#21644;&#36317;&#31163;...
&lt;/p&gt;
&lt;p&gt;
In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality. Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians. In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training. We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#20917;&#35299;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#19979;&#30340;&#36335;&#20917;&#22330;&#26223;&#35299;&#26512;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#26816;&#27979;&#36335;&#20917;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#28982;&#21518;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#36335;&#20917;&#21306;&#22495;&#25513;&#27169;&#65292;&#28982;&#21518;&#37319;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32593;&#32476;&#25552;&#21462;&#29305;&#24449;&#34920;&#31034;&#65292;&#26368;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#23545;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#21644;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2402.02985</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#29992;&#20110;&#36335;&#20917;&#22330;&#26223;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#20917;&#35299;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#19979;&#30340;&#36335;&#20917;&#22330;&#26223;&#35299;&#26512;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#26816;&#27979;&#36335;&#20917;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#28982;&#21518;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#36335;&#20917;&#21306;&#22495;&#25513;&#27169;&#65292;&#28982;&#21518;&#37319;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32593;&#32476;&#25552;&#21462;&#29305;&#24449;&#34920;&#31034;&#65292;&#26368;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#23545;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#21644;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#35299;&#26512;&#36335;&#20917;&#22330;&#26223;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#39640;&#20998;&#36776;&#29575;&#20351;&#24471;&#22788;&#29702;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#27880;&#26469;&#35757;&#32451;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#20917;&#35299;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#36817;&#26399;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22788;&#29702;&#36229;&#22823;&#20998;&#36776;&#29575;&#26080;&#20154;&#26426;&#22270;&#20687;&#65292;&#24555;&#36895;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#36335;&#20917;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#25509;&#19979;&#26469;&#65292;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;SAM&#20026;&#27809;&#26377;&#31867;&#21035;&#20449;&#24687;&#30340;&#36335;&#20917;&#21306;&#22495;&#29983;&#25104;&#25513;&#27169;&#12290;&#38543;&#21518;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32593;&#32476;&#20174;&#25152;&#26377;&#25513;&#27169;&#21306;&#22495;&#20013;&#25552;&#21462;&#29305;&#24449;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24212;&#29992;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#31639;&#27861;&#23545;&#36825;&#20123;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#24182;&#20026;&#27599;&#20010;&#31751;&#20998;&#37197;ID&#12290;&#28982;&#21518;&#65292;&#23558;&#25513;&#27169;&#21306;&#22495;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#25925;&#38556;&#35786;&#26029;&#21644;&#23481;&#38169;&#25511;&#21046;&#26041;&#26696;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#31561;&#23574;&#31471;&#25216;&#26415;&#23545;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#23481;&#38169;&#33021;&#21147;&#30340;&#21464;&#38761;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02980</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#25925;&#38556;&#35786;&#26029;&#21644;&#23481;&#38169;&#25511;&#21046;&#26041;&#26696;&#32508;&#36848;&#65306;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Review on Fault Diagnosis and Fault-Tolerant Control Scheme for Robotic Manipulators: Recent Advances in AI, Machine Learning, and Digital Twin
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02980
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#25925;&#38556;&#35786;&#26029;&#21644;&#23481;&#38169;&#25511;&#21046;&#26041;&#26696;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#31561;&#23574;&#31471;&#25216;&#26415;&#23545;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#23481;&#38169;&#33021;&#21147;&#30340;&#21464;&#38761;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;&#25991;&#31456;&#28145;&#20837;&#25506;&#35752;&#20102;&#38024;&#23545;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#37327;&#36523;&#23450;&#21046;&#30340;&#23481;&#38169;&#25511;&#21046;&#65288;FTC&#65289;&#26041;&#26696;&#30340;&#22797;&#26434;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#28085;&#30422;&#20102;FTC&#30340;&#21382;&#21490;&#28436;&#21464;&#65292;&#36861;&#28335;&#20854;&#38543;&#26102;&#38388;&#30340;&#21457;&#23637;&#65292;&#24182;&#35814;&#32454;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#65288;DTT&#65289;&#31561;&#23574;&#31471;&#25216;&#26415;&#30456;&#20114;&#34701;&#21512;&#25152;&#39537;&#21160;&#30340;&#26368;&#26032;&#31361;&#30772;&#12290;&#26412;&#25991;&#29305;&#21035;&#24378;&#35843;&#20102;&#36825;&#20123;&#24403;&#20195;&#36235;&#21183;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#25511;&#21046;&#21644;&#23481;&#38169;&#33021;&#21147;&#39046;&#22495;&#25152;&#20135;&#29983;&#30340;&#21464;&#38761;&#24433;&#21709;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#21382;&#21490;&#32972;&#26223;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;FTC&#26041;&#26696;&#28436;&#21464;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36825;&#19968;&#36807;&#31243;&#21253;&#25324;&#20102;&#20174;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#20449;&#21495;&#30340;&#26041;&#26696;&#36807;&#28193;&#21040;&#20256;&#24863;&#22120;&#30340;&#20316;&#29992;&#65292;&#20026;&#21518;&#32493;&#25506;&#32034;AI&#12289;ML&#21644;DTT&#25152;&#24102;&#26469;&#30340;&#24403;&#20170;&#33539;&#24335;&#36716;&#21464;&#25171;&#19979;&#22522;&#30784;&#12290;&#25105;&#20204;&#22312;&#35299;&#21078;&#36825;&#20010;&#22797;&#26434;&#30340;&#27675;&#22260;&#30340;&#21516;&#26102;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#36817;&#26399;&#30740;&#31350;&#25152;&#37319;&#29992;&#30340;AI&#12289;ML&#21644;DTT&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#25511;&#21046;&#21644;&#23481;&#38169;&#33021;&#21147;&#26041;&#38754;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This comprehensive review article delves into the intricate realm of fault-tolerant control (FTC) schemes tailored for robotic manipulators. Our exploration spans the historical evolution of FTC, tracing its development over time, and meticulously examines the recent breakthroughs fueled by the synergistic integration of cutting-edge technologies such as artificial intelligence (AI), machine learning (ML), and digital twin technologies (DTT). The article places a particular emphasis on the transformative influence these contemporary trends exert on the landscape of robotic manipulator control and fault tolerance.   By delving into the historical context, our aim is to provide a comprehensive understanding of the evolution of FTC schemes. This journey encompasses the transition from model-based and signal-based schemes to the role of sensors, setting the stage for an exploration of the present-day paradigm shift enabled by AI, ML, and DTT. The narrative unfolds as we dissect the intrica
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25552;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#25552;&#21319;&#31639;&#27861;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23558;&#26679;&#26412;&#21387;&#32553;&#26041;&#27861;&#25193;&#23637;&#21040;&#25903;&#25345;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26679;&#26412;&#22823;&#23567;&#19978;&#20855;&#26377;&#21333;&#23545;&#25968;&#20381;&#36182;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.02976</link><description>&lt;p&gt;
&#25552;&#21319;&#65292;&#25237;&#31080;&#20998;&#31867;&#22120;&#21644;&#38543;&#26426;&#37319;&#26679;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Boosting, Voting Classifiers and Randomized Sample Compression Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25552;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#25552;&#21319;&#31639;&#27861;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23558;&#26679;&#26412;&#21387;&#32553;&#26041;&#27861;&#25193;&#23637;&#21040;&#25903;&#25345;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26679;&#26412;&#22823;&#23567;&#19978;&#20855;&#26377;&#21333;&#23545;&#25968;&#20381;&#36182;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#21319;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#24369;&#23398;&#20064;&#22120;&#26469;&#20135;&#29983;&#19968;&#20010;&#24378;&#23398;&#20064;&#22120;&#12290;&#36825;&#20010;&#33539;&#24335;&#30340;&#26680;&#24515;&#26159;&#23558;&#24378;&#23398;&#20064;&#22120;&#24314;&#27169;&#20026;&#19968;&#20010;&#25237;&#31080;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#24369;&#23398;&#20064;&#22120;&#30340;&#21152;&#26435;&#22810;&#25968;&#25237;&#31080;&#12290;&#23613;&#31649;&#35768;&#22810;&#25104;&#21151;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#22914;&#26631;&#24535;&#24615;&#30340;AdaBoost&#65292;&#20135;&#29983;&#25237;&#31080;&#20998;&#31867;&#22120;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#24615;&#33021;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#19981;&#22815;&#20248;&#21270;&#65306;&#36804;&#20170;&#20026;&#27490;&#65292;&#24050;&#30693;&#30340;&#20351;&#25237;&#31080;&#20998;&#31867;&#22120;&#36798;&#21040;&#32473;&#23450;&#20934;&#30830;&#24615;&#25152;&#38656;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#30340;&#26368;&#20339;&#30028;&#38480;&#24635;&#26159;&#33267;&#23569;&#21253;&#21547;&#33267;&#22810;&#20004;&#20010;&#23545;&#25968;&#22240;&#23376;&#65292;&#32780;&#36825;&#24050;&#32463;&#36229;&#36807;&#20102;&#19968;&#33324;&#30340;&#24369;&#21040;&#24378;&#23398;&#20064;&#22120;&#25152;&#33021;&#23454;&#29616;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38543;&#26426;&#25552;&#21319;&#31639;&#27861;&#25171;&#30772;&#36825;&#19968;&#38556;&#30861;&#65292;&#35813;&#31639;&#27861;&#36755;&#20986;&#30340;&#25237;&#31080;&#20998;&#31867;&#22120;&#22312;&#26679;&#26412;&#22823;&#23567;&#19978;&#21253;&#21547;&#21333;&#23545;&#25968;&#20381;&#36182;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23558;&#26679;&#26412;&#21387;&#32553;&#26041;&#27861;&#25193;&#23637;&#21040;&#25903;&#25345;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#26469;&#33719;&#24471;&#36825;&#20010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms ba
&lt;/p&gt;</description></item><item><title>RetDream&#26159;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#24471;&#20998;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#21644;3D&#36164;&#28304;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02972</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24471;&#20998;&#33976;&#39311;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Score Distillation for Text-to-3D Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02972
&lt;/p&gt;
&lt;p&gt;
RetDream&#26159;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#24471;&#20998;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#21644;3D&#36164;&#28304;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;3D&#29983;&#25104;&#36890;&#36807;&#24341;&#20837;&#24378;&#22823;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#19981;&#36275;&#30340;3D&#20808;&#39564;&#30693;&#35782;&#20063;&#23548;&#33268;&#20102;3D&#20960;&#20309;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#35299;&#20915;3D&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;2D&#25968;&#25454;&#30456;&#27604;&#65292;3D&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26377;&#38480;&#65292;&#36825;&#23548;&#33268;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#22238;&#36991;&#36825;&#20123;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#38024;&#23545;&#24471;&#20998;&#33976;&#39311;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#21517;&#20026;RetDream&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#20351;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#21644;3D&#36164;&#28304;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#36136;&#37327;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#36164;&#28304;&#26469;&#34701;&#20837;&#20854;
&lt;/p&gt;
&lt;p&gt;
Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.02969</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#35789;&#25935;&#24863;&#24615;&#30340;&#29702;&#35299;&#65306;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02969
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;transformers&#24322;&#24120;&#25104;&#21151;&#32972;&#21518;&#21407;&#22240;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#20026;&#20160;&#20040;&#27880;&#24847;&#21147;&#23618;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#39044;&#27979;&#27169;&#22411;&#25429;&#25417;&#19978;&#19979;&#25991;&#21547;&#20041;&#65292;&#21363;&#20351;&#21477;&#23376;&#24456;&#38271;&#65292;&#36825;&#24448;&#24448;&#21462;&#20915;&#20110;&#19968;&#20010;&#25110;&#20960;&#20010;&#35789;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#38543;&#26426;&#29305;&#24449;&#30340;&#20856;&#22411;&#35774;&#32622;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#65292;&#31216;&#20026;&#35789;&#25935;&#24863;&#24615;&#65288;WS&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;WS&#65292;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#20010;&#21521;&#37327;&#65292;&#33021;&#22815;&#22823;&#24133;&#25200;&#21160;&#38543;&#26426;&#27880;&#24847;&#21147;&#29305;&#24449;&#26144;&#23556;&#12290;&#36825;&#20010;&#35770;&#28857;&#20851;&#38190;&#22320;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;softmax&#30340;&#20316;&#29992;&#65292;&#31361;&#26174;&#20102;&#23427;&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65289;&#30340;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#26631;&#20934;&#38543;&#26426;&#29305;&#24449;&#30340;WS&#26159;$1/\sqrt{n}$&#38454;&#30340;&#65292;$n$&#26159;&#25991;&#26412;&#26679;&#26412;&#20013;&#30340;&#21333;&#35789;&#25968;&#65292;&#22240;&#27492;&#23427;&#38543;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#32780;&#34928;&#20943;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20851;&#20110;&#35789;&#25935;&#24863;&#24615;&#30340;&#32467;&#26524;&#36716;&#21270;&#20026;&#27867;&#21270;&#30028;&#65306;&#30001;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to th
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35270;&#35273;&#29702;&#35299;&#22522;&#30784;&#27169;&#22411;&#22312;&#36947;&#36335;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#21508;&#31181;&#39550;&#39542;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#65292;&#20026;&#23454;&#29616;&#23545;&#21608;&#22260;&#22330;&#26223;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.02968</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#29702;&#35299;&#65306;&#20174;&#23398;&#20064;&#35270;&#35282;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35270;&#35273;&#29702;&#35299;&#22522;&#30784;&#27169;&#22411;&#22312;&#36947;&#36335;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#21508;&#31181;&#39550;&#39542;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#65292;&#20026;&#23454;&#29616;&#23545;&#21608;&#22260;&#22330;&#26223;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#30830;&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#25104;&#20026;&#26174;&#33879;&#22609;&#36896;&#26234;&#33021;&#31995;&#32479;&#33021;&#21147;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#22312;&#26234;&#33021;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21147;&#37327;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#21464;&#38761;&#24615;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#35270;&#35273;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35270;&#35273;&#29702;&#35299;&#22522;&#30784;&#27169;&#22411;(MM-VUFMs)&#20855;&#22791;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#24182;&#21516;&#26102;&#22788;&#29702;&#21508;&#31181;&#19982;&#39550;&#39542;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#65292;&#20026;&#23545;&#21608;&#22260;&#22330;&#26223;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#30340;MM-VUFMs&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#20165;&#26159;&#25552;&#20379;&#23545;&#24120;&#35265;&#23454;&#36341;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#28041;&#21450;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12289;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12289;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#20419;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#21518;&#39564;&#27010;&#29575;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#26469;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#27979;&#37327;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02964</link><description>&lt;p&gt;
&#28151;&#21512;&#22122;&#22768;&#19982;&#26465;&#20214;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#21518;&#39564;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mixed Noise and Posterior Estimation with Conditional DeepGEM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#21518;&#39564;&#27010;&#29575;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#26469;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#27979;&#37327;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28151;&#21512;&#22122;&#22768;&#27169;&#22411;&#30340;&#38388;&#25509;&#27979;&#37327;&#21644;&#32435;&#31859;&#35745;&#37327;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#30340;&#21518;&#39564;&#27010;&#29575;&#21644;&#22122;&#22768;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#24403;&#21069;&#30340;&#22122;&#22768;&#21442;&#25968;&#65292;&#25105;&#20204;&#22312;E&#27493;&#20013;&#23398;&#20064;&#20102;&#19968;&#20010;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#65292;&#20197;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#12290;&#22312;M&#27493;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20877;&#27425;&#36890;&#36807;EM&#31639;&#27861;&#25214;&#21040;&#22122;&#22768;&#21442;&#25968;&#30340;&#26356;&#26032;&#65292;&#20854;&#20855;&#26377;&#35299;&#26512;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#30340;&#35757;&#32451;&#19982;&#21069;&#21521;&#21644;&#21453;&#21521;KL&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#35768;&#22810;&#27979;&#37327;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#20687;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#21040;&#28909;&#25104;&#20687;&#30340;AI&#39537;&#21160;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#24314;&#31569;&#22806;&#35266;&#28909;&#25104;&#20687;&#26816;&#27979;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#36741;&#21161;&#26085;&#24120;&#24314;&#31569;&#26816;&#26597;&#25110;&#33258;&#21160;&#21270;&#26816;&#26597;&#22823;&#38754;&#31215;&#12290;</title><link>https://arxiv.org/abs/2402.02963</link><description>&lt;p&gt;
&#21033;&#29992;&#24425;&#33394;&#21040;&#28909;&#25104;&#20687;AI&#36827;&#34892;&#24314;&#31569;&#22806;&#35266;&#26816;&#27979;&#30340;&#21333;&#31867;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
One-class anomaly detection through color-to-thermal AI for building envelope inspection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02963
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#21040;&#28909;&#25104;&#20687;&#30340;AI&#39537;&#21160;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#24314;&#31569;&#22806;&#35266;&#28909;&#25104;&#20687;&#26816;&#27979;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#36741;&#21161;&#26085;&#24120;&#24314;&#31569;&#26816;&#26597;&#25110;&#33258;&#21160;&#21270;&#26816;&#26597;&#22823;&#38754;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#24314;&#31569;&#22806;&#35266;&#28909;&#25104;&#20687;&#26816;&#27979;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#30001;AI&#39537;&#21160;&#30340;&#20174;&#24425;&#33394;&#22270;&#20687;&#39044;&#27979;&#28909;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#19968;&#31867;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#35782;&#21035;&#39044;&#27979;&#21644;&#23454;&#38469;&#28909;&#20998;&#24067;&#20043;&#38388;&#26377;&#36739;&#22823;&#24046;&#24322;&#30340;&#28909;&#22270;&#21306;&#22495;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#30340;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#23558;&#26576;&#20123;&#29305;&#24449;&#26631;&#35782;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22312;&#19981;&#21516;&#23460;&#22806;&#28201;&#24230;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#28436;&#31034;&#20102;&#36825;&#20010;&#21407;&#29702;&#65292;&#20174;&#32780;&#26816;&#27979;&#21040;&#20102;&#28909;&#26725;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#36741;&#21161;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#26085;&#24120;&#24314;&#31569;&#26816;&#26597;&#65292;&#25110;&#19982;&#31227;&#21160;&#24179;&#21488;&#32467;&#21512;&#20197;&#33258;&#21160;&#21270;&#23545;&#22823;&#38754;&#31215;&#36827;&#34892;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a label-free method for detecting anomalies during thermographic inspection of building envelopes. It is based on the AI-driven prediction of thermal distributions from color images. Effectively the method performs as a one-class classifier of the thermal image regions with high mismatch between the predicted and actual thermal distributions. The algorithm can learn to identify certain features as normal or anomalous by selecting the target sample used for training. We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges. The method can be implemented to assist human professionals during routine building inspections or combined with mobile platforms for automating examination of large areas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#21644;&#29992;&#25143;&#20851;&#32852;&#25351;&#26631;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#19982;&#26080;&#20154;&#26426;&#30340;&#20851;&#32852;&#65292;&#20197;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#26041;&#38754;&#30340;&#21033;&#29992;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02957</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21327;&#21161;&#26080;&#20154;&#26426;&#21368;&#36733;&#34562;&#31389;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#21644;&#29992;&#25143;&#20851;&#32852;&#25351;&#26631;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#19982;&#26080;&#20154;&#26426;&#30340;&#20851;&#32852;&#65292;&#20197;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#26041;&#38754;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#30340;&#35299;&#20915;&#22320;&#38754;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#26234;&#33021;&#25968;&#25454;&#25910;&#38598;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#22320;&#38754;&#22522;&#31449;&#30340;&#26377;&#38480;&#39057;&#35889;&#21644;&#35206;&#30422;&#33539;&#22260;&#32473;&#32593;&#32476;&#29992;&#25143;&#30340;&#25968;&#25454;&#29575;&#38656;&#27714;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26080;&#20154;&#26426;&#20197;&#20854;&#39640;&#25935;&#25463;&#24615;&#12289;&#31227;&#21160;&#24615;&#21644;&#28789;&#27963;&#24615;&#32780;&#38395;&#21517;&#65292;&#20026;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#25163;&#27573;&#65292;&#25104;&#20026;&#39069;&#22806;&#30340;&#25509;&#20837;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#26041;&#38754;&#30340;&#21033;&#29992;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37325;&#28857;&#26159;&#22312;&#36136;&#37327;&#20445;&#35777;&#32422;&#26463;&#19979;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#21644;&#29992;&#25143;&#20851;&#32852;&#25351;&#26631;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#19982;&#26080;&#20154;&#26426;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#25152;&#21046;&#23450;&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#38382;&#39064;&#26159;&#38750;&#20984;&#21644;&#32452;&#21512;&#30340;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;&#26080;&#20154;&#26426;&#20197;&#38750;&#21512;&#20316;&#26041;&#24335;&#23547;&#27714;&#20174;&#29615;&#22659;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#24049;&#30340;&#31574;&#30053;.
&lt;/p&gt;
&lt;p&gt;
Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications. The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users. Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points. This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs. Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints. Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent reinforcement learning framework. In this framework, each UAV a
&lt;/p&gt;</description></item><item><title>AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02956</link><description>&lt;p&gt;
AdaTreeFormer: &#20174;&#19968;&#24352;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02956
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#24352;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#26469;&#20272;&#35745;&#21644;&#35745;&#25968;&#26641;&#26408;&#23494;&#24230;&#26159;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#39046;&#22495;&#20013;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19981;&#21516;&#22320;&#24418;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#26641;&#26408;&#31181;&#31867;&#20005;&#37325;&#38459;&#30861;&#20102;&#26641;&#26408;&#35745;&#25968;&#27169;&#22411;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#20174;&#20855;&#26377;&#36275;&#22815;&#26631;&#27880;&#26641;&#26408;&#30340;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AdaTreeFormer&#65292;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#29305;&#24449;&#12290;&#23427;&#36824;&#21253;&#25324;&#19977;&#20010;&#23376;&#32593;&#32476;&#65306;&#20004;&#20010;&#29992;&#20110;&#20998;&#21035;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#25552;&#21462;&#33258;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#36328;&#39046;&#22495;&#27880;&#24847;&#21147;&#22270;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#36866;&#24212;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#30740;&#31350;&#20102;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20998;&#35299;&#23376;&#28216;&#25103;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#24320;&#20102;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.02954</link><description>&lt;p&gt;
&#35299;&#20915;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65306;&#19968;&#31181;&#24191;&#20041;&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#30740;&#31350;&#20102;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20998;&#35299;&#23376;&#28216;&#25103;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#24320;&#20102;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#22810;&#20154;&#20998;&#25955;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#21333;&#20154;&#28216;&#25103;&#65292;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36125;&#23572;&#26364;&#30340;&#26368;&#20248;&#24615;&#21407;&#29702;&#36890;&#36807;&#23558;&#20854;&#20998;&#35299;&#20026;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#26469;&#35299;&#20915;&#21333;&#20154;&#28216;&#25103;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#27599;&#20010;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20013;&#32416;&#32544;&#20102;&#25152;&#26377;&#29609;&#23478;&#30340;&#20915;&#31574;&#21464;&#37327;&#65292;&#23548;&#33268;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#22791;&#20221;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#21069;&#25552;&#19979;&#35299;&#24320;&#36825;&#20123;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#36825;&#26159;&#25105;&#20204;&#31038;&#20250;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#31649;&#29702;&#39118;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#36890;&#36807;&#36827;&#19968;&#27493;&#23558;&#20219;&#20309;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#23376;&#28216;&#25103;&#26469;&#35299;&#20915;&#23427;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36880;&#27425;&#36827;&#34892;&#21333;&#20154;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20013;&#30340;&#24191;&#20041;&#21338;&#24328;&#35299;&#20915;&#26041;&#26696;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#35299;&#20915;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames. However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity. This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society. To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time. Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity. Our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26041;&#26696;&#65292;&#24182;&#37325;&#26032;&#23454;&#29616;&#20102;12&#20010;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02953</link><description>&lt;p&gt;
&#25581;&#31034;&#35299;&#20915;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Key of Machine Learning Solutions for Android Malware Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26041;&#26696;&#65292;&#24182;&#37325;&#26032;&#23454;&#29616;&#20102;12&#20010;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20316;&#20026;&#23545;&#24694;&#24847;&#24212;&#29992;&#31243;&#24207;&#30340;&#31532;&#19968;&#36947;&#38450;&#32447;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;ML&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22240;&#20854;&#33021;&#22815;&#33258;&#21160;&#25429;&#33719;Android APK&#20013;&#30340;&#24694;&#24847;&#27169;&#24335;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#24403;&#21069;&#30740;&#31350;&#36827;&#23637;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20351;&#24471;&#24456;&#38590;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#26377;&#19968;&#20010;&#20840;&#38754;&#30340;&#20102;&#35299;&#12290;&#26412;&#25991;&#23545;&#22522;&#20110;ML&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26681;&#25454;Android&#29305;&#24449;&#24037;&#31243;&#21644;ML&#24314;&#27169;&#36807;&#31243;&#23558;&#36129;&#29486;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;ML-based Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#37325;&#26032;&#23454;&#29616;&#20102;&#26469;&#33258;&#19981;&#21516;&#30740;&#31350;&#31038;&#21306;&#30340;12&#20010;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#24182;&#20174;&#19977;&#20010;&#26041;&#38754;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Android malware detection serves as the front line against malicious apps. With the rapid advancement of machine learning (ML), ML-based Android malware detection has attracted increasing attention due to its capability of automatically capturing malicious patterns from Android APKs. These learning-driven methods have reported promising results in detecting malware. However, the absence of an in-depth analysis of current research progress makes it difficult to gain a holistic picture of the state of the art in this area.   This paper presents a comprehensive investigation to date into ML-based Android malware detection with empirical and quantitative analysis. We first survey the literature, categorizing contributions into a taxonomy based on the Android feature engineering and ML modeling pipeline. Then, we design a general-propose framework for ML-based Android malware detection, re-implement 12 representative approaches from different research communities, and evaluate them from thr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#30830;&#23450;&#24615;MoE&#27169;&#22411;&#19979;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#24378;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#26469;&#25551;&#36848;&#19981;&#21516;&#31867;&#22411;&#19987;&#23478;&#20989;&#25968;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.02952</link><description>&lt;p&gt;
&#20851;&#20110;Softmax Gating&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Least Squares Estimation in Softmax Gating Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#30830;&#23450;&#24615;MoE&#27169;&#22411;&#19979;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#24378;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#26469;&#25551;&#36848;&#19981;&#21516;&#31867;&#22411;&#19987;&#23478;&#20989;&#25968;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#27169;&#22411;&#26159;&#19968;&#31181;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#65292;&#20351;&#29992;Softmax Gating&#20989;&#25968;&#32858;&#21512;&#22810;&#20010;&#19987;&#23478;&#32593;&#32476;&#65292;&#20197;&#24418;&#25104;&#19968;&#20010;&#26356;&#22797;&#26434;&#21644;&#34920;&#36798;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#30001;&#20110;&#21487;&#25193;&#23637;&#24615;&#32780;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;MoE&#27169;&#22411;&#30340;&#25968;&#23398;&#21644;&#32479;&#35745;&#24615;&#36136;&#22797;&#26434;&#19988;&#38590;&#20197;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#20197;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#27010;&#29575;MoE&#27169;&#22411;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#25968;&#25454;&#26159;&#30001;&#39640;&#26031;MoE&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#30830;&#23450;&#24615;MoE&#27169;&#22411;&#19979;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65288;LSE&#65289;&#30340;&#24615;&#33021;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#25968;&#25454;&#26681;&#25454;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31216;&#20026;&#24378;&#21487;&#35782;&#21035;&#24615;&#30340;&#26465;&#20214;&#65292;&#20197;&#34920;&#24449;&#19981;&#21516;&#31867;&#22411;&#19987;&#23478;&#20989;&#25968;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#24378;&#21487;&#35782;&#21035;&#19987;&#23478;&#30340;&#20272;&#35745;&#36895;&#24230;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
Mixture of experts (MoE) model is a statistical machine learning design that aggregates multiple expert networks using a softmax gating function in order to form a more intricate and expressive model. Despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of MoE models are complex and difficult to analyze. As a result, previous theoretical works have primarily focused on probabilistic MoE models by imposing the impractical assumption that the data are generated from a Gaussian MoE model. In this work, we investigate the performance of the least squares estimators (LSE) under a deterministic MoE model where the data are sampled according to a regression model, a setting that has remained largely unexplored. We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. We demonstrate that the rates for estimating strongly identifiable experts, namel
&lt;/p&gt;</description></item><item><title>$\textsf{DynaBRO}$&#26159;&#19968;&#31181;&#21160;&#24577;&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#20999;&#25442;&#25308;&#21344;&#24237;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#19978;&#19982;&#38745;&#24577;&#24773;&#20917;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#28176;&#21464;&#20272;&#35745;&#25216;&#26415;&#12289;&#24378;&#40065;&#26834;&#24037;&#20316;&#26426;&#21046;&#26356;&#26032;&#30340;&#32858;&#21512;&#21644;&#25925;&#38556;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#24341;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32463;&#21463;&#20303;$\mathcal{O}(\sqrt{T})$&#36718;&#25308;&#21344;&#24237;&#36523;&#20221;&#30340;&#25913;&#21464;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#30334;&#20998;&#27604;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.02951</link><description>&lt;p&gt;
&#21160;&#24577;&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#65306;&#36866;&#24212;&#20999;&#25442;&#25308;&#21344;&#24237;&#24037;&#20316;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02951
&lt;/p&gt;
&lt;p&gt;
$\textsf{DynaBRO}$&#26159;&#19968;&#31181;&#21160;&#24577;&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#20999;&#25442;&#25308;&#21344;&#24237;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#19978;&#19982;&#38745;&#24577;&#24773;&#20917;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#28176;&#21464;&#20272;&#35745;&#25216;&#26415;&#12289;&#24378;&#40065;&#26834;&#24037;&#20316;&#26426;&#21046;&#26356;&#26032;&#30340;&#32858;&#21512;&#21644;&#25925;&#38556;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#24341;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32463;&#21463;&#20303;$\mathcal{O}(\sqrt{T})$&#36718;&#25308;&#21344;&#24237;&#36523;&#20221;&#30340;&#25913;&#21464;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#30334;&#20998;&#27604;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#23481;&#38169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25216;&#26415;&#32771;&#34385;&#30340;&#26159;&#38745;&#24577;&#24773;&#20917;&#65292;&#20854;&#20013;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25308;&#21344;&#24237;&#26426;&#22120;&#30340;&#36523;&#20221;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#20551;&#35774;&#19981;&#33021;&#25429;&#25417;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21160;&#24577;&#25308;&#21344;&#24237;&#34892;&#20026;&#65292;&#21487;&#33021;&#21253;&#25324;&#30701;&#26242;&#25925;&#38556;&#25110;&#26377;&#38024;&#23545;&#24615;&#30340;&#26102;&#38388;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;$\textsf{DynaBRO}$&#65292;&#23427;&#33021;&#22815;&#32463;&#21463;&#20303;$\mathcal{O}(\sqrt{T})$&#36718;&#25308;&#21344;&#24237;&#36523;&#20221;&#30340;&#25913;&#21464;&#65288;&#20854;&#20013;$T$&#26159;&#24635;&#35757;&#32451;&#36718;&#25968;&#65289;&#65292;&#21516;&#26102;&#19982;&#38745;&#24577;&#24773;&#20917;&#19979;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#28176;&#21464;&#20272;&#35745;&#25216;&#26415;&#19982;&#24037;&#20316;&#26426;&#21046;&#26356;&#26032;&#30340;&#24378;&#40065;&#26834;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#25925;&#38556;&#23433;&#20840;&#36807;&#28388;&#22120;&#26469;&#38480;&#21046;&#21160;&#24577;&#25308;&#21344;&#24237;&#31574;&#30053;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#30334;&#20998;&#27604;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\textsf{DynaBRO}$ -- a new method capable of withstanding $\mathcal{O}(\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;PCA&#36827;&#34892;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#25104;&#20998;&#23376;&#31354;&#38388;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#30340;&#26377;&#25928;&#21306;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.02949</link><description>&lt;p&gt;
&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26680;PCA
&lt;/p&gt;
&lt;p&gt;
Kernel PCA for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;PCA&#36827;&#34892;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#25104;&#20998;&#23376;&#31354;&#38388;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#30340;&#26377;&#25928;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OoD&#65289;&#26816;&#27979;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;DNN&#29305;&#24449;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26816;&#27979;&#26469;&#33258;&#20869;&#20998;&#24067;&#65288;InD&#65289;&#25968;&#25454;&#30340;OoD&#25968;&#25454;&#26041;&#38754;&#19981;&#36275;&#22815;&#12290;PCA&#30340;&#22833;&#36133;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#22312;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#31616;&#21333;&#22788;&#29702;&#26080;&#27861;&#24456;&#22909;&#22320;&#23558;OoD&#21644;InD&#20013;&#30340;&#32593;&#32476;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#32780;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#26469;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;PCA&#65288;KPCA&#65289;&#26694;&#26550;&#36827;&#34892;OoD&#26816;&#27979;&#65292;&#23547;&#25214;OoD&#21644;InD&#29305;&#24449;&#20197;&#26174;&#33879;&#19981;&#21516;&#30340;&#27169;&#24335;&#20998;&#37197;&#30340;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#24449;&#26144;&#23556;&#65292;&#22312;KPCA&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#20869;&#26680;&#65292;&#20197;&#20419;&#36827;&#22312;&#20027;&#25104;&#20998;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#20013;InD&#21644;OoD&#25968;&#25454;&#20043;&#38388;&#30340;&#21487;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#22312;&#36825;&#31181;&#23376;&#31354;&#38388;&#20013;&#30340;&#37325;&#26500;&#35823;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24471;&#21040;$\mathcal{O}(1)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;HoughToRadon&#21464;&#25442;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22312;Hough&#21464;&#25442;&#23618;&#21518;&#36827;&#34892;&#25913;&#33391;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#36895;&#24230;&#24471;&#21040;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;97.7&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02946</link><description>&lt;p&gt;
HoughToRadon&#21464;&#25442;&#65306;&#25237;&#24433;&#31354;&#38388;&#29305;&#24449;&#25913;&#36827;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;HoughToRadon&#21464;&#25442;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22312;Hough&#21464;&#25442;&#23618;&#21518;&#36827;&#34892;&#25913;&#33391;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#36895;&#24230;&#24471;&#21040;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;97.7&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HoughToRadon&#21464;&#25442;&#23618;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#65292;&#26088;&#22312;&#25913;&#36827;&#19982;Hough&#21464;&#25442;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36895;&#24230;&#65292;&#20197;&#35299;&#20915;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;Hough&#21464;&#25442;&#23618;&#20043;&#21518;&#25918;&#32622;&#35813;&#23618;&#65292;&#8220;&#20869;&#37096;&#8221;&#21367;&#31215;&#21487;&#20197;&#25509;&#25910;&#21040;&#20855;&#26377;&#26032;&#30340;&#26377;&#30410;&#23646;&#24615;&#30340;&#20462;&#25913;&#29305;&#24449;&#22270;&#65292;&#22914;&#36739;&#23567;&#30340;&#22270;&#20687;&#22788;&#29702;&#21306;&#22495;&#21644;&#21442;&#25968;&#31354;&#38388;&#19982;&#35282;&#24230;&#21644;&#20559;&#31227;&#30340;&#32447;&#24615;&#24615;&#12290;&#36825;&#20123;&#23646;&#24615;&#22312;&#21333;&#29420;&#20351;&#29992;Hough&#21464;&#25442;&#26102;&#26159;&#19981;&#20855;&#22791;&#30340;&#12290;&#27492;&#22806;&#65292;HoughToRadon&#21464;&#25442;&#23618;&#20801;&#35768;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#26032;&#21442;&#25968;&#35843;&#25972;&#20013;&#38388;&#29305;&#24449;&#22270;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24179;&#34913;&#32467;&#26524;&#31070;&#32463;&#32593;&#32476;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#30340;MIDV-500&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#22312;&#25991;&#26723;&#20998;&#21106;&#20219;&#21153;&#20013;&#33410;&#30465;&#20102;&#26102;&#38388;&#65292;&#24182;&#23454;&#29616;&#20102;97.7&#65285;&#30340;&#26368;&#26032;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#22823;&#30340;HoughEncoder&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems. By placing it after a Hough Transform layer, "inner" convolutions receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift. These properties were not presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network. Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#28151;&#21512;&#26550;&#26500;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#26368;&#26032;&#28151;&#21512;CNN-ViT&#26550;&#26500;&#30340;&#32508;&#36848;&#12290;&#32763;&#35793;</title><link>https://arxiv.org/abs/2402.02941</link><description>&lt;p&gt;
&#25506;&#32034;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#28151;&#21512;&#26550;&#26500;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21327;&#21516;&#20316;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#28151;&#21512;&#26550;&#26500;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#26368;&#26032;&#28151;&#21512;CNN-ViT&#26550;&#26500;&#30340;&#32508;&#36848;&#12290;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#28151;&#21512;&#26550;&#26500;&#30340;&#21327;&#21516;&#20316;&#29992;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#26041;&#27861;&#65292;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21457;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#23545;&#26368;&#26032;&#30340;&#28151;&#21512;CNN-ViT&#26550;&#26500;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#35813;&#35843;&#26597;&#30340;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#65306;(1)&#23545;&#26222;&#36890;CNN&#21644;ViT&#30340;&#32972;&#26223;&#20171;&#32461;&#65292;(2)&#31995;&#32479;&#24615;&#22320;&#30740;&#31350;&#19981;&#21516;&#20998;&#31867;&#28151;&#21512;&#35774;&#35745;&#65292;&#25506;&#32034;&#34701;&#21512;CNN&#21644;ViT&#27169;&#22411;&#25152;&#23454;&#29616;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;(3)&#27604;&#36739;&#19981;&#21516;&#28151;&#21512;&#26550;&#26500;&#20043;&#38388;&#30340;&#24212;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#21327;&#21516;&#20316;&#29992;&#21644;&#20998;&#26512;&#65292;(4)&#28151;&#21512;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;(5)&#26368;&#21518;&#65292;&#35843;&#26597;&#24635;&#32467;&#20102;&#20851;&#38190;&#21457;&#29616;&#21644;&#24314;&#35758;&#12290;&#36890;&#36807;&#36825;&#31181;&#23545;&#28151;&#21512;&#35745;&#31639;&#26426;&#35270;&#35273;&#26550;&#26500;&#30340;&#25506;&#32034;&#65292;&#35813;&#35843;&#26597;&#26088;&#22312;&#20316;&#20026;&#19968;&#20010;&#25351;&#21335;&#65292;&#20419;&#36827;&#23545;CNN&#21644;ViT&#20043;&#38388;&#22797;&#26434;&#21160;&#24577;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The hybrid of Convolutional Neural Network (CNN) and Vision Transformers (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV). This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid CNN-ViT architectures, exploring the synergies between these two approaches. The main content of this survey includes: (1) a background on the vanilla CNN and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging CNNs and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and recommendations. Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between CNNs and V
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#20010;&#20351;&#29992;LSTM&#30340;&#33258;&#21160;&#21270;&#28798;&#38590;&#24674;&#22797;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#24555;&#36895;&#26816;&#27979;&#28798;&#38590;&#24182;&#22312;15&#31186;&#20869;&#33258;&#21160;&#20174;&#21478;&#19968;&#20010;Kubernetes&#38598;&#32676;&#24674;&#22797;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#20113;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#24674;&#22797;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02938</link><description>&lt;p&gt;
&#20351;&#29992;LSTM&#35774;&#35745;&#19982;&#23454;&#29616;Kubernetes&#38598;&#32676;&#30340;&#33258;&#21160;&#21270;&#28798;&#38590;&#24674;&#22797;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#20010;&#20351;&#29992;LSTM&#30340;&#33258;&#21160;&#21270;&#28798;&#38590;&#24674;&#22797;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#24555;&#36895;&#26816;&#27979;&#28798;&#38590;&#24182;&#22312;15&#31186;&#20869;&#33258;&#21160;&#20174;&#21478;&#19968;&#20010;Kubernetes&#38598;&#32676;&#24674;&#22797;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#20113;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#24674;&#22797;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#21830;&#19994;&#29615;&#22659;&#20013;&#25968;&#25454;&#30340;&#26085;&#30410;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#20445;&#25252;&#31574;&#30053;&#27491;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#22312;&#20113;&#29615;&#22659;&#20013;&#65292;&#25968;&#25454;&#20445;&#25252;&#23545;&#20110;&#32500;&#25252;&#20449;&#24687;&#36164;&#20135;&#21644;&#20445;&#25345;&#21487;&#25345;&#32493;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#32467;&#26500;&#65292;&#23558;Kubernetes&#31649;&#29702;&#24179;&#21488;&#19982;&#22791;&#20221;&#21644;&#24674;&#22797;&#24037;&#20855;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#31435;&#21363;&#26816;&#27979;&#28798;&#38590;&#65292;&#24182;&#33258;&#21160;&#20174;&#21478;&#19968;&#20010;Kubernetes&#38598;&#32676;&#24674;&#22797;&#24212;&#29992;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;15&#31186;&#20869;&#25191;&#34892;&#24674;&#22797;&#36807;&#31243;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#24674;&#22797;&#12290;&#36825;&#20174;&#26681;&#26412;&#19978;&#20943;&#23569;&#20102;&#19982;&#25163;&#21160;&#24674;&#22797;&#36807;&#31243;&#30456;&#27604;&#30340;&#28508;&#22312;&#24310;&#36831;&#21644;&#38169;&#35823;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20113;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#24674;&#22797;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27169;&#22411;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#39044;&#27979;&#20102;&#38598;&#32676;&#30340;CPU&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing importance of data in the modern business environment, effective data man-agement and protection strategies are gaining increasing research attention. Data protection in a cloud environment is crucial for safeguarding information assets and maintaining sustainable services. This study introduces a system structure that integrates Kubernetes management plat-forms with backup and restoration tools. This system is designed to immediately detect disasters and automatically recover applications from another kubernetes cluster. The experimental results show that this system executes the restoration process within 15 s without human intervention, enabling rapid recovery. This, in turn, significantly reduces the potential for delays and errors compared with manual recovery processes, thereby enhancing data management and recovery ef-ficiency in cloud environments. Moreover, our research model predicts the CPU utilization of the cluster using Long Short-Term Memory (LSTM). T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26223;&#22270;&#20687;&#20462;&#22797;&#26694;&#26550;&#65292;&#37319;&#29992;&#38376;&#25511;&#21367;&#31215;&#26469;&#21306;&#20998;&#26377;&#25928;&#20687;&#32032;&#21644;&#26080;&#25928;&#20687;&#32032;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#37325;&#24314;&#25439;&#22833;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#25214;&#21040;&#26368;&#36866;&#21512;&#20462;&#22797;&#32570;&#22833;&#21306;&#22495;&#30340;&#21442;&#32771;&#34917;&#19969;&#12290;</title><link>https://arxiv.org/abs/2402.02936</link><description>&lt;p&gt;
&#20855;&#26377;&#38376;&#25511;&#21367;&#31215;&#21644;&#19978;&#19979;&#25991;&#37325;&#24314;&#25439;&#22833;&#30340;&#20840;&#26223;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26223;&#22270;&#20687;&#20462;&#22797;&#26694;&#26550;&#65292;&#37319;&#29992;&#38376;&#25511;&#21367;&#31215;&#26469;&#21306;&#20998;&#26377;&#25928;&#20687;&#32032;&#21644;&#26080;&#25928;&#20687;&#32032;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#37325;&#24314;&#25439;&#22833;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#25214;&#21040;&#26368;&#36866;&#21512;&#20462;&#22797;&#32570;&#22833;&#21306;&#22495;&#30340;&#21442;&#32771;&#34917;&#19969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20840;&#26223;&#22270;&#20687;&#20462;&#34917;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24456;&#38590;&#21306;&#20998;&#26377;&#25928;&#20687;&#32032;&#21644;&#26080;&#25928;&#20687;&#32032;&#65292;&#24182;&#25214;&#21040;&#21512;&#36866;&#30340;&#21442;&#32771;&#21306;&#22495;&#26469;&#20462;&#34917;&#21463;&#25439;&#21306;&#22495;&#65292;&#20174;&#32780;&#23548;&#33268;&#20462;&#22797;&#32467;&#26524;&#20013;&#20986;&#29616;&#20266;&#24433;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26223;&#22270;&#20687;&#20462;&#22797;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;Face&#29983;&#25104;&#22120;&#65292;&#19968;&#20010;Cube&#29983;&#25104;&#22120;&#65292;&#19968;&#20010;&#20391;&#20998;&#25903;&#21644;&#20004;&#20010;&#21028;&#21035;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#31435;&#26041;&#20307;&#26144;&#23556;&#65288;CMP&#65289;&#26684;&#24335;&#20316;&#20026;&#32593;&#32476;&#36755;&#20837;&#12290;&#29983;&#25104;&#22120;&#37319;&#29992;&#38376;&#25511;&#21367;&#31215;&#26469;&#21306;&#20998;&#26377;&#25928;&#20687;&#32032;&#21644;&#26080;&#25928;&#20687;&#32032;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#20010;&#20391;&#20998;&#25903;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#37325;&#24314;&#65288;CR&#65289;&#25439;&#22833;&#26469;&#25351;&#23548;&#29983;&#25104;&#22120;&#25214;&#21040;&#26368;&#36866;&#21512;&#20462;&#22797;&#32570;&#22833;&#21306;&#22495;&#30340;&#21442;&#32771;&#34917;&#19969;&#12290;&#22312;PSNR&#21644;SSIM&#25351;&#26631;&#19979;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;SUN360&#34903;&#26223;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate tha
&lt;/p&gt;</description></item><item><title>InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.02933</link><description>&lt;p&gt;
InterpretCC: &#36866;&#20110;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02933
&lt;/p&gt;
&lt;p&gt;
InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#35299;&#37322;&#24615;&#22312;&#19977;&#20010;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;1&#65289;&#38656;&#35201;&#20154;&#31867;&#20449;&#20219;&#35299;&#37322;&#30340;&#36817;&#20284;&#65288;&#20363;&#22914;&#20107;&#21518;&#26041;&#27861;&#65289;&#65307;2&#65289;&#21066;&#24369;&#20102;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#65288;&#20363;&#22914;&#33258;&#21160;&#35782;&#21035;&#30340;&#29305;&#24449;&#25513;&#30721;&#65289;&#65307;3&#65289;&#21066;&#24369;&#20102;&#27169;&#22411;&#24615;&#33021;&#65288;&#20363;&#22914;&#20915;&#31574;&#26641;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#23545;&#20110;&#38754;&#21521;&#20154;&#31867;&#30340;&#39046;&#22495;&#65288;&#22914;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#25110;&#33258;&#28982;&#35821;&#35328;&#65289;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#20449;&#30340;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InterpretCC&#65288;&#21487;&#35299;&#37322;&#30340;&#26465;&#20214;&#35745;&#31639;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20043;&#21069;&#33258;&#36866;&#24212;&#21644;&#31232;&#30095;&#22320;&#28608;&#27963;&#29305;&#24449;&#65292;&#30830;&#20445;&#20154;&#31867;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#20026;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#20801;&#35768;&#20154;&#20204;&#31163;&#25955;&#22320;&#25351;&#23450;&#20852;&#36259;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separate
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#23545;&#24403;&#21069;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32858;&#31867;&#21644;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23558;&#22810;&#35821;&#20041;&#25628;&#32034;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02932</link><description>&lt;p&gt;
&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;- &#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation of Multilingual Semantic Search - Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02932
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#23545;&#24403;&#21069;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32858;&#31867;&#21644;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23558;&#22810;&#35821;&#20041;&#25628;&#32034;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#27010;&#36848;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#36827;&#34892;&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#24403;&#21069;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#26469;&#23545;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32858;&#31867;&#65292;&#22522;&#20110;&#23494;&#38598;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#37096;&#20998;&#65292;&#24182;&#27880;&#37325;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#23427;&#20204;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23558;&#22810;&#35821;&#20041;&#25628;&#32034;&#19982;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This literature review gives an overview of current approaches to perform domain adaptation in a low-resource and approaches to perform multilingual semantic search in a low-resource setting. We developed a new typology to cluster domain adaptation approaches based on the part of dense textual information retrieval systems, which they adapt, focusing on how to combine them efficiently. We also explore the possibilities of combining multilingual semantic search with domain adaptation approaches for dense retrievers in a low-resource setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#31163;&#25955;&#36951;&#20256;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30828;&#20214;&#36817;&#20284;&#30340;&#25928;&#30410;&#65292;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#38754;&#31215;&#21644;&#21151;&#32791;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02930</link><description>&lt;p&gt;
&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#31163;&#25955;&#22522;&#22240;&#35757;&#32451;&#20013;&#20197;&#29992;&#20110;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Embedding Hardware Approximations in Discrete Genetic-based Training for Printed MLPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#31163;&#25955;&#36951;&#20256;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30828;&#20214;&#36817;&#20284;&#30340;&#25928;&#30410;&#65292;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#38754;&#31215;&#21644;&#21151;&#32791;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#21047;&#30005;&#23376;&#26159;&#19968;&#31181;&#26377;&#30528;&#20302;&#25104;&#26412;&#21644;&#28789;&#27963;&#21046;&#36896;&#31561;&#29420;&#29305;&#29305;&#28857;&#30340;&#26377;&#26395;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#12290;&#19982;&#20256;&#32479;&#30340;&#30789;&#22522;&#25216;&#26415;&#19981;&#21516;&#65292;&#21360;&#21047;&#30005;&#23376;&#21487;&#20197;&#23454;&#29616;&#21487;&#20280;&#32553;&#12289;&#21487;&#36866;&#24212;&#12289;&#38750;&#27602;&#24615;&#30340;&#30828;&#20214;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21360;&#21047;&#30005;&#23376;&#30340;&#29305;&#24615;&#23610;&#23544;&#36739;&#22823;&#65292;&#35201;&#23454;&#29616;&#22797;&#26434;&#30340;&#30005;&#36335;&#22914;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#20284;&#35745;&#31639;&#34987;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#30005;&#36335;&#65288;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#30340;&#30828;&#20214;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26469;&#26368;&#22823;&#21270;&#36817;&#20284;&#35745;&#31639;&#30340;&#30410;&#22788;&#12290;&#30001;&#20110;&#30828;&#20214;&#36817;&#20284;&#30340;&#31163;&#25955;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#30828;&#20214;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#35774;&#35745;&#12290;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#25105;&#20204;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#22312;&#38754;&#31215;&#21644;&#21151;&#32791;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Printed Electronics (PE) stands out as a promisingtechnology for widespread computing due to its distinct attributes, such as low costs and flexible manufacturing. Unlike traditional silicon-based technologies, PE enables stretchable, conformal,and non-toxic hardware. However, PE are constrained by larger feature sizes, making it challenging to implement complex circuits such as machine learning (ML) classifiers. Approximate computing has been proven to reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs). In this paper, we maximize the benefits of approximate computing by integrating hardware approximation into the MLP training process. Due to the discrete nature of hardware approximation, we propose and implement a genetic-based, approximate, hardware-aware training approach specifically designed for printed MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction compared to the baseline while outperforming state of-the-art approximate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21382;&#21490;&#39134;&#26426;&#30340;XXL-CT&#23454;&#20363;&#20998;&#21106;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25506;&#32034;&#20102;&#33258;&#21160;&#25110;&#20132;&#20114;&#24335;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#22312;&#26377;&#25928;&#21246;&#21202;&#39134;&#26426;&#32452;&#20214;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02928</link><description>&lt;p&gt;
&#21382;&#21490;&#39134;&#26426;&#30340;XXL-CT&#23454;&#20363;&#20998;&#21106;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Instance Segmentation XXL-CT Challenge of a Historic Airplane
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02928
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21382;&#21490;&#39134;&#26426;&#30340;XXL-CT&#23454;&#20363;&#20998;&#21106;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25506;&#32034;&#20102;&#33258;&#21160;&#25110;&#20132;&#20114;&#24335;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#22312;&#26377;&#25928;&#21246;&#21202;&#39134;&#26426;&#32452;&#20214;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#25439;&#26816;&#27979;&#20013;&#65292;&#22797;&#21512;&#23545;&#35937;&#30340;XXL-CT&#25104;&#20687;&#23454;&#20363;&#20998;&#21106;&#38754;&#20020;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#26469;&#28304;&#20110;&#32570;&#20047;&#24050;&#30693;&#30340;&#21442;&#32771;&#20998;&#21106;&#26631;&#31614;&#12289;&#36866;&#29992;&#30340;&#20998;&#21106;&#24037;&#20855;&#26377;&#38480;&#20197;&#21450;&#37096;&#20998;&#36864;&#21270;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36827;&#34892;&#20102;&#8220;&#21382;&#21490;&#39134;&#26426;&#30340;XXL-CT&#23454;&#20363;&#20998;&#21106;&#25361;&#25112;&#8221;&#12290;&#35813;&#25361;&#25112;&#26088;&#22312;&#25506;&#32034;&#33258;&#21160;&#25110;&#20132;&#20114;&#24335;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#21246;&#21202;&#20986;&#39134;&#26426;&#30340;&#19981;&#21516;&#32452;&#20214;&#65292;&#20363;&#22914;&#34746;&#38025;&#12289;&#38086;&#38025;&#12289;&#37329;&#23646;&#26495;&#25110;&#21387;&#21147;&#31649;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20010;&#25361;&#25112;&#30340;&#32452;&#32455;&#21644;&#32467;&#26524;&#65292;&#24182;&#25551;&#36848;&#20102;&#25552;&#20132;&#30340;&#20998;&#21106;&#26041;&#27861;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation of compound objects in XXL-CT imagery poses a unique challenge in non-destructive testing. This complexity arises from the lack of known reference segmentation labels, limited applicable segmentation tools, as well as partially degraded image quality. To asses recent advancements in the field of machine learning-based image segmentation, the "Instance Segmentation XXL-CT Challenge of a Historic Airplane" was conducted. The challenge aimed to explore automatic or interactive instance segmentation methods for an efficient delineation of the different aircraft components, such as screws, rivets, metal sheets or pressure tubes. We report the organization and outcome of this challenge and describe the capabilities and limitations of the submitted segmentation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#30417;&#30563;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#38543;&#30528;&#36827;&#19968;&#27493;&#22686;&#21152;&#30417;&#30563;&#32780;&#31283;&#23450;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25509;&#21463;&#22810;&#20010;&#24207;&#21015;&#23545;&#40784;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02926</link><description>&lt;p&gt;
&#29992;&#20855;&#26377;&#21516;&#28304;&#36716;&#25442;&#22120;&#30340;&#30417;&#30563;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#30417;&#30563;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#38543;&#30528;&#36827;&#19968;&#27493;&#22686;&#21152;&#30417;&#30563;&#32780;&#31283;&#23450;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25509;&#21463;&#22810;&#20010;&#24207;&#21015;&#23545;&#40784;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#35821;&#35328;&#23398;&#20013;&#65292;&#35782;&#21035;&#30456;&#20851;&#35821;&#35328;&#20013;&#30340;&#21516;&#28304;&#35789;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#33258;&#21160;&#21516;&#28304;&#35782;&#21035;&#23545;&#20110;&#35782;&#21035;&#38899;&#20301;&#23545;&#24212;&#20851;&#31995;&#12289;&#21407;&#22987;&#35821;&#35328;&#37325;&#24314;&#12289;&#35821;&#31995;&#20998;&#31867;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#24110;&#21161;&#12290;&#20197;&#24448;&#21516;&#28304;&#35782;&#21035;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;&#36328;&#22810;&#35821;&#35328;&#35789;&#34920;&#35745;&#31639;&#30340;&#38899;&#32032;&#20998;&#24067;&#65292;&#23545;&#23450;&#20041;&#21516;&#28304;&#31751;&#20043;&#38388;&#38142;&#25509;&#30340;&#21516;&#28304;&#26631;&#31614;&#30340;&#20351;&#29992;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#35745;&#31639;&#29983;&#29289;&#23398;&#20026;&#28789;&#24863;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;&#12290;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#30417;&#30563;&#19979;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36827;&#19968;&#27493;&#22686;&#21152;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#21033;&#29992;&#26631;&#35760;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25509;&#21463;&#22810;&#20010;&#24207;&#21015;&#23545;&#40784;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification of cognates across related languages is one of the primary problems in historical linguistics. Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc. Previous state-of-the-art methods for cognate identification are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters. In this paper, we present a transformer-based architecture inspired by computational biology for the task of automated cognate detection. Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision, thereby proving the efficacy of utilizing the labeled information. We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#37327;&#35780;&#20272;&#25366;&#25496;&#26368;&#23567;&#30340;&#34892;&#20026;&#27169;&#24335;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20887;&#20313;&#27169;&#24335;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02921</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#37327;&#35780;&#20272;&#25366;&#25496;&#26368;&#23567;&#30340;&#34892;&#20026;&#27169;&#24335;&#38598;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#37327;&#35780;&#20272;&#25366;&#25496;&#26368;&#23567;&#30340;&#34892;&#20026;&#27169;&#24335;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20887;&#20313;&#27169;&#24335;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#25366;&#25496;&#25552;&#20379;&#20102;&#20998;&#26512;&#20449;&#24687;&#31995;&#32479;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#20107;&#20214;&#26085;&#24535;&#30340;&#26041;&#27861;&#12290;&#23427;&#25903;&#25345;&#20174;&#21307;&#30103;&#20445;&#20581;&#12289;&#21046;&#36896;&#19994;&#21040;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#30340;&#36807;&#31243;&#35774;&#35745;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;&#20026;&#20102;&#25506;&#32034;&#20855;&#26377;&#22823;&#37327;&#34892;&#20026;&#21487;&#21464;&#24615;&#30340;&#28789;&#27963;&#27969;&#31243;&#30340;&#35268;&#24459;&#24615;&#65292;&#24314;&#35758;&#25366;&#25496;&#20849;&#21516;&#25551;&#36848;&#24213;&#23618;&#27969;&#31243;&#30340;&#37325;&#22797;&#34892;&#20026;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34892;&#20026;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#27169;&#24335;&#20505;&#36873;&#20154;&#26102;&#21482;&#37319;&#29992;&#20102;&#22686;&#37327;&#35745;&#31639;&#65292;&#20294;&#22312;&#35780;&#20272;&#20854;&#36136;&#37327;&#26102;&#21364;&#27809;&#26377;&#20351;&#29992;&#22686;&#37327;&#35745;&#31639;&#65292;&#22240;&#27492;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#25366;&#25496;&#27169;&#24335;&#30340;&#36807;&#31243;&#20998;&#26512;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#33719;&#24471;&#20102;&#22823;&#37327;&#20887;&#20313;&#27169;&#24335;&#65292;&#23548;&#33268;&#25928;&#26524;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#20998;&#26512;&#22797;&#26434;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Process mining provides methods to analyse event logs generated by information systems during the execution of processes. It thereby supports the design, validation, and execution of processes in domains ranging from healthcare, through manufacturing, to e-commerce. To explore the regularities of flexible processes that show a large behavioral variability, it was suggested to mine recurrent behavioral patterns that jointly describe the underlying process. Existing approaches to behavioral pattern mining, however, suffer from two limitations. First, they show limited scalability as incremental computation is incorporated only in the generation of pattern candidates, but not in the evaluation of their quality. Second, process analysis based on mined patterns shows limited effectiveness due to an overwhelmingly large number of patterns obtained in practical application scenarios, many of which are redundant. In this paper, we address these limitations to facilitate the analysis of complex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.02910</link><description>&lt;p&gt;
DS-MS-TCN: &#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Otago&#36816;&#21160;&#35745;&#21010;&#26159;&#38024;&#23545;&#32769;&#24180;&#20154;&#30340;&#37325;&#35201;&#24247;&#22797;&#20030;&#25514;&#65292;&#26088;&#22312;&#22686;&#24378;&#24179;&#34913;&#21644;&#21147;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#65292;&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#35782;&#21035;Otago&#20307;&#25805;&#21160;&#20316;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#30740;&#31350;&#22312;&#23454;&#39564;&#23460;&#35774;&#32622;&#20013;&#25307;&#21215;&#20102;36&#21517;&#32769;&#24180;&#20154;&#65292;&#24182;&#23545;&#39069;&#22806;&#25307;&#21215;&#30340;7&#21517;&#32769;&#24180;&#20154;&#36827;&#34892;&#20102;&#23478;&#24237;&#35780;&#20272;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;(DS-MS-TCN)&#65292;&#29992;&#20110;&#20004;&#32423;&#24207;&#21015;&#21040;&#24207;&#21015;&#20998;&#31867;&#65292;&#23558;&#20854;&#32435;&#20837;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#27169;&#22411;&#19987;&#27880;&#20110;&#35782;&#21035;&#27599;&#20010;&#20307;&#25805;&#21160;&#20316;&#30340;&#37325;&#22797;&#27425;&#25968;(&#24494;&#26631;&#31614;)&#12290;&#38543;&#21518;&#30340;&#38454;&#27573;&#25193;&#23637;&#20102;&#35782;&#21035;&#33539;&#22260;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23454;&#26102;&#29289;&#32852;&#32593;&#25968;&#25454;&#19982;&#24314;&#31569;&#29289;&#30340;3D&#34920;&#31034;&#38598;&#25104;&#65292;&#26469;&#20419;&#36827;&#28784;&#30418;&#24314;&#27169;&#26469;&#30740;&#31350;&#24314;&#31569;&#30340;&#28909;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.02909</link><description>&lt;p&gt;
&#22810;&#23618;&#20303;&#23429;&#24314;&#31569;&#28909;&#21160;&#21147;&#23398;&#30340;&#28784;&#30418;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;
&lt;/p&gt;
&lt;p&gt;
Digital Twin for Grey Box modeling of Multistory residential building thermal dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23454;&#26102;&#29289;&#32852;&#32593;&#25968;&#25454;&#19982;&#24314;&#31569;&#29289;&#30340;3D&#34920;&#31034;&#38598;&#25104;&#65292;&#26469;&#20419;&#36827;&#28784;&#30418;&#24314;&#27169;&#26469;&#30740;&#31350;&#24314;&#31569;&#30340;&#28909;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#33021;&#25928;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#65292;&#30001;&#20110;&#23545;&#29615;&#22659;&#38382;&#39064;&#21644;&#33021;&#28304;&#29420;&#31435;&#24615;&#30340;&#20851;&#27880;&#65292;&#23427;&#27491;&#22312;&#36805;&#36895;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#21271;&#27431;&#65292;&#20165;&#20379;&#28909;&#33021;&#23601;&#21344;&#21040;&#24635;&#24314;&#31569;&#33021;&#32791;&#30340;70&#65285;&#12290;&#24037;&#19994;4.0&#25216;&#26415;&#65292;&#22914;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#12289;&#20113;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#20197;&#21450;&#39044;&#27979;&#24615;&#21644;&#20027;&#21160;&#24615;&#25968;&#23383;&#23402;&#29983;&#30340;&#21019;&#24314;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#36825;&#20010;&#25968;&#23383;&#12290;&#28982;&#32780;&#65292;&#24314;&#31569;&#30340;&#28909;&#21160;&#21147;&#23398;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#20381;&#36182;&#20110;&#35768;&#22810;&#21464;&#37327;&#12290;&#22240;&#27492;&#65292;&#24120;&#29992;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#30333;&#30418;&#27169;&#22411;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#24182;&#38656;&#35201;&#24191;&#27867;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;&#20027;&#35201;&#20381;&#36182;&#24314;&#31569;&#33021;&#32791;&#25968;&#25454;&#30340;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#22522;&#26412;&#30340;&#27934;&#23519;&#21147;&#65292;&#24182;&#19988;&#38459;&#30861;&#20102;&#20877;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#20197;&#22312;&#38598;&#25104;&#23454;&#26102;&#29289;&#32852;&#32593;&#25968;&#25454;&#19982;&#24314;&#31569;&#29289;&#30340;3D&#34920;&#31034;&#30340;&#21516;&#26102;&#20419;&#36827;&#24314;&#31569;&#28909;&#21160;&#21147;&#23398;&#30340;&#28784;&#30418;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Buildings energy efficiency is a widely researched topic, which is rapidly gaining popularity due to rising environmental concerns and the need for energy independence. In Northern Europe heating energy alone accounts for up to 70 percent of the total building energy consumption. Industry 4.0 technologies such as IoT, big data, cloud computing and machine learning, along with the creation of predictive and proactive digital twins, can help to reduce this number. However, buildings thermal dynamics is a very complex process that depends on many variables. As a result, commonly used physics-based white box models are time-consuming and require vast expertise. On the contrary, black box forecasting models, which rely primarily on building energy consumption data, lack fundamental insights and hinder re-use. In this study we propose an architecture to facilitate grey box modelling of building thermal dynamics while integrating real time IoT data with 3D representation of buildings. The arc
&lt;/p&gt;</description></item><item><title>ViewFusion &#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#21516;&#26102;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#21644;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02906</link><description>&lt;p&gt;
ViewFusion: &#23398;&#20064;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02906
&lt;/p&gt;
&lt;p&gt;
ViewFusion &#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#21516;&#26102;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#21644;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20026;&#26032;&#35270;&#35282;&#21512;&#25104;&#36825;&#20010;&#32769;&#38382;&#39064;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#26041;&#27861;&#21040;&#31471;&#21040;&#31471;&#30340;&#39118;&#26684;&#26550;&#26500;&#12290;&#27599;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#29305;&#23450;&#30340;&#20248;&#21183;&#65292;&#20294;&#20063;&#20855;&#26377;&#29305;&#23450;&#30340;&#36866;&#29992;&#24615;&#38480;&#21046;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;ViewFusion&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;ViewFusion&#21516;&#26102;&#23545;&#22330;&#26223;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#36755;&#20837;&#35270;&#35282;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#35270;&#35282;&#24471;&#21040;&#30340;&#22122;&#22768;&#26799;&#24230;&#19982;&#65288;&#25512;&#26029;&#24471;&#21040;&#30340;&#65289;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30456;&#32467;&#21512;&#65292;&#30830;&#20445;&#23545;&#20110;&#30446;&#26631;&#22330;&#26223;&#30340;&#27599;&#20010;&#21306;&#22495;&#65292;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36755;&#20837;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#20960;&#20010;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#21487;&#35757;&#32451;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#22810;&#20010;&#22330;&#26223;&#21644;&#29289;&#20307;&#31867;&#21035;&#65292;&#65288;2&#65289;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#22320;&#37319;&#29992;&#21487;&#21464;&#25968;&#37327;&#30340;&#26080;&#23039;&#24577;&#35270;&#22270;&#65292;&#65288;3&#65289;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#25104;&#21151;&#22797;&#21046;&#20102;&#20154;&#20307;&#32920;&#37096;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#31283;&#23450;&#32920;&#20851;&#33410;&#36816;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#20307;&#26356;&#39640;&#30340;&#38459;&#25239;&#65292;&#20026;&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#22312;&#31070;&#32463;&#26426;&#26800;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02904</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#20154;&#32920;&#25968;&#23383;&#23402;&#29983;&#20013;&#22797;&#21046;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#25104;&#21151;&#22797;&#21046;&#20102;&#20154;&#20307;&#32920;&#37096;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#31283;&#23450;&#32920;&#20851;&#33410;&#36816;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#20307;&#26356;&#39640;&#30340;&#38459;&#25239;&#65292;&#20026;&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#22312;&#31070;&#32463;&#26426;&#26800;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#20840;&#26032;&#20154;&#20307;&#36816;&#21160;&#20223;&#30495;&#24179;&#21488;MyoSuite&#65292;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#22797;&#21046;&#20102;&#20154;&#20307;&#31070;&#32463;&#26426;&#26800;&#23454;&#39564;&#30340;&#20808;&#39537;&#24615;&#24037;&#20316;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#19978;&#22797;&#21046;&#22810;&#31181;&#31867;&#22411;&#30340;&#20154;&#20307;&#32920;&#37096;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;&#65292;&#23558;&#30001;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25511;&#21046;&#30340;&#32920;&#20851;&#33410;&#36816;&#21160;&#19982;&#23454;&#38469;&#20154;&#20307;&#32920;&#20851;&#33410;&#22312;&#25197;&#30697;&#24178;&#25200;&#23454;&#39564;&#20013;&#35782;&#21035;&#21040;&#30340;&#38459;&#25239;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#31283;&#23450;&#30446;&#26631;&#32920;&#20851;&#33410;&#36816;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#20307;&#26356;&#39640;&#30340;&#32920;&#20851;&#33410;&#38459;&#25239;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#26356;&#30701;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#26356;&#22909;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#22312;&#31070;&#32463;&#26426;&#26800;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#20256;&#32479;&#23454;&#39564;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21021;&#27493;&#20294;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-contro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Tucker&#20998;&#35299;&#30340;&#40657;&#30418;&#36924;&#36817;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#31209;&#20998;&#35299;&#21644;&#25351;&#25968;&#36873;&#25321;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02890</link><description>&lt;p&gt;
&#40657;&#30418;&#36924;&#36817;&#19982;&#20998;&#23618;Tucker&#20998;&#35299;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Black-Box Approximation and Optimization with Hierarchical Tucker Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Tucker&#20998;&#35299;&#30340;&#40657;&#30418;&#36924;&#36817;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#31209;&#20998;&#35299;&#21644;&#25351;&#25968;&#36873;&#25321;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#20998;&#23618;Tucker&#20998;&#35299;&#21644;MaxVol&#25351;&#25968;&#36873;&#25321;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;HTBB&#65292;&#29992;&#20110;&#22810;&#32500;&#40657;&#30418;&#36924;&#36817;&#21644;&#26080;&#26799;&#24230;&#20248;&#21270;&#12290;14&#20010;&#22797;&#26434;&#27169;&#22411;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#39640;&#36798;1000&#26102;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#27604;&#20256;&#32479;&#30340;&#26080;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#21644;&#22522;&#20110;&#27969;&#34892;&#30340;&#24352;&#37327;&#32593;&#32476;&#30340;&#36924;&#36817;&#21644;&#20248;&#21270;&#26041;&#27861;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new method HTBB for the multidimensional black-box approximation and gradient-free optimization, which is based on the low-rank hierarchical Tucker decomposition with the use of the MaxVol indices selection procedure. Numerical experiments for 14 complex model problems demonstrate the robustness of the proposed method for dimensions up to 1000, while it shows significantly more accurate results than classical gradient-free optimization methods, as well as approximation and optimization methods based on the popular tensor train decomposition, which represents a simpler case of a tensor network.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36890;&#29992;&#38899;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Federated SSL&#26694;&#26550;FASSL&#65292;&#24182;&#21457;&#29616;&#22312;&#38899;&#39057;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#38899;&#39057;F-SSL&#26041;&#27861;&#19982;&#38598;&#20013;&#24335;&#38899;&#39057;-SSL&#26041;&#27861;&#30340;&#24615;&#33021;&#19981;&#30456;&#19978;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.02889</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#36890;&#29992;&#38899;&#39057;&#29702;&#35299;&#30340;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36890;&#29992;&#38899;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Federated SSL&#26694;&#26550;FASSL&#65292;&#24182;&#21457;&#29616;&#22312;&#38899;&#39057;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#38899;&#39057;F-SSL&#26041;&#27861;&#19982;&#38598;&#20013;&#24335;&#38899;&#39057;-SSL&#26041;&#27861;&#30340;&#24615;&#33021;&#19981;&#30456;&#19978;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#25972;&#21512;&#20026;&#21033;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#36890;&#29992;&#38899;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#29420;&#29305;&#32780;&#21327;&#21516;&#30340;&#32452;&#21512;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#38899;&#39057;&#28304;&#29983;&#25104;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-iid&#65289;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;SSL&#27169;&#22411;&#22312;FL&#27169;&#24335;&#19979;&#29992;&#20110;&#36890;&#29992;&#38899;&#39057;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#22312;&#20223;&#30495;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;FL&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#29305;&#24449;&#21305;&#37197;&#21644;&#39044;&#27979;&#38899;&#39057;-SSL&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Federated SSL&#65288;F-SSL&#65289;&#26694;&#26550;&#65292;&#21517;&#20026;FASSL&#65292;&#21487;&#20197;&#20174;&#22823;&#35268;&#27169;&#20998;&#25955;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#23398;&#20064;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#65292;&#23384;&#20648;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38899;&#39057;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#38899;&#39057;F-SSL&#26041;&#27861;&#19982;&#38598;&#20013;&#24335;&#38899;&#39057;-SSL&#26041;&#27861;&#30340;&#24615;&#33021;&#19981;&#30456;&#19978;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy. However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources. In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data. We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data. Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#12289;&#20869;&#23384;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24182;&#34892;&#32593;&#32476;&#65292;&#22312;&#19981;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#23545;&#39044;&#35757;&#32451;&#20027;&#24178;&#32593;&#32476;&#30340;&#29305;&#24449;&#36827;&#34892;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#19978;&#30340;&#39640;&#25928;&#12290;&#35813;&#26041;&#27861;&#22312;VTAB&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#26435;&#34913;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02887</link><description>&lt;p&gt;
&#26102;&#38388;&#12289;&#20869;&#23384;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Time-, Memory- and Parameter-Efficient Visual Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02887
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#12289;&#20869;&#23384;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24182;&#34892;&#32593;&#32476;&#65292;&#22312;&#19981;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#23545;&#39044;&#35757;&#32451;&#20027;&#24178;&#32593;&#32476;&#30340;&#29305;&#24449;&#36827;&#34892;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#19978;&#30340;&#39640;&#25928;&#12290;&#35813;&#26041;&#27861;&#22312;VTAB&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#26435;&#34913;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#25928;&#22320;&#23545;&#20854;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36866;&#24212;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#22312;&#35757;&#32451;&#30340;&#21442;&#25968;&#25968;&#37327;&#19978;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20173;&#28982;&#38656;&#35201;&#22312;&#25972;&#20010;&#27169;&#22411;&#20013;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#24182;&#27809;&#26377;&#26126;&#26174;&#38477;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#36890;&#36807;&#20027;&#24178;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24182;&#34892;&#32593;&#32476;&#65292;&#23545;&#20174;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#20027;&#24178;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#25805;&#20316;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#21442;&#25968;&#26041;&#38754;&#39640;&#25928;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20063;&#26159;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#29992;&#30340;VTAB&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#26435;&#34913;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.   We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the train
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#26377;&#23402;&#29983;&#21464;&#21387;&#22120;&#30340;&#36817;&#20284;&#24402;&#22240;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#30041;&#21407;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20934;&#30830;&#24402;&#22240;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36817;&#20284;&#21644;&#20934;&#30830;&#24402;&#22240;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#20851;&#27880;&#65292;&#24182;&#21457;&#29616;&#23402;&#29983;&#21464;&#21387;&#22120;&#20027;&#35201;&#24573;&#30053;&#21542;&#23450;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#21477;&#27861;&#35282;&#33394;&#30340;&#20851;&#27880;&#31243;&#24230;&#65292;&#20197;&#21450;&#22914;&#20309;&#21028;&#26029;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.02883</link><description>&lt;p&gt;
&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#36866;&#29992;&#20110;&#29616;&#26377;&#23402;&#29983;&#21464;&#21387;&#22120;&#30340;&#36817;&#20284;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximate Attributions for Off-the-Shelf Siamese Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02883
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#26377;&#23402;&#29983;&#21464;&#21387;&#22120;&#30340;&#36817;&#20284;&#24402;&#22240;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#30041;&#21407;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20934;&#30830;&#24402;&#22240;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36817;&#20284;&#21644;&#20934;&#30830;&#24402;&#22240;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#20851;&#27880;&#65292;&#24182;&#21457;&#29616;&#23402;&#29983;&#21464;&#21387;&#22120;&#20027;&#35201;&#24573;&#30053;&#21542;&#23450;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#21477;&#27861;&#35282;&#33394;&#30340;&#20851;&#27880;&#31243;&#24230;&#65292;&#20197;&#21450;&#22914;&#20309;&#21028;&#26029;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;: &#23402;&#29983;&#32534;&#30721;&#22120;&#22914;&#21477;&#23376;&#21464;&#25442;&#22120;&#26159;&#30446;&#21069;&#26368;&#19981;&#29702;&#35299;&#30340;&#28145;&#24230;&#27169;&#22411;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#24402;&#22240;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#36825;&#31181;&#27169;&#22411;&#31867;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#22788;&#29702;&#21333;&#20010;&#36755;&#20837;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#23402;&#29983;&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;(Moller&#31561;&#65292;2023)&#12290;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#21644;&#24494;&#35843;&#65292;&#22240;&#27492;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;(i)&#19968;&#31181;&#20855;&#26377;&#20934;&#30830;&#24402;&#22240;&#33021;&#21147;&#19988;&#20445;&#30041;&#21407;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;(ii)&#19968;&#31181;&#35745;&#31639;&#29616;&#26377;&#27169;&#22411;&#36817;&#20284;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24191;&#27867;&#27604;&#36739;&#20102;&#36817;&#20284;&#21644;&#20934;&#30830;&#24402;&#22240;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#20998;&#26512;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#20102;&#23402;&#29983;&#21464;&#21387;&#22120;&#23545;&#21477;&#27861;&#35282;&#33394;&#30340;&#20851;&#27880;&#31243;&#24230;&#65292;&#30830;&#35748;&#23427;&#20204;&#20027;&#35201;&#24573;&#30053;&#21542;&#23450;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#22914;&#20309;&#21028;&#26029;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33033;&#20914;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#24847;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#21487;&#33021;&#22312;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02880</link><description>&lt;p&gt;
&#37322;&#25918;&#33033;&#20914;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33033;&#20914;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#24847;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#21487;&#33021;&#22312;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#38656;&#35201;&#26368;&#20339;&#21033;&#29992;&#26377;&#38480;&#30340;&#37327;&#23376;&#36164;&#28304;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#38376;&#30340; QML &#27169;&#22411;&#23545;&#36719;&#20214;&#24037;&#31243;&#24072;&#24456;&#26041;&#20415;&#65292;&#20294;&#20854;&#34920;&#36798;&#33021;&#21147;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#30456;&#24178;&#26102;&#38388;&#20869;&#20801;&#35768;&#30340;&#30005;&#36335;&#28145;&#24230;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#33033;&#20914;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#30456;&#24178;&#26102;&#38388;&#20869;&#26500;&#24314;&#8220;&#26080;&#38480;&#8221;&#28145;&#24230;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#33021;&#20026;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#37322;&#25918;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#20174;&#37327;&#23376;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#36825;&#20010;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;&#33033;&#20914;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26469;&#33258;&#32534;&#30721;&#36807;&#31243;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#22522;&#20110;&#38376;&#27169;&#22411;&#20013;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#30340;&#36830;&#32493;&#26497;&#38480;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22522;&#30784;&#29289;&#29702;&#31995;&#32479;&#20855;&#26377;&#38598;&#21512;&#21487;&#25511;&#24615;&#30340;&#26465;&#20214;&#19979;&#65292;&#33033;&#20914;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#24847;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#22312;&#36825;&#31181;&#26465;&#20214;&#19979;&#65292;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#33033;&#20914;&#27169;&#22411;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#38376;&#30340;&#27169;&#22411;&#22312;&#29305;&#23450;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum (NISQ) devices requires the optimal utilization of limited quantum resources. The commonly used gate-based QML models are convenient for software engineers, but their expressivity is restricted by the permissible circuit depth within a finite coherence time. In contrast, pulse-based models enable the construction of "infinitely" deep quantum neural networks within the same coherence time, which may unleash greater expressive power for complex learning tasks. In this paper, we investigate this potential from the perspective of quantum control theory. We first indicate that the nonlinearity of pulse-based models comes from the encoding process that can be viewed as the continuous limit of data-reuploading in gate-based models. Subsequently, we prove that the pulse-based model can approximate arbitrary nonlinear functions when the underlying physical system is ensemble controllable. Under this condition, numerical si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.02872</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26159;&#19978;&#19979;&#25991;&#22836;&#37096;&#36827;&#34892;&#24230;&#37327;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#22312;&#27973;&#23618;&#20013;&#65292;&#28436;&#31034;&#30340;&#29305;&#24449;&#34987;&#21512;&#24182;&#21040;&#30456;&#24212;&#30340;&#26631;&#31614;&#20013;&#65292;&#36755;&#20837;&#25991;&#26412;&#30340;&#29305;&#24449;&#34987;&#32858;&#21512;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#12290;&#22312;&#28145;&#23618;&#20013;&#65292;&#19978;&#19979;&#25991;&#22836;&#37096;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#27599;&#20010;&#19978;&#19979;&#25991;&#22836;&#37096;&#20013;&#65292;&#20540;-&#36755;&#20986;&#30697;&#38453;&#25552;&#21462;&#20102;&#26631;&#31614;&#30340;&#29305;&#24449;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#35745;&#31639;&#20102;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#36234;&#22823;&#65292;&#36234;&#22810;&#30340;&#26631;&#31614;&#20449;&#24687;&#34987;&#20256;&#36755;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#21487;&#20197;&#34987;&#35270;&#20026;&#23398;&#20064;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19981;&#24179;&#34913;&#30340;&#26631;&#31614;&#21644;&#28436;&#31034;&#39034;&#24207;&#20250;&#24433;&#21709;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;GPT2&#22823;&#22411;&#12289;Llama 7B&#12289;13B&#21644;30B&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02870</link><description>&lt;p&gt;
&#27809;&#26377;&#35299;&#37322;&#30340;&#32479;&#35745;&#23398;&#65306;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20919;&#38745;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Statistics without Interpretation: A Sober Look at Explainable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02870
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#25991;&#29486;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#19981;&#28165;&#26970;&#25152;&#29992;&#20110;&#20309;&#22788;&#21450;&#20854;&#20351;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#22312;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#28165;&#26224;&#35299;&#37322;&#30340;&#22797;&#26434;&#32479;&#35745;&#26041;&#27861;&#24456;&#21487;&#33021;&#23548;&#33268;&#35299;&#37322;&#30340;&#38169;&#35823;&#65292;&#36825;&#19968;&#20107;&#23454;&#22312;&#25991;&#29486;&#20013;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#35770;&#25991;&#24212;&#26126;&#30830;&#35299;&#37322;&#31639;&#27861;&#30340;&#36755;&#20986;&#22914;&#20309;&#35299;&#37322;&#12290;&#20182;&#20204;&#36824;&#24212;&#28548;&#28165;&#22312;&#32473;&#20986;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22238;&#31572;&#21738;&#20123;&#20851;&#20110;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21738;&#20123;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#23427;&#20204;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#23427;&#36824;&#20381;&#36182;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation. Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature. In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted. They should also clarify what questions about the function can and cannot be answered given the explanations. Our argument is based on the distinction between statistics and their interpretation. It also relies on parallels between explainable machine learning and applied statistics.
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#20250;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#65292;&#30740;&#31350;&#21457;&#29616;&#24120;&#35265;&#19988;&#20855;&#26377;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02868</link><description>&lt;p&gt;
&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26263;&#22320;&#37324;&#26159;&#19968;&#31181;&#36951;&#24536;&#32531;&#35299;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02868
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#20250;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#65292;&#30740;&#31350;&#21457;&#29616;&#24120;&#35265;&#19988;&#20855;&#26377;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#20801;&#35768;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36716;&#31227;&#33021;&#21147;&#65292;&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#23601;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20174;&#21160;&#20316;&#21644;&#35266;&#23519;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#35282;&#24230;&#65292;&#23558;&#32454;&#35843;&#38454;&#27573;&#26410;&#35775;&#38382;&#21040;&#30340;&#19979;&#28216;&#20219;&#21153;&#29366;&#24577;&#23376;&#31354;&#38388;&#20013;&#30340;&#39044;&#35757;&#32451;&#33021;&#21147;&#36951;&#24536;&#38382;&#39064;&#20316;&#20026;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#30340;&#19968;&#20010;&#20855;&#20307;&#21407;&#22240;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#12290;&#27169;&#22411;&#22312;&#36825;&#20010;&#26410;&#35775;&#38382;&#21040;&#30340;&#29366;&#24577;&#23376;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#30001;&#20110;&#39044;&#35757;&#32451;&#20351;&#20854;&#22833;&#21435;&#20102;&#26399;&#26395;&#30340;&#36716;&#31227;&#20248;&#21183;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35813;&#38382;&#39064;&#21457;&#29983;&#30340;&#26465;&#20214;&#65292;&#34920;&#26126;&#23427;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;NetHack&#21644;Montezuma's Revenge&#29615;&#22659;&#36827;&#34892;&#35814;&#32454;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#20805;&#20998;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#20998;&#24067;&#30340;&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.02866</link><description>&lt;p&gt;
&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Normalizing Flows for Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#20998;&#24067;&#30340;&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#27969;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#65288;&#20363;&#22914;&#27491;&#24577;&#65289;&#20998;&#24067;&#30340;&#21452;&#23556;&#26144;&#23556;&#12290;&#19968;&#26086;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#26144;&#23556;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#24322;&#24120;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#37327;&#23376;&#26550;&#26500;&#30340;&#26631;&#20934;&#21270;&#27969;&#65292;&#25551;&#36848;&#20102;&#22914;&#20309;&#24314;&#27169;&#21644;&#20248;&#21270;&#36825;&#26679;&#30340;&#27969;&#65292;&#24182;&#22312;&#31034;&#20363;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#23396;&#31435;&#26862;&#26519;&#12289;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#65288;LOF&#65289;&#25110;&#21333;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23436;&#20840;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Normalizing Flow computes a bijective mapping from an arbitrary distribution to a predefined (e.g. normal) distribution. Such a flow can be used to address different tasks, e.g. anomaly detection, once such a mapping has been learned. In this work we introduce Normalizing Flows for Quantum architectures, describe how to model and optimize such a flow and evaluate our method on example datasets. Our proposed models show competitive performance for anomaly detection compared to classical methods, e.g. based on isolation forests, the local outlier factor (LOF) or single-class SVMs, while being fully executable on a quantum computer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22768;&#23398;&#21644;&#35843;&#21046;&#35889;&#22270;&#30456;&#32467;&#21512;&#30340;&#27880;&#24847;&#21147;LSTM&#31995;&#32479;&#23545;&#35821;&#38899;&#21487;&#25026;&#24615;&#32423;&#21035;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#36880;&#24103;&#35843;&#21046;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#20197;&#21450;&#20004;&#31181;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#27169;&#22411;&#22312;&#21253;&#21547;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#21475;&#21507;&#35821;&#38899;&#30340;UA-Speech&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02865</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#21644;&#35843;&#21046;&#35889;&#22270;&#32467;&#21512;&#30340;&#27880;&#24847;&#21147;LSTM&#31995;&#32479;&#23545;&#35821;&#38899;&#21487;&#25026;&#24615;&#32423;&#21035;&#36827;&#34892;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22768;&#23398;&#21644;&#35843;&#21046;&#35889;&#22270;&#30456;&#32467;&#21512;&#30340;&#27880;&#24847;&#21147;LSTM&#31995;&#32479;&#23545;&#35821;&#38899;&#21487;&#25026;&#24615;&#32423;&#21035;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#36880;&#24103;&#35843;&#21046;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#20197;&#21450;&#20004;&#31181;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#27169;&#22411;&#22312;&#21253;&#21547;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#21475;&#21507;&#35821;&#38899;&#30340;UA-Speech&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21487;&#25026;&#24615;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#22024;&#26434;&#29615;&#22659;&#12289;&#20449;&#36947;&#22833;&#30495;&#25110;&#29983;&#29702;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#21518;&#19968;&#31181;&#24773;&#20917;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#21487;&#25026;&#24615;&#32423;&#21035;&#30340;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#20197;&#21069;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#31532;&#19968;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#36880;&#24103;&#35843;&#21046;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#20013;&#33719;&#21462;&#20002;&#24323;&#37325;&#35201;&#26102;&#24207;&#20449;&#24687;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#31532;&#20108;&#65292;&#25506;&#35752;&#20102;&#23558;&#36880;&#24103;&#22768;&#23398;&#23545;&#25968;&#26757;&#23572;&#21644;&#35843;&#21046;&#35889;&#22270;&#22312;LSTM&#26694;&#26550;&#20013;&#36827;&#34892;&#20915;&#31574;&#32423;&#34701;&#21512;&#25110;&#21518;&#26399;&#34701;&#21512;&#20197;&#21450;&#35805;&#35821;&#32423;&#21152;&#26435;&#27744;&#21270;&#65288;WP&#65289;&#34701;&#21512;&#30340;&#20004;&#31181;&#19981;&#21516;&#31574;&#30053;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21253;&#21547;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#21475;&#21507;&#35821;&#38899;&#30340;UA-Speech&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech intelligibility can be affected by multiple factors, such as noisy environments, channel distortions or physiological issues. In this work, we deal with the problem of automatic prediction of the speech intelligibility level in this latter case. Starting from our previous work, a non-intrusive system based on LSTM networks with attention mechanism designed for this task, we present two main contributions. In the first one, it is proposed the use of per-frame modulation spectrograms as input features, instead of compact representations derived from them that discard important temporal information. In the second one, two different strategies for the combination of per-frame acoustic log-mel and modulation spectrograms into the LSTM framework are explored: at decision level or late fusion and at utterance level or Weighted-Pooling (WP) fusion. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. On the one
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22270;&#31070;&#32463;&#26426;&#22120;&#65288;GNM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#12290;GNM&#20351;&#29992;&#21516;&#27493;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#24182;&#29992;&#20960;&#20046;&#23436;&#20840;&#22270;&#20195;&#26367;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;GNM&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;MLP&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.02862</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#26426;&#22120;&#65306;&#19968;&#31181;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Machine: A New Model for Learning with Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22270;&#31070;&#32463;&#26426;&#22120;&#65288;GNM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#12290;GNM&#20351;&#29992;&#21516;&#27493;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#24182;&#29992;&#20960;&#20046;&#23436;&#20840;&#22270;&#20195;&#26367;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;GNM&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;MLP&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#23558;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22914;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22270;&#12290;&#20107;&#23454;&#19978;&#65292;MLP&#21487;&#20197;&#34920;&#31034;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26368;&#36817;&#24050;&#25104;&#20026;&#22312;&#22270;&#19978;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26631;&#20934;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MLP&#31561;&#20215;&#20110;&#19968;&#20010;&#22522;&#20110;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;MLP&#30340;&#22270;&#34920;&#31034;&#19978;&#25805;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;&#22270;&#31070;&#32463;&#26426;&#22120;&#65288;GNM&#65289;&#65292;&#23427;&#29992;&#19968;&#20010;&#20960;&#20046;&#23436;&#20840;&#22270;&#21462;&#20195;&#20102;MLP&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#24182;&#37319;&#29992;&#21516;&#27493;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#12290;&#25105;&#20204;&#34920;&#26126;&#21333;&#20010;GNM&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#22810;&#20010;MLP&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;GNM&#27169;&#22411;&#20248;&#20110;MLP&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in mapping data from different domains to graph structures. Among others, neural network models such as the multi-layer perceptron (MLP) can be modeled as graphs. In fact, MLPs can be represented as directed acyclic graphs. Graph neural networks (GNNs) have recently become the standard tool for performing machine learning tasks on graphs. In this work, we show that an MLP is equivalent to an asynchronous message passing GNN model which operates on the MLP's graph representation. We then propose a new machine learning model for tabular data, the so-called Graph Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a nearly complete graph and which employs a synchronous message passing scheme. We show that a single GNM model can simulate multiple MLP models. We evaluate the proposed model in several classification and regression datasets. In most cases, the GNM model outperforms the MLP architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#33258;&#22238;&#24402;&#23494;&#24230;&#32593;&#32476;&#21644;&#31070;&#32463;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#20351;&#29992;&#31070;&#32463;&#38598;&#21512;&#30340;&#26222;&#36941;&#35266;&#28857;&#65292;&#24182;&#21457;&#29616;&#21333;&#20010;&#33391;&#22909;&#26657;&#20934;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#23398;&#20064;&#30340;&#38745;&#24577;&#25351;&#26631;&#65292;&#24182;&#24471;&#20986;&#20102;&#20851;&#20110;&#20195;&#29702;&#26368;&#32456;&#24615;&#33021;&#30340;&#37325;&#35201;&#27169;&#22411;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02858</link><description>&lt;p&gt;
&#28145;&#24230;&#33258;&#22238;&#24402;&#23494;&#24230;&#32593;&#32476;&#19982;&#31070;&#32463;&#38598;&#21512;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#33258;&#22238;&#24402;&#23494;&#24230;&#32593;&#32476;&#21644;&#31070;&#32463;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#20351;&#29992;&#31070;&#32463;&#38598;&#21512;&#30340;&#26222;&#36941;&#35266;&#28857;&#65292;&#24182;&#21457;&#29616;&#21333;&#20010;&#33391;&#22909;&#26657;&#20934;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#23398;&#20064;&#30340;&#38745;&#24577;&#25351;&#26631;&#65292;&#24182;&#24471;&#20986;&#20102;&#20851;&#20110;&#20195;&#29702;&#26368;&#32456;&#24615;&#33021;&#30340;&#37325;&#35201;&#27169;&#22411;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20165;&#26377;&#31995;&#32479;&#36716;&#25442;&#38598;&#21512;&#21487;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20174;&#21487;&#29992;&#25968;&#25454;&#20013;&#25512;&#26029;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#22312;&#27169;&#22411;&#25512;&#28436;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#30495;&#23454;&#31995;&#32479;&#19978;&#30340;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#26159;&#20381;&#38752;&#38598;&#21512;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#21551;&#21457;&#24335;&#65292;&#24182;&#36991;&#20813;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#22826;&#22823;&#26102;&#21033;&#29992;&#27169;&#22411;&#12290;&#36890;&#36807;&#23637;&#31034;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#20351;&#29992;&#21333;&#20010;&#33391;&#22909;&#26657;&#20934;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#24517;&#39035;&#20351;&#29992;&#38598;&#21512;&#30340;&#26222;&#36941;&#35266;&#28857;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19982;&#27169;&#22411;&#23398;&#20064;&#26377;&#20851;&#30340;&#38745;&#24577;&#25351;&#26631;&#65292;&#24182;&#24471;&#20986;&#20102;&#20851;&#20110;&#20195;&#29702;&#30340;&#26368;&#32456;&#24615;&#33021;&#30340;&#37325;&#35201;&#27169;&#22411;&#29305;&#24615;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20855;&#26377;&#20559;&#24577;&#26799;&#24230;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;Adagrad&#21644;RMSProp&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#19982;&#26080;&#20559;&#24773;&#20917;&#30456;&#20284;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25910;&#25947;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#38477;&#20302;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02857</link><description>&lt;p&gt;
&#20559;&#24577;&#33258;&#36866;&#24212;&#38543;&#26426;&#36924;&#36817;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20855;&#26377;&#20559;&#24577;&#26799;&#24230;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;Adagrad&#21644;RMSProp&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#19982;&#26080;&#20559;&#24773;&#20917;&#30456;&#20284;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25910;&#25947;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#38477;&#20302;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#27493;&#38271;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#29616;&#22312;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22823;&#22810;&#25968;&#29702;&#35770;&#32467;&#26524;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#26080;&#20559;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#28982;&#32780;&#22312;&#19968;&#20123;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#21364;&#26080;&#27861;&#28385;&#36275;&#36825;&#19968;&#20551;&#35774;&#12290;&#26412;&#25991;&#23545;&#20855;&#26377;&#20559;&#24577;&#26799;&#24230;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38750;&#28176;&#36827;&#24615;&#20998;&#26512;&#65292;&#38024;&#23545;&#20984;&#21644;&#38750;&#20984;&#24179;&#28369;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#26102;&#21464;&#20559;&#24046;&#65292;&#24182;&#24378;&#35843;&#25511;&#21046;&#20559;&#24046;&#21644;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26799;&#24230;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#20559;&#24577;&#26799;&#24230;&#30340;Adagrad&#21644;RMSProp&#31639;&#27861;&#23545;&#20110;&#38750;&#20984;&#24179;&#28369;&#20989;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25991;&#29486;&#20013;&#26080;&#20559;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#30456;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#26041;&#27861;&#38477;&#20302;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#36890;&#36807;&#20174;&#22836;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#31232;&#30095;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02855</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65306;&#39640;&#25928;&#25512;&#33616;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#36890;&#36807;&#20174;&#22836;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#31232;&#30095;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#26085;&#30410;&#22686;&#38271;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#25968;&#37327;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#65292;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#36825;&#20010;&#25361;&#25112;&#20027;&#35201;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#22312;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#33616;&#12290;&#23613;&#31649;&#27169;&#22411;&#21387;&#32553;&#21644;&#26550;&#26500;&#25628;&#32034;&#26041;&#38754;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30528;&#26126;&#26174;&#30340;&#38480;&#21046;&#12290;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#21387;&#32553;&#20013;&#39044;&#35757;&#32451;/&#37325;&#26032;&#35757;&#32451;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#20197;&#21450;&#26550;&#26500;&#35774;&#35745;&#20013;&#24191;&#27867;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#20855;&#26377;&#20005;&#26684;&#26102;&#38388;&#25110;&#31354;&#38388;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#31649;&#29702;&#22797;&#26434;&#24615;&#21644;&#36981;&#23432;&#20869;&#23384;&#38480;&#21046;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#25512;&#33616;&#27169;&#22411;&#12290;DSL&#21019;&#26032;&#24615;&#22320;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#31232;&#30095;&#27169;&#22411;&#65292;&#21608;&#26399;&#30340;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#32452;&#21512;&#36890;&#29992;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.02851</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#22686;&#24378;&#32452;&#21512;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Compositional Generalization via Compositional Feature Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02851
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#32452;&#21512;&#36890;&#29992;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#32463;&#24120;&#38754;&#20020;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#22312;&#24120;&#35265;&#30340;&#22810;&#39046;&#22495;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#65292;&#38543;&#30528;&#31867;&#21035;&#21644;&#39046;&#22495;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24456;&#38590;&#20026;&#27599;&#20010;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20010;&#25361;&#25112;&#33258;&#28982;&#22320;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#32452;&#21512;&#36890;&#29992;&#24615;&#65288;CG&#65289;&#33021;&#21147;&#30340;&#27169;&#22411;&#30340;&#25506;&#32034;&#65292;&#21363;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;&#20026;&#20102;&#28145;&#20837;&#30740;&#31350;CG&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;CG-Bench&#65292;&#36825;&#26159;&#19968;&#22871;&#20174;&#29616;&#26377;&#23454;&#38469;&#22270;&#20687;&#25968;&#25454;&#38598;&#27966;&#29983;&#30340;CG&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#30446;&#21069;&#22312;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;DINOv2&#65289;&#19978;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#36825;&#20010;&#25361;&#25112;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65288;CFA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#24494;&#35843;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#19978;&#23398;&#20064;&#20004;&#20010;&#27491;&#20132;&#32447;&#24615;&#22836;&#37096;&#26469;&#23545;&#40784;&#31867;&#21035;&#21644;&#39046;&#22495;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain lab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#23545;&#25968;&#26757;&#23572;&#39057;&#35889;&#22270;&#26469;&#39044;&#27979;&#35821;&#38899;&#21487;&#25026;&#24615;&#27700;&#24179;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02850</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#31995;&#32479;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#35821;&#38899;&#21487;&#25026;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Attention Long Short-Term Memory based system for automatic classification of speech intelligibility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#23545;&#25968;&#26757;&#23572;&#39057;&#35889;&#22270;&#26469;&#39044;&#27979;&#35821;&#38899;&#21487;&#25026;&#24615;&#27700;&#24179;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21487;&#25026;&#24615;&#21487;&#33021;&#22240;&#22810;&#31181;&#22240;&#32032;&#32780;&#38477;&#20302;&#65292;&#22914;&#22024;&#26434;&#30340;&#29615;&#22659;&#12289;&#25216;&#26415;&#22256;&#38590;&#25110;&#29983;&#29289;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#26080;&#20405;&#20837;&#24615;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#30340;&#35821;&#38899;&#21487;&#25026;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#21644;&#23545;&#25968;&#26757;&#23572;&#39057;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#22522;&#20110;LSTM&#30340;&#31995;&#32479;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#20197;&#30830;&#23450;&#26356;&#30456;&#20851;&#30340;&#24103;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;UA-Speech&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#21475;&#21507;&#35821;&#38899;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27880;&#24847;&#21147;LSTM&#26550;&#26500;&#20248;&#20110;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#30340;&#21442;&#32771;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#22522;&#20110;&#24179;&#22343;&#27744;&#21270;&#30340;LSTM&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech intelligibility can be degraded due to multiple factors, such as noisy environments, technical difficulties or biological conditions. This work is focused on the development of an automatic non-intrusive system for predicting the speech intelligibility level in this latter case. The main contribution of our research on this topic is the use of Long Short-Term Memory (LSTM) networks with log-mel spectrograms as input features for this purpose. In addition, this LSTM-based system is further enhanced by the incorporation of a simple attention mechanism that is able to determine the more relevant frames to this task. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. Results show that the attention LSTM architecture outperforms both, a reference Support Vector Machine (SVM)-based system with hand-crafted features and a LSTM-based system with Mean-Pooling.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#38750;&#26230;&#30789;&#26448;&#26009;&#21046;&#20316;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968; (PUF) &#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#25915;&#20987;&#36825;&#20123;&#38598;&#25104;&#30005;&#36335;PUF&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNNs) &#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20294;&#20173;&#26080;&#27861;&#23436;&#20840;&#30772;&#35299;a-Si PUF&#30340;&#23433;&#20840;&#24615;&#65292;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#38750;&#26230;&#30789;PUFs&#30340;&#26426;&#22120;&#23398;&#20064;&#25239;&#24615;&#19982;&#20854;&#38750;&#32447;&#24615;&#21709;&#24212;&#30340;&#24378;&#24230;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.02846</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25239;&#24615;&#38750;&#26230;&#30789;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968; (PUFs) &#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Resistant Amorphous Silicon Physically Unclonable Functions (PUFs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02846
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#38750;&#26230;&#30789;&#26448;&#26009;&#21046;&#20316;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968; (PUF) &#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#25915;&#20987;&#36825;&#20123;&#38598;&#25104;&#30005;&#36335;PUF&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNNs) &#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20294;&#20173;&#26080;&#27861;&#23436;&#20840;&#30772;&#35299;a-Si PUF&#30340;&#23433;&#20840;&#24615;&#65292;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#38750;&#26230;&#30789;PUFs&#30340;&#26426;&#22120;&#23398;&#20064;&#25239;&#24615;&#19982;&#20854;&#38750;&#32447;&#24615;&#21709;&#24212;&#30340;&#24378;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#27874;&#21160;&#28151;&#27788;&#38750;&#26230;&#30789; (a-Si) &#33108;&#20307;&#20316;&#20026;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968; (PUFs) &#30340;&#24212;&#29992;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#23545;&#38598;&#25104;&#30005;&#23376;PUFs&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#22320;&#23545;PUF&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#24212;&#29992;&#21253;&#25324;&#32447;&#24615;&#22238;&#24402;&#12289;k-&#26368;&#36817;&#37051;&#12289;&#20915;&#31574;&#26641;&#38598;&#25104; (&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26641;) &#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNNs) &#22312;&#20869;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#23545;&#38598;&#25104;&#30340;&#38750;&#26230;&#30789;&#20809;&#23398;PUFs&#36827;&#34892;&#20102;&#25915;&#20987;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#31639;&#27861;&#20013;&#65292;DNNs&#34920;&#29616;&#26368;&#22909;&#65292;&#20294;&#20173;&#26080;&#27861;&#23436;&#20840;&#30772;&#35299;a-Si PUF&#30340;&#23433;&#20840;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31169;&#23494;&#20449;&#24687;&#24230;&#37327;&#23558;&#20854;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#26230;&#30789;PUFs&#30340;&#26426;&#22120;&#23398;&#20064;&#25239;&#24615;&#19982;&#20854;&#38750;&#32447;&#24615;&#21709;&#24212;&#30340;&#24378;&#24230;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate usage of nonlinear wave chaotic amorphous silicon (a-Si) cavities as physically unclonable functions (PUF). Machine learning attacks on integrated electronic PUFs have been demonstrated to be very effective at modeling PUF behavior. Such attacks on integrated a-Si photonic PUFs are investigated through application of algorithms including linear regression, k-nearest neighbor, decision tree ensembles (random forests and gradient boosted trees), and deep neural networks (DNNs). We found that DNNs performed the best among all the algorithms studied but still failed to completely break the a-Si PUF security which we quantify through a private information metric. Furthermore, machine learning resistance of a-Si PUFs were found to be directly related to the strength of their nonlinear response.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20852;&#36259;&#24314;&#27169;&#26694;&#26550;&#8220;Trinity&#8221;&#65292;&#36890;&#36807;&#21033;&#29992;&#38271;&#26399;&#32447;&#32034;&#26469;&#35299;&#20915;&#20852;&#36259;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#22810;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23454;&#26102;&#32858;&#31867;&#31995;&#32479;&#21644;&#35745;&#31639;&#32479;&#35745;&#20852;&#36259;&#30452;&#26041;&#22270;&#65292;Trinity&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.02842</link><description>&lt;p&gt;
Trinity&#65306;&#23558;&#22810;/&#23567;&#20247;/&#38271;&#26399;&#20852;&#36259;&#25972;&#21512;&#20026;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20852;&#36259;&#24314;&#27169;&#26694;&#26550;&#8220;Trinity&#8221;&#65292;&#36890;&#36807;&#21033;&#29992;&#38271;&#26399;&#32447;&#32034;&#26469;&#35299;&#20915;&#20852;&#36259;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#22810;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23454;&#26102;&#32858;&#31867;&#31995;&#32479;&#21644;&#35745;&#31639;&#32479;&#35745;&#20852;&#36259;&#30452;&#26041;&#22270;&#65292;Trinity&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20852;&#36259;&#24314;&#27169;&#19968;&#30452;&#26159;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#35768;&#22810;&#29616;&#26377;&#24037;&#20316;&#24050;&#32463;&#30740;&#31350;&#20102;&#20856;&#22411;&#30340;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#65288;&#20363;&#22914;&#22810;&#20852;&#36259;&#12289;&#23567;&#20247;&#20852;&#36259;&#21644;&#38271;&#26399;&#20852;&#36259;&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#21482;&#32771;&#34385;&#20102;&#20854;&#20013;&#19968;&#20010;&#20852;&#36259;&#65292;&#24182;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#20219;&#21153;&#38754;&#20020;&#20849;&#21516;&#30340;&#8220;&#20852;&#36259;&#36951;&#24536;&#8221;&#38382;&#39064;&#65292;&#32780;&#19988;&#23384;&#22312;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#38271;&#26399;&#32447;&#32034;&#21487;&#20197;&#25104;&#20026;&#22522;&#30707;&#65292;&#22240;&#20026;&#23427;&#20204;&#25581;&#31034;&#20102;&#22810;&#31181;&#20852;&#36259;&#24182;&#28548;&#28165;&#20102;&#23567;&#20247;&#20852;&#36259;&#12290;&#21463;&#21040;&#36825;&#20010;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26816;&#32034;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#8220;Trinity&#8221;&#65292;&#26469;&#35299;&#20915;&#20852;&#36259;&#36951;&#24536;&#38382;&#39064;&#24182;&#25913;&#21892;&#22810;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#25645;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#32858;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#29289;&#21697;&#25237;&#24433;&#21040;&#21487;&#26522;&#20030;&#30340;&#31751;&#20013;&#65292;&#24182;&#22312;&#36825;&#20123;&#31751;&#19978;&#35745;&#31639;&#32479;&#35745;&#20852;&#36259;&#30452;&#26041;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#30452;&#26041;&#22270;&#65292;Trinity&#21487;&#20197;&#35782;&#21035;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common "interest amnesia" problem, and a solution exists to mitigate it simultaneously. We figure that long-term cues can be the cornerstone since they reveal multi-interest and clarify long-tail interest. Inspired by the observation, we propose a novel and unified framework in the retrieval stage, "Trinity", to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31616;&#21333;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#26465;&#20214;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#37096;&#32626;&#22312;&#26412;&#22320;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;LLMs&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.02834</link><description>&lt;p&gt;
&#31616;&#21270;&#30340;LLaMA: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#28145;&#24230;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Shortened LLaMA: A Simple Depth Pruning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02834
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31616;&#21333;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#26465;&#20214;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#37096;&#32626;&#22312;&#26412;&#22320;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;LLMs&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32467;&#26500;&#21270;&#20462;&#21098;&#24050;&#25104;&#20026;&#38477;&#20302;&#20854;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#23485;&#24230;&#20462;&#21098;&#20943;&#23567;&#25237;&#24433;&#26435;&#37325;&#30697;&#38453;&#30340;&#22823;&#23567; (&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#27880;&#24847;&#21147;&#22836;)&#65292;&#21516;&#26102;&#20445;&#25345;&#23618;&#25968;&#19981;&#21464;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#28145;&#24230;&#20462;&#21098;&#21017;&#21024;&#38500;&#25972;&#20010;&#23618;&#25110;&#22359;&#65292;&#21516;&#26102;&#20445;&#25345;&#21097;&#20313;&#26435;&#37325;&#30340;&#22823;&#23567;&#19981;&#21464;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#23485;&#24230;&#20462;&#21098;&#25110;&#23485;&#24230;&#21644;&#28145;&#24230;&#20462;&#21098;&#30340;&#28151;&#21512;&#19978;&#65292;&#24456;&#23569;&#23545;&#20004;&#32773; (&#23485;&#24230;&#19982;&#28145;&#24230;) &#22312;&#23545;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#26032;&#30340;&#23485;&#24230;&#20462;&#21098;&#26041;&#27861;&#22312;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#20462;&#21098;&#26041;&#27861;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#23545;&#36816;&#34892;LLMs&#36827;&#34892;&#26377;&#38480;&#25209;&#27425;&#22823;&#23567;&#30340;&#26465;&#20214;&#65292;&#27492;&#26102;&#23485;&#24230;&#20462;&#21098;&#26080;&#25928;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#24110;&#21161;&#23558;LLMs&#37096;&#32626;&#22312;&#26412;&#22320;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.
&lt;/p&gt;</description></item><item><title>PowerGraph&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.02827</link><description>&lt;p&gt;
PowerGraph: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PowerGraph: A power grid benchmark dataset for graph neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02827
&lt;/p&gt;
&lt;p&gt;
PowerGraph&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#20351;&#29992;GNN&#65292;&#24182;&#22686;&#24378;GNN&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#20013;&#32570;&#20047;&#29992;&#20110;GNN&#24212;&#29992;&#30340;&#30005;&#21147;&#32593;&#26684;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;GNN&#21487;&#20197;&#28508;&#22312;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#30005;&#21147;&#32593;&#26684;&#29616;&#35937;&#12290;&#30005;&#21147;&#32593;&#26684;&#26159;&#22797;&#26434;&#30340;&#24037;&#31243;&#32593;&#32476;&#65292;&#22825;&#28982;&#36866;&#21512;&#20110;&#22270;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;GNN&#26377;&#28508;&#21147;&#25429;&#25417;&#21040;&#30005;&#21147;&#32593;&#26684;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#29992;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#32423;&#32852;&#25925;&#38556;&#20107;&#20214;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#23548;&#33268;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#21382;&#21490;&#26029;&#30005;&#25968;&#25454;&#38598;&#31232;&#32570;&#19988;&#19981;&#23436;&#25972;&#12290;&#36890;&#24120;&#36890;&#36807;&#35745;&#31639;&#26114;&#36149;&#30340;&#31163;&#32447;&#32423;&#32852;&#25925;&#38556;&#27169;&#25311;&#26469;&#35780;&#20272;&#33030;&#24369;&#24615;&#21644;&#35782;&#21035;&#20851;&#38190;&#32452;&#20214;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for GNN applications. Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to graph representations. Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures. Instead, we propose using machine learning models for the online detection of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#20083;&#22836;&#30244;&#30149;&#27602;&#29983;&#27542;&#22120;&#30115;&#12290;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02826</link><description>&lt;p&gt;
SynthVision - &#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#20135;&#20986;&#30340;&#26368;&#23567;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#20083;&#22836;&#30244;&#30149;&#27602;&#29983;&#27542;&#22120;&#30115;&#12290;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#23545;&#32039;&#24613;&#21307;&#30103;&#21361;&#26426;&#65288;&#22914;&#27969;&#34892;&#30149;&#25110;&#29983;&#29289;&#24656;&#24598;&#20027;&#20041;&#20107;&#20214;&#65289;&#26102;&#65292;&#24555;&#36895;&#24320;&#21457;&#30142;&#30149;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#20110;&#36825;&#20123;&#22330;&#26223;&#26469;&#35828;&#22826;&#24930;&#20102;&#65292;&#38656;&#35201;&#21019;&#26032;&#26041;&#27861;&#20174;&#24456;&#23569;&#30340;&#25968;&#25454;&#20013;&#24555;&#36895;&#29983;&#25104;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26469;&#26816;&#27979;&#20154;&#31867;&#20083;&#22836;&#30244;&#30149;&#27602;&#29983;&#27542;&#22120;&#30115;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#29992;&#20110;&#20174;10&#24352;HPV&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#22823;&#37327;&#22810;&#26679;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#19987;&#27880;&#20110;&#20934;&#30830;&#25551;&#32472;&#29983;&#27542;&#22120;&#30115;&#12290;&#31532;&#20108;&#38454;&#27573;&#28041;&#21450;&#20351;&#29992;&#35813;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#25193;&#25955;&#27169;&#22411;&#22312;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02823</link><description>&lt;p&gt;
&#36867;&#36991;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#65288;&#22826;&#65289;&#23481;&#26131;
&lt;/p&gt;
&lt;p&gt;
Evading Data Contamination Detection for Language Models is (too) Easy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#65292;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#32463;&#24120;&#25351;&#23548;&#29992;&#25143;&#23545;&#19968;&#20010;&#27169;&#22411;&#19982;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#35757;&#32451;&#30340;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#19982;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#21457;&#29983;&#27745;&#26579;&#65292;&#20174;&#32780;&#25439;&#23475;&#24615;&#33021;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#24320;&#21457;&#20102;&#19968;&#20123;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#26377;&#24847;&#36827;&#34892;&#27745;&#26579;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24773;&#20917;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#23545;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#21487;&#20449;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#26356;&#20005;&#26684;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#36825;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#28431;&#27934;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;EAL&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65292;&#26126;&#26174;&#25552;&#39640;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#65292;&#24182;&#23436;&#20840;&#36867;&#36991;&#20102;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FCVAE&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#38598;&#25104;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#29305;&#24449;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#37325;&#26500;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.02820</link><description>&lt;p&gt;
&#37325;&#35775;VAE&#65306;&#19968;&#31181;&#22522;&#20110;&#39057;&#29575;&#35270;&#35282;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FCVAE&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#38598;&#25104;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#29305;&#24449;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#37325;&#26500;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22312;Web&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#21508;&#31181;Web&#31995;&#32479;&#20381;&#36182;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23454;&#26102;&#30417;&#27979;&#21644;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#21551;&#21160;&#35786;&#26029;&#21644;&#20462;&#22797;&#31243;&#24207;&#12290;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#38477;&#22122;&#33021;&#21147;&#65292;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36825;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;VAE&#30340;&#26041;&#27861;&#22312;&#21516;&#26102;&#25429;&#25417;&#38271;&#21608;&#26399;&#24322;&#36136;&#27169;&#24335;&#21644;&#35814;&#32454;&#30701;&#21608;&#26399;&#36235;&#21183;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;&#39057;&#29575;&#22686;&#24378;&#30340;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;FCVAE&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;FCVAE&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#29305;&#24449;&#21516;&#26102;&#38598;&#25104;&#21040;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#26465;&#20214;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#37325;&#26500;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26469;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#24182;&#25214;&#21040;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#21644;&#24120;&#35265;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.02817</link><description>&lt;p&gt;
&#22522;&#20110;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;&#32447;&#24615;&#24046;&#24322;&#32422;&#26463;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26469;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#24182;&#25214;&#21040;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#21644;&#24120;&#35265;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#23427;&#20204;&#26159;&#27010;&#29575;&#20998;&#31867;&#22120;&#30340;&#32447;&#24615;&#20989;&#25968;&#65307;&#20197;&#21450;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#65292;&#23427;&#20204;&#22312;&#32676;&#20307;&#22238;&#24402;&#20989;&#25968;&#26041;&#38754;&#20063;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#24046;&#24322;&#24230;&#37327;&#65288;&#22914;&#20154;&#21475;&#24179;&#31561;&#12289;&#26426;&#20250;&#24179;&#31561;&#21644;&#39044;&#27979;&#24179;&#31561;&#65289;&#37117;&#26159;&#21452;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25581;&#31034;&#19982;Neyman-Pearson&#24341;&#29702;&#30340;&#36830;&#25509;&#65292;&#25214;&#21040;&#20102;&#22312;&#21333;&#19968;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#23545;&#20110;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#65292;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#21464;&#25104;&#20102;&#32676;&#20307;&#38408;&#20540;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#65288;&#22914;&#24179;&#31561;&#30340;&#20960;&#29575;&#65289;&#21644;&#21463;&#20445;&#25252;&#23646;&#24615;&#24120;&#35265;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. We introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02816</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intersectional Two-sided Fairness in Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26681;&#25454;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21487;&#20998;&#20026;&#29992;&#25143;&#20844;&#24179;&#24615;&#12289;&#29289;&#21697;&#20844;&#24179;&#24615;&#21644;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#21644;&#29289;&#21697;&#20844;&#24179;&#24615;&#30340;&#21452;&#36793;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#21363;&#20351;&#25512;&#33616;&#31995;&#32479;&#26159;&#21452;&#36793;&#20844;&#24179;&#30340;&#65292;&#20132;&#21449;&#21452;&#36793;&#19981;&#20844;&#24179;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#65292;&#36825;&#22312;&#26412;&#25991;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35266;&#23519;&#21644;&#23637;&#31034;&#65292;&#24182;&#19988;&#20197;&#21069;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#26469;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#26469;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#35843;&#25972;&#27491;&#38754;&#39044;&#27979;&#24471;&#20998;&#65292;&#20197;&#20844;&#24179;&#22320;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#22478;&#24066;&#22270;&#30340;&#19981;&#21516;&#37325;&#24314;&#26041;&#27861;&#65292;&#21253;&#25324;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#12289;&#29289;&#29702;&#39537;&#21160;&#21644;&#28151;&#21512;&#22411;&#65292;&#20197;&#21450;&#32467;&#21512;&#36229;&#32423;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#27861;&#22269;&#24052;&#40654;&#24066;&#20013;&#24515;&#30340;&#23454;&#26102;&#37325;&#24314;&#22478;&#24066;&#31354;&#27668;&#27745;&#26579;&#22320;&#22270;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.02812</link><description>&lt;p&gt;
&#22522;&#20110;&#32479;&#35745;&#12289;&#29289;&#29702;&#21644;&#36229;&#32423;&#23398;&#20064;&#22270;&#27169;&#22411;&#30340;&#22478;&#24066;&#31354;&#27668;&#27745;&#26579;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
State estimation of urban air pollution with statistical, physical, and super-learning graph models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#22478;&#24066;&#22270;&#30340;&#19981;&#21516;&#37325;&#24314;&#26041;&#27861;&#65292;&#21253;&#25324;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#12289;&#29289;&#29702;&#39537;&#21160;&#21644;&#28151;&#21512;&#22411;&#65292;&#20197;&#21450;&#32467;&#21512;&#36229;&#32423;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#27861;&#22269;&#24052;&#40654;&#24066;&#20013;&#24515;&#30340;&#23454;&#26102;&#37325;&#24314;&#22478;&#24066;&#31354;&#27668;&#27745;&#26579;&#22320;&#22270;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#23454;&#26102;&#37325;&#24314;&#22478;&#24066;&#31354;&#27668;&#27745;&#26579;&#22320;&#22270;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#21487;&#29992;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#28304;&#12289;&#30452;&#25509;&#27979;&#37327;&#30340;&#31232;&#32570;&#24615;&#12289;&#22122;&#22768;&#30340;&#23384;&#22312;&#20197;&#21450;&#38656;&#35201;&#32771;&#34385;&#30340;&#22823;&#38754;&#31215;&#65292;&#36825;&#20010;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22478;&#24066;&#22270;&#30340;&#19981;&#21516;&#37325;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#20998;&#20026;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#12289;&#29289;&#29702;&#39537;&#21160;&#25110;&#28151;&#21512;&#22411;&#65292;&#24182;&#32467;&#21512;&#20102;&#36229;&#32423;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#27861;&#22269;&#24052;&#40654;&#24066;&#20013;&#24515;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of real-time reconstruction of urban air pollution maps. The task is challenging due to the heterogeneous sources of available data, the scarcity of direct measurements, the presence of noise, and the large surfaces that need to be considered. In this work, we introduce different reconstruction methods based on posing the problem on city graphs. Our strategies can be classified as fully data-driven, physics-driven, or hybrid, and we combine them with super-learning models. The performance of the methods is tested in the case of the inner city of Paris, France.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.02805</link><description>&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20013;&#30340;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39034;&#24207;&#21644;&#24182;&#34892;&#35268;&#21010;&#20197;&#20248;&#21270;&#26102;&#38388;&#25104;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25104;&#21151;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;GPT-4&#21644;LLaMA-2&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;AsyncHow&#20013;&#65292;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#30340;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;PLaG&#33021;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#36973;&#21463;&#20005;&#37325;&#38477;&#32423;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;LLMs&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#35270;&#20026;&#23558;LLMs&#29992;&#20316;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02803</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Distilling Medication Recommendation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23427;&#26681;&#25454;&#24739;&#32773;&#29305;&#23450;&#30340;&#20581;&#24247;&#38656;&#27714;&#26469;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#21307;&#23398;&#25968;&#25454;&#30340;&#32454;&#24494;&#35821;&#20041;&#65292;&#32780;&#21482;&#26159;&#36807;&#24230;&#20381;&#36182;&#26631;&#35782;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#39318;&#27425;&#35775;&#38382;&#21307;&#38498;&#30340;&#24739;&#32773;&#30340;&#24773;&#20917;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#20043;&#21069;&#30340;&#22788;&#26041;&#21382;&#21490;&#21487;&#20197;&#21442;&#32771;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#36755;&#20837;&#19981;&#21487;&#30693;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;LLMs&#25913;&#36827;&#29616;&#26377;&#30340;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#65288;LEADER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20351;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#33616;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
&lt;/p&gt;</description></item><item><title>KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02801</link><description>&lt;p&gt;
KS-Lottery: &#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#35777;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02801
&lt;/p&gt;
&lt;p&gt;
KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#35777;&#20551;&#35828;&#35748;&#20026;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#8220;&#20013;&#22870;&#31080;&#8221;&#12290;&#22312;&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#20013;&#22870;&#31080;&#65311;&#25105;&#20204;&#22914;&#20309;&#25214;&#21040;&#36825;&#26679;&#30340;&#20013;&#22870;&#31080;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KS-Lottery&#65292;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22312;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#39640;&#24230;&#26377;&#25928;&#30340;LLM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#24494;&#35843;&#21069;&#21518;&#21442;&#25968;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29702;&#35770;&#35777;&#26126;&#20102;KS-Lottery&#21487;&#20197;&#22312;&#23884;&#20837;&#23618;&#20013;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#65292;&#24494;&#35843;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#23558;KS-Lottery&#19982;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KS-Lottery&#25214;&#21040;&#20102;&#19968;&#20010;&#26356;&#23567;&#30340;&#21442;&#25968;&#38598;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;18&#20010;&#26631;&#35760;&#30340;&#23884;&#20837;&#23618;
&lt;/p&gt;
&lt;p&gt;
The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27880;&#24847;&#24341;&#23548;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65288;JAFFNet&#65289;&#65292;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26126;&#26174;&#24615;&#26816;&#27979;&#12290;JAFFNet&#36890;&#36807;&#34701;&#21512;&#20302;&#23618;&#21644;&#39640;&#23618;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#31264;&#23494;&#24863;&#21463;&#37326;&#27169;&#22359;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#32570;&#38519;&#23610;&#24230;&#21464;&#21270;&#12289;&#22797;&#26434;&#32972;&#26223;&#12289;&#20302;&#23545;&#27604;&#24230;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02797</link><description>&lt;p&gt;
&#34920;&#38754;&#32570;&#38519;&#26126;&#26174;&#24615;&#26816;&#27979;&#30340;&#32852;&#21512;&#27880;&#24847;&#24341;&#23548;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02797
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27880;&#24847;&#24341;&#23548;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65288;JAFFNet&#65289;&#65292;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26126;&#26174;&#24615;&#26816;&#27979;&#12290;JAFFNet&#36890;&#36807;&#34701;&#21512;&#20302;&#23618;&#21644;&#39640;&#23618;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#31264;&#23494;&#24863;&#21463;&#37326;&#27169;&#22359;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#32570;&#38519;&#23610;&#24230;&#21464;&#21270;&#12289;&#22797;&#26434;&#32972;&#26223;&#12289;&#20302;&#23545;&#27604;&#24230;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#32570;&#38519;&#26816;&#27979;&#22312;&#24037;&#19994;&#21046;&#36896;&#21644;&#29983;&#20135;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#27493;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#32570;&#38519;&#23610;&#24230;&#21464;&#21270;&#12289;&#22797;&#26434;&#32972;&#26223;&#12289;&#20302;&#23545;&#27604;&#24230;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#30340;&#32852;&#21512;&#27880;&#24847;&#24341;&#23548;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65288;JAFFNet&#65289;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26126;&#26174;&#24615;&#26816;&#27979;&#12290;JAFFNet&#20027;&#35201;&#22312;&#35299;&#30721;&#38454;&#27573;&#34701;&#20837;&#20102;&#32852;&#21512;&#27880;&#24847;&#24341;&#23548;&#29305;&#24449;&#34701;&#21512;&#65288;JAFF&#65289;&#27169;&#22359;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#20302;&#23618;&#21644;&#39640;&#23618;&#29305;&#24449;&#12290;JAFF&#27169;&#22359;&#23398;&#20064;&#24378;&#35843;&#32570;&#38519;&#29305;&#24449;&#24182;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#65292;&#22312;&#26816;&#27979;&#20302;&#23545;&#27604;&#24230;&#32570;&#38519;&#26041;&#38754;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;JAFFNet&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#31264;&#23494;&#24863;&#21463;&#37326;&#65288;DRF&#65289;&#27169;&#22359;&#65292;&#22312;&#32534;&#30721;&#22120;&#21518;&#38754;&#25429;&#33719;&#20855;&#26377;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#19981;&#21516;&#23610;&#24230;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface defect inspection plays an important role in the process of industrial manufacture and production. Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on. To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network. JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features. The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects. In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23398;&#20064;&#30340;&#36793;&#32536;&#32531;&#23384;&#26694;&#26550;HR-Cache&#33021;&#22815;&#20248;&#21270;&#36793;&#32536;&#32531;&#23384;&#65292;&#25552;&#39640;&#23383;&#33410;&#21629;&#20013;&#29575;&#65292;&#38477;&#20302;&#32593;&#32476;&#36127;&#36733;&#65292;&#24182;&#21152;&#36895;&#20869;&#23481;&#20256;&#36882;&#32473;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2402.02795</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#36793;&#32536;&#20869;&#23481;&#20256;&#36882;&#32531;&#23384;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Learning-Based Caching Mechanism for Edge Content Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02795
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#36793;&#32536;&#32531;&#23384;&#26694;&#26550;HR-Cache&#33021;&#22815;&#20248;&#21270;&#36793;&#32536;&#32531;&#23384;&#65292;&#25552;&#39640;&#23383;&#33410;&#21629;&#20013;&#29575;&#65292;&#38477;&#20302;&#32593;&#32476;&#36127;&#36733;&#65292;&#24182;&#21152;&#36895;&#20869;&#23481;&#20256;&#36882;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#32593;&#32476;&#30340;&#20852;&#36215;&#21644;&#29289;&#32852;&#32593;(IoT)&#30340;&#21457;&#23637;&#65292;&#20869;&#23481;&#20256;&#36882;&#32593;&#32476;(CDNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#25193;&#23637;&#21040;&#32593;&#32476;&#36793;&#32536;&#12290;&#36825;&#31181;&#36716;&#21464;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#36793;&#32536;&#30340;&#26377;&#38480;&#32531;&#23384;&#23384;&#20648;&#21644;&#22810;&#26679;&#21270;&#30340;&#35831;&#27714;&#27169;&#24335;&#12290;&#36793;&#32536;&#29615;&#22659;&#21487;&#20197;&#25176;&#31649;&#20855;&#26377;&#19981;&#21516;&#23545;&#35937;&#22823;&#23567;&#20998;&#24067;&#21644;&#23545;&#35937;&#35775;&#38382;&#27169;&#24335;&#30340;&#27969;&#37327;&#31867;&#21035;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20351;&#24471;&#20256;&#32479;&#30340;&#32531;&#23384;&#31574;&#30053;&#24456;&#38590;&#21457;&#25381;&#20316;&#29992;&#65292;&#20256;&#32479;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#35831;&#27714;&#39057;&#29575;&#25110;&#26102;&#38388;&#38388;&#38548;&#31561;&#25351;&#26631;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#20248;&#21270;&#36793;&#32536;&#32531;&#23384;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36793;&#32536;&#23454;&#29616;&#26356;&#39640;&#30340;&#23383;&#33410;&#21629;&#20013;&#29575;&#19981;&#20165;&#21487;&#20197;&#20943;&#36731;&#32593;&#32476;&#39592;&#24178;&#30340;&#36127;&#36733;&#65292;&#36824;&#21487;&#20197;&#26368;&#23567;&#21270;&#36816;&#33829;&#25104;&#26412;&#24182;&#21152;&#24555;&#20869;&#23481;&#20256;&#36882;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HR-Cache&#30340;&#32508;&#21512;&#23398;&#20064;&#32531;&#23384;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#21361;&#38505;&#29575;(Hazard Rate)&#25490;&#24207;&#21407;&#21017;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#21021;&#29992;&#26469;&#35745;&#31639;&#24453;&#21629;&#26102;&#38388;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of 5G networks and the rise of the Internet of Things (IoT), Content Delivery Networks (CDNs) are increasingly extending into the network edge. This shift introduces unique challenges, particularly due to the limited cache storage and the diverse request patterns at the edge. These edge environments can host traffic classes characterized by varied object-size distributions and object-access patterns. Such complexity makes it difficult for traditional caching strategies, which often rely on metrics like request frequency or time intervals, to be effective. Despite these complexities, the optimization of edge caching is crucial. Improved byte hit rates at the edge not only alleviate the load on the network backbone but also minimize operational costs and expedite content delivery to end-users.   In this paper, we introduce HR-Cache, a comprehensive learning-based caching framework grounded in the principles of Hazard Rate (HR) ordering, a rule originally formulated to com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#21452;&#26354;&#27491;&#20999;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#65288;TeLU&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#19982;&#27969;&#34892;&#30340;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#65292;TeLU&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02790</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#26354;&#27491;&#20999;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#65288;TeLU&#65289;&#23454;&#29616;&#31283;&#23450;&#21644;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#21452;&#26354;&#27491;&#20999;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#65288;TeLU&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#19982;&#27969;&#34892;&#30340;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#65292;TeLU&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#21452;&#26354;&#27491;&#20999;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#65288;TeLU&#65289;&#65292;&#34920;&#31034;&#20026;$f(x) = x{\cdot}tanh(e^x)$&#12290;TeLU&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;GELU&#21644;Mish&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;TeLU&#22312;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#26377;&#25928;&#22320;&#23558;&#28608;&#27963;&#36755;&#20986;&#30340;&#22343;&#20540;&#35843;&#25972;&#20026;&#38646;&#65292;&#22686;&#24378;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#23545;&#21253;&#25324;Resnet-50&#22312;&#20869;&#30340;&#20808;&#36827;&#26550;&#26500;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#19982;&#27969;&#34892;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;ReLU&#12289;GELU&#12289;SiLU&#12289;Mish&#12289;Logish&#12289;Smish&#65289;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#32467;&#26524;&#26174;&#31034;TeLU&#20855;&#26377;&#36739;&#20302;&#30340;&#26041;&#24046;&#21644;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#38024;&#23545;&#20854;&#20182;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#26465;&#20214;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#21253;&#25324;CIFAR-10&#12289;CIFAR-100&#21644;TinyImageNet&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#65292;&#28085;&#30422;&#20102;860&#20010;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit (TeLU), a novel neural network activation function, represented as $f(x) = x{\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of conventional activation functions like ReLU, GELU, and Mish by addressing the vanishing and, to an extent, the exploding gradient problems. Our theoretical analysis and empirical assessments reveal that TeLU outperforms existing activation functions in stability and robustness, effectively adjusting activation outputs' mean towards zero for enhanced training stability and convergence. Extensive evaluations against popular activation functions (ReLU, GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including Resnet-50, demonstrate TeLU's lower variance and superior performance, even under hyperparameter conditions optimized for other functions. In large-scale tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet, encompassing 860 scenari
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#21644;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#26469;&#31283;&#23450;&#22320;&#33976;&#39311;&#30693;&#35782;&#24182;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02781</link><description>&lt;p&gt;
&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#39640;&#25928;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dual Knowledge Distillation for Efficient Sound Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#21644;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#26469;&#31283;&#23450;&#22320;&#33976;&#39311;&#30693;&#35782;&#24182;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;SED&#65289;&#23545;&#20110;&#35782;&#21035;&#29305;&#23450;&#22768;&#38899;&#21450;&#20854;&#22312;&#22768;&#23398;&#20449;&#21495;&#20013;&#30340;&#26102;&#38388;&#20301;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#22312;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#21464;&#24471;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;SED&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#20197;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#65288;TAKD&#65289;&#20026;&#24320;&#31471;&#65292;&#21033;&#29992;&#20174;&#23398;&#29983;&#27169;&#22411;&#21442;&#25968;&#30340;&#26102;&#24207;&#24179;&#22343;&#24471;&#21040;&#30340;&#24179;&#22343;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#38388;&#25509;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#23398;&#20064;&#65292;&#30830;&#20445;&#31283;&#23450;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#65288;EEFD&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#22312;&#23398;&#29983;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#23884;&#20837;&#33976;&#39311;&#23618;&#26469;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;DCASE 2023&#20219;&#21153;4A&#20844;&#20849;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;SED&#31995;&#32479;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#21152;&#36895;&#25311;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32500;&#25345;&#23545;&#19981;&#21516;&#36136;&#37327;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#21482;&#20351;&#29992;&#20102;&#24456;&#23569;&#30340;&#26597;&#35810;</title><link>https://arxiv.org/abs/2402.02774</link><description>&lt;p&gt;
&#36890;&#36807;&#24555;&#36895;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#20248;&#21270;&#21152;&#36895;&#25311;&#38453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Matroid Optimization through Fast Imprecise Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#21152;&#36895;&#25311;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32500;&#25345;&#23545;&#19981;&#21516;&#36136;&#37327;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#21482;&#20351;&#29992;&#20102;&#24456;&#23569;&#30340;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#22797;&#26434;&#27169;&#22411;&#20197;&#33719;&#24471;&#20934;&#30830;&#20449;&#24687;&#65288;&#20363;&#22914;&#27969;&#37327;&#27169;&#22411;&#12289;&#25968;&#25454;&#24211;&#31995;&#32479;&#12289;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#36739;&#38271;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#26597;&#35810;&#24378;&#27169;&#22411;&#35299;&#20915;&#19981;&#20934;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#37027;&#20040;&#20351;&#29992;&#33021;&#22815;&#24555;&#36895;&#32473;&#20986;&#19981;&#20934;&#30830;&#32467;&#26524;&#30340;&#36739;&#24369;&#27169;&#22411;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#22312;&#35745;&#31639;&#19968;&#20010;&#25311;&#38453;&#30340;&#26368;&#22823;&#26435;&#37325;&#22522;&#30784;&#30340;&#22522;&#30784;&#38382;&#39064;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#20010;&#24050;&#30693;&#27867;&#21270;&#12290;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#24178;&#20928;&#30340;&#26597;&#35810;&#25311;&#38453;&#20449;&#24687;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#39069;&#22806;&#25552;&#20379;&#20102;&#19968;&#20010;&#24555;&#36895;&#20294;&#33039;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#27169;&#25311;&#19968;&#20010;&#26410;&#30693;&#30340;&#12289;&#21487;&#33021;&#19981;&#21516;&#30340;&#25311;&#38453;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21482;&#20351;&#29992;&#24456;&#23569;&#25968;&#37327;&#30340;&#24178;&#20928;&#26597;&#35810;&#30456;&#23545;&#20110;&#33039;&#39044;&#27979;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#20219;&#24847;&#36136;&#37327;&#24046;&#30340;&#33039;&#25311;&#38453;&#30340;&#24378;&#20581;&#24615;&#65292;&#24182;&#25509;&#36817;&#32473;&#23450;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35768;&#22810;&#26041;&#38754;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#26368;&#20339;&#30340;
&lt;/p&gt;
&lt;p&gt;
Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times. Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model. In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information. We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid. We design and analyze practical algorithms which only use few clean queries w.r.t. the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem. Notably, we prove that our algorithms are, in many respects, best-possible
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#24120;&#21463;&#38480;&#20110;&#22522;&#30784;&#20998;&#24067;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;CDiffuser&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02772</link><description>&lt;p&gt;
&#23545;&#27604;&#25193;&#25955;&#22120;&#65306;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35268;&#21010;&#39640;&#22238;&#25253;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#24120;&#21463;&#38480;&#20110;&#22522;&#30784;&#20998;&#24067;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;CDiffuser&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25193;&#25955;&#30340;&#24314;&#27169;&#33021;&#21147;&#36827;&#34892;&#20219;&#24847;&#20998;&#24067;&#30340;&#35268;&#21010;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#35268;&#21010;&#29983;&#25104;&#20102;&#21518;&#32493;&#36712;&#36857;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#22522;&#30784;&#20998;&#24067;&#30340;&#38480;&#21046;&#65292;&#24182;&#24573;&#35270;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#19981;&#21516;&#29366;&#24577;&#20855;&#26377;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#23427;&#20204;&#20165;&#20165;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#19982;&#31163;&#32447;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#36712;&#36857;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#21363;&#20351;&#37197;&#22791;&#20102;&#24341;&#23548;&#27169;&#22411;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#21387;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDiffuser&#30340;&#26032;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36820;&#22238;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02769</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;: &#26131;&#20110;&#27169;&#20223;&#30340;&#21487;&#25512;&#24191;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;Learning from Teaching&#65292;&#31616;&#31216;LoT&#65289;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#20154;&#31867;&#25429;&#25417;&#31616;&#26126;&#25277;&#35937;&#27169;&#24335;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#25512;&#24191;&#30340;&#20851;&#31995;&#26356;&#23481;&#26131;&#25945;&#25480;&#12290;LoT&#36890;&#36807;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#25552;&#20379;&#21453;&#39304;&#26469;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#25913;&#36827;&#20027;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#26356;&#22810;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24341;&#20837;LoT&#30456;&#27604;&#20165;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;&#36825;&#34920;&#26126;&#20102;LoT&#22312;&#35782;&#21035;&#21487;&#25512;&#24191;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#22270;&#27010;&#35201;&#21644;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#20852;&#36890;&#20449;&#23454;&#29616;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#27010;&#35201;&#65292;&#35299;&#20915;&#20102;&#24212;&#29992;&#31243;&#24207;&#19982;&#32593;&#32476;&#20043;&#38388;&#22797;&#26434;&#30340;&#36890;&#20449;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02768</link><description>&lt;p&gt;
&#24847;&#22270;&#27010;&#35201;&#21644;&#36890;&#36807;&#26032;&#20852;&#36890;&#20449;&#36827;&#34892;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intent Profiling and Translation Through Emergent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#22270;&#27010;&#35201;&#21644;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#20852;&#36890;&#20449;&#23454;&#29616;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#27010;&#35201;&#65292;&#35299;&#20915;&#20102;&#24212;&#29992;&#31243;&#24207;&#19982;&#32593;&#32476;&#20043;&#38388;&#22797;&#26434;&#30340;&#36890;&#20449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#34920;&#36798;&#21644;&#28385;&#36275;&#32593;&#32476;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#31649;&#29702;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#20102;&#12290;&#22312;&#22522;&#20110;&#24847;&#22270;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#25143;&#21644;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#39640;&#32423;&#25277;&#35937;&#35821;&#35328;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#24847;&#22270;&#12290;&#23613;&#31649;&#36825;&#31181;&#25277;&#35937;&#31616;&#21270;&#20102;&#32593;&#32476;&#25805;&#20316;&#65292;&#20294;&#23427;&#23545;&#20110;&#26377;&#25928;&#22320;&#34920;&#36798;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#32593;&#32476;&#33021;&#21147;&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#22270;&#27010;&#35201;&#21644;&#32763;&#35793;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19982;&#32593;&#32476;&#20132;&#20114;&#30340;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#20182;&#20204;&#30340;&#39046;&#22495;&#35821;&#35328;&#26469;&#34920;&#36798;&#20182;&#20204;&#23545;&#32593;&#32476;&#26381;&#21153;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65288;&#21363;&#24212;&#29992;&#31243;&#24207;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#36890;&#20449;&#65289;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#29702;&#35299;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#39046;&#22495;&#35821;&#35328;&#65292;&#36825;&#26082;&#19981;&#23454;&#38469;&#20063;&#19981;&#21487;&#20280;&#32553;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26032;&#20852;&#36890;&#20449;&#30340;&#24847;&#22270;&#27010;&#35201;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#27010;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
To effectively express and satisfy network application requirements, intent-based network management has emerged as a promising solution. In intent-based methods, users and applications express their intent in a high-level abstract language to the network. Although this abstraction simplifies network operation, it induces many challenges to efficiently express applications' intents and map them to different network capabilities. Therefore, in this work, we propose an AI-based framework for intent profiling and translation. We consider a scenario where applications interacting with the network express their needs for network services in their domain language. The machine-to-machine communication (i.e., between applications and the network) is complex since it requires networks to learn how to understand the domain languages of each application, which is neither practical nor scalable. Instead, a framework based on emergent communication is proposed for intent profiling, in which applica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#28966;&#28857;&#35843;&#21046;&#32593;&#32476;&#65288;FocalNets&#65289;&#26469;&#35299;&#20915;&#38899;&#39057;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#65292;&#39318;&#27425;&#22312;&#29615;&#22659;&#22768;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#65292;&#24182;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#31867;&#20284;&#22823;&#23567;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#21644;&#19987;&#38376;&#29992;&#20110;&#38899;&#39057;&#39046;&#22495;&#30340;PIQ&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02754</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22768;&#38899;&#20998;&#31867;&#30340;&#28966;&#28857;&#35843;&#21046;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Focal Modulation Networks for Interpretable Sound Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#28966;&#28857;&#35843;&#21046;&#32593;&#32476;&#65288;FocalNets&#65289;&#26469;&#35299;&#20915;&#38899;&#39057;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#65292;&#39318;&#27425;&#22312;&#29615;&#22659;&#22768;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#65292;&#24182;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#31867;&#20284;&#22823;&#23567;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#21644;&#19987;&#38376;&#29992;&#20110;&#38899;&#39057;&#39046;&#22495;&#30340;PIQ&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#26029;&#25104;&#21151;&#24341;&#21457;&#20102;&#19982;&#20854;&#40657;&#30418;&#26412;&#36136;&#30456;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20449;&#20219;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#23545;&#35299;&#37322;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38899;&#39057;&#39046;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#21364;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20107;&#21518;&#35299;&#37322;&#19978;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#27880;&#24847;&#21147;&#28966;&#28857;&#35843;&#21046;&#32593;&#32476;&#65288;FocalNets&#65289;&#65292;&#22312;&#38899;&#39057;&#39046;&#22495;&#20013;&#39318;&#27425;&#24212;&#29992;FocalNets&#26469;&#35299;&#20915;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;FocalNets&#24212;&#29992;&#20110;&#29615;&#22659;&#22768;&#38899;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;ESC-50&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#31867;&#20284;&#22823;&#23567;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#19982;&#19987;&#38376;&#29992;&#20110;&#38899;&#39057;&#39046;&#22495;&#20107;&#21518;&#35299;&#37322;&#30340;PIQ&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing success of deep neural networks has raised concerns about their inherent black-box nature, posing challenges related to interpretability and trust. While there has been extensive exploration of interpretation techniques in vision and language, interpretability in the audio domain has received limited attention, primarily focusing on post-hoc explanations. This paper addresses the problem of interpretability by-design in the audio domain by utilizing the recently proposed attention-free focal modulation networks (FocalNets). We apply FocalNets to the task of environmental sound classification for the first time and evaluate their interpretability properties on the popular ESC-50 dataset. Our method outperforms a similarly sized vision transformer both in terms of accuracy and interpretability. Furthermore, it is competitive against PIQ, a method specifically designed for post-hoc interpretation in the audio domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02750</link><description>&lt;p&gt;
KIVI&#65306;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#26381;&#21153;&#38656;&#35201;&#23558;&#35768;&#22810;&#35831;&#27714;&#25209;&#37327;&#22788;&#29702;&#20197;&#20943;&#23569;&#27599;&#20010;&#35831;&#27714;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#20197;&#36991;&#20813;&#37325;&#26032;&#35745;&#31639;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#26174;&#33879;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25104;&#20026;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#29942;&#39048;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;GPU&#30340;SRAM&#24517;&#39035;&#20174;&#20027;GPU&#20869;&#23384;&#20013;&#21152;&#36733;&#25972;&#20010;KV&#32531;&#23384;&#20197;&#29983;&#25104;&#27599;&#20010;&#26631;&#35760;&#65292;&#23548;&#33268;&#35745;&#31639;&#26680;&#24515;&#22312;&#27492;&#36807;&#31243;&#20013;&#22788;&#20110;&#31354;&#38386;&#29366;&#24577;&#12290;&#20943;&#23567;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#19968;&#20010;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37327;&#21270;&#65292;&#36890;&#36807;&#20943;&#23569;KV&#32531;&#23384;&#25152;&#38656;&#30340;&#24635;&#23383;&#33410;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;KV&#32531;&#23384;&#20803;&#32032;&#20998;&#24067;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#20197;&#20102;&#35299;KV&#32531;&#23384;&#37327;&#21270;&#30340;&#38590;&#24230;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#20803;&#32032;&#20998;&#24067;&#30740;&#31350;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut
&lt;/p&gt;</description></item><item><title>&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02746</link><description>&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#36275;&#20197;&#24212;&#23545;
&lt;/p&gt;
&lt;p&gt;
Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02746
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#20351;&#29992;&#26631;&#20934; Gaussian &#36807;&#31243;&#65288;GP&#65289;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#21363;&#26631;&#20934; BO&#65292;&#22312;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#36825;&#31181;&#35266;&#24565;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110; Gaussian &#36807;&#31243;&#22312;&#21327;&#26041;&#24046;&#24314;&#27169;&#21644;&#20989;&#25968;&#20272;&#35745;&#20013;&#23545;&#39640;&#32500;&#36755;&#20837;&#30340;&#22256;&#38590;&#12290;&#34429;&#28982;&#36825;&#20123;&#25285;&#24551;&#30475;&#36215;&#26469;&#21512;&#29702;&#65292;&#20294;&#32570;&#20047;&#25903;&#25345;&#36825;&#31181;&#35266;&#28857;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#22238;&#24402;&#36827;&#34892;&#39640;&#32500;&#20248;&#21270;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26631;&#20934; GP &#30340;&#34920;&#29616;&#22987;&#32456;&#20301;&#20110;&#26368;&#20339;&#33539;&#22260;&#20869;&#65292;&#24448;&#24448;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#29616;&#26377; BO &#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#21051;&#26495;&#21360;&#35937;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934; GP &#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;&#20989;&#25968;&#30340;&#33021;&#21147;&#24378;&#22823;&#30340;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#24378;&#32467;&#26500;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#36827;&#34892; BO &#21487;&#20197;&#33719;&#24471;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;Koopman&#31639;&#23376;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#36229;&#26799;&#24230;&#30340;&#36712;&#36857;&#26469;&#39640;&#25928;&#22320;&#36817;&#20284;&#20840;&#23616;&#36229;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#36229;&#21442;&#25968;&#30340;&#36138;&#23146;&#20248;&#21270;&#65292;&#20860;&#20855;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02741</link><description>&lt;p&gt;
&#20855;&#26377;Koopman&#31639;&#23376;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Glocal Hypergradient Estimation with Koopman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;Koopman&#31639;&#23376;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#36229;&#26799;&#24230;&#30340;&#36712;&#36857;&#26469;&#39640;&#25928;&#22320;&#36817;&#20284;&#20840;&#23616;&#36229;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#36229;&#21442;&#25968;&#30340;&#36138;&#23146;&#20248;&#21270;&#65292;&#20860;&#20855;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#20351;&#29992;&#36229;&#26799;&#24230;&#26469;&#26356;&#26032;&#36229;&#21442;&#25968;&#65292;&#21363;&#20803;&#26631;&#20934;&#30340;&#26799;&#24230;&#19982;&#36229;&#21442;&#25968;&#30340;&#20851;&#31995;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26356;&#26032;&#31574;&#30053;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992;&#27169;&#22411;&#35757;&#32451;&#23436;&#25104;&#21518;&#24471;&#21040;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#26469;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#20043;&#21518;&#24471;&#21040;&#30340;&#23616;&#37096;&#36229;&#26799;&#24230;&#12290;&#34429;&#28982;&#20840;&#23616;&#36229;&#26799;&#24230;&#20855;&#26377;&#21487;&#38752;&#24615;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#26174;&#33879;&#65307;&#30456;&#21453;&#65292;&#23616;&#37096;&#36229;&#26799;&#24230;&#36895;&#24230;&#24555;&#20294;&#24120;&#24120;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;glocal&#36229;&#26799;&#24230;&#20272;&#35745;&#65292;&#23558;&#8220;&#20840;&#23616;&#8221;&#30340;&#36136;&#37327;&#19982;&#8220;&#23616;&#37096;&#8221;&#30340;&#25928;&#29575;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;Koopman&#31639;&#23376;&#29702;&#35770;&#26469;&#32447;&#24615;&#21270;&#36229;&#26799;&#24230;&#30340;&#21160;&#24577;&#65292;&#20197;&#20415;&#21487;&#20197;&#20165;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#36229;&#26799;&#24230;&#30340;&#36712;&#36857;&#26469;&#39640;&#25928;&#22320;&#36817;&#20284;&#20840;&#23616;&#36229;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20272;&#35745;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#36138;&#23146;&#22320;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending "global" quality with "local" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#22320;&#25506;&#31350;&#20102;&#21518;&#38376;&#25193;&#25955;&#27169;&#22411;&#20013;&#27602;&#22122;&#22768;&#36755;&#20837;&#30340;&#21487;&#26816;&#27979;&#24615;&#65292;&#21457;&#29616;&#20998;&#24067;&#24046;&#24322;&#22312;&#26408;&#39532;&#26816;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#35302;&#21457;&#26816;&#27979;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.02739</link><description>&lt;p&gt;
DisDet: &#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#21487;&#26816;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#22320;&#25506;&#31350;&#20102;&#21518;&#38376;&#25193;&#25955;&#27169;&#22411;&#20013;&#27602;&#22122;&#22768;&#36755;&#20837;&#30340;&#21487;&#26816;&#27979;&#24615;&#65292;&#21457;&#29616;&#20998;&#24067;&#24046;&#24322;&#22312;&#26408;&#39532;&#26816;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#35302;&#21457;&#26816;&#27979;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20196;&#20154;&#20852;&#22859;&#30340;&#29983;&#25104;&#22411;AI&#26102;&#20195;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#24378;&#22823;&#19988;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#20869;&#23481;&#29983;&#25104;&#21644;&#32534;&#36753;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#28508;&#22312;&#23433;&#20840;&#39118;&#38505;&#38750;&#24120;&#24517;&#35201;&#19988;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24320;&#21019;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#23545;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#21628;&#21505;&#23545;&#36825;&#31181;&#27969;&#34892;&#19988;&#22522;&#30784;&#30340;AI&#25216;&#26415;&#30340;&#23433;&#20840;&#25361;&#25112;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#21644;&#35843;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#31995;&#32479;&#22320;&#25506;&#31350;&#20102;&#21518;&#38376;&#25193;&#25955;&#27169;&#22411;&#20013;&#27602;&#22122;&#22768;&#36755;&#20837;&#30340;&#21487;&#26816;&#27979;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#65292;&#20294;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#20174;&#38450;&#24481;&#32773;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#29616;&#26377;&#25193;&#25955;&#21518;&#38376;&#25915;&#20987;&#20013;&#35302;&#21457;&#27169;&#24335;&#30340;&#23646;&#24615;&#65292;&#21457;&#29616;&#20998;&#24067;&#24046;&#24322;&#22312;&#26408;&#39532;&#26816;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#35302;&#21457;&#26816;&#27979;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#34701;&#21512;&#31574;&#30053;&#30340;&#35270;&#35282;&#35780;&#20272;&#20102;LiDAR-Camera&#34701;&#21512;&#27169;&#22411;&#23545;&#25239;&#24120;&#35265;&#22825;&#27668;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#26435;&#37325;&#34701;&#21512;&#31574;&#30053;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02738</link><description>&lt;p&gt;
&#22522;&#20110;&#34701;&#21512;&#31574;&#30053;&#35270;&#35282;&#30340;LiDAR-Camera&#34701;&#21512;&#27169;&#22411;&#23545;&#25239;&#22825;&#27668;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34701;&#21512;&#31574;&#30053;&#30340;&#35270;&#35282;&#35780;&#20272;&#20102;LiDAR-Camera&#34701;&#21512;&#27169;&#22411;&#23545;&#25239;&#24120;&#35265;&#22825;&#27668;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#26435;&#37325;&#34701;&#21512;&#31574;&#30053;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;LiDAR-Camera&#34701;&#21512;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#29289;&#29702;&#19990;&#30028;&#20013;&#65292;&#23427;&#20204;&#23545;&#24120;&#35265;&#30340;&#22825;&#27668;&#24178;&#25200;&#65288;&#22914;&#38654;&#12289;&#38632;&#12289;&#38634;&#21644;&#38451;&#20809;&#65289;&#30340;&#40065;&#26834;&#24615;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20174;&#34701;&#21512;&#31574;&#30053;&#30340;&#35270;&#35282;&#35780;&#20272;&#20102;&#34701;&#21512;&#27169;&#22411;&#22312;&#21463;&#25439;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#35780;&#20272;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#32780;&#23454;&#29992;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;&#28789;&#27963;&#26435;&#37325;&#34701;&#21512;&#26469;&#33258;LiDAR&#21644;&#30456;&#26426;&#28304;&#30340;&#29305;&#24449;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#22825;&#27668;&#22330;&#26223;&#65292;&#20197;&#22686;&#24378;&#34701;&#21512;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#22235;&#31181;&#34701;&#21512;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving. However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored. In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset. Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios. Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#26131;&#33719;&#24471;&#30340;&#26410;&#27880;&#37322;&#35270;&#39057;&#26469;&#30417;&#30563;&#21333;&#24103;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#12290;&#36890;&#36807;&#35745;&#31639;&#36830;&#32493;&#24103;&#30340;&#23039;&#21183;&#21644;&#20809;&#27969;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#20445;&#25345;&#22270;&#20687;&#20809;&#27969;&#19982;&#23039;&#21183;&#21464;&#21270;&#20809;&#27969;&#30340;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20248;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#24182;&#19982;&#20351;&#29992;&#26356;&#22810;&#24102;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.02736</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#36816;&#21160;&#32447;&#32034;&#30417;&#30563;&#21333;&#24103;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#26131;&#33719;&#24471;&#30340;&#26410;&#27880;&#37322;&#35270;&#39057;&#26469;&#30417;&#30563;&#21333;&#24103;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#12290;&#36890;&#36807;&#35745;&#31639;&#36830;&#32493;&#24103;&#30340;&#23039;&#21183;&#21644;&#20809;&#27969;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#20445;&#25345;&#22270;&#20687;&#20809;&#27969;&#19982;&#23039;&#21183;&#21464;&#21270;&#20809;&#27969;&#30340;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20248;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#24182;&#19982;&#20351;&#29992;&#26356;&#22810;&#24102;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26377;&#36275;&#22815;&#30340;&#24102;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#25668;&#20687;&#26426;&#26469;&#20272;&#35745;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#12290;&#24403;&#36825;&#20123;&#25968;&#25454;&#19981;&#36275;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20854;&#20182;&#20449;&#24687;&#28304;&#65288;&#22914;&#36523;&#20307;&#24418;&#29366;&#25968;&#25454;&#24211;&#65289;&#26469;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#26469;&#25913;&#36827;&#20272;&#35745;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20449;&#24687;&#28304;&#20063;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#26131;&#33719;&#24471;&#30340;&#26410;&#27880;&#37322;&#35270;&#39057;&#26469;&#25552;&#20379;&#25152;&#38656;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#32473;&#23450;&#20351;&#29992;&#19981;&#36275;&#30340;&#24102;&#27880;&#37322;&#25968;&#25454;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#35745;&#31639;&#36830;&#32493;&#24103;&#20013;&#30340;&#23039;&#21183;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20809;&#27969;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#20809;&#27969;&#21644;&#20174;&#19968;&#20010;&#24103;&#21040;&#19979;&#19968;&#20010;&#30340;&#23039;&#21183;&#21464;&#21270;&#20013;&#25512;&#26029;&#20986;&#30340;&#20809;&#27969;&#20043;&#38388;&#24378;&#21046;&#20445;&#25345;&#19968;&#33268;&#24615;&#12290;&#36825;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#38468;&#21152;&#30417;&#30563;&#26469;&#26377;&#25928;&#22320;&#20248;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#24182;&#19982;&#20351;&#29992;&#26356;&#22810;&#24102;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26679;&#26412;&#20998;&#24067;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#29992;&#20110;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.02732</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generative Approach to Surrogate-based Black-box Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26679;&#26412;&#20998;&#24067;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#29992;&#20110;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19968;&#32452;&#26679;&#26412;&#65292;&#36890;&#36807;&#40657;&#30418;&#30446;&#26631;&#21453;&#39304;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#37492;&#21035;&#24615;&#26367;&#20195;&#27169;&#22411;&#26469;&#27169;&#25311;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#23398;&#20064;&#30446;&#26631;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#28982;&#21518;&#65292;&#30333;&#30418;&#25915;&#20987;&#38024;&#23545;&#26367;&#20195;&#27169;&#22411;&#29983;&#25104;&#19982;&#21407;&#22987;&#26679;&#26412;&#30456;&#20284;&#20294;&#23646;&#20110;&#20854;&#20182;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#37492;&#21035;&#24615;&#26367;&#20195;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#23398;&#20064;&#30446;&#26631;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#22240;&#27492;&#36825;&#20123;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#36739;&#20302;&#12290;&#19982;&#37492;&#21035;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#30446;&#26631;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#25110;&#30456;&#37051;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#30340;&#20998;&#24067;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-based black-box attacks have exposed the heightened vulnerability of DNNs. These attacks are designed to craft adversarial examples for any samples with black-box target feedback for only a given set of samples. State-of-the-art surrogate-based attacks involve training a discriminative surrogate that mimics the target's outputs. The goal is to learn the decision boundaries of the target. The surrogate is then attacked by white-box attacks to craft adversarial examples similar to the original samples but belong to other classes. With limited samples, the discriminative surrogate fails to accurately learn the target's decision boundaries, and these surrogate-based attacks suffer from low success rates. Different from the discriminative approach, we propose a generative surrogate that learns the distribution of samples residing on or close to the target's decision boundaries. The distribution learned by the generative surrogate can be used to craft adversarial examples that have
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#21512;&#20316;&#24335;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#26041;&#27861;&#65288;GAN-CRME&#65289;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#26080;&#32447;&#30005;&#22320;&#22270;&#65292;&#26080;&#38656;&#21457;&#23556;&#26426;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#31227;&#21160;&#29992;&#25143;&#22788;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#27979;&#37327;&#21644;&#22320;&#29702;&#22320;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#26080;&#32447;&#30005;&#22320;&#22270;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#23398;&#20064;&#31639;&#27861;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02729</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#30340;&#24555;&#36895;&#20934;&#30830;&#21512;&#20316;&#24335;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#21512;&#20316;&#24335;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#26041;&#27861;&#65288;GAN-CRME&#65289;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#26080;&#32447;&#30005;&#22320;&#22270;&#65292;&#26080;&#38656;&#21457;&#23556;&#26426;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#31227;&#21160;&#29992;&#25143;&#22788;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#27979;&#37327;&#21644;&#22320;&#29702;&#22320;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#26080;&#32447;&#30005;&#22320;&#22270;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#23398;&#20064;&#31639;&#27861;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;6G&#26102;&#20195;&#65292;&#25903;&#25345;&#22810;&#26679;&#21270;&#26080;&#32447;&#24212;&#29992;&#30340;&#23454;&#26102;&#26080;&#32447;&#36164;&#28304;&#30417;&#27979;&#21644;&#31649;&#29702;&#25104;&#20026;&#36843;&#20999;&#38656;&#27714;&#12290;&#36825;&#35201;&#27714;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#26080;&#32447;&#36164;&#28304;&#30340;&#20998;&#24067;&#65292;&#36890;&#24120;&#20197;&#22320;&#29702;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#20449;&#21495;&#21151;&#29575;&#24378;&#24230;&#34920;&#31034;&#65292;&#21363;&#26080;&#32447;&#30005;&#22320;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#21512;&#20316;&#24335;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#65288;CRME&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;GAN-CRME&#65292;&#23427;&#20855;&#22791;&#24555;&#36895;&#20934;&#30830;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#33021;&#21147;&#65292;&#26080;&#38656;&#21457;&#23556;&#26426;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#31227;&#21160;&#29992;&#25143;&#22788;&#20998;&#24067;&#24335;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#27979;&#37327;&#19982;&#22320;&#29702;&#22320;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#25512;&#26029;&#26080;&#32447;&#30005;&#22320;&#22270;&#65292;&#20174;&#32780;&#38477;&#20302;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#30340;&#25512;&#26029;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications. This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map. In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information. The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity. Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDNet&#30340;&#39057;&#22495;&#21435;&#22122;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35782;&#21035;&#30001;&#35825;&#23548;&#22810;&#33021;&#24178;&#32454;&#32990;&#20998;&#21270;&#32780;&#26469;&#30340;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#12290;&#30001;&#20110;&#32972;&#26223;&#21644;&#24178;&#25200;&#20449;&#24687;&#30340;&#23384;&#22312;&#65292;&#35266;&#23519;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;IAI704&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02724</link><description>&lt;p&gt;
FDNet: &#39057;&#22495;&#21435;&#22122;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35782;&#21035;&#30001;&#35825;&#23548;&#22810;&#33021;&#24178;&#32454;&#32990;&#20998;&#21270;&#32780;&#26469;&#30340;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;
&lt;/p&gt;
&lt;p&gt;
FDNet: Frequency Domain Denoising Network For Cell Segmentation in Astrocytes Derived From Induced Pluripotent Stem Cells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDNet&#30340;&#39057;&#22495;&#21435;&#22122;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35782;&#21035;&#30001;&#35825;&#23548;&#22810;&#33021;&#24178;&#32454;&#32990;&#20998;&#21270;&#32780;&#26469;&#30340;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#12290;&#30001;&#20110;&#32972;&#26223;&#21644;&#24178;&#25200;&#20449;&#24687;&#30340;&#23384;&#22312;&#65292;&#35266;&#23519;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;IAI704&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20307;&#32454;&#32990;&#20154;&#24037;&#29983;&#25104;&#30340;&#35825;&#23548;&#22810;&#33021;&#24178;&#32454;&#32990;&#65288;iPSCs&#65289;&#22312;&#30142;&#30149;&#24314;&#27169;&#21644;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#33647;&#29289;&#31579;&#36873;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20174;iPSCs&#20998;&#21270;&#20986;&#30340;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#26159;&#30740;&#31350;&#31070;&#32463;&#20195;&#35874;&#30340;&#37325;&#35201;&#30446;&#26631;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#20998;&#21270;&#38454;&#27573;&#35266;&#23519;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#35266;&#23519;&#21040;&#30340;&#24418;&#24577;&#21464;&#21270;&#26469;&#30417;&#27979;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#30340;&#20998;&#21270;&#36827;&#31243;&#65292;&#28982;&#21518;&#36890;&#36807;&#20998;&#23376;&#29983;&#29289;&#23398;&#25216;&#26415;&#22312;&#25104;&#29087;&#26102;&#30830;&#23450;&#12290;&#28982;&#32780;&#65292;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#36890;&#24120;&#20250;&#8220;&#23436;&#32654;&#22320;&#8221;&#34701;&#20837;&#32972;&#26223;&#20013;&#65292;&#32780;&#19988;&#20854;&#20013;&#19968;&#20123;&#32454;&#32990;&#20250;&#34987;&#24178;&#25200;&#20449;&#24687;&#65288;&#20363;&#22914;&#27515;&#32454;&#32990;&#12289;&#22521;&#20859;&#22522;&#27785;&#31215;&#29289;&#21644;&#32454;&#32990;&#27531;&#39608;&#65289;&#25513;&#30422;&#65292;&#36825;&#20351;&#24471;&#35266;&#23519;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#21464;&#24471;&#22256;&#38590;&#12290;&#30001;&#20110;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#26143;&#24418;&#33014;&#36136;&#32454;&#32990;&#20998;&#21106;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;IAI704&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;704&#20010;&#22270;&#20687;&#21450;&#20854;co
&lt;/p&gt;
&lt;p&gt;
Artificially generated induced pluripotent stem cells (iPSCs) from somatic cells play an important role for disease modeling and drug screening of neurodegenerative diseases. Astrocytes differentiated from iPSCs are important targets to investigate neuronal metabolism. The astrocyte differentiation progress can be monitored through the variations of morphology observed from microscopy images at different differentiation stages, then determined by molecular biology techniques upon maturation. However, the astrocytes usually ``perfectly'' blend into the background and some of them are covered by interference information (i.e., dead cells, media sediments, and cell debris), which makes astrocytes difficult to observe. Due to the lack of annotated datasets, the existing state-of-the-art deep learning approaches cannot be used to address this issue. In this paper, we introduce a new task named astrocyte segmentation with a novel dataset, called IAI704, which contains 704 images and their co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25240;&#25187;&#33258;&#36866;&#24212;&#22312;&#32447;&#39044;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#20110;&#22797;&#26434;&#30340;&#25439;&#22833;&#24207;&#21015;&#21644;&#27604;&#36739;&#22120;&#65292;&#24182;&#25913;&#36827;&#20102;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#26080;&#38656;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.02720</link><description>&lt;p&gt;
&#25240;&#25187;&#33258;&#36866;&#24212;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Discounted Adaptive Online Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25240;&#25187;&#33258;&#36866;&#24212;&#22312;&#32447;&#39044;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#20110;&#22797;&#26434;&#30340;&#25439;&#22833;&#24207;&#21015;&#21644;&#27604;&#36739;&#22120;&#65292;&#24182;&#25913;&#36827;&#20102;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#26080;&#38656;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#24182;&#19981;&#24635;&#26159;&#35201;&#35760;&#20303;&#19968;&#20999;&#12290;&#30001;&#20110;&#26410;&#26469;&#22312;&#32479;&#35745;&#19978;&#21487;&#33021;&#19982;&#36807;&#21435;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#22312;&#26032;&#25968;&#25454;&#21040;&#26469;&#26102;&#20248;&#38597;&#22320;&#24536;&#35760;&#21382;&#21490;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#36816;&#29992;&#26368;&#36817;&#21457;&#23637;&#30340;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#37325;&#26032;&#24605;&#32771;&#20102;&#32463;&#20856;&#30340;&#25240;&#25187;&#36951;&#25022;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#65292;&#23427;&#36866;&#24212;&#20110;&#25439;&#22833;&#24207;&#21015;&#21644;&#27604;&#36739;&#22120;&#30340;&#22797;&#26434;&#24615;&#65292;&#25913;&#36827;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;-&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19988;&#20855;&#26377;&#24658;&#23450;&#30340;&#23398;&#20064;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#19981;&#38656;&#35201;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#65292;&#21482;&#35201;&#27714;&#20984;&#24615;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#32463;&#36807;&#35777;&#26126;&#23545;&#27425;&#20248;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#26469;&#23637;&#31034;&#36825;&#20123;&#22909;&#22788;&#65292;&#32780;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#24102;&#26377;&#38598;&#21512;&#25104;&#21592;&#20915;&#31574;&#30340;&#19979;&#28216;&#22312;&#32447;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online learning is not always about memorizing everything. Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in. To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning. Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate. In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning. We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#35268;&#21010;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02716</link><description>&lt;p&gt;
&#29702;&#35299;LLM&#20195;&#29702;&#30340;&#35268;&#21010;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the planning of LLM agents: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#35268;&#21010;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#26234;&#33021;&#65292;&#23558;LLM&#29992;&#20316;&#33258;&#20027;&#20195;&#29702;&#30340;&#35268;&#21010;&#27169;&#22359;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#26356;&#22810;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#35268;&#21010;&#30340;&#39318;&#20010;&#31995;&#32479;&#35270;&#35282;&#65292;&#28085;&#30422;&#20102;&#26088;&#22312;&#25552;&#39640;&#35268;&#21010;&#33021;&#21147;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM-&#20195;&#29702;&#35268;&#21010;&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#35745;&#21010;&#36873;&#25321;&#12289;&#22806;&#37096;&#27169;&#22359;&#12289;&#21453;&#24605;&#21644;&#35760;&#24518;&#12290;&#23545;&#20110;&#27599;&#20010;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02713</link><description>&lt;p&gt;
&#19968;&#31687;&#20301;&#32622;&#35770;&#25991;: &#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26377;&#20160;&#20040;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: What Can Large Language Models Tell Us about Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02713
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#33021;&#21147;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#37327;&#27169;&#22411;&#35843;&#25972;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#30446;&#21069;&#30340;LLM&#20855;&#26377;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#25928;&#30340;&#20915;&#31574;&#21644;&#25512;&#36827;&#21521;&#26356;&#26222;&#36866;&#24418;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#21457;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#21487;&#20197;&#25171;&#24320;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#21253;&#25324;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#12290;&#25105;&#20204;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#35748;&#35782;&#21040;LLM&#22312;&#25512;&#36827;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#23545;&#36825;&#20123;&#30456;&#20851;&#24037;&#20316;&#30340;&#20449;&#20219;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#31070;&#32463;&#26550;&#26500;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#39640;&#26031;&#28608;&#27963;&#20989;&#25968;&#22312;&#26377;&#25928;&#35757;&#32451;PINNs&#26102;&#36229;&#36807;&#20854;&#20182;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#20123;&#21457;&#29616;&#22312;&#22810;&#20010;PDEs&#30340;&#39564;&#35777;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;</title><link>https://arxiv.org/abs/2402.02711</link><description>&lt;p&gt;
&#29992;&#20110;&#20248;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Architectural Strategies for the optimization of Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#31070;&#32463;&#26550;&#26500;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#39640;&#26031;&#28608;&#27963;&#20989;&#25968;&#22312;&#26377;&#25928;&#35757;&#32451;PINNs&#26102;&#36229;&#36807;&#20854;&#20182;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#20123;&#21457;&#29616;&#22312;&#22810;&#20010;PDEs&#30340;&#39564;&#35777;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#22522;&#30784;&#29289;&#29702;&#21407;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#20013;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#23613;&#31649;PINNs&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#19968;&#31995;&#21015;PDEs&#30340;&#35757;&#32451;&#20013;&#20063;&#22240;&#20854;&#22256;&#38590;&#32780;&#22768;&#21517;&#29436;&#34249;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#31070;&#32463;&#26550;&#26500;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;PINN&#20248;&#21270;&#30340;&#22797;&#26434;&#24615;&#12290;&#21033;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#26377;&#25928;&#35757;&#32451;PINNs&#26102;&#65292;&#39640;&#26031;&#28608;&#27963;&#20989;&#25968;&#20248;&#20110;&#20960;&#31181;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#12290;&#20511;&#37492;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#23450;&#21046;&#26550;&#26500;&#22914;&#20309;&#22686;&#24378;&#20248;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#36890;&#36807;&#23545;&#31185;&#23398;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;PDEs&#36827;&#34892;&#20005;&#26684;&#39564;&#35777;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) offer a promising avenue for tackling both forward and inverse problems in partial differential equations (PDEs) by incorporating deep learning with fundamental physics principles. Despite their remarkable empirical success, PINNs have garnered a reputation for their notorious training challenges across a spectrum of PDEs. In this work, we delve into the intricacies of PINN optimization from a neural architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study reveals that Gaussian activations surpass several alternate activations when it comes to effectively training PINNs. Building on insights from numerical linear algebra, we introduce a preconditioned neural architecture, showcasing how such tailored architectures enhance the optimization process. Our theoretical findings are substantiated through rigorous validation against established PDEs within the scientific literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02705</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#30340;&#34920;&#24449;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Representation Surgery for Multi-Task Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#23558;&#22810;&#20010;&#20219;&#21153;&#30340;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#39592;&#24178;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#25509;&#21512;&#24182;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25191;&#34892;MTL&#65292;&#32780;&#19981;&#26159;&#25910;&#38598;&#23427;&#20204;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;MTL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#30340;&#34920;&#31034;&#20998;&#24067;&#65292;&#25105;&#20204;&#21457;&#29616;&#21512;&#24182;&#27169;&#22411;&#24448;&#24448;&#38754;&#20020;&#34920;&#31034;&#20559;&#24046;&#30340;&#22256;&#22659;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#21512;&#24182;&#27169;&#22411;&#19982;&#20010;&#20307;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#20998;&#24067;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#21512;&#24182;MTL&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#21512;&#24182;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;Surgery&#8221;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#23427;&#20197;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#20026;&#36755;&#20837;&#65292;&#24182;&#35797;&#22270;&#36755;&#20986;&#20854;&#20013;&#21253;&#21547;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called "Surgery" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#26159;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.02701</link><description>&lt;p&gt;
&#29702;&#35299;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#22240;&#32032;&#65306;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#26159;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#21162;&#21147;&#33268;&#21147;&#20110;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#23545;&#36830;&#32493;&#25511;&#21046;&#26377;&#29992;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#31574;&#30053;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#27979;&#35797;&#29615;&#22659;&#21487;&#33021;&#19982;&#35757;&#32451;&#29615;&#22659;&#19981;&#21516;&#65292;&#20363;&#22914;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#24178;&#25200;&#22240;&#32032;&#12290;&#35768;&#22810;&#23454;&#38469;&#31639;&#27861;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#20013;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#22240;&#32032;&#20197;&#21450;&#20026;&#20160;&#20040;&#20182;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#29702;&#35770;&#19978;&#22238;&#31572;&#24433;&#21709;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#65288;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#65289;&#23545;&#20110;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#30340;&#25928;&#30410;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;DM&#25968;&#25454;&#30340;&#23454;&#35777;&#35777;&#25454;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#34920;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#35777;&#26126;&#23427;&#20204;&#20855;&#26377;&#25152;&#38656;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20854;&#20013;&#65292;&#23545;&#20110;&#31532;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#21435;&#38500;&#21487;&#36798;&#24615;&#20551;&#35774;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02700</link><description>&lt;p&gt;
&#32447;&#24615;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity Characterization for Linear Contextual MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#34920;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#35777;&#26126;&#23427;&#20204;&#20855;&#26377;&#25152;&#38656;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20854;&#20013;&#65292;&#23545;&#20110;&#31532;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#21435;&#38500;&#21487;&#36798;&#24615;&#20551;&#35774;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#25551;&#36848;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#36716;&#31227;&#20869;&#26680;&#21644;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#30001;&#19968;&#20010;&#19978;&#19979;&#25991;&#21464;&#37327;&#32034;&#24341;&#30340;&#19981;&#21516;MDPs&#12290;&#34429;&#28982;CMDPs&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#20855;&#26377;&#26102;&#21464;&#29615;&#22659;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#27169;&#22411;&#19979;&#30340;CMDPs&#65306;&#27169;&#22411;I&#20855;&#26377;&#19978;&#19979;&#25991;&#21464;&#21270;&#34920;&#31034;&#21644;&#25152;&#26377;&#19978;&#19979;&#25991;&#20844;&#20849;&#32447;&#24615;&#26435;&#37325;&#65307;&#20197;&#21450;&#27169;&#22411;II&#20855;&#26377;&#25152;&#26377;&#19978;&#19979;&#25991;&#30340;&#20844;&#20849;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#32447;&#24615;&#26435;&#37325;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#20855;&#26377;&#25152;&#38656;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#20445;&#35777;&#30340;&#949;-&#27425;&#20248;&#38388;&#38553;&#12290;&#29305;&#21035;&#26159;&#65292;&#23558;&#25105;&#20204;&#23545;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#23454;&#20363;&#21270;&#20026;&#34920;&#26684;CMDP&#65292;&#36890;&#36807;&#21435;&#38500;&#21487;&#36798;&#24615;&#20551;&#35774;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#25105;&#20204;&#23545;&#31532;&#20108;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual Markov decision processes (CMDPs) describe a class of reinforcement learning problems in which the transition kernels and reward functions can change over time with different MDPs indexed by a context variable. While CMDPs serve as an important framework to model many real-world applications with time-varying environments, they are largely unexplored from theoretical perspective. In this paper, we study CMDPs under two linear function approximation models: Model I with context-varying representations and common linear weights for all contexts; and Model II with common representations for all contexts and context-varying linear weights. For both models, we propose novel model-based algorithms and show that they enjoy guaranteed $\epsilon$-suboptimality gap with desired polynomial sample complexity. In particular, instantiating our result for the first model to the tabular CMDP improves the existing result by removing the reachability assumption. Our result for the second mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#28145;&#24230;&#35828;&#35805;&#20154;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#22686;&#24378;&#20998;&#31867;&#22120;&#26469;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#22686;&#24378;&#65292;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#23545;&#35813;&#20998;&#31867;&#22120;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#38754;&#23545;&#19981;&#30456;&#20851;&#22768;&#23398;&#21464;&#21270;&#26102;&#30340;&#27867;&#21270;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02699</link><description>&lt;p&gt;
&#38024;&#23545;&#40065;&#26834;&#24615;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Adversarial Data Augmentation for Robust Speaker Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#28145;&#24230;&#35828;&#35805;&#20154;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#22686;&#24378;&#20998;&#31867;&#22120;&#26469;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#22686;&#24378;&#65292;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#23545;&#35813;&#20998;&#31867;&#22120;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#38754;&#23545;&#19981;&#30456;&#20851;&#22768;&#23398;&#21464;&#21270;&#26102;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#30001;&#20110;&#20854;&#26131;&#20110;&#23454;&#29616;&#21644;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#28145;&#24230;&#35828;&#35805;&#20154;&#27169;&#22411;&#20013;&#24191;&#27867;&#21463;&#21040;&#27426;&#36814;&#12290;&#23427;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#30340;&#22768;&#23398;&#21464;&#21270;&#26469;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#19982;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#24573;&#30053;&#19981;&#30456;&#20851;&#30340;&#22768;&#23398;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#30340;DA&#23384;&#22312;&#19968;&#20010;&#28508;&#22312;&#38382;&#39064;&#65292;&#21363;&#22686;&#24378;&#27531;&#24046;&#65292;&#21363;&#19981;&#21516;&#31867;&#22411;&#30340;&#22686;&#24378;&#24341;&#36215;&#30340;&#19981;&#24517;&#35201;&#22833;&#30495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#65288;A-DA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;DA&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#28041;&#21450;&#19968;&#20010;&#39069;&#22806;&#30340;&#22686;&#24378;&#20998;&#31867;&#22120;&#26469;&#20998;&#31867;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#22686;&#24378;&#31867;&#22411;&#12290;&#36825;&#31181;&#23545;&#25239;&#24615;&#23398;&#20064;&#20351;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#21487;&#20197;&#27450;&#39575;&#22686;&#24378;&#20998;&#31867;&#22120;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#21040;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#22312;&#38754;&#23545;&#19981;&#21516;&#31867;&#22411;&#22686;&#24378;&#26102;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation (DA) has gained widespread popularity in deep speaker models due to its ease of implementation and significant effectiveness. It enriches training data by simulating real-life acoustic variations, enabling deep neural networks to learn speaker-related representations while disregarding irrelevant acoustic variations, thereby improving robustness and generalization. However, a potential issue with the vanilla DA is augmentation residual, i.e., unwanted distortion caused by different types of augmentation. To address this problem, this paper proposes a novel approach called adversarial data augmentation (A-DA) which combines DA with adversarial learning. Specifically, it involves an additional augmentation classifier to categorize various augmentation types used in data augmentation. This adversarial learning empowers the network to generate speaker embeddings that can deceive the augmentation classifier, making the learned speaker embeddings more robust in the face of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#25512;&#24191;&#20102;&#38543;&#26426;&#20248;&#21183;&#30340;&#27010;&#24565;&#20197;&#20351;&#20854;&#33021;&#22815;&#22312;&#20219;&#24847;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#22788;&#29702;&#36830;&#32493;&#24615;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02698</link><description>&lt;p&gt;
&#36229;&#36234;&#26399;&#26395;: &#29616;&#23454;&#20013;&#23454;&#29616;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Expectations: Learning with Stochastic Dominance Made Practical
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#25512;&#24191;&#20102;&#38543;&#26426;&#20248;&#21183;&#30340;&#27010;&#24565;&#20197;&#20351;&#20854;&#33021;&#22815;&#22312;&#20219;&#24847;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#22788;&#29702;&#36830;&#32493;&#24615;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21183;&#27169;&#22411;&#23545;&#20915;&#31574;&#26102;&#20855;&#26377;&#39118;&#38505;&#21388;&#24694;&#20559;&#22909;&#30340;&#19981;&#30830;&#23450;&#32467;&#26524;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#27604;&#20110;&#20165;&#20165;&#20381;&#36182;&#26399;&#26395;&#20540;&#65292;&#33258;&#28982;&#22320;&#25429;&#25417;&#20102;&#24213;&#23618;&#19981;&#30830;&#23450;&#24615;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#38543;&#26426;&#20248;&#21183;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#21364;&#24456;&#23569;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20197;&#19979;&#25361;&#25112;&#65306;$\textbf{i)}$ &#38543;&#26426;&#20248;&#21183;&#30340;&#21407;&#22987;&#27010;&#24565;&#20165;&#25552;&#20379;&#20102;$\textit{&#37096;&#20998;&#24207;}$&#65292;&#22240;&#27492;&#19981;&#33021;&#20316;&#20026;&#26368;&#20248;&#24615;&#20934;&#21017;&#65307;&#21644; $\textbf{ii)}$ &#30001;&#20110;&#35780;&#20272;&#38543;&#26426;&#20248;&#21183;&#30340;&#36830;&#32493;&#24615;&#26412;&#36136;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#39640;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24314;&#31435;&#19968;&#20010;&#19982;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;&#30456;&#20851;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38543;&#26426;&#20248;&#21183;&#27010;&#24565;&#25512;&#24191;&#65292;&#20351;&#24471;&#20219;&#24847;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#12290;&#25509;&#19979;&#26469;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#38543;&#26426;&#20248;&#21183;&#30340;&#36830;&#32493;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes, which naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations. Despite theoretically appealing, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\textbf{i)}$, the original concept of stochastic dominance only provides a $\textit{partial order}$, therefore, is not amenable to serve as an optimality criterion; and $\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.%, which barriers its application for machine learning.   In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance. We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables. We next develop a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#21644;&#26174;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#27973;&#26174;&#24335;&#32593;&#32476;&#26469;&#23454;&#29616;&#19982;&#32473;&#23450;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30456;&#21516;&#30340;&#29305;&#24449;&#20809;&#35889;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.02697</link><description>&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#19982;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#19981;&#22826;&#28145;&#30340;&#26174;&#24335;&#27169;&#22411;&#20960;&#20046;&#31561;&#20215;
&lt;/p&gt;
&lt;p&gt;
Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#21644;&#26174;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#27973;&#26174;&#24335;&#32593;&#32476;&#26469;&#23454;&#29616;&#19982;&#32473;&#23450;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30456;&#21516;&#30340;&#29305;&#24449;&#20809;&#35889;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#65288;DEQs&#65289;&#20316;&#20026;&#20856;&#22411;&#30340;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#38544;&#24335;DEQ&#21644;&#26174;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#24046;&#24322;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#22312;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23545;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36755;&#20837;&#25968;&#25454;&#19979;&#65292;&#38544;&#24335;DEQ&#30340;&#20849;&#36717;&#26680;&#65288;CK&#65289;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30697;&#38453;&#30340;&#29305;&#24449;&#20809;&#35889;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#38544;&#24335;-CKs&#21644;NTKs&#30340;&#20809;&#35889;&#34892;&#20026;&#21462;&#20915;&#20110;DEQ&#28608;&#27963;&#20989;&#25968;&#21644;&#21021;&#22987;&#26435;&#37325;&#26041;&#24046;&#65292;&#20294;&#20165;&#36890;&#36807;&#19968;&#32452;&#22235;&#20010;&#38750;&#32447;&#24615;&#26041;&#31243;&#12290;&#20316;&#20026;&#36825;&#19968;&#29702;&#35770;&#32467;&#26524;&#30340;&#30452;&#25509;&#24433;&#21709;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#31934;&#24515;&#35774;&#35745;&#19968;&#20010;&#27973;&#26174;&#24335;&#32593;&#32476;&#26469;&#20135;&#29983;&#19982;&#32473;&#23450;DEQ&#30456;&#21516;&#30340;CK&#25110;NTK&#12290;&#23613;&#31649;&#36825;&#37324;&#26159;&#38024;&#23545;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#25512;&#23548;&#30340;&#65292;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks. There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models. In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture. We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations. As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data, empirical results 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#30740;&#31350;&#20102;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#65292;&#24378;&#35843;&#20102;&#20854;&#23545;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#22495;&#27867;&#21270;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.02696</link><description>&lt;p&gt;
&#23545;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Selection for Responsible Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#30740;&#31350;&#20102;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#65292;&#24378;&#35843;&#20102;&#20854;&#23545;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#22495;&#27867;&#21270;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#24050;&#32463;&#20986;&#29616;&#65292;&#37325;&#28857;&#26159;&#23558;ML&#27169;&#22411;&#19982;&#20262;&#29702;&#21644;&#31038;&#20250;&#20215;&#20540;&#30456;&#19968;&#33268;&#65292;&#21516;&#26102;&#22686;&#24378;&#20854;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#36127;&#36131;&#20219;&#30340;ML&#28041;&#21450;&#35768;&#22810;&#38382;&#39064;&#12290;&#26412;&#35843;&#26597;&#28041;&#21450;&#22235;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21487;&#35299;&#37322;&#24615;&#65292;&#20844;&#24179;&#24615;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#22495;&#27867;&#21270;&#12290;&#29305;&#24449;&#36873;&#25321;&#22312;&#36127;&#36131;&#20219;&#30340;ML&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#22522;&#20110;&#21464;&#37327;&#20043;&#38388;&#30340;&#32479;&#35745;&#30456;&#20851;&#24615;&#26500;&#24314;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#24102;&#26377;&#20559;&#35265;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#34394;&#20551;&#27169;&#24335;&#12290;&#26412;&#35843;&#26597;&#19987;&#27880;&#20110;&#24403;&#21069;&#23545;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#30340;&#30740;&#31350;&#65306;&#20160;&#20040;&#26159;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#20197;&#21450;&#23427;&#22914;&#20309;&#21152;&#24378;&#36127;&#36131;&#20219;ML&#30340;&#22235;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#32467;&#26524;&#20135;&#29983;&#22240;&#26524;&#24433;&#21709;&#24182;&#21306;&#20998;&#22240;&#26524;&#21644;&#30456;&#20851;&#24615;&#65292;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;ML&#27169;&#22411;&#22312;&#20262;&#29702;&#21644;&#31038;&#20250;&#19978;&#36127;&#36131;&#20219;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has become an integral aspect of many real-world applications. As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness. Responsible ML involves many issues. This survey addresses four main issues: interpretability, fairness, adversarial robustness, and domain generalization. Feature selection plays a pivotal role in the responsible ML tasks. However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance. This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML. By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in hi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02695</link><description>&lt;p&gt;
&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploiting Class Probabilities for Black-box Sentence-level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#32423;&#25915;&#20987;&#26159;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#21477;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#20123;&#21477;&#23376;&#19982;&#27491;&#30830;&#20998;&#31867;&#30340;&#21477;&#23376;&#21516;&#20041;&#65292;&#20294;&#34987;&#20998;&#31867;&#22120;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#65292;&#20998;&#31867;&#22120;&#21482;&#33021;&#36890;&#36807;&#23545;&#26597;&#35810;&#36755;&#20837;&#30340;&#21453;&#39304;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20027;&#35201;&#20197;&#31867;&#21035;&#27010;&#29575;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#23613;&#31649;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#20294;&#30001;&#20110;&#22312;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#19981;&#20351;&#29992;&#21453;&#39304;&#65292;&#35201;&#20040;&#20165;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#19978;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#26159;&#21542;&#20540;&#24471;&#25110;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#19982;&#22522;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ICME 2024 Grand Challenge&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25506;&#32034;&#20102;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#39046;&#22495;&#36716;&#31227;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#21319;&#22768;&#22330;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02694</link><description>&lt;p&gt;
IEEE ICME 2024&#22823;&#25361;&#25112;&#36187;: &#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02694
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ICME 2024 Grand Challenge&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25506;&#32034;&#20102;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#39046;&#22495;&#36716;&#31227;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#21319;&#22768;&#22330;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#22330;&#20998;&#31867;&#26159;&#35745;&#31639;&#22768;&#22330;&#20998;&#26512;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#29615;&#22659;&#30340;&#29420;&#29305;&#22768;&#23398;&#29305;&#24449;&#12290;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#25152;&#24341;&#36215;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#30340;&#22768;&#22330;&#20998;&#31867;&#25361;&#25112;&#24050;&#32463;&#22312;&#35774;&#22791;&#36890;&#29992;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#20294;&#28041;&#21450;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#25991;&#21270;&#21644;&#35821;&#35328;&#31561;&#29305;&#24449;&#30340;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#39046;&#22495;&#36716;&#31227;&#30340;&#25361;&#25112;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#26410;&#26631;&#35760;&#30340;&#22768;&#22330;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#30740;&#31350;&#21033;&#29992;&#36825;&#20123;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#33021;&#26041;&#27861;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;ICME 2024&#22823;&#25361;&#25112;&#36187;&#20013;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#22768;&#22330;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acoustic scene classification (ASC) is a crucial research problem in computational auditory scene analysis, and it aims to recognize the unique acoustic characteristics of an environment. One of the challenges of the ASC task is domain shift caused by a distribution gap between training and testing data. Since 2018, ASC challenges have focused on the generalization of ASC models across different recording devices. Although this task in recent years has achieved substantial progress in device generalization, the challenge of domain shift between different regions, involving characteristics such as time, space, culture, and language, remains insufficiently explored at present. In addition, considering the abundance of unlabeled acoustic scene data in the real world, it is important to study the possible ways to utilize these unlabelled data. Therefore, we introduce the task Semi-supervised Acoustic Scene Classification under Domain Shift in the ICME 2024 Grand Challenge. We encourage par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LG-GNN&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#35745;&#31639;&#36793;&#32536;&#27010;&#29575;&#26469;&#39044;&#27979;&#22270;&#20013;&#30340;&#38142;&#25509;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#32479;&#35745;&#20445;&#35777;&#12290;&#36825;&#31181;&#26550;&#26500;&#23545;&#20110;&#31232;&#30095;&#21644;&#31264;&#23494;&#22270;&#37117;&#36866;&#29992;&#65292;&#24182;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02692</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees for Link Prediction using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LG-GNN&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#35745;&#31639;&#36793;&#32536;&#27010;&#29575;&#26469;&#39044;&#27979;&#22270;&#20013;&#30340;&#38142;&#25509;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#32479;&#35745;&#20445;&#35777;&#12290;&#36825;&#31181;&#26550;&#26500;&#23545;&#20110;&#31232;&#30095;&#21644;&#31264;&#23494;&#22270;&#37117;&#36866;&#29992;&#65292;&#24182;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#30001;&#22270;&#19978;&#29983;&#25104;&#30340;&#22270;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#25512;&#23548;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;GNN&#26550;&#26500;&#65288;LG-GNN&#65289;&#65292;&#21487;&#20197;&#20135;&#29983;&#23545;&#28508;&#22312;&#36793;&#32536;&#27010;&#29575;&#30340;&#19968;&#33268;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;&#22343;&#26041;&#35823;&#24046;&#36827;&#34892;&#20102;&#30028;&#23450;&#65292;&#24182;&#23545;LG-GNN&#22312;&#26816;&#27979;&#39640;&#27010;&#29575;&#36793;&#32536;&#30340;&#33021;&#21147;&#32473;&#20986;&#20102;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#36866;&#29992;&#20110;&#31232;&#30095;&#21644;&#31264;&#23494;&#22270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#20856;GCN&#26550;&#26500;&#30340;&#19968;&#20123;&#32570;&#28857;&#65292;&#24182;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper derives statistical guarantees for the performance of Graph Neural Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We propose a linear GNN architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities. We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges. Our guarantees hold for both sparse and dense graphs. Finally, we demonstrate some of the shortcomings of the classical GCN architecture, as well as verify our results on real and synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#36807;&#31243;&#30340;&#26032;&#22411;&#25490;&#21517;&#26367;&#20195;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#31216;&#20026;&#27850;&#26494;&#36807;&#31243;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;PoPBO&#65289;&#30340;&#39640;&#25928;BO&#26694;&#26550;&#65292;&#24182;&#20174;&#32463;&#20856;&#30340;LCB&#21644;EI&#27169;&#22411;&#20013;&#24471;&#20986;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#25910;&#38598;&#20989;&#25968;&#20197;&#36866;&#24212;&#23427;&#12290;</title><link>https://arxiv.org/abs/2402.02687</link><description>&lt;p&gt;
&#27850;&#26494;&#36807;&#31243;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Poisson Process for Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#36807;&#31243;&#30340;&#26032;&#22411;&#25490;&#21517;&#26367;&#20195;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#31216;&#20026;&#27850;&#26494;&#36807;&#31243;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;PoPBO&#65289;&#30340;&#39640;&#25928;BO&#26694;&#26550;&#65292;&#24182;&#20174;&#32463;&#20856;&#30340;LCB&#21644;EI&#27169;&#22411;&#20013;&#24471;&#20986;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#25910;&#38598;&#20989;&#25968;&#20197;&#36866;&#24212;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#27010;&#29575;&#26367;&#20195;&#27169;&#22411;&#24314;&#31435;&#40657;&#30418;&#20989;&#25968;&#30340;&#32477;&#23545;&#20989;&#25968;&#21709;&#24212;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#26500;&#24314;&#36825;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;Parzen&#20272;&#35745;&#26041;&#27861;(TPE)&#12289;&#38543;&#26426;&#26862;&#26519;(SMAC)&#21644;&#39640;&#26031;&#36807;&#31243;(GP)&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#39033;&#30340;&#30456;&#23545;&#25490;&#21517;&#65292;&#30456;&#23545;&#25490;&#21517;&#21487;&#20197;&#27604;&#32477;&#23545;&#20989;&#25968;&#21709;&#24212;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22312;&#20989;&#25968;&#21709;&#24212;&#38590;&#20197;&#22788;&#29702;&#20294;&#20559;&#22909;&#21487;&#20197;&#33719;&#21462;&#26102;&#26356;&#20855;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#36807;&#31243;&#30340;&#26032;&#22411;&#25490;&#21517;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;BO&#26694;&#26550;&#65292;&#31216;&#20026;&#27850;&#26494;&#36807;&#31243;&#36125;&#21494;&#26031;&#20248;&#21270;(PoPBO)&#12290;&#36827;&#19968;&#27493;&#20174;&#32463;&#20856;&#30340;LCB&#21644;EI&#27169;&#22411;&#20013;&#24471;&#20986;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#25910;&#38598;&#20989;&#25968;&#20197;&#36866;&#24212;&#23427;&#12290;&#19982;&#32463;&#20856;&#30340;GP-BO&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PoPBO&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#23545;&#22122;&#22768;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
BayesianOptimization(BO) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian process (GP). However, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired. To this end, we propose a novel ranking-based surrogate model based on the Poisson process and introduce an efficient BO framework, namely Poisson Process Bayesian Optimization (PoPBO). Two tailored acquisition functions are further derived from classic LCB and EI to accommodate it. Compared to the classic GP-BO method, our PoPBO has lower computation costs and better robustness to noise, which is verified b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#30340;&#26041;&#21521;&#24615;&#36890;&#35759;&#12290;&#36890;&#36807;&#24314;&#31435;LDS&#19982;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.02686</link><description>&lt;p&gt;
&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#65306;&#19968;&#31181;&#21457;&#29616;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#26041;&#21521;&#24615;&#36890;&#35759;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#30340;&#26041;&#21521;&#24615;&#36890;&#35759;&#12290;&#36890;&#36807;&#24314;&#31435;LDS&#19982;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#19981;&#21516;&#33041;&#21306;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#31070;&#32463;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#21508;&#31181;&#32479;&#35745;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#28508;&#22312;&#30340;&#36890;&#35759;&#12290;&#20004;&#20010;&#20027;&#35201;&#30340;&#31867;&#21035;&#26159;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#65292;&#27599;&#20010;&#26041;&#27861;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#28508;&#22312;&#21464;&#37327;&#65292;&#22914;&#39057;&#24102;&#21644;&#36890;&#35759;&#26041;&#21521;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;LDS&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#39640;&#65292;&#20294;&#22312;&#28508;&#22312;&#34920;&#31034;&#26041;&#38754;&#32570;&#20047;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#19982;&#22810;&#36755;&#20986;GP&#30456;&#23545;&#24212;&#30340;LDS&#65292;&#21363;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#65288;MRM-GP&#65289;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#21512;&#20108;&#20026;&#19968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#24314;&#31435;&#20102;LDS&#21644;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#31070;&#32463;&#35760;&#24405;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26126;&#30830;&#24314;&#27169;&#20102;&#39057;&#29575;&#21644;&#30456;&#20301;&#24310;&#36831;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#22312;&#26102;&#38388;&#28857;&#19978;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional repre
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02681</link><description>&lt;p&gt;
&#31561;&#21464;&#23545;&#31216;&#30772;&#32570;&#38598;
&lt;/p&gt;
&lt;p&gt;
Equivariant Symmetry Breaking Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#28041;&#21450;&#28508;&#22312;&#23545;&#31216;&#24615;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;ENN&#22312;&#32473;&#23450;&#26356;&#39640;&#23545;&#31216;&#24615;&#36755;&#20837;&#26102;&#26080;&#27861;&#20135;&#29983;&#36739;&#20302;&#23545;&#31216;&#24615;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29289;&#29702;&#31995;&#32479;&#20013;&#20250;&#21457;&#29983;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#19968;&#20010;&#21021;&#22987;&#39640;&#24230;&#23545;&#31216;&#30340;&#29366;&#24577;&#33719;&#24471;&#19968;&#20010;&#36739;&#19981;&#23545;&#31216;&#30340;&#31283;&#23450;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#22914;&#20309;&#31995;&#32479;&#22320;&#22312;ENN&#20013;&#30772;&#22351;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#26032;&#22411;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#38598;&#65288;SBS&#65289;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#26159;&#37325;&#26032;&#35774;&#35745;&#29616;&#26377;&#30340;&#32593;&#32476;&#65292;&#32780;&#26159;&#35774;&#35745;&#20102;&#19968;&#32452;&#23545;&#31216;&#30772;&#32570;&#23545;&#35937;&#65292;&#26681;&#25454;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#23545;&#31216;&#24615;&#23558;&#20854;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#38598;&#21512;&#19978;&#23450;&#20041;&#31561;&#21464;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#23427;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;... (the abstract is incomplete and cut off)
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.02680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22320;&#29702;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Geographically Biased
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#22312;&#22320;&#21547;&#26377;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#20260;&#23475;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#38543;&#30528;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20559;&#35265;&#23545;&#20110;&#23454;&#29616;&#20844;&#27491;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22320;&#29702;&#35270;&#35282;&#30740;&#31350;LLMs&#23545;&#25105;&#20204;&#25152;&#29983;&#27963;&#30340;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#24378;&#22823;&#65292;&#22240;&#20026;&#23545;&#20154;&#31867;&#29983;&#27963;&#20013;&#35832;&#22810;&#19982;&#22320;&#29702;&#31354;&#38388;&#30456;&#20851;&#30340;&#26041;&#38754;&#65288;&#22914;&#25991;&#21270;&#12289;&#31181;&#26063;&#12289;&#35821;&#35328;&#12289;&#25919;&#27835;&#21644;&#23447;&#25945;&#65289;&#26377;&#30528;&#26126;&#26174;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#38382;&#39064;&#22320;&#29702;&#20559;&#35265;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20013;&#30340;&#31995;&#32479;&#38169;&#35823;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;LLMs&#33021;&#22815;&#36827;&#34892;&#31934;&#30830;&#30340;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#65292;&#20197;&#35780;&#32423;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20854;&#19982;&#30495;&#23454;&#24773;&#20917;&#20043;&#38388;&#21576;&#29616;&#20986;&#24378;&#28872;&#30340;&#21333;&#35843;&#30456;&#20851;&#24615;&#65288;Spearman's &#961;&#26368;&#39640;&#21487;&#36798;0.89&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#20010;&#23458;&#35266;&#21644;&#23376;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#20849;&#21516;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#27010;&#29575;&#21644;&#39069;&#22806;&#30340;&#22240;&#26524;&#32467;&#26500;&#20808;&#39564;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#22240;&#26524;&#22270;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#24212;&#29992;&#20110;&#20449;&#36151;&#35780;&#32423;&#31561;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.02678</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#21457;&#29616;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#19982;&#20449;&#35465;&#35780;&#32423;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#27010;&#29575;&#21644;&#39069;&#22806;&#30340;&#22240;&#26524;&#32467;&#26500;&#20808;&#39564;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#22240;&#26524;&#22270;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#24212;&#29992;&#20110;&#20449;&#36151;&#35780;&#32423;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26377;&#21161;&#20110;&#38416;&#26126;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#36890;&#36807;&#23637;&#31034;&#20854;&#39044;&#27979;&#22522;&#30784;&#26469;&#22686;&#24378;&#20854;&#21487;&#38752;&#24615;&#12290;&#20960;&#31181;XAI&#27169;&#22411;&#32771;&#34385;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#30740;&#31350;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#22522;&#20110;&#21453;&#20107;&#23454;&#27010;&#29575;&#26469;&#35299;&#37322;&#65292;&#24182;&#20551;&#35774;&#22240;&#26524;&#22270;&#24050;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22686;&#21152;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#29305;&#24449;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;XAI&#26694;&#26550;&#65292;&#25918;&#23485;&#20102;&#22240;&#26524;&#22270;&#24050;&#30693;&#30340;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21453;&#20107;&#23454;&#27010;&#29575;&#21644;&#20851;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#39069;&#22806;&#20808;&#39564;&#20449;&#24687;&#65292;&#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20272;&#35745;&#20986;&#30340;&#22240;&#26524;&#22270;&#19982;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02675</link><description>&lt;p&gt;
&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Verifiable evaluations of machine learning models using zkSNARKs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#26469;&#36234;&#22810;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#65292;&#24320;&#21457;&#32773;&#30340;&#27169;&#22411;&#35780;&#20272;&#24517;&#39035;&#34987;&#24403;&#20316;&#38754;&#20540;&#25509;&#21463;&#12290;&#36825;&#20123;&#35780;&#20272;&#32467;&#26524;&#65292;&#26080;&#35770;&#26159;&#20219;&#21153;&#20934;&#30830;&#24615;&#12289;&#20559;&#24046;&#35780;&#20272;&#36824;&#26159;&#23433;&#20840;&#26816;&#26597;&#65292;&#20256;&#32479;&#19978;&#26080;&#27861;&#36890;&#36807;&#37325;&#26032;&#25191;&#34892;&#40657;&#31665;&#27169;&#22411;&#36755;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;zkSNARKs&#36827;&#34892;&#27169;&#22411;&#25512;&#29702;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#21487;&#20197;&#25171;&#21253;&#25104;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#26174;&#31034;&#20855;&#26377;&#22266;&#23450;&#31169;&#26377;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#36798;&#21040;&#20102;&#25152;&#36848;&#30340;&#24615;&#33021;&#25110;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#20123;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#21487;&#20197;&#22312;&#20219;&#20309;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#35745;&#31639;&#35201;&#27714;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#21644;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02672</link><description>&lt;p&gt;
&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65306;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23545;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATEs&#65289;&#30340;&#20272;&#35745;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#22914;&#26524;&#20998;&#24067;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#21487;&#20197;&#38598;&#20013;&#65292;&#21487;&#20197;&#23545;CATEs&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#65292;&#21017;&#24456;&#38590;&#36827;&#34892;&#25968;&#25454;&#32858;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;CATE&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#24635;&#32467;&#22914;&#19979;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#36827;&#34892;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#12290;&#21322;&#21442;&#25968;&#25110;&#38750;&#21442;&#25968;&#30340;CATE&#27169;&#22411;&#33021;&#22815;&#27604;&#21442;&#25968;&#27169;&#22411;&#26356;&#31283;&#20581;&#22320;&#36827;&#34892;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#23545;&#20110;&#27169;&#22411;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#26356;&#24378;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#25552;&#20986;&#26377;&#25928;&#30340;&#36890;&#20449;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#23558;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#21333;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20855;&#26377;&#22810;&#20010;&#28508;&#22312;&#30410;&#22788;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.02665</link><description>&lt;p&gt;
&#22522;&#20110;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#32479;&#19968;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#23558;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#21333;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20855;&#26377;&#22810;&#20010;&#28508;&#22312;&#30410;&#22788;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#21033;&#29992;&#29615;&#22659;&#22870;&#21169;&#21644;&#23450;&#20041;&#29992;&#25143;&#20174;&#36825;&#20123;&#22870;&#21169;&#20013;&#33719;&#24471;&#30340;&#25928;&#29992;&#30340;&#20989;&#25968;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#33539;&#24335;&#25193;&#23637;&#21040;&#21333;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#27010;&#36848;&#20102;&#22810;&#20010;&#28508;&#22312;&#30410;&#22788;&#65292;&#21253;&#25324;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#30446;&#26631;&#30456;&#20851;&#30340;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#22810;&#31574;&#30053;&#23398;&#20064;&#12289;&#39118;&#38505;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#12289;&#25240;&#25187;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#37319;&#29992;&#22522;&#20110;&#25928;&#29992;&#30340;&#26041;&#27861;&#30340;&#31639;&#27861;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in multi-objective reinforcement learning (MORL) has introduced the utility-based paradigm, which makes use of both environmental rewards and a function that defines the utility derived by the user from those rewards. In this paper we extend this paradigm to the context of single-objective reinforcement learning (RL), and outline multiple potential benefits including the ability to perform multi-policy learning across tasks relating to uncertain objectives, risk-aware RL, discounting, and safe RL. We also examine the algorithmic implications of adopting a utility-based approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#25991;&#31456;&#25506;&#35752;&#20102;&#22240;&#26524;&#27010;&#24565;&#19982;&#32431;&#31929;&#27010;&#29575;&#27010;&#24565;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#21457;&#29616;&#35745;&#31639;&#19978;&#30340;&#20844;&#27491;&#24182;&#19981;&#31561;&#21516;&#20110;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#24179;&#31561;&#12290;&#21516;&#26102;&#36824;&#32416;&#27491;&#20102;&#19968;&#20123;&#26377;&#20851;&#35745;&#31639;&#19978;&#30340;&#20844;&#27491;&#30340;&#35823;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02663</link><description>&lt;p&gt;
&#35745;&#31639;&#19978;&#30340;&#20844;&#27491;&#24182;&#38750;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#24179;&#31561;&#65292;&#20197;&#21450;&#20854;&#20182;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Fairness Is Not Demographic Parity, and Other Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02663
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#25991;&#31456;&#25506;&#35752;&#20102;&#22240;&#26524;&#27010;&#24565;&#19982;&#32431;&#31929;&#27010;&#29575;&#27010;&#24565;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#21457;&#29616;&#35745;&#31639;&#19978;&#30340;&#20844;&#27491;&#24182;&#19981;&#31561;&#21516;&#20110;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#24179;&#31561;&#12290;&#21516;&#26102;&#36824;&#32416;&#27491;&#20102;&#19968;&#20123;&#26377;&#20851;&#35745;&#31639;&#19978;&#30340;&#20844;&#27491;&#30340;&#35823;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#35880;&#24910;&#23545;&#24453;&#22312;&#22240;&#26524;&#27010;&#24565;&#19982;&#32431;&#31929;&#27010;&#29575;&#27010;&#24565;&#20043;&#38388;&#36827;&#34892;&#31561;&#20215;&#24615;&#30340;&#26029;&#35328;&#12290;&#22312;&#26412;&#31616;&#30701;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#23545;&#26368;&#36817;&#19968;&#20010;&#22768;&#31216;&#35745;&#31639;&#19978;&#30340;&#20844;&#27491;&#31561;&#21516;&#20110;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#24179;&#31561;&#30340;&#20027;&#24352;&#36827;&#34892;&#20102;&#23457;&#26597;&#12290;&#20180;&#32454;&#30740;&#31350;&#21518;&#21457;&#29616;&#35813;&#20027;&#24352;&#19981;&#25104;&#31435;&#12290;&#25105;&#23558;&#20511;&#27492;&#26426;&#20250;&#35299;&#20915;&#19968;&#20123;&#20851;&#20110;&#35745;&#31639;&#19978;&#30340;&#20844;&#27491;&#30340;&#26356;&#24191;&#27867;&#35823;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blanket statements of equivalence between causal concepts and purely probabilistic concepts should be approached with care. In this short note, I examine a recent claim that counterfactual fairness is equivalent to demographic parity. The claim fails to hold up upon closer examination. I will take the opportunity to address some broader misunderstandings about counterfactual fairness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20687;-&#23383;&#24149;&#32534;&#30721;&#65288;ICE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20272;&#26102;&#23545;&#22270;&#20687;&#21644;&#23383;&#24149;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#36827;&#34892;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#26469;&#25913;&#21892;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02662</link><description>&lt;p&gt;
&#25552;&#21319;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22270;&#20687;-&#23383;&#24149;&#32534;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image-Caption Encoding for Improving Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20687;-&#23383;&#24149;&#32534;&#30721;&#65288;ICE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20272;&#26102;&#23545;&#22270;&#20687;&#21644;&#23383;&#24149;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#36827;&#34892;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#26469;&#25913;&#21892;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#27604;&#26041;&#27861;&#19982;&#29983;&#25104;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#22522;&#30784;&#19978;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#31561;&#19979;&#28216;&#25512;&#26029;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#20854;&#22312;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24403;&#19968;&#20010;&#20998;&#24067;&#22806;&#25968;&#25454;&#28857;&#34987;&#38169;&#35823;&#20998;&#31867;&#26102;&#65292;&#27491;&#30830;&#31867;&#21035;&#36890;&#24120;&#21487;&#20197;&#22312;&#21069;K&#20010;&#39044;&#27979;&#31867;&#21035;&#20013;&#25214;&#21040;&#12290;&#20026;&#20102;&#23558;&#27169;&#22411;&#39044;&#27979;&#23548;&#21521;&#21069;K&#20010;&#39044;&#27979;&#31867;&#21035;&#20013;&#30340;&#27491;&#30830;&#31867;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#20687;-&#23383;&#24149;&#32534;&#30721;&#65288;ICE&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#30452;&#25509;&#22312;&#35780;&#20272;&#26102;&#21482;&#22312;&#22270;&#20687;&#26465;&#20214;&#21644;&#23383;&#24149;&#26465;&#20214;&#19979;&#36827;&#34892;&#19968;&#33268;&#24615;&#32422;&#26463;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29983;&#25104;&#30340;&#23383;&#24149;&#30340;&#29420;&#29305;&#23646;&#24615;&#26469;&#25351;&#23548;&#25105;&#20204;&#22312;&#21069;K&#20010;&#39044;&#27979;&#31867;&#21035;&#20013;&#23547;&#25214;&#27491;&#30830;&#31867;&#21035;&#26631;&#31614;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20854;&#20182;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02658</link><description>&lt;p&gt;
&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#30340;&#39564;&#35777;&#22120;&#65306;&#20851;&#20110;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#30417;&#30563;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#39564;&#35777;&#22120;&#26469;&#35780;&#20272;&#25512;&#29702;&#22120;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#24050;&#32463;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#22312;&#39564;&#35777;&#22120;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#25454;&#25972;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;MiPS&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#20934;&#30830;&#23436;&#25104;&#30340;&#27604;&#20363;&#23450;&#20041;&#20026;&#20934;&#30830;&#29575;&#12290;&#25512;&#29702;&#22120;&#20013;&#30340;&#38169;&#35823;&#20250;&#23548;&#33268;MiPS&#20302;&#20272;&#20013;&#38388;&#27493;&#39588;&#30340;&#20934;&#30830;&#29575;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#65292;&#32780;&#19981;&#26159;&#20302;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65288;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;+0.67&#65285;&#65292;&#25968;&#23398;&#19978;&#30340;&#20934;&#30830;&#29575;+4.16&#65285;&#65292;MBPP&#19978;&#30340;&#20934;&#30830;&#29575;+0.92&#65285;&#19982;&#36755;&#20986;s&#30456;&#27604;&#12290;&#65289;
&lt;/p&gt;
&lt;p&gt;
Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#21407;&#22411;&#36827;&#34892;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22806;&#37096;&#20998;&#24067;&#30340;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02653</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22411;&#30340;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning with Mixture of Prototypes for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#21407;&#22411;&#36827;&#34892;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22806;&#37096;&#20998;&#24067;&#30340;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#36828;&#31163;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#35757;&#32451;&#25968;&#25454;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#36825;&#23545;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36317;&#31163;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26679;&#26412;&#19982;ID&#31867;&#21035;&#20013;&#24515;&#25110;&#21407;&#22411;&#30340;&#36317;&#31163;&#26469;&#35782;&#21035;&#26410;&#30693;OOD&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#23398;&#20064;&#34920;&#31034;&#26102;&#20381;&#36182;&#20110;&#36807;&#20110;&#31616;&#21270;&#30340;&#25968;&#25454;&#20551;&#35774;&#65292;&#20363;&#22914;&#65292;&#29992;&#19968;&#20010;&#20013;&#24515;&#31867;&#21407;&#22411;&#24314;&#27169;&#27599;&#20010;&#31867;&#30340;ID&#25968;&#25454;&#65292;&#25110;&#32773;&#20351;&#29992;&#19981;&#36866;&#29992;&#20110;OOD&#26816;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24573;&#35270;&#20102;&#25968;&#25454;&#20013;&#30340;&#33258;&#28982;&#22810;&#26679;&#24615;&#12290;&#23558;&#27599;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#26679;&#26412;&#24378;&#21046;&#32039;&#20945;&#22320;&#22260;&#32469;&#19968;&#20010;&#21407;&#22411;&#36827;&#34892;&#24314;&#27169;&#20250;&#23548;&#33268;&#23545;&#30495;&#23454;&#25968;&#25454;&#24314;&#27169;&#19981;&#22815;&#20805;&#20998;&#65292;&#24615;&#33021;&#21463;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#21407;&#22411;&#27169;&#22411;&#30340;&#21407;&#22411;&#23398;&#20064;&#19982;&#28151;&#21512;&#65288;PALM&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sam
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02651</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models Provide Promptable Representations for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#20195;&#29702;&#36890;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#22823;&#37327;&#36890;&#29992;&#21644;&#21487;&#32034;&#24341;&#30340;&#19990;&#30028;&#30693;&#35782;&#26469;&#36827;&#34892;&#20855;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;VLMs&#29992;&#20316;&#21487;&#25552;&#31034;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#31574;&#30053;&#65306;&#36825;&#20123;&#23884;&#20837;&#22312;&#35270;&#35273;&#35266;&#23519;&#20013;&#20855;&#26377;&#22522;&#30784;&#65292;&#24182;&#26681;&#25454;VLM&#30340;&#20869;&#37096;&#30693;&#35782;&#32534;&#30721;&#35821;&#20041;&#29305;&#24449;&#65292;&#36890;&#36807;&#25552;&#20379;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#22312;Minecraft&#21644;Habitat&#20013;&#30340;&#35270;&#35273;&#22797;&#26434;&#12289;&#38271;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36890;&#29992;&#22411;VLMs&#25552;&#21462;&#30340;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#32988;&#36807;&#20351;&#29992;&#36890;&#29992;&#30340;&#12289;&#19981;&#21487;&#25552;&#31034;&#30340;&#22270;&#20687;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#36981;&#24490;&#25351;&#31034;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02644</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#30340;&#26041;&#27861;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Variational DAG Estimation via State Augmentation With Stochastic Permutations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02644
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#21363;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#65292;&#26159;&#19968;&#20010;&#22312;&#32479;&#35745;&#21644;&#35745;&#31639;&#19978;&#37117;&#24456;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#31561;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#24212;&#29992;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#26041;&#38754;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#22788;&#29702;&#20247;&#25152;&#21608;&#30693;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#12290;&#20174;&#27010;&#29575;&#25512;&#26029;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#65288;i&#65289;&#34920;&#31034;&#28385;&#36275;DAG&#32422;&#26463;&#30340;&#22270;&#30340;&#20998;&#24067;&#21644;&#65288;ii&#65289;&#20272;&#35745;&#24213;&#23618;&#32452;&#21512;&#31354;&#38388;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;DAG&#21644;&#25490;&#21015;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26500;&#24314;&#32852;&#21512;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#21518;&#39564;&#20272;&#35745;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;&#31163;&#25955;&#20998;&#24067;&#30340;&#36830;&#32493;&#26494;&#24347;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#33021;&#22815;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and re
&lt;/p&gt;</description></item><item><title>LLMDB&#26159;&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#21644;&#20302;&#20934;&#30830;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02643</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#22411;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM-Enhanced Data Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02643
&lt;/p&gt;
&lt;p&gt;
LLMDB&#26159;&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#21644;&#20302;&#20934;&#30830;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#20248;&#21270;&#25968;&#25454;&#31649;&#29702;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24191;&#27867;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#65288;&#36866;&#24212;&#19981;&#21516;&#24773;&#26223;&#65289;&#21644;&#25512;&#29702;&#33021;&#21147;&#65288;&#29702;&#35299;&#19978;&#19979;&#25991;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20154;&#31867;&#31454;&#20105;&#33021;&#21147;&#65292;&#23545;&#20110;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#65288;&#22914;&#25968;&#25454;&#24211;&#35786;&#26029;&#12289;&#25968;&#25454;&#24211;&#35843;&#20248;&#65289;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#20197;&#21450;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#20302;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLMDB&#65292;&#19968;&#31181;&#20351;&#29992;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#34394;&#26500;&#65292;&#38477;&#20302;&#20102;LLM&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;LLMDB&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#20197;&#36991;&#20813;&#34394;&#26500;&#12290;LLMDB&#36890;&#36807;&#20943;&#23569;LLMs&#30340;&#39640;&#25104;&#26412; addresses challenges: hallucination, high cost, low accuracy-
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by 
&lt;/p&gt;</description></item><item><title>$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#26159;&#23558;$C^*$-&#20195;&#25968;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#36890;&#36807;&#32479;&#19968;&#29616;&#26377;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#26356;&#22810;&#20803;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02637</link><description>&lt;p&gt;
$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#65306;&#36808;&#21521;&#26032;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
$C^*$-Algebraic Machine Learning: Moving in a New Direction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02637
&lt;/p&gt;
&lt;p&gt;
$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#26159;&#23558;$C^*$-&#20195;&#25968;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#36890;&#36807;&#32479;&#19968;&#29616;&#26377;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#26356;&#22810;&#20803;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#25968;&#23398;&#30340;&#20960;&#20010;&#39046;&#22495;&#65288;&#22914;&#32479;&#35745;&#23398;&#12289;&#27010;&#29575;&#35770;&#21644;&#32447;&#24615;&#20195;&#25968;&#65289;&#26377;&#30528;&#38271;&#26399;&#30340;&#21512;&#20316;&#20256;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#26041;&#21521;&#65306;$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#65292;&#36825;&#26159;$C^*$-&#20195;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#20132;&#27969;&#21644;&#30456;&#20114;&#28363;&#20859;&#12290;$C^*$-&#20195;&#25968;&#26159;&#22797;&#25968;&#31354;&#38388;&#30340;&#33258;&#28982;&#25512;&#24191;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#32479;&#19968;&#29616;&#26377;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#26356;&#22810;&#20803;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;$C^*$-&#20195;&#25968;&#30340;&#21407;&#22240;&#21644;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#35774;&#35745;$C^*$-&#20195;&#25968;&#23398;&#20064;&#27169;&#22411;&#30340;&#25216;&#26415;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#21457;&#23637;&#21644;&#24212;&#29992;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02636</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#23398;&#20064;&#29420;&#31435;&#30340;&#22240;&#26524;&#26426;&#21046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Learn Independent Causal Mechanisms?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#19981;&#24120;&#35265;&#30340;&#29615;&#22659;&#35774;&#32622;&#25110;&#20998;&#24067;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#30446;&#21069;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#65292;&#22240;&#20026;&#20219;&#21153;&#30340;&#33539;&#22260;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#25110;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#37027;&#20123;&#23398;&#20064;&#25277;&#35937;&#21464;&#37327;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#31995;&#32479;&#65292;&#22914;&#22240;&#26524;&#27169;&#22411;&#65292;&#21487;&#20197;&#34920;&#29616;&#20986;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#26356;&#24378;&#31283;&#20581;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#23384;&#22312;&#24182;&#20351;&#29992;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65288;ICMs&#65289;&#65292;&#34920;&#31034;&#21482;&#31232;&#30095;&#20132;&#20114;&#30340;&#39640;&#23618;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#20004;&#20010;&#27010;&#24565;&#65292;&#22312;LLMs&#20013;&#23398;&#20064;ICMs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#31232;&#30095;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;LLM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#22270;&#21464;&#25442;&#22120;&#65288;KGT&#65289;&#29992;&#20110;&#39640;&#25928;&#22320;&#36827;&#34892;&#22270;&#20687;&#20462;&#22797;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#36830;&#25509;&#20851;&#38190;&#33410;&#28857;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;6&#20010;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02634</link><description>&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#30340;&#20851;&#38190;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Key-Graph Transformer for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02634
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#22270;&#21464;&#25442;&#22120;&#65288;KGT&#65289;&#29992;&#20110;&#39640;&#25928;&#22320;&#36827;&#34892;&#22270;&#20687;&#20462;&#22797;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#36830;&#25509;&#20851;&#38190;&#33410;&#28857;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;6&#20010;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#25928;&#30340;&#22270;&#20687;&#20462;&#22797;&#20013;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23558;&#36825;&#20123;&#25552;&#31034;&#38598;&#25104;&#21040;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#20013;&#20250;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#21152;&#37325;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#36755;&#20837;&#20998;&#36776;&#29575;&#19979;&#12290;&#27492;&#22806;&#65292;&#21464;&#25442;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#23481;&#26131;&#32771;&#34385;&#19982;&#26080;&#20851;&#23545;&#35937;&#25110;&#21306;&#22495;&#30340;&#19981;&#24517;&#35201;&#20840;&#23616;&#25552;&#31034;&#65292;&#20174;&#32780;&#24341;&#20837;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#20851;&#38190;&#22270;&#21464;&#25442;&#22120;&#65288;KGT&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KGT&#23558;&#22270;&#20687;&#22359;&#29305;&#24449;&#35270;&#20026;&#22270;&#33410;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#20851;&#38190;&#22270;&#26500;&#36896;&#22120;&#36890;&#36807;&#20165;&#36873;&#25321;&#20851;&#38190;&#33410;&#28857;&#32780;&#19981;&#26159;&#25152;&#26377;&#33410;&#28857;&#26469;&#39640;&#25928;&#22320;&#24418;&#25104;&#31232;&#30095;&#20294;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20851;&#38190;&#22270;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#31383;&#21475;&#20869;&#65292;&#21482;&#22312;&#25152;&#36873;&#25321;&#33410;&#28857;&#20043;&#38388;&#24341;&#23548;&#20851;&#38190;&#22270;&#30340;&#25351;&#23548;&#19979;&#36827;&#34892;&#20851;&#38190;&#22270;&#27880;&#24847;&#25805;&#20316;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#32447;&#24615;&#12290;&#23545;6&#20010;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;KGT&#22312;&#24615;&#33021;&#19978;&#30340;&#39046;&#20808;&#22320;&#20301;&#65292;&#23637;&#31034;&#20102;&#21452;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
While it is crucial to capture global information for effective image restoration (IR), integrating such cues into transformer-based methods becomes computationally expensive, especially with high input resolution. Furthermore, the self-attention mechanism in transformers is prone to considering unnecessary global cues from unrelated objects or regions, introducing computational inefficiencies. In response to these challenges, we introduce the Key-Graph Transformer (KGT) in this paper. Specifically, KGT views patch features as graph nodes. The proposed Key-Graph Constructor efficiently forms a sparse yet representative Key-Graph by selectively connecting essential nodes instead of all the nodes. Then the proposed Key-Graph Attention is conducted under the guidance of the Key-Graph only among selected nodes with linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed KGT's state-of-the-art performance, showcasing advancements both
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#39046;&#22495;&#30456;&#20284;&#24615;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02633</link><description>&lt;p&gt;
&#39044;&#27979;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#65306;&#39046;&#22495;&#30456;&#20284;&#24615;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#39046;&#22495;&#30456;&#20284;&#24615;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRLs&#65289;&#65292;&#32454;&#35843;&#21644;&#27979;&#35797;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#24573;&#35270;&#20102;LRLs&#21644;&#36328;&#39046;&#22495;&#30340;&#36716;&#21464;&#12290;&#38024;&#23545;LRLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#22240;&#32032;&#65306;&#32454;&#35843;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#65292;&#32454;&#35843;&#35821;&#26009;&#24211;&#19982;&#27979;&#35797;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#39046;&#22495;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#32463;&#20856;&#22238;&#24402;&#27169;&#22411;&#35780;&#20272;&#36825;&#20123;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#39046;&#22495;&#30456;&#20284;&#24615;&#23545;&#20110;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs). While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors: the size of the fine-tuning corpus, the domain similarity between fine-tuning and testing corpora, and the language similarity between source and target languages. We employ classical regression models to assess how these factors impact the model's performance. Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;Mobius&#21464;&#25442;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#38750;&#38646;&#31995;&#25968;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#24674;&#22797;Mobius&#21464;&#25442;&#65292;&#24182;&#25581;&#31034;&#20102;&#32676;&#20307;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02631</link><description>&lt;p&gt;
&#23398;&#20064;&#29702;&#35299;&#65306;&#36890;&#36807;Mobius&#21464;&#25442;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Understand: Identifying Interactions via the Mobius Transform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;Mobius&#21464;&#25442;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#38750;&#38646;&#31995;&#25968;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#24674;&#22797;Mobius&#21464;&#25442;&#65292;&#24182;&#25581;&#31034;&#20102;&#32676;&#20307;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25214;&#21040;&#25105;&#20204;&#23398;&#20064;&#30340;&#20989;&#25968;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#12290;Mobius&#21464;&#25442;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#30340;&#31995;&#25968;&#23545;&#24212;&#20110;&#36755;&#20837;&#21464;&#37327;&#38598;&#21512;&#19978;&#30340;&#21807;&#19968;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;Mobius&#21464;&#25442;&#19982;Shapley&#20540;&#30340;&#27010;&#24565;&#23494;&#20999;&#30456;&#20851;&#65288;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#31561;&#20215;&#30340;&#65289;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21338;&#24328;&#35770;&#37325;&#35201;&#24615;&#27010;&#24565;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#22312;$n$&#20010;&#36755;&#20837;&#20043;&#38388;&#30340;&#25152;&#26377;$2^n$&#20010;&#21487;&#33021;&#20132;&#20114;&#20043;&#20013;&#65292;&#38750;&#38646;Mobius&#31995;&#25968;&#65288;&#21644;&#22240;&#27492;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65289;&#30340;&#27604;&#20363;&#23567;&#20110;&#38750;&#38646;&#31995;&#25968;&#24635;&#25968;&#30340;&#65288;&#20856;&#22411;&#65289;&#24773;&#20917;&#12290;&#24403;&#26377;$K = O(2^{n \delta})$&#20010;&#65292;&#20854;&#20013;$\delta \leq \frac{1}{3}$&#30340;&#38750;&#38646;&#31995;&#25968;&#20197;&#22343;&#21248;&#38543;&#26426;&#26041;&#24335;&#36873;&#25321;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;$O(Kn)$&#20010;&#26679;&#26412;&#21644;$O(Kn^2)$&#30340;&#26102;&#38388;&#20869;&#23436;&#20840;&#24674;&#22797;Mobius&#21464;&#25442;&#65292;&#24182;&#19988;&#38543;&#30528;$K \rightarrow \infty$&#65292;&#35823;&#24046;&#36235;&#20110;&#38646;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#32676;&#20307;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn. The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables. The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance. This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs. When there are $K = O(2^{n \delta})$ with $\delta \leq \frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \rightarrow \infty$, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#31181;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#65306;&#37327;&#21270;&#21644;&#31561;&#20215;&#26597;&#35810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;O2RNN&#21644;&#22522;&#20110;&#37327;&#21270;&#30340;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02627</link><description>&lt;p&gt;
&#23545;&#22810;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#31181;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#65306;&#37327;&#21270;&#21644;&#31561;&#20215;&#26597;&#35810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;O2RNN&#21644;&#22522;&#20110;&#37327;&#21270;&#30340;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#31181;&#31454;&#20105;&#30340;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#65306;&#37327;&#21270;&#21644;&#31561;&#20215;&#26597;&#35810;&#12290;&#25105;&#20204;&#20351;&#29992;&#37327;&#21270;&#26041;&#27861;&#65288;k-means&#21644;SOM&#65289;&#20174;3600&#20010;RNN&#27169;&#22411;&#20013;&#25552;&#21462;&#20102;18000&#20010;DFA&#65292;&#20351;&#29992;&#31561;&#20215;&#26597;&#35810;&#65288;$L^{*}$&#65289;&#26041;&#27861;&#20174;3600&#20010;DFA&#27169;&#22411;&#20013;&#25552;&#21462;&#20102;10&#20010;&#21021;&#22987;&#21270;&#31181;&#23376;&#12290;&#25105;&#20204;&#20174;7&#20010;Tomita&#35821;&#27861;&#21644;4&#20010;Dyck&#35821;&#27861;&#20013;&#25277;&#26679;&#24471;&#21040;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;4&#20010;RNN&#21333;&#20803;&#65288;LSTM&#12289;GRU&#12289;O2RNN&#21644;MIRNN&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;O2RNN&#21644;&#22522;&#20110;&#37327;&#21270;&#30340;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#24403;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#35757;&#32451;&#26102;&#65292;$L^{*}$&#22312;Tomita&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#19982;&#37327;&#21270;&#26041;&#27861;&#31867;&#20284;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#37096;&#20998;&#35757;&#32451;&#30340;RNN&#65292;$L^{*}$&#22312;DFA&#29366;&#24577;&#25968;&#37327;&#19978;&#34920;&#29616;&#19981;&#31283;&#23450;&#65292;&#20363;&#22914;&#23545;&#20110;Tomita 5&#21644;Tomita 6&#35821;&#35328;&#65292;$L^{*}$&#20135;&#29983;&#20102;&#36229;&#36807;100&#20010;&#29366;&#24577;&#30340;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#37327;&#21270;&#26041;&#27861;&#30340;&#35268;&#21017;&#29366;&#24577;&#25968;&#37327;&#38750;&#24120;&#25509;&#36817;&#30495;&#23454;&#30340;DFA&#12290;&#22312;RNN&#20013;&#65292;O2RNN&#21644;quantization-based&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are perfectly trained. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with number of states very close to ground truth DFA. Among RNN c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#30340;&#21160;&#24577;&#25490;&#21517;&#31995;&#32479;&#30340;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25991;&#26723;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#22312;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#39640;&#26041;&#24046;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#20272;&#35745;&#20301;&#32622;&#20559;&#24046;&#30340;&#24517;&#35201;&#24615;&#65292;&#24314;&#35758;&#21516;&#26102;&#20351;&#29992;&#26377;&#20559;&#21644;&#26080;&#20559;&#30340;&#20301;&#32622;&#20559;&#24046;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.02626</link><description>&lt;p&gt;
&#29305;&#24449;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Position bias in features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#30340;&#21160;&#24577;&#25490;&#21517;&#31995;&#32479;&#30340;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25991;&#26723;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#22312;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#39640;&#26041;&#24046;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#20272;&#35745;&#20301;&#32622;&#20559;&#24046;&#30340;&#24517;&#35201;&#24615;&#65292;&#24314;&#35758;&#21516;&#26102;&#20351;&#29992;&#26377;&#20559;&#21644;&#26080;&#20559;&#30340;&#20301;&#32622;&#20559;&#24046;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#20013;&#24314;&#27169;&#25991;&#20214;&#30456;&#20851;&#24615;&#30340;&#30446;&#30340;&#26159;&#22312;&#21518;&#32493;&#25628;&#32034;&#20013;&#26356;&#22909;&#22320;&#25490;&#21517;&#12290;&#25991;&#26723;&#29305;&#23450;&#30340;&#21382;&#21490;&#28857;&#20987;&#29575;&#21487;&#20197;&#20316;&#20026;&#21160;&#24577;&#25490;&#21517;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#38543;&#30528;&#25105;&#20204;&#31215;&#32047;&#26356;&#22810;&#26679;&#26412;&#65292;&#31995;&#32479;&#20250;&#36827;&#34892;&#26356;&#26032;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20960;&#31181;&#36825;&#26679;&#30340;&#29305;&#24449;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#25511;&#21046;&#23454;&#39564;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23558;&#21453;&#21521;&#20542;&#21521;&#21152;&#26435;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26723;&#19978;&#21487;&#20197;&#20135;&#29983;&#23545;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#36825;&#20010;&#29305;&#24449;&#21487;&#20197;&#20934;&#30830;&#22320;&#36817;&#20284;&#30456;&#20851;&#24615;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20855;&#26377;&#39640;&#26041;&#24046;&#65292;&#19988;&#38543;&#30528;&#20301;&#32622;&#20559;&#24046;&#30340;&#31243;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#19981;&#20934;&#30830;&#30340;&#20301;&#32622;&#20559;&#24046;&#20272;&#35745;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#29305;&#24449;&#30340;&#34920;&#29616;&#21487;&#33021;&#19981;&#22914;&#20559;&#20506;&#30340;&#28857;&#20987;&#29575;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#20934;&#30830;&#30340;&#20301;&#32622;&#20559;&#24046;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#29420;&#29305;&#22320;&#24314;&#35758;&#21516;&#26102;&#20351;&#29992;&#26377;&#20559;&#21644;&#26080;&#20559;&#30340;&#20301;&#32622;&#20559;&#24046;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of modeling document relevance for search engines is to rank better in subsequent searches. Document-specific historical click-through rates can be important features in a dynamic ranking system which updates as we accumulate more sample. This paper describes the properties of several such features, and tests them in controlled experiments. Extending the inverse propensity weighting method to documents creates an unbiased estimate of document relevance. This feature can approximate relevance accurately, leading to near-optimal ranking in ideal circumstances. However, it has high variance that is increasing with respect to the degree of position bias. Furthermore, inaccurate position bias estimation leads to poor performance. Under several scenarios this feature can perform worse than biased click-through rates. This paper underscores the need for accurate position bias estimation, and is unique in suggesting simultaneous use of biased and unbiased position bias features.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.02625</link><description>&lt;p&gt;
&#29992;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#22686;&#24378;Transformer RNNs
&lt;/p&gt;
&lt;p&gt;
Enhancing Transformer RNNs with Multiple Temporal Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02625
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20854;&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#32500;&#25252;&#20808;&#21069;&#36935;&#21040;&#30340;&#25991;&#26412;&#30340;&#22810;&#26679;&#26102;&#38388;&#35270;&#22270;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#32435;&#20837;&#20102;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#35813;&#26550;&#26500;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#20445;&#30041;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#26368;&#23569;&#65288;&#20165;&#20026;&#26368;&#21021;&#21442;&#25968;&#25968;&#37327;&#30340;0.04%&#65289;&#65292;&#20063;&#23454;&#29616;&#20102;&#27492;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#25152;&#38656;&#30340;&#39069;&#22806;&#21442;&#25968;&#32463;&#36807;&#24494;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#23436;&#20840;&#39044;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#25552;&#31034;&#25512;&#26029;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#26435;&#37325;&#21464;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36816;&#21160;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#21160;&#24577;&#35843;&#25972;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#65292;&#21487;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26368;&#20248;&#38381;&#29615;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02624</link><description>&lt;p&gt;
&#19968;&#31181;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#26435;&#37325;&#21464;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#26435;&#37325;&#21464;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36816;&#21160;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#21160;&#24577;&#35843;&#25972;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#65292;&#21487;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26368;&#20248;&#38381;&#29615;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#26368;&#20339;&#20195;&#20215;&#20989;&#25968;&#21442;&#25968;&#20197;&#20248;&#21270;&#22810;&#20010;&#25511;&#21046;&#30446;&#26631;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#25216;&#26415;&#36890;&#36807;&#30830;&#23450;MPC&#30340; Pareto &#26368;&#20248;&#21442;&#25968;&#38598;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#24403;MPC&#30340;&#25805;&#20316;&#26465;&#20214;&#19978;&#19979;&#25991;&#22312;&#20854;&#36816;&#34892;&#36807;&#31243;&#20013;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#21333;&#20010;&#21442;&#25968;&#38598;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#26368;&#20248;&#30340;&#38381;&#29615;&#25511;&#21046;&#24615;&#33021;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#36816;&#34892;&#26102;&#35843;&#25972;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(RL)&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26368;&#20248;&#21442;&#25968;&#38598;&#65292;&#24182;&#22312;&#26435;&#37325;&#21464;&#21270;&#30340;MPC&#20013;&#36827;&#34892;&#21160;&#24577;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#20174;&#22836;&#23398;&#20064;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#25805;&#20316;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;RL&#30340;&#21160;&#20316;&#38480;&#21046;&#22312;&#23433;&#20840;&#23398;&#20064;&#31354;&#38388;&#20869;&#65292;&#35813;&#31354;&#38388;&#34920;&#31034;&#32463;&#36807;&#39044;&#20248;&#21270;&#30340;BO Pareto&#26368;&#20248;&#26435;&#37325;&#30340;&#30446;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight se
&lt;/p&gt;</description></item><item><title>DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02622</link><description>&lt;p&gt;
DenseFormer: &#36890;&#36807;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#22686;&#24378;Transformer&#20013;&#30340;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02622
&lt;/p&gt;
&lt;p&gt;
DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;Vaswani&#31561;&#20154;&#65288;2017&#65289;&#30340;Transformer&#26550;&#26500;&#29616;&#24050;&#26222;&#36941;&#24212;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#35821;&#38899;&#22788;&#29702;&#21644;&#22270;&#20687;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DenseFormer&#65292;&#36825;&#26159;&#23545;&#26631;&#20934;&#26550;&#26500;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#19981;&#22686;&#21152;&#20854;&#22823;&#23567;-&#23545;&#20110;&#25317;&#26377;100B&#21442;&#25968;&#33539;&#22260;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#21482;&#38656;&#28155;&#21152;&#20960;&#21315;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#20381;&#38752;&#39069;&#22806;&#30340;&#24179;&#22343;&#27493;&#39588;&#65292;&#35745;&#31639;&#24403;&#21069;&#21644;&#36807;&#21435;&#34920;&#31034;&#30340;&#21152;&#26435;&#24179;&#22343;-&#25105;&#20204;&#23558;&#36825;&#20010;&#25805;&#20316;&#31216;&#20026;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65288;DWA&#65289;&#12290;&#23398;&#21040;&#30340;DWA&#26435;&#37325;&#23637;&#29616;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#36828;&#23618;&#30340;&#28608;&#27963;&#30340;&#24378;&#22823;&#19988;&#32467;&#26500;&#21270;&#30340;&#37325;&#22797;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#36798;&#21040;&#27604;&#26356;&#28145;&#30340;transformer&#27169;&#22411;&#30456;&#21516;&#30340;&#22256;&#24785;&#24230;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#65292;&#36825;&#20123;&#26032;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;transformer&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#21152;&#36895;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#20013;&#24378;&#21270;&#23398;&#20064;&#23376;&#31243;&#24207;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#19982;&#19987;&#23478;&#25968;&#25454;&#20998;&#24067;&#25509;&#36817;&#30340;&#24754;&#35266;&#20027;&#20041;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02616</link><description>&lt;p&gt;
&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#20013;&#24754;&#35266;&#20027;&#20041;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
The Virtues of Pessimism in Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#21152;&#36895;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#20013;&#24378;&#21270;&#23398;&#20064;&#23376;&#31243;&#24207;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#19982;&#19987;&#23478;&#25968;&#25454;&#20998;&#24067;&#25509;&#36817;&#30340;&#24754;&#35266;&#20027;&#20041;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#38656;&#35201;&#22312;&#20869;&#24490;&#29615;&#20013;&#21453;&#22797;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#26114;&#36149;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#20943;&#23569;&#25506;&#32034;&#36127;&#25285;&#22312;&#20869;&#24490;&#29615;&#30340;RL&#20013;&#38750;&#24120;&#21487;&#21462;&#12290;&#20363;&#22914;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#23398;&#20064;&#32773;&#37325;&#32622;&#21040;&#19987;&#23478;&#29366;&#24577;&#26469;&#25351;&#23548;&#23398;&#20064;&#32773;&#22312;&#39640;&#22238;&#25253;&#19987;&#23478;&#29366;&#24577;&#19979;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#21152;&#36895;IRL&#20013;RL&#23376;&#31243;&#24207;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#24754;&#35266;&#20027;&#20041;&#65292;&#21363;&#20445;&#25345;&#19982;&#19987;&#23478;&#25968;&#25454;&#20998;&#24067;&#25509;&#36817;&#65292;&#36890;&#36807;&#31163;&#32447;RL&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;RL&#21644;IRL&#20043;&#38388;&#24418;&#25104;&#20102;&#19968;&#20010;&#36830;&#25509;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#31163;&#32447;RL&#31639;&#27861;&#26469;&#25552;&#39640;IRL&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#23637;&#31034;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#25506;&#32034;&#36127;&#25285;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20351;&#29992;&#19987;&#23478;&#31034;&#33539;&#65292;&#20943;&#23569;&#23545;&#20869;&#37096;&#24378;&#21270;&#23398;&#20064;&#24490;&#29615;&#20013;&#30340;&#30828;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;MaxEntIRL&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.02608</link><description>&lt;p&gt;
&#21152;&#36895;&#36870;&#24378;&#21270;&#23398;&#20064;&#19982;&#19987;&#23478;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Accelerating Inverse Reinforcement Learning with Expert Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20351;&#29992;&#19987;&#23478;&#31034;&#33539;&#65292;&#20943;&#23569;&#23545;&#20869;&#37096;&#24378;&#21270;&#23398;&#20064;&#24490;&#29615;&#20013;&#30340;&#30828;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;MaxEntIRL&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;MaxEntIRL&#65292;f-IRL&#65289;&#22312;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#24182;&#22312;&#20869;&#24490;&#29615;&#20013;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#36896;&#25104;&#20102;&#19968;&#20010;&#27604;&#36739;&#22855;&#24618;&#30340;&#20498;&#32622;&#65292;&#19968;&#20010;&#26356;&#38590;&#30340;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#65292;&#20301;&#20110;&#19968;&#20010;&#30456;&#23545;&#36739;&#26131;&#30340;&#38382;&#39064;&#65292;&#27169;&#20223;&#23398;&#20064;&#30340;&#20869;&#24490;&#29615;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#19987;&#23478;&#31034;&#33539;&#21487;&#20197;&#20943;&#23569;&#20869;&#37096;&#24378;&#21270;&#23398;&#20064;&#24490;&#29615;&#20013;&#23545;&#30828;&#25506;&#32034;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#21152;&#36895;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#23558;&#19987;&#23478;&#36716;&#22330;&#25918;&#20837;&#20869;&#37096;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;Soft-Actor Critic&#65289;&#30340;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#30452;&#25509;&#21521;&#23398;&#20064;&#32773;&#25552;&#20379;&#39640;&#22870;&#21169;&#29366;&#24577;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#24191;&#27867;&#30340;&#25506;&#32034;&#26469;&#21457;&#29616;&#23427;&#20204;&#65292;&#65288;2&#65289;&#22312;Q&#20540;&#24341;&#23548;&#20013;&#20351;&#29992;&#19987;&#23478;&#34892;&#20026;&#65292;&#20197;&#25913;&#36827;&#30446;&#26631;Q&#20540;&#20272;&#35745;&#24182;&#26356;&#20934;&#30830;&#22320;&#25551;&#36848;&#39640;&#20215;&#20540;&#30340;&#19987;&#23478;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MaxEntIRL&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL) search over candidate reward functions and solve a reinforcement learning problem in the inner loop. This creates a rather strange inversion where a harder problem, reinforcement learning, is in the inner loop of a presumably easier problem, imitation learning. In this work, we show that better utilization of expert demonstrations can reduce the need for hard exploration in the inner RL loop, hence accelerating learning. Specifically, we propose two simple recipes: (1) placing expert transitions into the replay buffer of the inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner about high reward states instead of forcing the learner to discover them through extensive exploration, and (2) using expert actions in Q value bootstrapping in order to improve the target Q value estimates and more accurately describe high value expert states. Our methods show significant gains over a MaxEntIRL ba
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;&#21152;&#23494;&#24037;&#20855;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25104;&#21151;&#22320;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#65292;&#35268;&#36991;&#20102;&#26368;&#20808;&#36827;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#65292;&#24182;&#19988;&#22312;&#35268;&#36991;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20351;&#29992;&#39640;&#32423;&#20462;&#25913;&#26041;&#27861;&#30340;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.02600</link><description>&lt;p&gt;
&#32469;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#28151;&#28102;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;&#21152;&#23494;&#24037;&#20855;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25104;&#21151;&#22320;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#65292;&#35268;&#36991;&#20102;&#26368;&#20808;&#36827;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#65292;&#24182;&#19988;&#22312;&#35268;&#36991;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20351;&#29992;&#39640;&#32423;&#20462;&#25913;&#26041;&#27861;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#29983;&#25104;&#65288;AMG&#65289;&#26159;&#29983;&#25104;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#20197;&#21152;&#24378;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#24050;&#25104;&#20026;&#20027;&#21160;&#24335;&#32593;&#32476;&#38450;&#24481;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#25552;&#20379;&#23545;&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#24494;&#23567;&#25200;&#21160;&#25110;&#28155;&#21152;&#65292;&#24182;&#27809;&#26377;&#25506;&#32034;&#20840;&#25991;&#20214;&#28151;&#28102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24320;&#28304;&#21152;&#23494;&#24037;&#20855;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#20197;&#35268;&#36991;&#26368;&#20808;&#36827;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#24341;&#25806;&#65292;&#24182;&#36229;&#36234;&#20351;&#29992;&#39640;&#32423;&#20462;&#25913;&#26041;&#27861;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#35268;&#36991;&#29575;&#25552;&#39640;&#20102;27%-49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Malware Generation (AMG), the gen- eration of adversarial malware variants to strengthen Deep Learning (DL)-based malware detectors has emerged as a crucial tool in the development of proactive cyberdefense. However, the majority of extant works offer subtle perturbations or additions to executable files and do not explore full-file obfuscation. In this study, we show that an open-source encryption tool coupled with a Reinforcement Learning (RL) framework can successfully obfuscate malware to evade state-of-the-art malware detection engines and outperform techniques that use advanced modification methods. Our results show that the proposed method improves the evasion rate from 27%-49% compared to widely- used state-of-the-art reinforcement learning-based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21452;&#20869;&#28857;&#20248;&#21270;&#23398;&#20064;&#21644;&#21452;&#36229;&#26799;&#24230;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#26377;&#30028;&#21464;&#37327;&#30340;&#21442;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#32422;&#26463;&#23545;&#24212;&#30340;&#23545;&#20598;&#21464;&#37327;&#65292;&#30830;&#20445;&#23545;&#20598;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#39640;&#20445;&#30495;&#24230;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#21644;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.02596</link><description>&lt;p&gt;
&#21452;&#20869;&#28857;&#20248;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual Interior-Point Optimization Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21452;&#20869;&#28857;&#20248;&#21270;&#23398;&#20064;&#21644;&#21452;&#36229;&#26799;&#24230;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#26377;&#30028;&#21464;&#37327;&#30340;&#21442;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#32422;&#26463;&#23545;&#24212;&#30340;&#23545;&#20598;&#21464;&#37327;&#65292;&#30830;&#20445;&#23545;&#20598;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#39640;&#20445;&#30495;&#24230;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#21644;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#20869;&#28857;&#23398;&#20064;&#65288;DIPL&#65289;&#21644;&#21452;&#36229;&#26799;&#24230;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#20197;&#23398;&#20064;&#24102;&#26377;&#26377;&#30028;&#21464;&#37327;&#30340;&#21442;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#65292;&#36825;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#37117;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;DIPL&#27169;&#25311;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#20598;&#20869;&#28857;&#31639;&#27861;&#65292;&#32780;DSL&#21017;&#27169;&#25311;&#20102;&#32463;&#20856;&#30340;&#23545;&#20598;&#36229;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#19982;&#32422;&#26463;&#20851;&#32852;&#30340;&#23545;&#20598;&#21464;&#37327;&#65292;DIPL&#21644;DSL&#20445;&#35777;&#23545;&#20598;&#21487;&#34892;&#24615;&#65292;&#28982;&#21518;&#21033;&#29992;&#23545;&#20110;&#32422;&#26463;&#30028;&#38480;&#30340;&#23545;&#20598;&#30340;&#28789;&#27963;&#24615;&#12290;DIPL&#21644;DSL&#36890;&#36807;&#25552;&#20379;&#36136;&#37327;&#35777;&#26126;&#26469;&#34917;&#20805;&#29616;&#26377;&#30340;&#21407;&#22987;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#20204;&#33021;&#22815;&#20026;&#22823;&#35268;&#27169;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#65292;&#24182;&#22312;0.5%&#30340;&#20248;&#21270;&#24046;&#36317;&#19979;&#25552;&#20379;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Dual Interior Point Learning (DIPL) and Dual Supergradient Learning (DSL) to learn dual feasible solutions to parametric linear programs with bounded variables, which are pervasive across many industries. DIPL mimics a novel dual interior point algorithm while DSL mimics classical dual supergradient ascent. DIPL and DSL ensure dual feasibility by predicting dual variables associated with the constraints then exploiting the flexibility of the duals of the bound constraints. DIPL and DSL complement existing primal learning methods by providing a certificate of quality. They are shown to produce high-fidelity dual-feasible solutions to large-scale optimal power flow problems providing valid dual bounds under 0.5% optimality gap.
&lt;/p&gt;</description></item><item><title>&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30828;&#20214;&#23454;&#29616;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.02593</link><description>&lt;p&gt;
&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02593
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30828;&#20214;&#23454;&#29616;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#27169;&#25311;&#31995;&#32479;&#22266;&#26377;&#22320;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20687;GELU&#21644;SiLU&#36825;&#26679;&#30340;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#31283;&#20581;&#22320;&#20256;&#25773;&#26799;&#24230;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#26222;&#36941;&#23384;&#22312;&#20110;&#25152;&#26377;&#27169;&#25311;&#31995;&#32479;&#20013;&#30340;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#22312;&#37327;&#21270;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#21367;&#31215;&#12289;&#32447;&#24615;&#21644;Transformer&#32593;&#32476;&#30340;&#20998;&#26512;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#20462;&#27491;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#65292;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#22312;&#25239;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#19982;ReLU&#30456;&#27604;&#65292;&#22312;&#25509;&#36817;&#38646;&#26102;&#26799;&#24230;&#35823;&#24046;&#39640;&#20986;100&#20493;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#39640;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#30340;&#30828;&#20214;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.02592</link><description>&lt;p&gt;
&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Unified Training of Universal Time Series Forecasting Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#19968;&#27169;&#22411;&#30340;&#26694;&#26550;&#19979;&#36816;&#20316;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#36890;&#29992;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#28304;&#20110;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35774;&#24819;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#30340;&#21333;&#19968;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;i) &#36328;&#39057;&#29575;&#23398;&#20064;&#65292;ii) &#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#20219;&#24847;&#25968;&#37327;&#30340;&#21464;&#37327;&#65292;&#20197;&#21450;iii) &#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23545;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Masked Encoder&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#65288;Moirai&#65289;&#12290;&#22312;&#25105;&#20204;&#26032;&#24341;&#20837;&#30340;&#22823;&#35268;&#27169;&#24320;&#25918;&#26102;&#38388;&#24207;&#21015;&#23384;&#26723;&#65288;LOTSA&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#21463;&#22122;&#22768;&#24433;&#21709;&#30340;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#21512;&#25104;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#31283;&#23450;&#25152;&#26377;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#21487;&#33021;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35299;&#20915;&#20102;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#36890;&#29992;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02588</link><description>&lt;p&gt;
&#20174;&#26377;&#22122;&#22768;&#36755;&#20837;&#21644;&#26377;&#22122;&#22768;&#36755;&#20986;&#25968;&#25454;&#20013;&#21512;&#25104;&#25511;&#21046;&#22120;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Controller Synthesis from Noisy-Input Noisy-Output Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#21463;&#22122;&#22768;&#24433;&#21709;&#30340;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#21512;&#25104;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#31283;&#23450;&#25152;&#26377;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#21487;&#33021;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35299;&#20915;&#20102;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#36890;&#29992;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#20165;&#21463;&#27979;&#37327;&#22122;&#22768;&#24433;&#21709;&#30340;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#65292;&#20026;&#32447;&#24615;&#31995;&#32479;&#21512;&#25104;&#19968;&#20010;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#12290;&#20026;&#22788;&#29702;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#21407;&#31995;&#32479;&#30340;&#36741;&#21161;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#31283;&#23450;&#25152;&#26377;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#21487;&#33021;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#36890;&#29992;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31995;&#32479;&#12290;&#25968;&#20540;&#20363;&#23376;&#23545;&#25152;&#24471;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of synthesizing a dynamic output-feedback controller for a linear system, using solely input-output data corrupted by measurement noise. To handle input-output data, an auxiliary representation of the original system is introduced. By exploiting the structure of the auxiliary system, we design a controller that robustly stabilizes all possible systems consistent with data. Notably, we also provide a novel solution to extend the results to generic multi-input multi-output systems. The findings are illustrated by numerical examples.
&lt;/p&gt;</description></item><item><title>ClipFormer&#31639;&#27861;&#29992;&#20110;&#20943;&#36731;&#21464;&#21387;&#22120;&#22312;&#20132;&#21449;&#26639;&#19978;&#30340;&#20889;&#20837;&#22122;&#22768;&#23545;&#27880;&#24847;&#21147;&#23618;&#20013;&#38190;&#20540;&#30697;&#38453;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02586</link><description>&lt;p&gt;
ClipFormer:&#29992;&#20110;&#20943;&#36731;&#23384;&#20648;&#22120;&#30005;&#38459;&#20132;&#21449;&#28857;&#19978;&#21464;&#21387;&#22120;&#20889;&#20837;&#22122;&#22768;&#30340;&#38190;-&#20540;&#21098;&#36753;
&lt;/p&gt;
&lt;p&gt;
ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02586
&lt;/p&gt;
&lt;p&gt;
ClipFormer&#31639;&#27861;&#29992;&#20110;&#20943;&#36731;&#21464;&#21387;&#22120;&#22312;&#20132;&#21449;&#26639;&#19978;&#30340;&#20889;&#20837;&#22122;&#22768;&#23545;&#27880;&#24847;&#21147;&#23618;&#20013;&#38190;&#20540;&#30697;&#38453;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#24050;&#32463;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21508;&#31181;&#29616;&#23454;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#20256;&#32479;&#30340;&#20911;&#183;&#35834;&#20381;&#26364;&#35745;&#31639;&#33539;&#20363;&#22312;&#21152;&#36895;&#21464;&#21387;&#22120;&#26102;&#38754;&#20020;&#30528;&#20869;&#23384;&#21644;&#24102;&#23485;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#38750;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#65288;NVM&#65289;&#30340;&#20869;&#23384;&#35745;&#31639;&#65288;IMC&#65289;&#20132;&#21449;&#26639;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#20197;&#39640;&#33021;&#25928;&#36827;&#34892;&#39640;&#24230;&#24182;&#34892;&#30340;&#30697;&#38453;-&#21521;&#37327;&#20056;&#27861;&#65288;MVM&#65289;&#65292;&#24050;&#25104;&#20026;&#21152;&#36895;&#21464;&#21387;&#22120;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#26639;&#20013;&#30340;&#27169;&#25311;MVM&#25805;&#20316;&#24341;&#20837;&#20102;&#38750;&#29702;&#24819;&#24615;&#65292;&#22914;&#38543;&#26426;&#35835;&#20889;&#22122;&#22768;&#65292;&#36825;&#20123;&#22122;&#22768;&#20250;&#24433;&#21709;&#37096;&#32626;&#30340;&#21464;&#21387;&#22120;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30001;&#20110;&#20889;&#22122;&#22768;&#23545;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#21160;&#24577;&#29983;&#25104;&#30340;&#38190;&#65288;K&#65289;&#21644;&#20540;&#65288;V&#65289;&#30697;&#38453;&#30340;&#24433;&#21709;&#32780;&#23481;&#26131;&#21463;&#21040;&#20132;&#21449;&#26639;&#30340;&#24433;&#21709;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#31181;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21019;&#26032;&#30340;ClipFormer&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have revolutionized various real-world applications from natural language processing to computer vision. However, traditional von-Neumann computing paradigm faces memory and bandwidth limitations in accelerating transformers owing to their massive model sizes. To this end, In-memory Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs) with high energy-efficiencies, have emerged as a promising solution for accelerating transformers. However, analog MVM operations in crossbars introduce non-idealities, such as stochastic read &amp; write noise, which affect the inference accuracy of the deployed transformers. Specifically, we find pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the impact of write noise on the dynamically-generated Key (K) and Value (V) matrices in the attention layers, an effect not accounted for in prior studies. We, thus, propose Cl
&lt;/p&gt;</description></item><item><title>DiffEditor&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29616;&#26377;&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#30340;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#25552;&#31034;&#21644;&#32454;&#33410;&#25805;&#20316;&#65292;&#20197;&#21450;&#32467;&#21512;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#26799;&#24230;&#24341;&#23548;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02583</link><description>&lt;p&gt;
DiffEditor: &#25552;&#21319;&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#30340;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02583
&lt;/p&gt;
&lt;p&gt;
DiffEditor&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29616;&#26377;&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#30340;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#25552;&#31034;&#21644;&#32454;&#33410;&#25805;&#20316;&#65292;&#20197;&#21450;&#32467;&#21512;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#26799;&#24230;&#24341;&#23548;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#20855;&#22791;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23558;&#36825;&#20123;&#33021;&#21147;&#24212;&#29992;&#20110;&#32454;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffEditor&#26469;&#25913;&#21892;&#29616;&#26377;&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#32534;&#36753;&#32467;&#26524;&#36890;&#24120;&#32570;&#20047;&#32534;&#36753;&#20934;&#30830;&#24615;&#24182;&#19988;&#20250;&#20986;&#29616;&#24847;&#22806;&#30340;&#20266;&#24433;&#65307;&#65288;2&#65289;&#32570;&#20047;&#28789;&#27963;&#24615;&#20197;&#21327;&#35843;&#32534;&#36753;&#25805;&#20316;&#65292;&#20363;&#22914;&#24341;&#20837;&#26032;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#20013;&#24341;&#20837;&#22270;&#20687;&#25552;&#31034;&#65292;&#19982;&#25991;&#26412;&#25552;&#31034;&#19968;&#36215;&#26356;&#22909;&#22320;&#25551;&#36848;&#32534;&#36753;&#20869;&#23481;&#12290;&#20026;&#20102;&#22686;&#21152;&#28789;&#27963;&#24615;&#21516;&#26102;&#20445;&#25345;&#20869;&#23481;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#23616;&#37096;&#32467;&#21512;&#21040;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#37319;&#26679;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#22522;&#20110;&#21306;&#22495;&#24471;&#20998;&#30340;&#26799;&#24230;&#24341;&#23548;&#21644;&#26102;&#38388;&#26053;&#34892;&#31574;&#30053;&#32467;&#21512;&#21040;&#25193;&#25955;&#37319;&#26679;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31934;&#30830;&#21098;&#20999;&#27979;&#37327;&#65292;&#24182;&#25506;&#31350;&#20102;&#28857;&#25193;&#25955;&#20989;&#25968;&#38169;&#35823;&#20272;&#35745;&#21644;&#26143;&#31995;&#31181;&#32676;&#20559;&#24046;&#23545;&#27979;&#37327;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02578</link><description>&lt;p&gt;
&#35823;&#24046;&#20272;&#35745;&#19979;PSF&#38169;&#35823;&#20272;&#35745;&#21644;&#26143;&#31995;&#31181;&#32676;&#20559;&#24046;&#23545;&#20351;&#29992;CNN&#36827;&#34892;&#31934;&#30830;&#21098;&#20999;&#27979;&#37327;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of PSF misestimation and galaxy population bias on precision shear measurement using a CNN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31934;&#30830;&#21098;&#20999;&#27979;&#37327;&#65292;&#24182;&#25506;&#31350;&#20102;&#28857;&#25193;&#25955;&#20989;&#25968;&#38169;&#35823;&#20272;&#35745;&#21644;&#26143;&#31995;&#31181;&#32676;&#20559;&#24046;&#23545;&#27979;&#37327;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#36828;&#26143;&#31995;&#30340;&#24369;&#24341;&#21147;&#36879;&#38236;&#25552;&#20379;&#20102;&#25506;&#27979;&#26263;&#33021;&#37327;&#30340;&#24378;&#22823;&#25163;&#27573;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#31934;&#30830;&#21098;&#20999;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#27973;&#23618;&#30340;CNN&#65292;&#30740;&#31350;&#20102;&#28857;&#25193;&#25955;&#20989;&#25968;&#65288;PSF&#65289;&#38169;&#35823;&#20272;&#35745;&#20197;&#21450;&#8220;&#26143;&#31995;&#31181;&#32676;&#20559;&#24046;&#8221;&#65288;&#21253;&#25324;&#8220;&#20998;&#24067;&#20559;&#24046;&#8221;&#21644;&#8220;&#24418;&#24577;&#20559;&#24046;&#8221;&#65289;&#23545;&#19979;&#19968;&#20195;&#35843;&#26597;&#30340;&#31934;&#30830;&#24230;&#35201;&#27714;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#21253;&#21547;&#22122;&#22768;&#30340;&#22278;&#30424;&#29366;&#21644;&#26925;&#22278;&#29366;&#26143;&#31995;&#31181;&#32676;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20010;&#20195;&#34920;&#31867;&#20284;&#27431;&#20960;&#37324;&#24471;&#35843;&#26597;&#30340;PSF&#12290;&#25105;&#20204;&#20551;&#35774;&#20272;&#35745;&#21098;&#20999;&#21644;&#30495;&#23454;&#21098;&#20999;&#20043;&#38388;&#23384;&#22312;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#38750;&#24120;&#35268;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20943;&#36731;&#22122;&#22768;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#27979;&#37327;&#20102;&#20056;&#27861;&#65288;m&#65289;&#21644;&#21152;&#27861;&#65288;c&#65289;&#20559;&#24046;&#12290;&#24403;&#25105;&#20204;&#20351;&#29992;&#19981;&#27491;&#30830;&#30340;&#26143;&#31995;&#26925;&#22278;&#24230;&#20998;&#24067;&#25110;&#23610;&#23544;-&#26143;&#31561;&#20851;&#31995;&#65292;&#25110;&#38169;&#35823;&#27604;&#20363;&#26102;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;$m$&#21644;$c$&#12290;
&lt;/p&gt;
&lt;p&gt;
Weak gravitational lensing of distant galaxies provides a powerful probe of dark energy. The aim of this study is to investigate the application of convolutional neural networks (CNNs) to precision shear estimation. In particular, using a shallow CNN, we explore the impact of point spread function (PSF) misestimation and `galaxy population bias' (including `distribution bias' and `morphology bias'), focusing on the accuracy requirements of next generation surveys. We simulate a population of noisy disk and elliptical galaxies and adopt a PSF that is representative of a Euclid-like survey. We quantify the accuracy achieved by the CNN assuming a linear relationship between the estimated and true shears and measure the multiplicative ($m$) and additive ($c$) biases. We make use of an unconventional loss function to mitigate the effects of noise bias and measure $m$ and $c$ when we use either: (i) an incorrect galaxy ellipticity distribution or size-magnitude relation, or the wrong ratio o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#19982;&#26893;&#29289;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Cosserat&#26438;&#30340;Gazebo&#20223;&#30495;&#24179;&#21488;&#25554;&#20214;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29289;&#29702;&#24341;&#25806;&#23545;&#38750;&#21018;&#24615;&#29289;&#20307;&#30340;&#38480;&#21046;&#65292;&#26377;&#21161;&#20110;&#20892;&#19994;&#26426;&#22120;&#20154;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.02570</link><description>&lt;p&gt;
Gazebo&#26893;&#29289;&#65306;&#20351;&#29992;Cosserat Rods&#27169;&#25311;&#26893;&#29289;&#19982;&#26426;&#22120;&#20154;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Gazebo Plants: Simulating Plant-Robot Interaction with Cosserat Rods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#19982;&#26893;&#29289;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Cosserat&#26438;&#30340;Gazebo&#20223;&#30495;&#24179;&#21488;&#25554;&#20214;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29289;&#29702;&#24341;&#25806;&#23545;&#38750;&#21018;&#24615;&#29289;&#20307;&#30340;&#38480;&#21046;&#65292;&#26377;&#21161;&#20110;&#20892;&#19994;&#26426;&#22120;&#20154;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#37319;&#25688;&#26377;&#28508;&#21147;&#23545;&#20892;&#19994;&#29983;&#20135;&#21147;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25913;&#21892;&#39135;&#21697;&#36136;&#37327;&#65292;&#25552;&#39640;&#21487;&#25345;&#32493;&#24615;&#65292;&#24182;&#35299;&#20915;&#21171;&#21160;&#21147;&#30701;&#32570;&#38382;&#39064;&#12290;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#39046;&#22495;&#39134;&#36895;&#21457;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#24050;&#25104;&#20026;&#24517;&#35201;&#12290;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#20197;&#33258;&#21160;&#21270;&#24213;&#23618;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#31867;&#65289;&#20063;&#20005;&#37325;&#20381;&#36182;&#20110;&#36825;&#20123;&#34394;&#25311;&#29615;&#22659;&#65292;&#22240;&#20026;&#24448;&#24448;&#38656;&#35201;&#21512;&#25104;&#25968;&#25454;&#26469;&#20811;&#26381;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#30701;&#32570;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#29289;&#29702;&#24341;&#25806;&#22914;ODE&#12289;Simbody&#12289;Bullet&#21644;DART&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20027;&#35201;&#25903;&#25345;&#21018;&#20307;&#30340;&#36816;&#21160;&#21644;&#30896;&#25758;&#20132;&#20114;&#65292;&#36825;&#20010;&#22266;&#26377;&#30340;&#38480;&#21046;&#38459;&#30861;&#20102;&#23545;&#38750;&#21018;&#24615;&#29289;&#20307;&#65288;&#22914;&#26893;&#29289;&#21644;&#20316;&#29289;&#65289;&#30340;&#22788;&#29702;&#23454;&#39564;&#21644;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Cosserat&#26438;&#30340;Gazebo&#20223;&#30495;&#24179;&#21488;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic harvesting has the potential to positively impact agricultural productivity, reduce costs, improve food quality, enhance sustainability, and to address labor shortage. In the rapidly advancing field of agricultural robotics, the necessity of training robots in a virtual environment has become essential. Generating training data to automatize the underlying computer vision tasks such as image segmentation, object detection and classification, also heavily relies on such virtual environments as synthetic data is often required to overcome the shortage and lack of variety of real data sets. However, physics engines commonly employed within the robotics community, such as ODE, Simbody, Bullet, and DART, primarily support motion and collision interaction of rigid bodies. This inherent limitation hinders experimentation and progress in handling non-rigid objects such as plants and crops. In this contribution, we present a plugin for the Gazebo simulation platform based on Cosserat ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#26377;&#38480;&#21644;&#24179;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21333;&#26426;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#25214;&#21040;&#949;&#27425;&#20248;&#35299;&#65292;&#20219;&#20309;&#26799;&#24230;&#26041;&#27861;&#37117;&#38656;&#35201;&#33267;&#23569;&#937;(n+&#954;&#8730;nlog(1/&#949;))&#20010;IFO&#35843;&#29992;&#12290;&#22312;&#20998;&#24067;&#24335;&#24773;&#20917;&#19979;&#65292;&#26368;&#23567;&#21270;PL&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;&#19979;&#30028;&#20026;&#937;(&#954;/&#8730;&#947;log(1/&#949;))&#65292;&#937;((&#954;+&#964;&#954;/&#8730;&#947;)log(1/&#949;))&#21644;&#937;(n+&#954;&#8730;...</title><link>https://arxiv.org/abs/2402.02569</link><description>&lt;p&gt;
&#20851;&#20110;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#26377;&#38480;&#21644;&#24179;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of Finite-Sum Smooth Optimization under the Polyak-{\L}ojasiewicz Condition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#26377;&#38480;&#21644;&#24179;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21333;&#26426;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#25214;&#21040;&#949;&#27425;&#20248;&#35299;&#65292;&#20219;&#20309;&#26799;&#24230;&#26041;&#27861;&#37117;&#38656;&#35201;&#33267;&#23569;&#937;(n+&#954;&#8730;nlog(1/&#949;))&#20010;IFO&#35843;&#29992;&#12290;&#22312;&#20998;&#24067;&#24335;&#24773;&#20917;&#19979;&#65292;&#26368;&#23567;&#21270;PL&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;&#19979;&#30028;&#20026;&#937;(&#954;/&#8730;&#947;log(1/&#949;))&#65292;&#937;((&#954;+&#964;&#954;/&#8730;&#947;)log(1/&#949;))&#21644;&#937;(n+&#954;&#8730;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24335;&#20026;$\min_{{\bf x}\in{\mathbb R}^d} f({\bf x})\triangleq \frac{1}{n}\sum_{i=1}^n f_i({\bf x})$&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;$f(\cdot)$&#28385;&#36275;&#21442;&#25968;$\mu$&#30340;Polyak-Lojasiewicz&#65288;PL&#65289;&#26465;&#20214;&#65292;&#32780;$\{f_i(\cdot)\}_{i=1}^n$&#26159;$L$&#22343;&#26041;&#24179;&#28369;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#26799;&#24230;&#26041;&#27861;&#22312;&#23547;&#25214;$\epsilon$-&#27425;&#20248;&#35299;&#26102;&#37117;&#38656;&#35201;&#33267;&#23569;$\Omega(n+\kappa\sqrt{n}\log(1/\epsilon))$&#20010;&#22686;&#37327;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;IFO&#65289;&#35843;&#29992;&#65292;&#20854;&#20013;$\kappa\triangleq L/\mu$&#26159;&#38382;&#39064;&#30340;&#26465;&#20214;&#25968;&#12290;&#36825;&#20010;&#32467;&#26524;&#20960;&#20046;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19968;&#38454;&#26041;&#27861;&#30340;IFO&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#26368;&#23567;&#21270;PL&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20010;&#20307;$f_1(\cdot),\dots,f_n(\cdot)$&#20301;&#20110;&#19968;&#20010;&#30001;$n$&#20010;&#33410;&#28857;&#32452;&#25104;&#30340;&#36830;&#36890;&#32593;&#32476;&#19978;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;$\Omega(\kappa/\sqrt{\gamma}\,\log(1/\epsilon))$&#65292;$\Omega((\kappa+\tau\kappa/\sqrt{\gamma}\,)\log(1/\epsilon))$&#21644;$\Omega\big(n+\kappa\sqrt{...
&lt;/p&gt;
&lt;p&gt;
This paper considers the optimization problem of the form $\min_{{\bf x}\in{\mathbb R}^d} f({\bf x})\triangleq \frac{1}{n}\sum_{i=1}^n f_i({\bf x})$, where $f(\cdot)$ satisfies the Polyak--{\L}ojasiewicz (PL) condition with parameter $\mu$ and $\{f_i(\cdot)\}_{i=1}^n$ is $L$-mean-squared smooth. We show that any gradient method requires at least $\Omega(n+\kappa\sqrt{n}\log(1/\epsilon))$ incremental first-order oracle (IFO) calls to find an $\epsilon$-suboptimal solution, where $\kappa\triangleq L/\mu$ is the condition number of the problem. This result nearly matches upper bounds of IFO complexity for best-known first-order methods. We also study the problem of minimizing the PL function in the distributed setting such that the individuals $f_1(\cdot),\dots,f_n(\cdot)$ are located on a connected network of $n$ agents. We provide lower bounds of $\Omega(\kappa/\sqrt{\gamma}\,\log(1/\epsilon))$, $\Omega((\kappa+\tau\kappa/\sqrt{\gamma}\,)\log(1/\epsilon))$ and $\Omega\big(n+\kappa\sqrt{
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#33021;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#32039;&#20945;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.02561</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#32858;&#31867;&#20248;&#21270;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Foundation Model Makes Clustering a Better Initialization for Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#33021;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#32039;&#20945;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#20197;&#28385;&#36275;&#26377;&#38480;&#30340;&#26631;&#27880;&#39044;&#31639;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#26041;&#27861;&#38024;&#23545;&#21021;&#22987;&#21270;&#27169;&#22411;&#21518;&#30340;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#24517;&#19981;&#21487;&#23569;&#30340;&#21021;&#22987;&#21270;&#38454;&#27573;&#65292;&#21364;&#27809;&#26377;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#22823;&#22810;&#25968;&#37117;&#37319;&#29992;&#38543;&#26426;&#25277;&#26679;&#25110;&#32773;&#31616;&#21333;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#25277;&#26679;&#23481;&#26131;&#20135;&#29983;&#27874;&#21160;&#65292;&#32780;&#31616;&#21333;&#32858;&#31867;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#25968;&#25454;&#65289;&#26102;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#32858;&#31867;&#26041;&#27861;&#32467;&#21512;&#65292;&#29992;&#20110;&#36873;&#25321;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#38454;&#27573;&#30340;&#26679;&#26412;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#25351;&#22312;&#33258;&#30417;&#30563;&#33539;&#24335;&#19979;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#32039;&#20945;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20020;&#24202;&#35797;&#39564;&#21644;&#27169;&#22411;&#23545;&#33258;&#28982;&#36923;&#36753;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02558</link><description>&lt;p&gt;
&#25552;&#21319;&#29983;&#29289;&#21307;&#23398;NLI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#20020;&#24202;&#35797;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20020;&#24202;&#35797;&#39564;&#21644;&#27169;&#22411;&#23545;&#33258;&#28982;&#36923;&#36753;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#34892;&#19994;&#20013;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#22914;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#21830;&#19994;&#26234;&#33021;&#21644;&#21307;&#23398;&#31561;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#24212;&#29992;&#26159;&#20998;&#26512;&#21644;&#35843;&#26597;&#19982;&#34164;&#21547;&#20219;&#21153;&#30456;&#20851;&#30340;&#20020;&#24202;&#35797;&#39564;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#25463;&#24452;&#23398;&#20064;&#12289;&#20107;&#23454;&#19981;&#19968;&#33268;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#21363;&#20415;&#22312;&#19978;&#19979;&#25991;&#21464;&#21270;&#24456;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#36827;&#34892;&#20102;&#23545;&#25239;&#24615;&#21644;&#40065;&#26834;&#24615;&#27979;&#35797;&#20197;&#30830;&#20445;&#27169;&#22411;&#30340;&#36755;&#20986;&#23436;&#25972;&#24615;&#65292;&#20294;&#27169;&#31946;&#24615;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#30830;&#20445;&#25512;&#29702;&#30340;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#30340;&#21477;&#27861;&#35821;&#20041;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25506;&#27979;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20351;&#29992;&#20102;&#35760;&#24518;&#25506;&#27979;&#26041;&#27861;&#26469;&#30740;&#31350;&#22312;&#20020;&#24202;&#35797;&#39564;&#19978;&#35757;&#32451;&#30340;Sci-five&#27169;&#22411;&#12290;&#25105;&#35843;&#26597;&#20102;&#35813;&#27169;&#22411;&#22312;&#33258;&#28982;&#36923;&#36753;&#26041;&#38754;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#23454;&#29616;&#30446;&#26631;&#65292;&#25105;&#35757;&#32451;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#25506;&#27979;&#22120;&#12290;&#20351;&#29992;&#36825;&#20123;&#25506;&#27979;&#22120;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02554</link><description>&lt;p&gt;
DeSparsify&#65306;&#23545;&#35270;&#35273;Transformer&#20013;&#30340;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#65289;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38543;&#20351;&#29992;&#30340;Token&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Token&#31232;&#30095;&#21270;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#37319;&#29992;&#20102;&#19968;&#31181;&#20381;&#36182;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#23558;&#26080;&#20851;&#30340;Token&#20174;&#35745;&#31639;&#27969;&#31243;&#20013;&#20002;&#24323;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21160;&#24577;&#24615;&#21644;&#24179;&#22343;&#24773;&#20917;&#20551;&#35774;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961; - &#32463;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#33021;&#22815;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#30340;&#21487;&#29992;&#24615;&#12290;&#35813;&#25915;&#20987;&#26088;&#22312;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e
&lt;/p&gt;</description></item><item><title>Neur2BiLO&#26159;&#19968;&#20010;&#38024;&#23545;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#24341;&#20837;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02552</link><description>&lt;p&gt;
Neur2BiLO: &#31070;&#32463;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2BiLO: Neural Bilevel Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02552
&lt;/p&gt;
&lt;p&gt;
Neur2BiLO&#26159;&#19968;&#20010;&#38024;&#23545;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#24341;&#20837;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22788;&#29702;&#23884;&#22871;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#39046;&#23548;&#32773;&#39318;&#20808;&#20570;&#20986;&#20915;&#31574;&#20197;&#26368;&#23567;&#21270;&#33258;&#24049;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36861;&#38543;&#32773;&#30340;&#26368;&#22909;&#21453;&#24212;&#12290;&#25972;&#25968;&#21464;&#37327;&#32422;&#26463;&#30340;&#21452;&#23618;&#38382;&#39064;&#29305;&#21035;&#38590;&#20197;&#22788;&#29702;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#21452;&#23618;&#20248;&#21270;&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#20294;&#23427;&#20204;&#22312;&#38382;&#39064;&#35268;&#27169;&#36739;&#22823;&#26102;&#24448;&#24448;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#24773;&#20917;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38382;&#39064;&#29305;&#23450;&#30340;&#31639;&#27861;&#65288;&#31934;&#30830;&#21644;&#21551;&#21457;&#24335;&#65289;&#23616;&#38480;&#20110;&#29305;&#23450;&#33539;&#22260;&#12290;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;Neur2BiLO&#23558;&#36890;&#36807;&#30417;&#30563;&#22238;&#24402;&#35757;&#32451;&#30340;&#39046;&#23548;&#32773;&#25110;&#36861;&#38543;&#32773;&#30340;&#20540;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#23884;&#20837;&#21040;&#26131;&#20110;&#35299;&#20915;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20013;&#12290; Neur2BiLO&#20316;&#20026;&#19968;&#31181;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21452;&#23618;&#32972;&#21253;&#25318;&#25130;&#38382;&#39064;&#65292;&#21363;&#8220;&#20851;&#38190;n&#20010;&#38382;&#39064;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization deals with nested problems in which a leader takes the first decision to minimize their objective function while accounting for a follower's best-response reaction. Constrained bilevel problems with integer variables are particularly notorious for their hardness. While exact solvers have been proposed for mixed-integer linear bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case. On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope. Under a data-driven setting in which similar instances of a bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds a neural network approximation of the leader's or follower's value function, trained via supervised regression, into an easy-to-solve mixed-integer program. Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for the bilevel knapsack interdiction problem, the "critical n
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#65292;&#32469;&#36807;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.02551</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#22120;&#19982;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;
&lt;/p&gt;
&lt;p&gt;
Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#65292;&#32469;&#36807;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#29616;&#20195;&#31574;&#30053;&#24448;&#24448;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#65292;&#20854;&#29305;&#28857;&#26159;&#40657;&#30418;&#24615;&#36136;&#22797;&#26434;&#65292;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#21487;&#33021;&#22312;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26080;&#38556;&#30861;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36712;&#36857;&#35268;&#21010;&#22120;&#19982;&#26032;&#39062;&#30340;&#33258;&#21160;&#35843;&#35856;&#20302;&#32423;&#21644;&#20851;&#33410;&#32423;&#25511;&#21046;&#31574;&#30053;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#24182;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#38454;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#32469;&#36807;&#20102;&#19982;&#35745;&#31639;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#26080;&#27169;&#22411;DRL&#20195;&#29702;&#22312;&#20851;&#33410;&#32423;&#25512;&#29702;&#20219;&#21153;&#31354;&#38388;&#20013;&#36827;&#34892;&#36895;&#24230;&#38480;&#21046;&#21644;&#26080;&#38556;&#30861;&#36816;&#21160;&#35268;&#21010;&#65292;&#28982;&#21518;&#23558;&#35813;&#35268;&#21010;&#36755;&#20837;&#21040;&#31283;&#20581;&#30340;&#23376;&#31995;&#32479;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#20013;&#65292;&#20135;&#29983;&#25152;&#38656;&#30340;&#25197;&#30697;&#65292;&#32780;&#26460;&#40515;&#25628;&#32034;&#20248;&#21270;&#65288;CSO&#65289;&#31639;&#27861;&#22686;&#24378;&#20102;&#25511;&#21046;&#22686;&#30410;&#20197;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02549</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#36866;&#21512;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Table-based Fact-Checkers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#39564;&#35777;&#65288;TFV&#65289;&#26088;&#22312;&#25552;&#21462;&#35821;&#21477;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;&#29616;&#26377;&#22522;&#20110;&#23567;&#35268;&#27169;&#27169;&#22411;&#30340;TFV&#26041;&#27861;&#22312;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21644;&#38646;&#26679;&#26412;&#33021;&#21147;&#34180;&#24369;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;TFV&#39046;&#22495;&#30340;&#28508;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;LLMs&#26159;&#21542;&#36866;&#21512;&#20316;&#20026;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#22120;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35821;&#26469;&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;LLMs&#22312;TFV&#26041;&#38754;&#65292;&#21363;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;TFV&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#21644;&#26500;&#24314;&#20102;TFV&#25351;&#23548;&#20197;&#30740;&#31350;LLMs&#30340;&#25351;&#23548;&#35843;&#25972;&#24102;&#26469;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;LLMs&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;TFV&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#32467;&#26524;&#65292;&#32780;&#25351;&#23548;&#35843;&#25972;&#21017;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25506;&#32034;&#20102;&#23545;&#32593;&#29699;&#21160;&#20316;&#36827;&#34892;&#20998;&#31867;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23398;&#26415;&#32593;&#29699;&#25968;&#25454;&#38598;THETIS&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#26368;&#20339;&#27169;&#22411;&#36798;&#21040;&#20102;74%&#30340;&#27867;&#21270;&#20934;&#30830;&#29575;&#65292;&#20026;&#32593;&#29699;&#21160;&#20316;&#20998;&#31867;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#23545;&#26368;&#20339;&#27169;&#22411;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#25913;&#36827;&#32593;&#29699;&#25968;&#25454;&#38598;&#30340;&#26041;&#21521;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#25968;&#25454;&#38598;&#21644;&#24403;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#29699;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.02545</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#32593;&#29699;&#21160;&#20316;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Tennis Actions Using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25506;&#32034;&#20102;&#23545;&#32593;&#29699;&#21160;&#20316;&#36827;&#34892;&#20998;&#31867;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23398;&#26415;&#32593;&#29699;&#25968;&#25454;&#38598;THETIS&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#26368;&#20339;&#27169;&#22411;&#36798;&#21040;&#20102;74%&#30340;&#27867;&#21270;&#20934;&#30830;&#29575;&#65292;&#20026;&#32593;&#29699;&#21160;&#20316;&#20998;&#31867;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#23545;&#26368;&#20339;&#27169;&#22411;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#25913;&#36827;&#32593;&#29699;&#25968;&#25454;&#38598;&#30340;&#26041;&#21521;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#25968;&#25454;&#38598;&#21644;&#24403;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#29699;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#22312;&#35270;&#39057;&#20013;&#35782;&#21035;&#29305;&#23450;&#20107;&#20214;&#21464;&#24471;&#26356;&#21152;&#31934;&#30830;&#12290;&#36825;&#22312;&#32593;&#29699;&#31561;&#36816;&#21160;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#30456;&#20851;&#24615;&#65292;&#20363;&#22914;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#27604;&#36187;&#32479;&#35745;&#20449;&#24687;&#65292;&#25110;&#32773;&#22238;&#25918;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#21160;&#20316;&#20197;&#29992;&#20110;&#28216;&#25103;&#31574;&#30053;&#25110;&#29699;&#21592;&#25552;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#32593;&#29699;&#21160;&#20316;&#20998;&#31867;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;SlowFast&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#19977;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23398;&#26415;&#32593;&#29699;&#25968;&#25454;&#38598;THETIS&#36827;&#34892;&#27979;&#35797;&#12290;&#26368;&#20339;&#27169;&#22411;&#36798;&#21040;&#20102;74%&#30340;&#27867;&#21270;&#20934;&#30830;&#29575;&#65292;&#35777;&#26126;&#20102;&#32593;&#29699;&#21160;&#20316;&#20998;&#31867;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;&#26368;&#20339;&#27169;&#22411;&#25552;&#20379;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#25913;&#36827;&#32593;&#29699;&#25968;&#25454;&#38598;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#65292;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#29699;&#25968;&#25454;&#38598;&#30340;&#19968;&#33324;&#38480;&#21046;&#20197;&#21450;&#26410;&#26469;&#38656;&#35201;&#37319;&#21462;&#30340;&#27493;&#39588;&#20197;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances of deep learning makes it possible to identify specific events in videos with greater precision. This has great relevance in sports like tennis in order to e.g., automatically collect game statistics, or replay actions of specific interest for game strategy or player improvements. In this paper, we investigate the potential and the challenges of using deep learning to classify tennis actions. Three models of different size, all based on the deep learning architecture SlowFast were trained and evaluated on the academic tennis dataset THETIS. The best models achieve a generalization accuracy of 74 %, demonstrating a good performance for tennis action classification. We provide an error analysis for the best model and pinpoint directions for improvement of tennis datasets in general. We discuss the limitations of the data set, general limitations of current publicly available tennis data-sets, and future steps needed to make progress.
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CompeteSMoE&#26041;&#27861;&#65292;&#36890;&#36807;&#31454;&#20105;&#26426;&#21046;&#35299;&#20915;&#20102;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.02526</link><description>&lt;p&gt;
CompeteSMoE - &#36890;&#36807;&#31454;&#20105;&#23454;&#29616;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CompeteSMoE&#26041;&#27861;&#65292;&#36890;&#36807;&#31454;&#20105;&#26426;&#21046;&#35299;&#20915;&#20102;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;SMoE&#65289;&#20026;&#36229;&#36234;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#25110;&#23485;&#24230;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#35757;&#32451;SMoE&#30340;&#25361;&#25112;&#22312;&#20110;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#23548;&#33268;&#21442;&#25968;&#20887;&#20313;&#21644;&#26377;&#38480;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31454;&#20105;&#26426;&#21046;&#26469;&#35299;&#20915;&#34920;&#31034;&#23849;&#28291;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#36890;&#36807;&#21482;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#20855;&#26377;&#26368;&#39640;&#31070;&#32463;&#21709;&#24212;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#31454;&#20105;&#20139;&#26377;&#19982;&#26368;&#20248;&#20272;&#35745;&#22120;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;CompeteSMoE&#65292;&#19968;&#31181;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#31616;&#21333;&#30340;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#31454;&#20105;&#32467;&#26524;&#30340;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;CompeteSMoE&#22312;&#31454;&#20105;&#36335;&#30001;&#31574;&#30053;&#26041;&#38754;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;Transformer&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, effective training of SMoE has proven to be challenging due to the representation collapse issue, which causes parameter redundancy and limited representation potentials. In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse. By routing inputs only to experts with the highest neural response, we show that, under mild assumptions, competition enjoys the same convergence rate as the optimal estimator. We further propose CompeteSMoE, an effective and efficient algorithm to train large language models by deploying a simple router that predicts the competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains from the competition routing policy while having low computation overheads. Our extensive empirical evaluations on two transformer architectures a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#27169;&#22411;&#36873;&#25321;&#20013;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.02522</link><description>&lt;p&gt;
&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#30340;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;
&lt;/p&gt;
&lt;p&gt;
Absolute convergence and error thresholds in non-active adaptive sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#27169;&#22411;&#36873;&#25321;&#20013;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#26159;&#19968;&#31181;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#21644;&#33258;&#21160;&#22320;&#30830;&#23450;&#20445;&#35777;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26080;&#35770;&#25152;&#37319;&#29992;&#30340;&#35843;&#24230;&#21644;&#29983;&#25104;&#24369;&#39044;&#27979;&#22120;&#30340;&#31574;&#30053;&#22914;&#20309;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#35745;&#31639;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#36136;&#37327;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#32477;&#23545;&#22320;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#36827;&#34892;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#35813;&#25216;&#26415;&#22312;&#24037;&#20316;&#20551;&#35774;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#21644;&#23436;&#22791;&#24615;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#37319;&#26679;&#26041;&#26696;&#30340;&#40065;&#26834;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#31526;&#21512;&#25105;&#20204;&#30340;&#39044;&#26399;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#35789;&#24615;&#26631;&#27880;&#29983;&#25104;&#20026;&#26696;&#20363;&#30740;&#31350;&#26469;&#35828;&#26126;&#36825;&#19968;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;(SIMPL)&#65292;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#20197;&#21450;&#20351;&#29992;Bernstein&#22522;&#22810;&#39033;&#24335;&#23545;&#36830;&#32493;&#36712;&#36857;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#20934;&#30830;&#30340;&#36816;&#21160;&#39044;&#27979;&#65292;&#20026;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02519</link><description>&lt;p&gt;
SIMPL:&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;(SIMPL)&#65292;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#20197;&#21450;&#20351;&#29992;Bernstein&#22522;&#22810;&#39033;&#24335;&#23545;&#36830;&#32493;&#36712;&#36857;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#20934;&#30830;&#30340;&#36816;&#21160;&#39044;&#27979;&#65292;&#20026;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;&#65292;&#21517;&#20026;SIMPL&#12290;&#19982;&#20256;&#32479;&#30340;&#20197;&#26234;&#33021;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#34429;&#28982;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#20294;&#35745;&#31639;&#37325;&#22797;&#65292;&#20197;&#21450;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#34429;&#28982;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#26377;&#25152;&#22949;&#21327;&#65292;SIMPL&#21487;&#20197;&#23454;&#26102;&#12289;&#20934;&#30830;&#22320;&#39044;&#27979;&#25152;&#26377;&#30456;&#20851;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#36816;&#21160;&#12290;&#20026;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#23454;&#29616;&#25913;&#36827;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#20197;&#23545;&#31216;&#26041;&#24335;&#25191;&#34892;&#23450;&#21521;&#28040;&#24687;&#20256;&#36882;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#25152;&#26377;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#26410;&#26469;&#36816;&#21160;&#65292;&#24182;&#20943;&#36731;&#35270;&#35282;&#36716;&#25442;&#24102;&#26469;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;Bernstein&#22522;&#22810;&#39033;&#24335;&#23545;&#36830;&#32493;&#36712;&#36857;&#21442;&#25968;&#21270;&#65292;&#20801;&#35768;&#22312;&#20219;&#20309;&#25152;&#38656;&#26102;&#38388;&#28857;&#35780;&#20272;&#36712;&#36857;&#21644;&#20854;&#39640;&#38454;&#23548;&#25968;&#65292;&#36825;&#23545;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;SIMPL&#25171;&#30772;&#20102;&#27169;&#24335;&#24182;&#25552;&#20379;&#20102;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28508;&#22312;&#22270;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21644;&#39044;&#27979;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02518</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#25193;&#25955;&#65306;&#19968;&#31181;&#22312;&#22270;&#19978;&#29983;&#25104;&#21644;&#39044;&#27979;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Diffusion: A Unified Framework for Generation and Prediction on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28508;&#22312;&#22270;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21644;&#39044;&#27979;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#65288;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#65289;&#21644;&#21508;&#31867;&#22411;&#65288;&#29983;&#25104;&#12289;&#22238;&#24402;&#21644;&#20998;&#31867;&#65289;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#22270;&#25193;&#25955;&#65288;LGD&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21644;&#29305;&#24449;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#35299;&#30721;&#65292;&#28982;&#21518;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#30446;&#26631;&#12290;LGD&#36824;&#21487;&#20197;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22238;&#24402;&#21644;&#20998;&#31867;&#31561;&#39044;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#65288;&#26465;&#20214;&#65289;&#29983;&#25104;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;LGD&#33021;&#22815;&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#26469;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results acr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.02516</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#35843;&#24230;&#29992;&#20110;&#33258;&#36866;&#24212;&#37319;&#26679;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adaptive scheduling for adaptive sampling in POS taggers construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#20316;&#20026;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#25439;&#22833;&#24615;&#33021;&#12290;&#19982;&#20043;&#21069;&#20351;&#29992;&#38543;&#26426;&#12289;&#22266;&#23450;&#25110;&#23450;&#26399;&#22686;&#21152;&#23454;&#20363;&#20043;&#38388;&#38388;&#38548;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20309;&#19978;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#32467;&#21512;&#21151;&#33021;&#27169;&#22411;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#12290;&#35813;&#31639;&#27861;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20551;&#35774;&#19978;&#34987;&#35777;&#26126;&#26159;&#24418;&#24335;&#19978;&#27491;&#30830;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#26696;&#20363;&#65292;&#19979;&#19968;&#20010;&#26696;&#20363;&#26159;&#26368;&#36817;&#30340;&#65292;&#30830;&#20445;&#20174;&#21069;&#32773;&#20013;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#65292;&#21487;&#20197;&#35843;&#33410;&#27492;&#26465;&#20214;&#30340;&#35201;&#27714;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26356;&#21152;&#20851;&#27880;&#22312;&#35757;&#32451;&#25968;&#25454;&#24211;&#20013;&#20020;&#26102;&#24615;&#33021;&#33192;&#32960;&#30340;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#38450;&#27490;&#23398;&#20064;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#23398;&#20064;&#26354;&#32447;&#28436;&#21270;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#26469;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.02515</link><description>&lt;p&gt;
&#23398;&#20064;&#26354;&#32447;&#24314;&#27169;&#21450;&#20854;&#22312;&#35789;&#24615;&#26631;&#27880;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Modeling of learning curves with applications to pos tagging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#23398;&#20064;&#26354;&#32447;&#28436;&#21270;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#26469;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#21644;&#20351;&#29992;&#21151;&#33021;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#23398;&#20064;&#26354;&#32447;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#36924;&#36817;&#25152;&#38656;&#26102;&#38388;&#28857;&#30340;&#24453;&#27714;&#20540;&#65292;&#29420;&#31435;&#20110;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#32463;&#36807;&#19968;&#23450;&#30340;&#36807;&#31243;&#28857;&#65288;&#31216;&#20026;&#39044;&#27979;&#32423;&#21035;&#65289;&#21518;&#12290;&#35813;&#25552;&#26696;&#22312;&#24037;&#20316;&#20551;&#35774;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#24418;&#24335;&#19978;&#27491;&#30830;&#30340;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20010;&#21487;&#38752;&#30340;&#36817;&#20284;&#26465;&#20214;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#22522;&#20110;&#26368;&#32456;&#21487;&#23454;&#29616;&#30340;&#20934;&#30830;&#24230;&#26469;&#35774;&#23450;&#25910;&#25947;&#38408;&#20540;&#65292;&#36825;&#25193;&#23637;&#20102;&#20572;&#27490;&#20934;&#21017;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#23384;&#22312;&#25197;&#26354;&#35266;&#23519;&#32467;&#26524;&#65292;&#20063;&#20284;&#20046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#22521;&#35757;&#24037;&#20316;&#37327;&#65292;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#26469;&#20943;&#23569;&#23398;&#20064;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#20154;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#35813;&#25552;&#26696;&#22312;&#33267;&#23569;&#19977;&#20010;&#25805;&#20316;&#31243;&#24207;&#20013;&#24456;&#26377;&#20852;&#36259;&#12290;&#31532;&#19968;&#20010;&#26159;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20266;&#26631;&#31614;&#30340;&#24418;&#24577;&#27880;&#24847;&#21147;&#28145;&#24230;&#30417;&#30563;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38750;&#23545;&#27604;&#24230;CT&#33145;&#20027;&#21160;&#33033;&#20998;&#21106;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#30041;&#20027;&#21160;&#33033;&#30340;&#24418;&#24577;&#29305;&#24449;&#24182;&#20943;&#36731;&#27169;&#31946;&#36793;&#30028;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.02514</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#20266;&#26631;&#31614;&#30340;&#24418;&#24577;&#27880;&#24847;&#21147;&#28145;&#24230;&#30417;&#30563;&#22312;&#38750;&#23545;&#27604;&#24230;CT&#33145;&#20027;&#21160;&#33033;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Supervision by Gaussian Pseudo-label-based Morphological Attention for Abdominal Aorta Segmentation in Non-Contrast CTs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20266;&#26631;&#31614;&#30340;&#24418;&#24577;&#27880;&#24847;&#21147;&#28145;&#24230;&#30417;&#30563;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38750;&#23545;&#27604;&#24230;CT&#33145;&#20027;&#21160;&#33033;&#20998;&#21106;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#30041;&#20027;&#21160;&#33033;&#30340;&#24418;&#24577;&#29305;&#24449;&#24182;&#20943;&#36731;&#27169;&#31946;&#36793;&#30028;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38750;&#23545;&#27604;&#24230;CT&#24433;&#20687;&#20013;&#33145;&#37096;&#20027;&#21160;&#33033;&#30340;&#20998;&#21106;&#38382;&#39064;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#20869;&#33108;&#23548;&#33322;&#30340;&#19968;&#20010;&#22797;&#26434;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#36866;&#21512;&#20351;&#29992;&#23545;&#27604;&#21058;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#27169;&#22411;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#22522;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#24378;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#23545;&#27604;&#24230;CT&#20013;&#30001;&#20110;&#20027;&#21160;&#33033;&#36793;&#30028;&#30340;&#22266;&#26377;&#27169;&#31946;&#65292;&#24378;&#26631;&#31614;&#30340;&#21487;&#38752;&#24615;&#21487;&#33021;&#21463;&#21040;&#25439;&#23475;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#30417;&#30563;&#23558;&#20854;&#38598;&#25104;&#21040;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#24418;&#24577;&#27880;&#24847;&#21147;&#65288;MA&#65289;&#30340;&#22686;&#24378;&#12290;&#30001;&#20110;&#39640;&#26031;&#20266;&#26631;&#31614;&#20445;&#30041;&#20102;&#20027;&#21160;&#33033;&#30340;&#24418;&#24577;&#29305;&#24449;&#65292;&#32780;&#27809;&#26377;&#26126;&#30830;&#22320;&#34920;&#31034;&#20854;&#36793;&#30028;&#20998;&#24067;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#20445;&#25345;&#20027;&#21160;&#33033;&#30340;&#24418;&#24577;&#23398;&#65292;&#21516;&#26102;&#20943;&#36731;&#27169;&#31946;&#36793;&#30028;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#38477;&#20302;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The segmentation of the abdominal aorta in non-contrast CT images is a non-trivial task for computer-assisted endovascular navigation, particularly in scenarios where contrast agents are unsuitable. While state-of-the-art deep learning segmentation models have been proposed recently for this task, they are trained on manually annotated strong labels. However, the inherent ambiguity in the boundary of the aorta in non-contrast CT may undermine the reliability of strong labels, leading to potential overfitting risks. This paper introduces a Gaussian-based pseudo label, integrated into conventional deep learning models through deep supervision, to achieve Morphological Attention (MA) enhancement. As the Gaussian pseudo label retains the morphological features of the aorta without explicitly representing its boundary distribution, we suggest that it preserves aortic morphology during training while mitigating the negative impact of ambiguous boundaries, reducing the risk of overfitting. It
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02513</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#30456;&#20851;&#22312;&#32447;&#25351;&#26631;&#26469;&#25552;&#21069;&#20572;&#27490;
&lt;/p&gt;
&lt;p&gt;
Early stopping by correlating online indicators in neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#22312;&#35757;&#32451;&#23398;&#20064;&#32773;&#26102;&#35782;&#21035;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#36825;&#20351;&#24471;&#25903;&#25345;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35813;&#31867;&#22411;&#24314;&#27169;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21363;&#29992;&#20110;&#25351;&#31034;&#19968;&#32452;&#20551;&#35774;&#26159;&#21542;&#28385;&#36275;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#19982;&#20174;&#37329;&#19997;&#38592;&#21028;&#26029;&#20013;&#26500;&#24314;&#30340;&#19968;&#31995;&#21015;&#29420;&#31435;&#20572;&#27490;&#26465;&#20214;&#30456;&#32852;&#31995;&#65292;&#20197;&#35780;&#20272;&#36807;&#25311;&#21512;&#30340;&#23384;&#22312;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#20026;&#20915;&#31574;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30340;&#22522;&#30784;&#65292;&#20197;&#20013;&#26029;&#23398;&#20064;&#36807;&#31243;&#12290;&#19982;&#20043;&#21069;&#19987;&#27880;&#20110;&#21333;&#19968;&#26631;&#20934;&#30340;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#20043;&#38388;&#30340;&#38468;&#24102;&#25928;&#24212;&#65292;&#23547;&#27714;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#21644;&#26356;&#22823;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced. This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling. Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting. That way, we provide a formal basis for decision making in terms of interrupting the learning process.   As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability. With a view to illustrating the effectiveness of the halting condition described, 
&lt;/p&gt;</description></item><item><title>PoCo&#26159;&#19968;&#31181;&#31574;&#30053;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#20219;&#21153;&#32423;&#32452;&#21512;&#21644;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.02511</link><description>&lt;p&gt;
PoCo: &#26469;&#33258;&#21644;&#20026;&#24322;&#26500;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#31574;&#30053;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
PoCo: Policy Composition from and for Heterogeneous Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02511
&lt;/p&gt;
&lt;p&gt;
PoCo&#26159;&#19968;&#31181;&#31574;&#30053;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#20219;&#21153;&#32423;&#32452;&#21512;&#21644;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#20197;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#22312;&#39068;&#33394;&#12289;&#28145;&#24230;&#12289;&#35302;&#35273;&#21644;&#23039;&#24577;&#24863;&#31561;&#19981;&#21516;&#27169;&#24577;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#22312;&#27169;&#25311;&#12289;&#30495;&#23454;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#35270;&#39057;&#31561;&#19981;&#21516;&#39046;&#22495;&#25910;&#38598;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#25910;&#38598;&#24182;&#27719;&#38598;&#19968;&#20010;&#39046;&#22495;&#30340;&#25152;&#26377;&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#31574;&#30053;&#26469;&#22788;&#29702;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#24322;&#26500;&#24615;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#21644;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#31574;&#30053;&#32452;&#21512;&#65292;&#36890;&#36807;&#32452;&#21512;&#29992;&#25193;&#25955;&#27169;&#22411;&#34920;&#31034;&#30340;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#65292;&#23558;&#36328;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23398;&#20064;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#25805;&#20316;&#20013;&#20351;&#29992;&#20219;&#21153;&#32423;&#32452;&#21512;&#65292;&#24182;&#19982;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#32452;&#21512;&#65292;&#20197;&#22312;&#25512;&#29702;&#26102;&#35843;&#25972;&#31574;&#30053;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#12289;&#20154;&#31867;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real 
&lt;/p&gt;</description></item><item><title>&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65288;HFL&#65289;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#20998;&#37197;&#32473;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#35299;&#20915;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#32593;&#32476;&#25317;&#22622;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;K-Center&#31639;&#27861;&#29992;&#20110;&#35774;&#22791;&#35843;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#35774;&#22791;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.02506</link><description>&lt;p&gt;
&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#35774;&#22791;&#35843;&#24230;&#21644;&#20998;&#37197;&#22312;&#29289;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Device Scheduling and Assignment in Hierarchical Federated Learning for Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02506
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65288;HFL&#65289;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#20998;&#37197;&#32473;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#35299;&#20915;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#32593;&#32476;&#25317;&#22622;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;K-Center&#31639;&#27861;&#29992;&#20110;&#35774;&#22791;&#35843;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#35774;&#22791;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#29289;&#32852;&#32593;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#22312;IoT&#35774;&#22791;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#24517;&#39035;&#35299;&#20915;&#32593;&#32476;&#25317;&#22622;&#38382;&#39064;&#12290;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65288;HFL&#65289;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#20998;&#37197;&#32473;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36890;&#20449;&#24320;&#38144;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25152;&#26377;IoT&#35774;&#22791;&#21516;&#26102;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#23454;&#38469;&#30340;HFL&#26041;&#26696;&#36873;&#25321;&#19968;&#37096;&#20998;IoT&#35774;&#22791;&#21442;&#19982;&#35757;&#32451;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#35774;&#22791;&#35843;&#24230;&#30340;&#27010;&#24565;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#21482;&#26377;&#36873;&#25321;&#30340;IoT&#35774;&#22791;&#34987;&#23433;&#25490;&#21442;&#19982;&#20840;&#23616;&#35757;&#32451;&#65292;&#27599;&#20010;&#35774;&#22791;&#34987;&#20998;&#37197;&#32473;&#19968;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#12290;&#29616;&#26377;&#30340;HFL&#20998;&#37197;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25628;&#32034;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#23547;&#25214;&#26368;&#20248;&#20998;&#37197;&#26102;&#23384;&#22312;&#39640;&#24310;&#36831;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;K-Center&#31639;&#27861;&#29992;&#20110;&#35774;&#22791;&#35843;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising machine learning approach for Internet of Things (IoT), but it has to address network congestion problems when the population of IoT devices grows. Hierarchical FL (HFL) alleviates this issue by distributing model aggregation to multiple edge servers. Nevertheless, the challenge of communication overhead remains, especially in scenarios where all IoT devices simultaneously join the training process. For scalability, practical HFL schemes select a subset of IoT devices to participate in the training, hence the notion of device scheduling. In this setting, only selected IoT devices are scheduled to participate in the global training, with each of them being assigned to one edge server. Existing HFL assignment methods are primarily based on search mechanisms, which suffer from high latency in finding the optimal assignment. This paper proposes an improved K-Center algorithm for device scheduling and introduces a deep reinforcement learning-based appr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02500</link><description>&lt;p&gt;
&#28857;&#20113;&#38382;&#39064;:&#37325;&#26032;&#24605;&#32771;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;RGB&#65292;RGB-D&#21644;&#28857;&#20113;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;17&#20010;&#19981;&#21516;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#28041;&#21450;&#20004;&#20010;&#22522;&#20934;&#21644;&#20223;&#30495;&#22120;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;RGB&#21644;RGB-D&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#22312;&#30456;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#32972;&#26223;&#22806;&#35266;&#31561;&#21508;&#31181;&#20960;&#20309;&#21644;&#35270;&#35273;&#32447;&#32034;&#26041;&#38754;&#65292;&#37117;&#33021;&#25552;&#39640;&#31574;&#30053;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#32500;&#28857;&#20113;&#26159;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#35266;&#28857;&#33021;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#39044;&#27979;&#21644;&#20849;&#20139;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#36712;&#36857;&#21644;&#36741;&#21161;&#25805;&#20316;&#32773;&#30340;&#24847;&#22270;&#65292;&#21487;&#20197;&#20943;&#36731;&#25805;&#20316;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#21644;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02499</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;Trajectron&#65306;&#22522;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20849;&#20139;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Robot Trajectron: Trajectory Prediction-based Shared Control for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#39044;&#27979;&#21644;&#20849;&#20139;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#36712;&#36857;&#21644;&#36741;&#21161;&#25805;&#20316;&#32773;&#30340;&#24847;&#22270;&#65292;&#21487;&#20197;&#20943;&#36731;&#25805;&#20316;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#21644;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;(a)&#22914;&#20309;&#39044;&#27979;&#25163;&#33218;&#21040;&#36798;&#21160;&#20316;&#30340;&#36712;&#36857;&#65292;&#22522;&#20110;&#36816;&#21160;&#30340;&#24320;&#22987;&#30340;&#20960;&#31186;&#38047;&#65307;(b)&#22914;&#20309;&#21033;&#29992;&#36825;&#20010;&#39044;&#27979;&#22120;&#26469;&#20419;&#36827;&#20849;&#20139;&#25511;&#21046;&#25805;&#20316;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#39044;&#26399;&#36816;&#21160;&#26041;&#21521;&#19978;&#36741;&#21161;&#25805;&#20316;&#32773;&#26469;&#20943;&#36731;&#35748;&#30693;&#36127;&#33655;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#20272;&#35745;&#22120;&#65292;&#21517;&#20026;Robot Trajectron (RT)&#65292;&#23427;&#26681;&#25454;&#26426;&#22120;&#20154;&#26368;&#36817;&#30340;&#20301;&#32622;&#12289;&#36895;&#24230;&#21644;&#21152;&#36895;&#24230;&#21382;&#21490;&#26469;&#20135;&#29983;&#26426;&#22120;&#20154;&#39044;&#26399;&#36712;&#36857;&#30340;&#27010;&#29575;&#34920;&#31034;&#12290;&#32771;&#34385;&#21040;&#25163;&#33218;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;RT&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#25805;&#20316;&#32773;&#30340;&#24847;&#22270;&#65292;&#30456;&#27604;&#20854;&#20182;&#21482;&#20351;&#29992;&#25163;&#33218;&#20301;&#32622;&#30340;&#29366;&#24577;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;RT&#29305;&#21035;&#36866;&#21512;&#22312;&#25805;&#20316;&#32773;&#24847;&#22270;&#26131;&#21463;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#25552;&#20379;&#36741;&#21161;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20849;&#20139;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;RT&#30340;&#39044;&#27979;&#33021;&#21147;&#19982;&#28508;&#22312;&#30446;&#26631;&#20301;&#32622;&#30340;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;RT&#22312;&#24847;&#22270;&#20272;&#35745;&#21644;
&lt;/p&gt;
&lt;p&gt;
We address the problem of (a) predicting the trajectory of an arm reaching motion, based on a few seconds of the motion's onset, and (b) leveraging this predictor to facilitate shared-control manipulation tasks, easing the cognitive load of the operator by assisting them in their anticipated direction of motion. Our novel intent estimator, dubbed the \emph{Robot Trajectron} (RT), produces a probabilistic representation of the robot's anticipated trajectory based on its recent position, velocity and acceleration history. Taking arm dynamics into account allows RT to capture the operator's intent better than other SOTA models that only use the arm's position, making it particularly well-suited to assist in tasks where the operator's intent is susceptible to change. We derive a novel shared-control solution that combines RT's predictive capacity to a representation of the locations of potential reaching targets. Our experiments demonstrate RT's effectiveness in both intent estimation and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;2-WL&#27979;&#35797;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#28857;&#20113;&#20013;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20462;&#25913;&#30340;PPGN&#26550;&#26500;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.02484</link><description>&lt;p&gt;
Weisfeiler Leman&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#31561;&#21464;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler Leman for Euclidean Equivariant Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;2-WL&#27979;&#35797;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#28857;&#20113;&#20013;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20462;&#25913;&#30340;PPGN&#26550;&#26500;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k-Weisfeiler Leman (k-WL)&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#26159;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34920;&#36798;&#33021;&#21147;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#35777;&#26126;&#20102;2-WL&#27979;&#35797;&#22312;&#32534;&#30721;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#21152;&#26435;&#22270;&#19978;&#26159;&#23436;&#22791;&#30340;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#19982;2-WL&#27979;&#35797;&#31561;&#20215;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;GNNs&#21487;&#20197;&#34987;&#35777;&#26126;&#22312;&#28857;&#20113;&#19978;&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#32467;&#26524;&#20165;&#38480;&#20110;&#28857;&#20113;&#19978;&#30340;&#19981;&#21464;&#36830;&#32493;&#20989;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#26041;&#38754;&#23545;&#36825;&#19968;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;:&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;2-WL&#27979;&#35797;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#28857;&#20113;&#65292;&#36825;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PPGN (Maron&#31561;&#20154;&#65292;2019)&#21487;&#20197;&#22312;&#20302;&#22797;&#26434;&#24230;&#19979;&#22312;&#25152;&#26377;&#28857;&#20113;&#19978;&#19968;&#33268;&#22320;&#27169;&#25311;2-WL&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36825;&#20010;PPGN&#26550;&#26500;&#30340;&#31616;&#21333;&#20462;&#25913;&#21487;&#20197;&#29992;&#26469;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds. Yet, this result is limited to invariant continuous functions on point clouds.   In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications. Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity. Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly.   Building
&lt;/p&gt;</description></item><item><title>BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02479</link><description>&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02479
&lt;/p&gt;
&lt;p&gt;
BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#32487;Proximal Policy Optimization (PPO)&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22914;Sequence Likelihood Calibration (SLiC)&#21644;Direct Policy Optimization (DPO)&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#31163;&#32447;&#30340;&#65292;&#24182;&#19988;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20351;&#29992;&#22870;&#21169;&#12290;&#36825;&#20123;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;DPO&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#65292;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;LLM&#23545;&#40784;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36951;&#28431;&#20102;PPO&#26041;&#27861;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#35832;&#22914;SLiC&#25110;RRHF&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;(RM)&#36827;&#34892;&#25490;&#24207;/&#20559;&#22909;&#65292;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#24573;&#30053;&#20102;RM&#30340;&#21442;&#25968;&#24418;&#24335;(&#20363;&#22914;Bradley-Terry&#12289;Plackett-Luce)&#65307;&#32780;&#35832;&#22914;DPO&#30340;&#26041;&#27861;&#29978;&#33267;&#19981;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;BRAIn&#65292;&#23427;&#23558;RM&#20316;&#20026;&#20998;&#24067;&#21305;&#37197;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#37325;&#26032;&#24341;&#20837;&#12290;BRAIn&#32771;&#34385;&#21040;&#20102;LLM&#20998;&#24067;&#22312;&#20551;&#35774;&#36755;&#20986;&#36136;&#37327;&#33391;&#22909;&#30340;&#26465;&#20214;&#19979;&#65292;&#24182;&#24212;&#29992;B...
&lt;/p&gt;
&lt;p&gt;
Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#23545;&#20026;&#20160;&#20040;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#24378;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02478</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#65311;&#20851;&#20110;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Why are hyperbolic neural networks effective? A study on hierarchical representation capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#23545;&#20026;&#20160;&#20040;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#24378;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#65288;HNNs&#65289;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36816;&#20316;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#21160;&#26426;&#26159;&#21452;&#26354;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#31181;&#20248;&#21270;&#23884;&#20837;&#65292;&#33021;&#22815;&#27604;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#26356;&#20934;&#30830;&#22320;&#20445;&#30041;&#25968;&#25454;&#30340;&#23618;&#32423;&#20851;&#31995;&#65288;&#31216;&#20026;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#65292;HRC&#65289;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;HNN&#21487;&#20197;&#36798;&#21040;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#23884;&#20837;&#65292;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#30740;&#31350;&#24314;&#31435;&#22312;&#38169;&#35823;&#30340;&#21160;&#26426;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;HRC&#30340;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#23545;HNN&#20026;&#20309;&#26377;&#25928;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#21463;&#21040;&#20998;&#26512;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#24378;HRC&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HNN&#26080;&#27861;&#23454;&#29616;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#23884;&#20837;&#12290;HRC&#21463;&#20248;&#21270;&#30446;&#26631;&#21644;&#23618;&#32423;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been widely applied in recent years, motivated by the existence of an optimal embedding in hyperbolic space that can preserve data hierarchical relationships (termed Hierarchical Representation Capability, HRC) more accurately than Euclidean space. However, there is no evidence to suggest that HNNs can achieve this theoretical optimal embedding, leading to much research being built on flawed motivations. In this paper, we propose a benchmark for evaluating HRC and conduct a comprehensive analysis of why HNNs are effective through large-scale experiments. Inspired by the analysis results, we propose several pre-training strategies to enhance HRC and improve the performance of downstream tasks, further validating the reliability of the analysis. Experiments show that HNNs cannot achieve the theoretical optimal embedding. The HRC is significantly affected by the optimization objectives and hierarchical structures, and 
&lt;/p&gt;</description></item><item><title>TimeSiam&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Siamese&#32593;&#32476;&#21644;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#23398;&#20064;&#20869;&#37096;&#26102;&#24207;&#20381;&#36182;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.02475</link><description>&lt;p&gt;
TimeSiam&#65306;&#19968;&#31181;&#29992;&#20110;&#23402;&#29983;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02475
&lt;/p&gt;
&lt;p&gt;
TimeSiam&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Siamese&#32593;&#32476;&#21644;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#23398;&#20064;&#20869;&#37096;&#26102;&#24207;&#20381;&#36182;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#38477;&#20302;&#26631;&#27880;&#30340;&#25104;&#26412;&#24182;&#21463;&#30410;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#22312;&#35270;&#35273;&#25110;&#35821;&#35328;&#39046;&#22495;&#24191;&#20026;&#35748;&#21487;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#22914;&#36974;&#34109;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#36974;&#34109;&#26102;&#38388;&#24207;&#21015;&#25110;&#35745;&#31639;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23558;&#23548;&#33268;&#20002;&#22833;&#25110;&#24573;&#35270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20851;&#38190;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#24378;&#35843;&#26102;&#38388;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimeSiam&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TimeSiam&#23545;&#23402;&#29983;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25429;&#25417;&#38543;&#26426;&#37319;&#26679;&#30340;&#36807;&#21435;&#21644;&#24403;&#21069;&#23376;&#24207;&#21015;&#20043;&#38388;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;&#20363;&#22914;&#36974;&#34109;&#65289;&#65292;TimeSiam&#21487;&#20197;&#20174;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#23376;&#24207;&#21015;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#20174;&#36807;&#21435;&#21040;&#24403;&#21069;&#30340;&#37325;&#24314;&#26469;&#23398;&#20064;&#20869;&#37096;&#26102;&#24207;&#20381;&#36182;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#21487;&#23398;&#20064;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02468</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#25506;&#32034;&#30340;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#21516;&#20276;
&lt;/p&gt;
&lt;p&gt;
Fast Peer Adaptation with Context-aware Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#65292;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#35782;&#21035;&#21516;&#20276;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26159;&#36866;&#24212;&#20013;&#36827;&#34892;&#26368;&#20339;&#21453;&#24212;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24403;&#28216;&#25103;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#19988;&#26102;&#38388;&#36328;&#24230;&#24456;&#38271;&#26102;&#65292;&#25506;&#32034;&#26410;&#30693;&#21516;&#20276;&#30340;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#20276;&#35782;&#21035;&#22870;&#21169;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#29615;&#22659;&#19979;&#65288;&#20363;&#22914;&#22810;&#20010;&#22238;&#21512;&#30340;&#35266;&#23519;&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#35782;&#21035;&#21516;&#20276;&#30340;&#34892;&#20026;&#27169;&#24335;&#26469;&#22870;&#21169;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36825;&#20010;&#22870;&#21169;&#28608;&#21169;&#26234;&#33021;&#20307;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#21363;&#22312;&#23545;&#21516;&#20276;&#31574;&#30053;&#19981;&#30830;&#23450;&#26102;&#31215;&#26497;&#23547;&#25214;&#21644;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02464</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#21315;&#35328;&#65306;&#20351;&#29992;&#32431;Transformer&#23558;&#22270;&#24418;&#27431;&#25289;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#24314;&#27169;&#20026;&#32431;&#35821;&#35328;&#29978;&#33267;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22266;&#26377;&#20449;&#24687;&#65311;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#19968;&#30452;&#26159;&#22270;&#24418;&#24314;&#27169;&#20013;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;GNN&#21644;Graphformer&#21162;&#21147;&#23558;&#22270;&#24418;&#32534;&#30721;&#20026;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#20294;&#20174;&#21521;&#37327;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#22270;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GraphsGPT&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#22270;&#24418;&#21333;&#35789;&#30340;Graph2Seq&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#20174;&#22270;&#24418;&#21333;&#35789;&#37325;&#26500;&#21407;&#22987;&#22270;&#24418;&#20197;&#30830;&#20445;&#20449;&#24687;&#31561;&#20215;&#24615;&#30340;GraphGPT&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;100M&#20010;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#20102;GraphsGPT&#65292;&#24182;&#24471;&#21040;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;(1) &#39044;&#35757;&#32451;&#30340;Graph2Seq&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;8/9&#20010;&#22270;&#24418;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;(2) &#39044;&#35757;&#32451;&#30340;GraphGPT&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#29983;&#25104;&#22120;&#65292;&#20854;&#33021;&#22815;&#36827;&#34892;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#30340;&#22270;&#24418;&#29983;&#25104;&#12290;(3) Graph2Seq+Gr
&lt;/p&gt;
&lt;p&gt;
Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35299;&#20915;Lasso&#21644;Logistic Lasso&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#20027;&#21160;&#38598;&#26041;&#27861;&#21644;&#36866;&#24403;&#30340;&#27714;&#35299;&#22120;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21152;&#36895;&#12290;&#22312;&#21387;&#32553;&#24863;&#30693;&#12289;Lasso&#22238;&#24402;&#21644;Logistic Lasso&#22238;&#24402;&#23454;&#39564;&#20013;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#33021;&#25552;&#39640;&#32422;30&#20493;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.02463</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#30340;Lasso&#21644;Logistic Lasso&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Method for Lasso and Logistic Lasso
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35299;&#20915;Lasso&#21644;Logistic Lasso&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#20027;&#21160;&#38598;&#26041;&#27861;&#21644;&#36866;&#24403;&#30340;&#27714;&#35299;&#22120;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21152;&#36895;&#12290;&#22312;&#21387;&#32553;&#24863;&#30693;&#12289;Lasso&#22238;&#24402;&#21644;Logistic Lasso&#22238;&#24402;&#23454;&#39564;&#20013;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#33021;&#25552;&#39640;&#32422;30&#20493;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#12289;Lasso&#22238;&#24402;&#21644;Logistic Lasso&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20027;&#21160;&#38598;&#26041;&#27861;&#36845;&#20195;&#36816;&#34892;&#36866;&#24403;&#30340;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#26032;&#20027;&#21160;&#38598;&#30340;&#31574;&#30053;&#65292;&#30456;&#27604;&#20110;&#21333;&#27425;&#35843;&#29992;&#22810;&#20010;&#27714;&#35299;&#22120;&#65288;&#21253;&#25324;Matlab&#30340;lassoglm&#21644;glmnet&#20197;&#21450;&#29992;&#20110;&#31232;&#30095;&#37325;&#26500;&#30340;&#26799;&#24230;&#25237;&#24433;&#31639;&#27861;GPSR&#65289;&#65292;&#33021;&#22815;&#23454;&#29616;&#22823;&#24133;&#21152;&#36895;&#12290;&#23545;&#20110;&#21387;&#32553;&#24863;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;GPSR&#30340;&#28151;&#21512;&#24179;&#22343;&#36895;&#24230;&#25552;&#39640;&#20102;31.41&#20493;&#65288;&#23545;&#20110;&#39640;&#26031;&#31995;&#21015;&#65289;&#21644;25.64&#20493;&#65288;&#23545;&#20110;&#20108;&#36827;&#21046;&#31995;&#21015;&#65289;&#12290;&#22312;Lasso&#22238;&#24402;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;GPSR&#30340;&#28151;&#21512;&#24179;&#22343;&#36895;&#24230;&#25552;&#39640;&#20102;30.67&#20493;&#12290;&#22312;Logistic Lasso&#22238;&#24402;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;lassoglm&#30340;&#28151;&#21512;&#24179;&#22343;&#36895;&#24230;&#25552;&#39640;&#20102;11.95&#20493;&#65292;&#19982;glmnet&#30340;&#28151;&#21512;&#24179;&#22343;&#36895;&#24230;&#25552;&#39640;&#20102;1.40&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a fast method for solving compressed sensing, Lasso regression, and Logistic Lasso regression problems that iteratively runs an appropriate solver using an active set approach. We design a strategy to update the active set that achieves a large speedup over a single call of several solvers, including gradient projection for sparse reconstruction (GPSR), lassoglm of Matlab, and glmnet. For compressed sensing, the hybrid of our method and GPSR is 31.41 times faster than GPSR on average for Gaussian ensembles and 25.64 faster on average for binary ensembles. For Lasso regression, the hybrid of our method and GPSR achieves a 30.67-fold average speedup in our experiments. In our experiments on Logistic Lasso regression, the hybrid of our method and lassoglm gives an 11.95-fold average speedup, and the hybrid of our method and glmnet gives a 1.40-fold average speedup.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#25506;&#35752;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#24212;&#29992;&#20197;&#21450;&#34701;&#21512;&#25216;&#26415;&#30340;&#35780;&#20272;&#12290;&#37325;&#28857;&#20851;&#27880;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02460</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of multimodal machine learning approaches in healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#25506;&#35752;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#24212;&#29992;&#20197;&#21450;&#34701;&#21512;&#25216;&#26415;&#30340;&#35780;&#20272;&#12290;&#37325;&#28857;&#20851;&#27880;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20256;&#32479;&#19978;&#27880;&#37325;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#22797;&#21046;&#20020;&#24202;&#23454;&#36341;&#20013;&#25972;&#21512;&#22810;&#31181;&#20449;&#24687;&#26469;&#28304;&#20197;&#25913;&#21892;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#20020;&#24202;&#21307;&#29983;&#36890;&#24120;&#20381;&#36182;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#23454;&#39564;&#23460;&#25968;&#25454;&#12289;&#29983;&#21629;&#20307;&#24449;&#21644;&#21508;&#31181;&#24433;&#20687;&#25968;&#25454;&#27169;&#24577;&#26469;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#24182;&#23545;&#20854;&#21457;&#29616;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26356;&#39640;&#25928;&#34701;&#21512;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#22320;&#20195;&#34920;&#21307;&#29983;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#20840;&#38754;&#27010;&#36848;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#65292;&#29305;&#21035;&#24378;&#35843;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#34701;&#21512;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#30740;&#31350;&#24120;&#35265;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods in healthcare have traditionally focused on using data from a single modality, limiting their ability to effectively replicate the clinical practice of integrating multiple sources of information for improved decision making. Clinicians typically rely on a variety of data sources including patients' demographic information, laboratory data, vital signs and various imaging data modalities to make informed decisions and contextualise their findings. Recent advances in machine learning have facilitated the more efficient incorporation of multimodal data, resulting in applications that better represent the clinician's approach. Here, we provide a review of multimodal machine learning approaches in healthcare, offering a comprehensive overview of recent literature. We discuss the various data modalities used in clinical diagnosis, with a particular emphasis on imaging data. We evaluate fusion techniques, explore existing multimodal datasets and examine common traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#36857;&#22240;&#23376;&#20998;&#26512;&#65288;MTFA&#65289;&#30340;&#25918;&#26494;&#29256;&#26412;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#22240;&#24322;&#26041;&#24046;&#22122;&#22768;&#36896;&#25104;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#22240;&#23376;&#20998;&#26512;&#21644;&#35889;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#24322;&#24120;&#24773;&#20917;&#21644;&#30149;&#24577;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02459</link><description>&lt;p&gt;
&#22312;&#26368;&#23567;&#21270;&#36857;&#22240;&#23376;&#20998;&#26512;&#20013; - &#36825;&#39318;&#32769;&#27468;&#20197;&#26032;&#30340;&#26041;&#24335;&#28436;&#21809;
&lt;/p&gt;
&lt;p&gt;
On Minimum Trace Factor Analysis - An Old Song Sung to a New Tune
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#36857;&#22240;&#23376;&#20998;&#26512;&#65288;MTFA&#65289;&#30340;&#25918;&#26494;&#29256;&#26412;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#22240;&#24322;&#26041;&#24046;&#22122;&#22768;&#36896;&#25104;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#22240;&#23376;&#20998;&#26512;&#21644;&#35889;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#24322;&#24120;&#24773;&#20917;&#21644;&#30149;&#24577;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#38477;&#20302;&#26041;&#27861;&#65292;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#22240;&#23376;&#20998;&#26512;&#65292;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#26159;&#24456;&#24120;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#26174;&#33879;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#23547;&#25214;&#31283;&#20581;&#30340;&#20302;&#32500;&#36924;&#36817;&#23384;&#22312;&#26126;&#26174;&#19988;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#23567;&#21270;&#36857;&#22240;&#23376;&#20998;&#26512;&#65288;MTFA&#65289;&#30340;&#25918;&#26494;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#31181;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#26681;&#28304;&#21487;&#20197;&#36861;&#28335;&#21040;1940&#24180;Ledermann&#30340;&#24037;&#20316;&#12290;&#36825;&#31181;&#25918;&#26494;&#26041;&#27861;&#22312;&#19981;&#36807;&#24230;&#25311;&#21512;&#24322;&#26041;&#24046;&#25200;&#21160;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#35299;&#20915;&#20102;&#22240;&#32032;&#20998;&#26512;&#20013;&#32463;&#24120;&#34987;&#25552;&#21040;&#30340;Heywood&#26696;&#20363;&#21644;&#26368;&#36817;&#21457;&#29616;&#30340;&#29616;&#26377;&#35889;&#26041;&#27861;&#20013;"&#30149;&#24577;&#35781;&#21650;"&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#25152;&#24471;&#20302;&#31209;&#23376;&#31354;&#38388;&#30340;&#31934;&#30830;&#24230;&#21644;&#25152;&#25552;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19978;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#21253;&#25324;HeteroPCA&#65292;Lasso&#21644;Soft-Impute&#65289;&#30340;&#19968;&#20123;&#26377;&#36259;&#32852;&#31995;&#65292;&#20197;&#22635;&#34917;&#19968;&#20123;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction methods, such as principal component analysis (PCA) and factor analysis, are central to many problems in data science. There are, however, serious and well-understood challenges to finding robust low dimensional approximations for data with significant heteroskedastic noise. This paper introduces a relaxed version of Minimum Trace Factor Analysis (MTFA), a convex optimization method with roots dating back to the work of Ledermann in 1940. This relaxation is particularly effective at not overfitting to heteroskedastic perturbations and addresses the commonly cited Heywood cases in factor analysis and the recently identified "curse of ill-conditioning" for existing spectral methods. We provide theoretical guarantees on the accuracy of the resulting low rank subspace and the convergence rate of the proposed algorithm to compute that matrix. We develop a number of interesting connections to existing methods, including HeteroPCA, Lasso, and Soft-Impute, to fill an i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GPTN-SS&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;TN&#32467;&#26500;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02456</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02456
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GPTN-SS&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;TN&#32467;&#26500;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;TN-SS&#65289;&#26088;&#22312;&#25628;&#32034;&#36866;&#21512;&#34920;&#31034;&#39640;&#32500;&#38382;&#39064;&#30340;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#32467;&#26500;&#65292;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;TN&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29616;&#26377;&#31639;&#27861;&#25214;&#21040;&#28385;&#24847;&#30340;TN&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#31639;&#27861;&#24182;&#36991;&#20813;&#20154;&#21147;&#23494;&#38598;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#26469;&#33258;&#21160;&#35774;&#35745;TN-SS&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;GPTN-SS&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#20197;&#31867;&#20284;&#36827;&#21270;&#30340;&#26041;&#24335;&#36816;&#34892;&#12290;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPTN-SS&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#24320;&#21457;&#20986;&#26356;&#22909;&#22320;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#22411;TN-SS&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;TN&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network structure search (TN-SS), aiming at searching for suitable tensor network (TN) structures in representing high-dimensional problems, largely promotes the efficacy of TN in various machine learning applications. Nonetheless, finding a satisfactory TN structure using existing algorithms remains challenging. To develop more effective algorithms and avoid the human labor-intensive development process, we explore the knowledge embedded in large language models (LLMs) for the automatic design of TN-SS algorithms. Our approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner. The experimental results, derived from real-world data, demonstrate that GPTN-SS can effectively leverage the insights gained from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation. These algorithms exhibit superior performance in searching the high-quality TN structur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#38544;&#24335;&#20559;&#24046;&#30340;&#36215;&#28304;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#30001;&#26435;&#37325;&#21021;&#22987;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#21021;&#22987;&#21270;&#23545;&#20248;&#21270;&#21644;&#27867;&#21270;&#30340;&#24726;&#35770;&#30340;&#20316;&#29992;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02454</link><description>&lt;p&gt;
&#20851;&#20110;&#21021;&#22987;&#21270;&#23545;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#38544;&#24335;&#20559;&#24046;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Initialization on the Implicit Bias in Deep Linear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#38544;&#24335;&#20559;&#24046;&#30340;&#36215;&#28304;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#30001;&#26435;&#37325;&#21021;&#22987;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#21021;&#22987;&#21270;&#23545;&#20248;&#21270;&#21644;&#27867;&#21270;&#30340;&#24726;&#35770;&#30340;&#20316;&#29992;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#20102;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#24726;&#35770;&#26159;&#65292;&#20256;&#32479;&#35266;&#28857;&#19981;&#40723;&#21169;&#23436;&#32654;&#25311;&#21512;&#25968;&#25454;&#65292;&#32780;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21017;&#34987;&#35774;&#35745;&#25104;&#20570;&#36825;&#20214;&#20107;&#65292;&#28982;&#32780;&#23427;&#20204;&#33021;&#26377;&#25928;&#22320;&#27867;&#21270;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#32034;&#19982;&#27492;&#29616;&#35937;&#30456;&#20851;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;&#24050;&#32463;&#30830;&#23450;&#20102;&#21508;&#31181;&#38544;&#24335;&#20559;&#24046;&#30340;&#26469;&#28304;&#65292;&#20363;&#22914;&#27493;&#38271;&#12289;&#26435;&#37325;&#21021;&#22987;&#21270;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30740;&#31350;&#30001;&#26435;&#37325;&#21021;&#22987;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#35299;&#20915;&#27424;&#23450;&#32447;&#24615;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#26102;&#21021;&#22987;&#21270;&#23545;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#38416;&#26126;&#20102;&#21021;&#22987;&#21270;&#22312;&#20248;&#21270;&#21644;&#27867;&#21270;&#24726;&#35770;&#20013;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Deep Learning's (DL) empirical success, our theoretical understanding of its efficacy remains limited. One notable paradox is that while conventional wisdom discourages perfect data fitting, deep neural networks are designed to do just that, yet they generalize effectively. This study focuses on exploring this phenomenon attributed to the implicit bias at play. Various sources of implicit bias have been identified, such as step size, weight initialization, optimization algorithm, and number of parameters. In this work, we focus on investigating the implicit bias originating from weight initialization. To this end, we examine the problem of solving underdetermined linear systems in various contexts, scrutinizing the impact of initialization on the implicit regularization when using deep networks to solve such systems. Our findings elucidate the role of initialization in the optimization and generalization paradoxes, contributing to a more comprehensive understanding of DL's perf
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#36164;&#28304;&#23454;&#39564;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26089;&#26399;&#23398;&#20064;&#26354;&#32447;&#20272;&#35745;&#20316;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#27169;&#22411;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#36139;&#20047;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02449</link><description>&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#24314;&#27169;PoS&#26631;&#35760;&#22120;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Surfing the modeling of PoS taggers in low-resource scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02449
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#23454;&#39564;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26089;&#26399;&#23398;&#20064;&#26354;&#32447;&#20272;&#35745;&#20316;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#27169;&#22411;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#36139;&#20047;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#32467;&#26500;&#25216;&#26415;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#36235;&#21183;&#25581;&#31034;&#20102;&#24222;&#22823;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20351;&#24471;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#37325;&#26032;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#20173;&#28982;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#35774;&#32622;&#20013;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#36873;&#25321;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#20197;&#22312;&#21512;&#29702;&#25104;&#26412;&#20869;&#25552;&#21319;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#28041;&#21450;&#21040;&#35757;&#32451;&#21644;/&#25110;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#30340;&#39046;&#22495;&#26102;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#26089;&#26399;&#23398;&#20064;&#26354;&#32447;&#20272;&#35745;&#20316;&#20026;&#22312;&#36164;&#28304;&#36139;&#20047;&#29615;&#22659;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#27169;&#22411;&#30340;&#23454;&#29992;&#26426;&#21046;&#12290;&#22522;&#20110;&#20808;&#21069;&#22312;&#35757;&#32451;&#21644;&#39564;&#35777;&#36164;&#28304;&#20805;&#36275;&#26465;&#20214;&#19979;&#35780;&#20272;&#30340;&#24418;&#24335;&#21270;&#36924;&#36817;&#27169;&#22411;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#19988;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent trend towards the application of deep structured techniques has revealed the limits of huge models in natural language processing. This has reawakened the interest in traditional machine learning algorithms, which have proved still to be competitive in certain contexts, in particular low-resource settings. In parallel, model selection has become an essential task to boost performance at reasonable cost, even more so when we talk about processes involving domains where the training and/or computational resources are scarce. Against this backdrop, we evaluate the early estimation of learning curves as a practical mechanism for selecting the most appropriate model in scenarios characterized by the use of non-deep learners in resource-lean settings. On the basis of a formal approximation model previously evaluated under conditions of wide availability of training and validation resources, we study the reliability of such an approach in a different and much more demanding operati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25913;&#36827;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;&#21644;&#20248;&#21270;&#22120;&#31561;&#21508;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#22823;&#35268;&#27169;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26032;&#27700;&#24179;&#30340;BERT&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02447</link><description>&lt;p&gt;
&#25171;&#30772;MLPerf&#35757;&#32451;&#65306;&#20248;&#21270;BERT&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Breaking MLPerf Training: A Case Study on Optimizing BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02447
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;&#21644;&#20248;&#21270;&#22120;&#31561;&#21508;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#22823;&#35268;&#27169;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26032;&#27700;&#24179;&#30340;BERT&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#36895;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#25913;&#36827;&#21253;&#25324;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;&#12289;&#20248;&#21270;&#22120;&#31561;&#35757;&#32451;&#30340;&#21508;&#20010;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#22823;&#35268;&#27169;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#27599;&#20010;&#32452;&#20214;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;BERT&#35757;&#32451;&#24615;&#33021;&#30340;&#26032;&#27700;&#24179;&#12290;&#22312;&#20998;&#24067;&#24335;BERT&#35757;&#32451;&#20013;&#65292;&#36127;&#36733;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#26681;&#25454;&#19981;&#21516;&#38271;&#24230;&#30340;&#26679;&#26412;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#12290;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#35268;&#27169;&#25104;&#27491;&#27604;&#30340;&#36890;&#20449;&#25104;&#26412;&#38656;&#35201;&#36890;&#36807;&#26377;&#29992;&#30340;&#35745;&#31639;&#26469;&#38544;&#34255;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#22120;&#65292;&#22914;ADAM&#12289;LAMB&#31561;&#65292;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20180;&#32454;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#24819;&#27861;&#65292;&#21363;&#22522;&#20110;&#25968;&#25454;&#38598;&#20998;&#23618;&#30340;&#26412;&#22320;&#39044;&#25490;&#24207;&#36827;&#34892;&#36127;&#36733;&#24179;&#34913;&#21644;&#20840;&#32422;&#20943;&#20043;&#21069;&#30340;&#25353;&#26742;&#26799;&#24230;&#35009;&#21098;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#26799;&#24230;&#35745;&#31639;&#21644;&#21516;&#27493;&#30340;&#37325;&#21472;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization
&lt;/p&gt;</description></item><item><title>LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.02446</link><description>&lt;p&gt;
LQER: &#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#37325;&#24314;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
LQER: Low-Rank Quantization Error Reconstruction for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02446
&lt;/p&gt;
&lt;p&gt;
LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#20943;&#23569;&#65288;LQER&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#37327;&#21270;&#21644;&#20302;&#31209;&#36924;&#36817;&#26469;&#24674;&#22797;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;LQER&#21033;&#29992;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#23558;&#37327;&#21270;&#35823;&#24046;&#30340;&#22855;&#24322;&#20540;&#20998;&#24067;&#25512;&#21521;&#26399;&#26395;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;LLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#36817;&#20046;&#26080;&#25439;&#30340;W4A8&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#12289;&#32593;&#26684;&#25628;&#32034;&#25110;&#22522;&#20110;&#26799;&#24230;&#30340;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;LQER&#30340;&#35745;&#31639;&#27169;&#24335;&#28040;&#38500;&#20102;&#20174;&#19981;&#35268;&#21017;&#20869;&#23384;&#20301;&#32622;&#25910;&#38598;&#39640;&#31934;&#24230;&#26435;&#37325;&#25152;&#38656;&#30340;&#19987;&#29992;Scatter&#21644;Gather&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;W4A8 LLMs&#22312;&#20845;&#20010;&#28909;&#38376;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#30828;&#20214;&#36164;&#28304;&#27604;&#39046;&#20808;&#30340;&#26368;&#26032;&#26041;&#27861;&#23569;1.36&#20493;&#12290;&#19968;&#26086;&#35770;&#25991;&#34987;&#25509;&#21463;&#65292;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#25209;&#37327;&#23545;&#27604;&#24335;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65288;BECLR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#32858;&#31867;&#20869;&#23384;&#65288;DyCE&#65289;&#27169;&#22359;&#21644;&#36845;&#20195;&#26368;&#20248;&#20256;&#36755;&#30340;&#20998;&#24067;&#23545;&#40784;&#31574;&#30053;&#65288;OpTA&#65289;&#65292;&#22312;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20998;&#21035;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#37319;&#26679;&#21644;&#26679;&#26412;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02444</link><description>&lt;p&gt;
BECLR&#65306;&#22686;&#24378;&#25209;&#37327;&#23545;&#27604;&#24335;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BECLR: Batch Enhanced Contrastive Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#25209;&#37327;&#23545;&#27604;&#24335;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65288;BECLR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#32858;&#31867;&#20869;&#23384;&#65288;DyCE&#65289;&#27169;&#22359;&#21644;&#36845;&#20195;&#26368;&#20248;&#20256;&#36755;&#30340;&#20998;&#24067;&#23545;&#40784;&#31574;&#30053;&#65288;OpTA&#65289;&#65292;&#22312;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20998;&#21035;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#37319;&#26679;&#21644;&#26679;&#26412;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#26102;&#20195;&#65292;&#24555;&#36895;&#20174;&#24456;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#20013;&#23398;&#20064;&#26159;&#21306;&#20998;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#22522;&#26412;&#29305;&#28857;&#12290;&#26080;&#30417;&#30563;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;U-FSL&#65289;&#24076;&#26395;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#20002;&#24323;&#23545;&#27880;&#37322;&#30340;&#20381;&#36182;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;U-FSL&#39046;&#22495;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#20174;&#39044;&#35757;&#32451;&#21644;&#21518;&#32493;&#25512;&#29702;&#38454;&#27573;&#32467;&#26500;&#24615;&#22320;&#35299;&#20915;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#32858;&#31867;&#20869;&#23384;&#65288;DyCE&#65289;&#27169;&#22359;&#65292;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#38454;&#27573;&#27491;&#26679;&#26412;&#37319;&#26679;&#30340;&#39640;&#24230;&#21487;&#20998;&#31163;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#23558;&#38544;&#24335;&#31867;&#21035;&#32423;&#21035;&#30340;&#27934;&#23519;&#21147;&#34701;&#20837;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#30452;&#34987;&#24573;&#35270;&#20294;&#21364;&#33267;&#20851;&#37325;&#35201;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#38454;&#27573;&#30340;&#26679;&#26412;&#20559;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#26368;&#20248;&#20256;&#36755;&#30340;&#20998;&#24067;&#23545;&#40784;&#65288;OpTA&#65289;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages. We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning. We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;ReLU&#30340;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02442</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;ReLU&#30340;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02442
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;ReLU&#30340;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#23494;&#20999;&#32852;&#31995;&#65292;&#20154;&#20204;&#23545;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#65288;NMD&#65289;&#30340;&#25506;&#32034;&#26085;&#30410;&#22686;&#22810;&#12290;NMD&#26088;&#22312;&#20174;&#31232;&#30095;&#30340;&#38750;&#36127;&#30697;&#38453;&#20013;&#25214;&#21040;&#19968;&#20010;&#20302;&#31209;&#30697;&#38453;&#65292;&#20854;&#20013;&#27599;&#20010;&#20803;&#32032;&#37117;&#32463;&#36807;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#20856;&#22411;&#30340;&#36873;&#25321;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30340;ReLU-NMD&#27169;&#22411;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Tikhonov&#27491;&#21017;&#21270;&#30340;ReLU-NMD&#27169;&#22411;&#65292;&#31216;&#20026;ReLU-NMD-T&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;ReLU-NMD-T&#27169;&#22411;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#32467;&#21512;&#20102;&#27491;&#21160;&#37327;&#21644;&#36127;&#21160;&#37327;&#21442;&#25968;&#12290;&#25105;&#20204;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#20195;&#30721;&#21487;&#22312;https://github.com/nothing2wang/NMD-TM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in the exploration of Nonlinear Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims to find a low-rank matrix from a sparse nonnegative matrix with a per-element nonlinear function. A typical choice is the Rectified Linear Unit (ReLU) activation function. To address over-fitting in the existing ReLU-based NMD model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for handling the ReLU-NMD-T model. A distinctive feature, setting our work apart from most existing studies, is the incorporation of both positive and negative momentum parameters in our algorithm. Our numerical experiments on real-world datasets show the effectiveness of the proposed model and algorithm. Moreover, the code is available at https://github.com/nothing2wang/NMD-TM.
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02439</link><description>&lt;p&gt;
DiffStitch: &#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02439
&lt;/p&gt;
&lt;p&gt;
DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#26368;&#20339;&#36712;&#36857;&#65292;&#36825;&#32473;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#24517;&#39035;&#33719;&#24471;&#21040;&#36798;&#39640;&#22870;&#21169;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#65288;DiffStitch&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#23427;&#21487;&#20197;&#31995;&#32479;&#22320;&#29983;&#25104;&#36712;&#36857;&#20043;&#38388;&#30340;&#25340;&#25509;&#36716;&#25442;&#12290;DiffStitch&#21487;&#20197;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;DiffStitch&#22312;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DiffStitch&#22312;&#19968;&#27493;&#26041;&#27861;&#65288;IQL&#65289;&#12289;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;TD3+BC&#65289;&#21644;&#36712;&#36857;&#26041;&#27861;&#65288;PPO&#65289;&#30340;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#23569;&#37327;&#32500;&#24230;&#30340;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#26469;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#39640;&#32500;&#25955;&#20081;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.02438</link><description>&lt;p&gt;
&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and interpretable Support Vector Classification based on the truncated ANOVA decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02438
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#23569;&#37327;&#32500;&#24230;&#30340;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#26469;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#39640;&#32500;&#25955;&#20081;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#22312;&#25955;&#20081;&#25968;&#25454;&#19978;&#36827;&#34892;&#20998;&#31867;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36890;&#24120;&#38656;&#35201;&#22788;&#29702;&#35768;&#22810;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#19977;&#35282;&#20989;&#25968;&#25110;&#23567;&#27874;&#30340;&#29305;&#24449;&#26144;&#23556;&#26469;&#35299;&#20915;SVM&#30340;&#21407;&#22987;&#24418;&#24335;&#12290;&#22312;&#23567;&#32500;&#24230;&#35774;&#32622;&#20013;&#65292;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#21644;&#30456;&#20851;&#26041;&#27861;&#26159;&#22788;&#29702;&#25152;&#32771;&#34385;&#22522;&#20989;&#25968;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;&#32500;&#25968;&#28798;&#38590;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;FFT&#30340;&#26041;&#27861;&#21464;&#24471;&#20302;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38480;&#21046;&#33258;&#24049;&#20351;&#29992;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#65292;&#27599;&#20010;&#22522;&#20989;&#25968;&#21482;&#20381;&#36182;&#20110;&#23569;&#25968;&#20960;&#20010;&#32500;&#24230;&#12290;&#36825;&#26159;&#30001;&#20110;&#25928;&#24212;&#30340;&#31232;&#30095;&#24615;&#21644;&#26368;&#36817;&#20851;&#20110;&#20989;&#25968;&#20174;&#25955;&#20081;&#25968;&#25454;&#20013;&#30340;&#25130;&#26029;&#26041;&#24046;&#20998;&#35299;&#30340;&#37325;&#24314;&#30340;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#21160;&#26426;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#23427;&#20204;&#30340;&#32806;&#21512;&#26041;&#38754;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machines (SVMs) are an important tool for performing classification on scattered data, where one usually has to deal with many data points in high-dimensional spaces. We propose solving SVMs in primal form using feature maps based on trigonometric functions or wavelets. In small dimensional settings the Fast Fourier Transform (FFT) and related methods are a powerful tool in order to deal with the considered basis functions. For growing dimensions the classical FFT-based methods become inefficient due to the curse of dimensionality. Therefore, we restrict ourselves to multivariate basis functions, each one of them depends only on a small number of dimensions. This is motivated by the well-known sparsity of effects and recent results regarding the reconstruction of functions from scattered data in terms of truncated analysis of variance (ANOVA) decomposition, which makes the resulting model even interpretable in terms of importance of the features as well as their coupling
&lt;/p&gt;</description></item><item><title>&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#32773;&#26159;&#23545;&#24863;&#30693;&#32773;&#27169;&#22411;&#30340;&#20116;&#31181;&#21464;&#20307;&#65292;&#23427;&#20204;&#33021;&#22815;&#33719;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.02433</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#32773;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Perceiver
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02433
&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#32773;&#26159;&#23545;&#24863;&#30693;&#32773;&#27169;&#22411;&#30340;&#20116;&#31181;&#21464;&#20307;&#65292;&#23427;&#20204;&#33021;&#22815;&#33719;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#32773;&#27169;&#22411;&#23545;&#20110;&#36755;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#27809;&#26377;&#36807;&#22810;&#30340;&#24314;&#27169;&#20551;&#35774;&#65292;&#20854;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#26102;&#38388;&#19978;&#20855;&#26377;&#20108;&#27425;&#21487;&#25193;&#23637;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#24863;&#30693;&#32773;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#36229;&#36807;&#25110;&#19982;ResNet-50&#21644;ViT&#30456;&#31454;&#20105;&#12290;&#28982;&#32780;&#65292;&#24863;&#30693;&#32773;&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#26657;&#20934;&#38382;&#39064;&#12290;&#24863;&#30693;&#32773;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#12289;&#19977;&#20010;&#27169;&#22411;&#12289;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;&#21644;&#19968;&#20010;&#36229;&#21442;&#25968;&#35774;&#32622;&#19978;&#37117;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#31967;&#31957;&#30340;&#26159;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24863;&#30693;&#32773;&#27169;&#22411;&#30340;&#24615;&#33021;&#25913;&#36827;&#24133;&#24230;&#24456;&#23567;&#12290;&#27492;&#22806;&#65292;&#24863;&#30693;&#32773;&#27169;&#22411;&#23545;&#20110;&#26550;&#26500;&#20808;&#39564;&#30340;&#20943;&#23569;&#24182;&#19981;&#23454;&#36136;&#24615;&#65292;&#24182;&#19981;&#33021;&#31561;&#21516;&#20110;&#20854;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#21457;&#26126;&#20102;&#20116;&#31181;&#21464;&#20307;&#30340;&#24863;&#30693;&#32773;&#27169;&#22411;&#65292;&#21363;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#32773;&#65292;&#23427;&#20204;&#21487;&#20197;&#33719;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#22312;&#19977;&#20010;&#25351;&#26631;&#19978;&#36827;&#34892;&#27979;&#37327;&#12290;&#36890;&#36807;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#32773;&#30456;&#27604;&#24863;&#30693;&#32773;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Perceiver makes few architectural assumptions about the relationship among its inputs with quadratic scalability on its memory and computation time. Indeed, the Perceiver model outpaces or is competitive with ResNet-50 and ViT in terms of accuracy to some degree. However, the Perceiver does not take predictive uncertainty and calibration into account. The Perceiver also generalizes its performance on three datasets, three models, one evaluation metric, and one hyper-parameter setting. Worst of all, the Perceiver's relative performance improvement against other models is marginal. Furthermore, its reduction of architectural prior is not substantial; is not equivalent to its quality. Thereby, I invented five mutations of the Perceiver, the Uncertainty-Aware Perceivers, that obtain uncertainty estimates and measured their performance on three metrics. Experimented with CIFAR-10 and CIFAR-100, the Uncertainty-Aware Perceivers make considerable performance enhancement compared to the Pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#65292;&#29992;&#20110;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;&#12290;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#23618;&#65288;me-GC&#65289;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#32422;&#26463;&#65292;&#24182;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.02431</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#20197;&#23454;&#29616;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#65292;&#29992;&#20110;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;&#12290;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#23618;&#65288;me-GC&#65289;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#32422;&#26463;&#65292;&#24182;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20132;&#20114;&#21160;&#20316;&#65292;&#21253;&#25324;&#25163;&#23545;&#25163;&#20132;&#20114;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#65292;&#22312;&#35270;&#39057;&#20998;&#26512;&#21644;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;&#22270;&#21367;&#31215;&#22312;&#24314;&#27169;&#39592;&#39612;&#25968;&#25454;&#30340;&#25299;&#25169;&#24863;&#30693;&#29305;&#24449;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#21367;&#31215;&#24212;&#29992;&#20110;&#29420;&#31435;&#23454;&#20307;&#65292;&#24182;&#22312;&#20132;&#20114;&#21160;&#20316;&#35782;&#21035;&#26102;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#65292;&#36825;&#20960;&#20046;&#26080;&#27861;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#35821;&#20041;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#65288;me-GC&#65289;&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;me-GC&#20351;&#29992;&#30456;&#20114;&#25299;&#25169;&#28608;&#21169;&#27169;&#22359;&#39318;&#20808;&#20174;&#21333;&#20010;&#23454;&#20307;&#20013;&#25552;&#21462;&#37051;&#25509;&#30697;&#38453;&#65292;&#28982;&#21518;&#33258;&#36866;&#24212;&#22320;&#23545;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#32422;&#26463;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;me-GC&#36827;&#19968;&#27493;&#20351;&#29992;&#30456;&#20114;&#29305;&#24449;&#28608;&#21169;&#27169;&#22359;&#20174;&#25104;&#23545;&#23454;&#20307;&#20013;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20302;&#23618;&#27425;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#36947;&#36335;&#20998;&#21106;&#65292;&#22312;&#20027;&#27969;&#32593;&#32476;&#27169;&#22411;&#30340;&#20027;&#35201;&#38454;&#27573;&#23454;&#29616;&#20102;&#22823;&#37096;&#20998;&#36947;&#36335;&#20687;&#32032;&#30340;&#20934;&#30830;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#20197;&#20302;&#23618;&#27425;&#29305;&#24449;&#20026;&#20027;&#23548;&#30340;&#36947;&#36335;&#20998;&#21106;&#32593;&#32476;&#65288;LFD-RoadSeg&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.02430</link><description>&lt;p&gt;
&#21033;&#29992;&#20302;&#23618;&#27425;&#34920;&#31034;&#36827;&#34892;&#36229;&#24555;&#36895;&#36947;&#36335;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting Low-level Representations for Ultra-Fast Road Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20302;&#23618;&#27425;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#36947;&#36335;&#20998;&#21106;&#65292;&#22312;&#20027;&#27969;&#32593;&#32476;&#27169;&#22411;&#30340;&#20027;&#35201;&#38454;&#27573;&#23454;&#29616;&#20102;&#22823;&#37096;&#20998;&#36947;&#36335;&#20687;&#32032;&#30340;&#20934;&#30830;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#20197;&#20302;&#23618;&#27425;&#29305;&#24449;&#20026;&#20027;&#23548;&#30340;&#36947;&#36335;&#20998;&#21106;&#32593;&#32476;&#65288;LFD-RoadSeg&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#30340;&#23454;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#19968;&#30452;&#26159;&#36947;&#36335;&#20998;&#21106;&#26041;&#27861;&#30340;&#36861;&#27714;&#12290;&#20026;&#27492;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#36731;&#37327;&#32423;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#36947;&#36335;&#26159;&#8220;&#29289;&#36136;&#8221;&#65288;&#32972;&#26223;&#25110;&#29615;&#22659;&#20803;&#32032;&#65289;&#32780;&#19981;&#26159;&#8220;&#19996;&#35199;&#8221;&#65288;&#29305;&#23450;&#21487;&#35782;&#21035;&#30340;&#23545;&#35937;&#65289;&#30340;&#20107;&#23454;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25506;&#32034;&#29992;&#20302;&#23618;&#27425;&#29305;&#24449;&#32780;&#19981;&#26159;&#39640;&#23618;&#27425;&#29305;&#24449;&#26469;&#34920;&#31034;&#36947;&#36335;&#30340;&#21487;&#34892;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20027;&#27969;&#32593;&#32476;&#27169;&#22411;&#30340;&#20027;&#35201;&#38454;&#27573;&#36275;&#20197;&#34920;&#31034;&#22823;&#37096;&#20998;&#20687;&#32032;&#30340;&#36947;&#36335;&#36827;&#34892;&#20998;&#21106;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20302;&#23618;&#27425;&#29305;&#24449;&#20026;&#20027;&#23548;&#30340;&#36947;&#36335;&#20998;&#21106;&#32593;&#32476;&#65288;LFD-RoadSeg&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LFD-RoadSeg&#37319;&#29992;&#20102;&#21452;&#36793;&#32467;&#26500;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#31354;&#38388;&#32454;&#33410;&#20998;&#25903;&#65292;&#36890;&#36807;ResNet-18&#30340;&#31532;&#19968;&#38454;&#27573;&#25552;&#21462;&#36947;&#36335;&#30340;&#20302;&#23618;&#27425;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#35774;&#35745;&#20102;&#19978;&#19979;&#25991;&#35821;&#20041;&#20998;&#25903;&#65292;&#20197;&#25233;&#21046;&#20302;&#23618;&#27425;&#29305;&#24449;&#20013;&#38169;&#35823;&#22320;&#23558;&#26080;&#32441;&#29702;&#21306;&#22495;&#35823;&#35748;&#20026;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving real-time and accuracy on embedded platforms has always been the pursuit of road segmentation methods. To this end, they have proposed many lightweight networks. However, they ignore the fact that roads are "stuff" (background or environmental elements) rather than "things" (specific identifiable objects), which inspires us to explore the feasibility of representing roads with low-level instead of high-level features. Surprisingly, we find that the primary stage of mainstream network models is sufficient to represent most pixels of the road for segmentation. Motivated by this, we propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg). Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail branch is firstly designed to extract low-level feature representation for the road by the first stage of ResNet-18. To suppress texture-less regions mistaken as the road in the low-level feature, the context semantic branch is then designed to ext
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02429</link><description>&lt;p&gt;
&#26397;&#30528;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#20316;&#20026;&#31163;&#32447;RL&#21644;&#20803;RL&#30340;&#32467;&#21512;&#65292;&#22312;&#23454;&#29616;RL&#26234;&#33021;&#20307;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#20197;&#21450;&#23433;&#20840;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;OMRL&#65288;COMRL&#65289;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#24335;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26377;&#25928;&#20219;&#21153;&#34920;&#31034;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;COMRL&#39046;&#22495;&#30340;&#20960;&#20010;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#36825;&#20123;&#30475;&#20284;&#29420;&#31435;&#30340;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20013;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;COMRL&#31639;&#27861;&#26412;&#36136;&#19978;&#26159;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#36817;&#20284;&#30028;&#38480;&#26469;&#20248;&#21270;&#20219;&#21153;&#21464;&#37327;$\boldsymbol{M}$&#21644;&#20854;&#28508;&#22312;&#34920;&#31034;$\boldsymbol{Z}$&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#22522;&#20110;&#29702;&#35770;&#27934;&#23519;&#21147;&#21644;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;R&#38382;&#39064;&#35889;&#19978;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\boldsymbol{M}$ and its latent representation $\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of R
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28151;&#21512;&#39044;&#27979;&#32508;&#21512;&#35268;&#21010;&#65288;HPP&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#36793;&#32536;&#26465;&#20214;&#19979;&#30340;&#21344;&#29992;&#39044;&#27979;&#21644;&#21338;&#24328;&#35770;&#36816;&#21160;&#39044;&#27979;&#22120;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#27169;&#22359;&#25972;&#21512;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02426</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#28151;&#21512;&#39044;&#27979;&#32508;&#21512;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Hybrid-Prediction Integrated Planning for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28151;&#21512;&#39044;&#27979;&#32508;&#21512;&#35268;&#21010;&#65288;HPP&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#36793;&#32536;&#26465;&#20214;&#19979;&#30340;&#21344;&#29992;&#39044;&#27979;&#21644;&#21338;&#24328;&#35770;&#36816;&#21160;&#39044;&#27979;&#22120;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#27169;&#22359;&#25972;&#21512;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#24182;&#39044;&#27979;&#21608;&#22260;&#29615;&#22659;&#65292;&#20197;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#26368;&#36817;&#23398;&#20064;&#22411;&#31995;&#32479;&#30340;&#36827;&#23637;&#20984;&#26174;&#20102;&#25972;&#21512;&#39044;&#27979;&#21644;&#35268;&#21010;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#24102;&#26469;&#20102;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#21333;&#19968;&#39044;&#27979;&#30340;&#22266;&#26377;&#26435;&#34913;&#65292;&#39044;&#27979;&#27169;&#24335;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#39044;&#27979;&#21644;&#35268;&#21010;&#20013;&#30340;&#31038;&#20132;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28151;&#21512;&#39044;&#27979;&#32508;&#21512;&#35268;&#21010;&#65288;HPP&#65289;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#19977;&#20010;&#21019;&#26032;&#35774;&#35745;&#30340;&#27169;&#22359;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36793;&#32536;&#26465;&#20214;&#19979;&#30340;&#21344;&#29992;&#39044;&#27979;&#65292;&#20197;&#20351;&#32852;&#21512;&#21344;&#29992;&#19982;&#20195;&#29702;&#24863;&#30693;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;MS-OccFormer&#27169;&#22359;&#36890;&#36807;&#20195;&#29702;&#26041;&#21521;&#36816;&#21160;&#39044;&#27979;&#23454;&#29616;&#20102;&#22810;&#38454;&#27573;&#30340;&#21344;&#29992;&#39044;&#27979;&#23545;&#40784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#36816;&#21160;&#39044;&#27979;&#22120;GTFormer&#65292;&#26469;&#27169;&#25311;&#20010;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving systems require the ability to fully understand and predict the surrounding environment to make informed decisions in complex scenarios. Recent advancements in learning-based systems have highlighted the importance of integrating prediction and planning modules. However, this integration has brought forth three major challenges: inherent trade-offs by sole prediction, consistency between prediction patterns, and social coherence in prediction and planning. To address these challenges, we introduce a hybrid-prediction integrated planning (HPP) system, which possesses three novelly designed modules. First, we introduce marginal-conditioned occupancy prediction to align joint occupancy with agent-wise perceptions. Our proposed MS-OccFormer module achieves multi-stage alignment per occupancy forecasting with consistent awareness from agent-wise motion predictions. Second, we propose a game-theoretic motion predictor, GTFormer, to model the interactive future among indivi
&lt;/p&gt;</description></item><item><title>EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02425</link><description>&lt;p&gt;
EuLagNet: &#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#27431;&#25289;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02425
&lt;/p&gt;
&lt;p&gt;
EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#23545;&#27668;&#35937;&#23398;&#12289;&#28023;&#27915;&#23398;&#21644;&#31354;&#27668;&#21160;&#21147;&#23398;&#31561;&#24191;&#27867;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#36890;&#24120;&#20174;&#27431;&#25289;&#35282;&#24230;&#35266;&#23519;&#65292;&#20854;&#27963;&#36291;&#21644;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#22312;&#38745;&#27490;&#30340;&#32593;&#26684;&#20013;&#20005;&#37325;&#34987;&#25513;&#30422;&#21644;&#28151;&#28102;&#65292;&#32473;&#39044;&#27979;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#20026;&#23548;&#21521;&#30340;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#21452;&#37325;&#36882;&#24402;&#32593;&#32476;&#65288;EuLagNet&#65289;&#65292;&#36890;&#36807;&#36319;&#36394;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#24182;&#38543;&#26102;&#38388;&#31215;&#32047;&#21160;&#21147;&#23398;&#20449;&#24687;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;EuLag&#22359;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#26102;&#21051;&#21644;&#23610;&#24230;&#19978;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#27431;&#25289;&#21644;&#25289;&#26684;&#26391;&#26085;&#29305;&#24449;&#65292;&#20854;&#20013;&#36319;&#36394;&#31890;&#23376;&#30340;&#36816;&#21160;&#26159;&#20174;&#27431;&#25289;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#31215;&#32047;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#34987;&#32435;&#20837;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
&lt;/p&gt;</description></item><item><title>Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02423</link><description>&lt;p&gt;
Uni-RLHF: &#29992;&#20110;&#22810;&#26679;&#21270;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#29992;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02423
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#25163;&#21160;&#22870;&#21169;&#35774;&#35745;&#65292;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#29615;&#22659;&#20013;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#31867;&#22411;&#23545;RLHF&#30340;&#36827;&#27493;&#36827;&#34892;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27880;&#37322;&#24179;&#21488;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#32479;&#19968;&#22522;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Uni-RLHF&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;RLHF&#37327;&#36523;&#23450;&#21046;&#30340;&#32508;&#21512;&#31995;&#32479;&#23454;&#29616;&#12290;&#23427;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#23436;&#25972;&#30340;&#20174;&#30495;&#23454;&#20154;&#31867;&#21453;&#39304;&#21040;&#23454;&#38469;&#38382;&#39064;&#21457;&#23637;&#30340;&#24037;&#20316;&#27969;&#12290;Uni-RLHF&#21253;&#21547;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#36890;&#29992;&#30340;&#22810;&#21453;&#39304;&#27880;&#37322;&#24179;&#21488;&#65292;2&#65289;&#22823;&#35268;&#27169;&#30340;&#20247;&#21253;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;3&#65289;&#27169;&#22359;&#21270;&#30340;&#31163;&#32447;RLHF&#22522;&#20934;&#23454;&#29616;&#12290;Uni-RLHF&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#21453;&#39304;&#31867;&#22411;&#65292;&#24182;&#19982;&#20027;&#35201;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of main
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#25972;&#21512;&#21040;&#19968;&#20010;&#26816;&#32034;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#36879;&#35270;&#26816;&#32034;&#26469;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#12290;&#20351;&#29992;&#35299;&#37322;&#26041;&#27861;&#20998;&#26512;&#40657;&#30418;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#23558;&#37325;&#35201;&#24615;&#20998;&#25968;&#20316;&#20026;&#34917;&#20805;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;&#25552;&#21319;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#21644;&#20107;&#23454;&#26816;&#39564;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02418</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#22810;&#36879;&#35270;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
eXplainable Bayesian Multi-Perspective Generative Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#25972;&#21512;&#21040;&#19968;&#20010;&#26816;&#32034;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#36879;&#35270;&#26816;&#32034;&#26469;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#12290;&#20351;&#29992;&#35299;&#37322;&#26041;&#27861;&#20998;&#26512;&#40657;&#30418;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#23558;&#37325;&#35201;&#24615;&#20998;&#25968;&#20316;&#20026;&#34917;&#20805;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;&#25552;&#21319;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#21644;&#20107;&#23454;&#26816;&#39564;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30830;&#23450;&#24615;&#26816;&#32034;&#27969;&#27700;&#32447;&#27880;&#37325;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#23548;&#33268;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#25972;&#21512;&#21040;&#26816;&#32034;&#27969;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#36879;&#35270;&#26816;&#32034;&#26469;&#26657;&#20934;&#26816;&#32034;&#27969;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#32467;&#21512;LIME&#21644;SHAP&#31561;&#25216;&#26415;&#26469;&#20998;&#26512;&#40657;&#30418;&#37325;&#25490;&#24207;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20174;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#20013;&#24471;&#20986;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#20316;&#20026;&#34917;&#20805;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;&#20197;&#22686;&#24378;&#22522;&#30784;&#37325;&#25490;&#24207;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#38382;&#31572;&#21644;&#20107;&#23454;&#26816;&#39564;&#20219;&#21153;&#19978;&#35780;&#20272;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#37325;&#25490;&#23454;&#29616;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;KILT&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deterministic retrieval pipelines prioritize achieving state-of-the-art performance but often lack interpretability in decision-making. These models face challenges in assessing uncertainty, leading to overconfident predictions. To overcome these limitations, we integrate uncertainty calibration and interpretability into a retrieval pipeline. Specifically, we introduce Bayesian methodologies and multi-perspective retrieval to calibrate uncertainty within a retrieval pipeline. We incorporate techniques such as LIME and SHAP to analyze the behavior of a black-box reranker model. The importance scores derived from these explanation methodologies serve as supplementary relevance scores to enhance the base reranker model. We evaluate the resulting performance enhancements achieved through uncertainty calibration and interpretable reranking on Question Answering and Fact Checking tasks. Our methods demonstrate substantial performance improvements across three KILT datasets.
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.02408</link><description>&lt;p&gt;
GLaPE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02408
&lt;/p&gt;
&lt;p&gt;
GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#20219;&#21153;&#24615;&#33021;&#20173;&#28982;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;LLM&#33258;&#36523;&#20316;&#20026;&#20248;&#21270;&#22120;&#26469;&#35782;&#21035;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26368;&#20248;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25552;&#31034;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#38590;&#20197;&#33719;&#21462;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#26631;&#31614;&#65292;&#20197;&#35745;&#31639;&#27599;&#20010;&#20505;&#36873;&#25552;&#31034;&#30340;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#24191;&#27867;&#30340;&#23454;&#26045;&#21644;&#36890;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65288;GLaPE&#65289;&#65292;&#20197;&#20943;&#23569;&#23545;&#37329;&#26631;&#31614;&#30340;&#20381;&#36182;&#12290;&#21463;&#21040;&#33258;&#19968;&#33268;&#24615;&#21644;&#31572;&#26696;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#36827;&#34892;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GLaPE&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20845;&#20010;&#20219;&#21153;&#65292;GLaPE&#22312;&#32477;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#35780;&#20272;&#32467;&#26524;&#19982;&#20351;&#29992;&#30495;&#23454;&#37329;&#26631;&#31614;&#35780;&#20272;&#30340;&#32467;&#26524;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#30028;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#65292;&#19982;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#30456;&#20851;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#26029;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.02407</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Defining Neural Network Architecture through Polytope Structures of Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#30028;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#65292;&#19982;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#30456;&#20851;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#26029;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#22823;&#22411;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#24443;&#24213;&#20998;&#31867;&#65292;&#28982;&#32780;&#36825;&#31181;&#20851;&#31995;&#30340;&#20855;&#20307;&#24615;&#36136;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#30340;&#19978;&#19979;&#30028;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#30028;&#38480;&#26159;&#30001;&#25152;&#35752;&#35770;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#25152;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#21407;&#21017;&#22312;&#21333;&#32431;&#22797;&#21512;&#20307;&#21644;&#29305;&#23450;&#22810;&#26679;&#26354;&#38754;&#24418;&#29366;&#19978;&#30340;&#24212;&#29992;&#65292;&#35299;&#37322;&#20102;&#32593;&#32476;&#23485;&#24230;&#38656;&#27714;&#22914;&#20309;&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#32780;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#30740;&#31350;&#19968;&#31181;&#30456;&#21453;&#24773;&#20917;&#65292;&#21363;&#21487;&#20197;&#20174;&#30456;&#24212;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20986;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#12289;Fashion-MNIST&#21644;CIFAR10&#65289;&#21487;&#20197;&#29992;&#21482;&#26377;&#23569;&#25968;&#38754;&#30340;&#20004;&#20010;&#22810;&#38754;&#20307;&#26377;&#25928;&#22320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current theoretical and empirical research in neural networks suggests that complex datasets require large network architectures for thorough classification, yet the precise nature of this relationship remains unclear. This paper tackles this issue by defining upper and lower bounds for neural network widths, which are informed by the polytope structure of the dataset in question. We also delve into the application of these principles to simplicial complexes and specific manifold shapes, explaining how the requirement for network width varies in accordance with the geometric complexity of the dataset. Moreover, we develop an algorithm to investigate a converse situation where the polytope structure of a dataset can be inferred from its corresponding trained neural networks. Through our algorithm, it is established that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently encapsulated using no more than two polytopes with a small number of faces.
&lt;/p&gt;</description></item><item><title>FreDF&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26631;&#31614;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.02399</link><description>&lt;p&gt;
FreDF: &#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FreDF: Learning to Forecast in Frequency Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02399
&lt;/p&gt;
&lt;p&gt;
FreDF&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26631;&#31614;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#22312;&#21382;&#21490;&#24207;&#21015;&#21644;&#26631;&#31614;&#24207;&#21015;&#20013;&#37117;&#38754;&#20020;&#33258;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22788;&#29702;&#21382;&#21490;&#24207;&#21015;&#20013;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#33258;&#30456;&#20851;&#23384;&#22312;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26032;&#20852;&#30340;&#39044;&#27979;&#27169;&#22411;&#20027;&#35201;&#36981;&#24490;&#30452;&#25509;&#39044;&#27979;&#65288;DF&#65289;&#33539;&#24335;&#65292;&#22312;&#26631;&#31614;&#24207;&#21015;&#20013;&#20551;&#35774;&#26465;&#20214;&#29420;&#31435;&#24615;&#19979;&#29983;&#25104;&#22810;&#27493;&#39044;&#27979;&#12290;&#36825;&#31181;&#20551;&#35774;&#24573;&#35270;&#20102;&#26631;&#31614;&#24207;&#21015;&#20013;&#22266;&#26377;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22522;&#20110;DF&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39057;&#22495;&#22686;&#24378;&#30452;&#25509;&#39044;&#27979;&#65288;FreDF&#65289;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#26469;&#36991;&#20813;&#26631;&#31614;&#33258;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreDF&#22312;&#24615;&#33021;&#19978;&#22823;&#22823;&#36229;&#36807;&#20102;&#21253;&#25324;iTransformer&#22312;&#20869;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models. In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods including iTransformer and is compatible with a variety of forecast models.
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAGE&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#36741;&#21161;&#39564;&#35777;&#21644;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02388</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#39564;&#35777;&#30340;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#29983;&#25104;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAGE&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#36741;&#21161;&#39564;&#35777;&#21644;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#25552;&#20986;&#21644;&#39564;&#35777;&#38024;&#23545;&#22797;&#26434;&#31995;&#32479;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#35299;&#20915;&#26041;&#26696;&#25110;&#25919;&#31574;&#65292;&#24182;&#23454;&#29616;&#21508;&#31181;&#30446;&#26631;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#36328;&#23398;&#31185;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#20855;&#26377;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#32534;&#31243;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26377;&#28508;&#21147;&#20943;&#36731;&#36825;&#20010;&#36807;&#31243;&#30340;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;LLM&#25797;&#38271;&#22788;&#29702;&#24207;&#21015;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;ABM&#20013;&#30340;&#22797;&#26434;&#20132;&#20114;&#21644;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;LLM&#32570;&#20047;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#20165;&#20165;&#20381;&#38752;LLM&#26159;&#26080;&#27861;&#26377;&#25928;&#23436;&#25104;&#36825;&#20010;&#36807;&#31243;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAGE&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35270;&#35273;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02382</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35270;&#35273;&#35843;&#25972;&#20013;&#25552;&#31034;&#35789;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Power of Prompt for Visual Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35270;&#35273;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#26159;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#35789;&#26469;&#23450;&#21046;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;VPT&#21450;&#20854;&#21464;&#31181;&#32463;&#24120;&#36935;&#21040;&#35832;&#22914;&#25552;&#31034;&#21021;&#22987;&#21270;&#12289;&#25552;&#31034;&#38271;&#24230;&#21644;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#24615;&#33021;&#19981;&#20339;&#31561;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#25104;&#21151;&#30340;&#19978;&#19979;&#25991;&#36866;&#24212;&#12290;&#26412;&#30740;&#31350;&#20174;&#25506;&#32034;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#31034;&#35789;&#19982;&#34917;&#19969;&#20196;&#29260;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#28436;&#21464;&#24320;&#22987;&#12290;&#21463;&#21040;&#25552;&#31034;&#20196;&#29260;&#19982;&#34917;&#19969;&#20196;&#29260;&#20043;&#38388;&#24448;&#24448;&#20855;&#26377;&#39640;&#20114;&#20449;&#24687;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#35813;&#31574;&#30053;&#24615;&#21021;&#22987;&#21270;&#26126;&#26174;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;VPT&#65292;&#26080;&#38656;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#27969;&#31243;&#20445;&#25345;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#35814;&#23613;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe
&lt;/p&gt;</description></item><item><title>NOAH&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22836;&#37096;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#25104;&#23545;&#23545;&#35937;&#31867;&#21035;&#27880;&#24847;&#21147;(POCA)&#26469;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#26367;&#20195;&#29616;&#26377;&#30340;DNN&#22836;&#37096;&#65292;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02377</link><description>&lt;p&gt;
NOAH: &#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#25104;&#23545;&#23545;&#35937;&#31867;&#21035;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
NOAH: Learning Pairwise Object Category Attentions for Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02377
&lt;/p&gt;
&lt;p&gt;
NOAH&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22836;&#37096;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#25104;&#23545;&#23545;&#35937;&#31867;&#21035;&#27880;&#24847;&#21147;(POCA)&#26469;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#26367;&#20195;&#29616;&#26377;&#30340;DNN&#22836;&#37096;&#65292;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#36890;&#24120;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#30340;&#20027;&#24178;&#21644;&#29992;&#20110;&#29305;&#24449;&#32534;&#30721;&#21644;&#31867;&#21035;&#39044;&#27979;&#30340;&#22836;&#37096;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20027;&#27969;DNN&#30340;&#22836;&#37096;&#32467;&#26500;&#37319;&#29992;&#31867;&#20284;&#30340;&#29305;&#24449;&#32534;&#30721;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#20381;&#36182;&#24615;&#32780;&#24573;&#30053;&#20102;&#23616;&#37096;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#29305;&#24449;&#32534;&#30721;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#31216;&#20026;&#25104;&#23545;&#23545;&#35937;&#31867;&#21035;&#27880;&#24847;&#21147;(POCA)&#30340;&#26032;&#22411;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;Non-glObal Attentive Head (NOAH)&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#23494;&#38598;&#30340;&#29305;&#23450;&#31867;&#21035;&#30340;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;NOAH&#24341;&#20837;&#20102;&#19968;&#31181;&#25972;&#27905;&#30340;&#29305;&#24449;&#20998;&#21106;&#12289;&#36716;&#25442;&#21644;&#21512;&#24182;&#25805;&#20316;&#30340;&#32452;&#21512;&#65292;&#20197;&#22312;&#23616;&#37096;&#21040;&#20840;&#23616;&#33539;&#22260;&#20869;&#23398;&#20064;POCA&#12290;&#20316;&#20026;&#19968;&#31181;&#26131;&#20110;&#26367;&#20195;&#21508;&#31181;&#31867;&#22411;DNN&#29616;&#26377;&#22836;&#37096;&#30340;&#35774;&#35745;&#65292;NOAH&#33021;&#22815;&#22312;&#20445;&#25345;&#31867;&#20284;&#30340;&#27169;&#22411;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A modern deep neural network (DNN) for image classification tasks typically consists of two parts: a backbone for feature extraction, and a head for feature encoding and class predication. We observe that the head structures of mainstream DNNs adopt a similar feature encoding pipeline, exploiting global feature dependencies while disregarding local ones. In this paper, we revisit the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that relies on a new form of dot-product attention called pairwise object category attention (POCA), efficiently exploiting spatially dense category-specific attentions to augment classification performance. NOAH introduces a neat combination of feature split, transform and merge operations to learn POCAs at local to global scales. As a drop-in design, NOAH can be easily used to replace existing heads of various types of DNNs, improving classification performance while maintaining similar model efficiency. We validate the effectiveness 
&lt;/p&gt;</description></item><item><title>AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02370</link><description>&lt;p&gt;
AutoTimes: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoTimes: Autoregressive Time Series Forecasters via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02370
&lt;/p&gt;
&lt;p&gt;
AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#23578;&#26410;&#23436;&#20840;&#21457;&#23637;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#30456;&#20284;&#39034;&#24207;&#32467;&#26500;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#30340;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#23545;LLM&#28508;&#21147;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20174;&#35821;&#35328;&#24314;&#27169;&#20013;&#23398;&#21040;&#30340;&#36890;&#29992;&#20196;&#29260;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoTimes&#65292;&#23558;LLM&#37325;&#26032;&#29992;&#20316;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36825;&#19982;LLM&#30340;&#33719;&#21462;&#21644;&#21033;&#29992;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#22788;&#29702;&#28789;&#27963;&#30340;&#31995;&#21015;&#38271;&#24230;&#65292;&#24182;&#23454;&#29616;&#19982;&#27969;&#34892;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02368</link><description>&lt;p&gt;
&#35745;&#26102;&#22120;: &#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Timer: Transformers for Time Series Analysis at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#27169;&#22411;&#21487;&#33021;&#36935;&#21040;&#24615;&#33021;&#29942;&#39048;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#20013;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#39281;&#21644;&#32780;&#38544;&#34109;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;&#22823;&#27169;&#22411;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21462;&#24471;&#20102;&#25345;&#32493;&#30340;&#36827;&#23637;&#65292;&#22312;&#23569;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#26041;&#38754;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#22312;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#25913;&#21464;&#30446;&#21069;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23567;&#27169;&#22411;&#30340;&#20570;&#27861;&#65292;&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#21253;&#21547;10&#20159;&#20010;&#26102;&#38388;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#32479;&#19968;&#20026;&#21333;&#24207;&#21015;&#24207;&#21015;&#65288;S3&#65289;&#26684;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#38754;&#21521;LTSM&#30340;GPT&#39118;&#26684;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet 
&lt;/p&gt;</description></item><item><title>Transolver&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;Transformer&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#27880;&#24847;&#21147;&#21644;&#28789;&#27963;&#24418;&#29366;&#30340;&#29255;&#27573;&#26469;&#23398;&#20064;&#31163;&#25955;&#21270;&#20960;&#20309;&#24418;&#29366;&#32972;&#21518;&#38544;&#34255;&#30340;&#20869;&#22312;&#29289;&#29702;&#29366;&#24577;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#19979;&#30340;&#22797;&#26434;&#29289;&#29702;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20855;&#22791;&#20869;&#29983;&#20960;&#20309;&#29983;&#25104;&#33021;&#21147;&#30340;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.02366</link><description>&lt;p&gt;
Transolver&#65306;&#19968;&#31181;&#29992;&#20110;&#19968;&#33324;&#20960;&#20309;&#20307;&#19978;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#24555;&#36895;Transformer&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transolver: A Fast Transformer Solver for PDEs on General Geometries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02366
&lt;/p&gt;
&lt;p&gt;
Transolver&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;Transformer&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#27880;&#24847;&#21147;&#21644;&#28789;&#27963;&#24418;&#29366;&#30340;&#29255;&#27573;&#26469;&#23398;&#20064;&#31163;&#25955;&#21270;&#20960;&#20309;&#24418;&#29366;&#32972;&#21518;&#38544;&#34255;&#30340;&#20869;&#22312;&#29289;&#29702;&#29366;&#24577;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#19979;&#30340;&#22797;&#26434;&#29289;&#29702;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20855;&#22791;&#20869;&#29983;&#20960;&#20309;&#29983;&#25104;&#33021;&#21147;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#23454;&#29616;&#20102;&#24456;&#22810;&#37324;&#31243;&#30865;&#24335;&#30340;&#25104;&#23601;&#65292;&#26368;&#36817;&#24320;&#22987;&#24212;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;PDEs&#36890;&#24120;&#34987;&#31163;&#25955;&#21270;&#25104;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#30340;&#22823;&#35268;&#27169;&#32593;&#26684;&#65292;&#23545;Transformer&#26469;&#35828;&#30452;&#25509;&#20174;&#22823;&#37327;&#21333;&#20010;&#28857;&#20013;&#25429;&#25417;&#22797;&#26434;&#30340;&#29289;&#29702;&#30456;&#20851;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#36229;&#36234;&#32932;&#27973;&#32780;&#31528;&#37325;&#30340;&#32593;&#26684;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#26356;&#22522;&#30784;&#30340;&#24605;&#24819;&#25552;&#20986;&#20102;Transolver&#65292;&#21363;&#23398;&#20064;&#31163;&#25955;&#21270;&#20960;&#20309;&#24418;&#29366;&#32972;&#21518;&#38544;&#34255;&#30340;&#20869;&#22312;&#29289;&#29702;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#31163;&#25955;&#21270;&#30340;&#22495;&#33258;&#36866;&#24212;&#22320;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;&#21487;&#23398;&#20064;&#30340;&#28789;&#27963;&#24418;&#29366;&#30340;&#29255;&#27573;&#65292;&#20855;&#26377;&#30456;&#20284;&#29289;&#29702;&#29366;&#24577;&#30340;&#32593;&#26684;&#28857;&#23558;&#34987;&#24402;&#23646;&#20110;&#21516;&#19968;&#20010;&#29255;&#27573;&#12290;&#36890;&#36807;&#35745;&#31639;&#20174;&#29255;&#27573;&#32534;&#30721;&#30340;&#20855;&#26377;&#29289;&#29702;&#24847;&#35782;&#30340;&#35760;&#21495;&#30340;&#27880;&#24847;&#21147;&#65292;Transovler&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#19979;&#30340;&#22797;&#26434;&#29289;&#29702;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20351;&#27714;&#35299;&#22120;&#20855;&#22791;&#20869;&#29983;&#20960;&#20309;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-genera
&lt;/p&gt;</description></item><item><title>&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02364</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#21457;&#23637;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
The Developmental Landscape of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02364
&lt;/p&gt;
&lt;p&gt;
&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;transformers&#20013;&#65292;&#24403;&#23427;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25110;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22914;&#20309;&#20197;&#31163;&#25955;&#30340;&#21457;&#23637;&#38454;&#27573;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#38548;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#36890;&#36807;&#25506;&#27979;&#21442;&#25968;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#31181;&#32676;&#25439;&#22833;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#30740;&#31350;&#36825;&#20123;&#26032;&#26041;&#27861;&#25581;&#31034;&#30340;&#38454;&#27573;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29289;&#29702;&#23398;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21407;&#29702;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21457;&#29616;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#20887;&#20313;&#21487;&#20197;&#35299;&#37322;&#20026;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#24182;&#35777;&#26126;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#20887;&#20313;&#22312;&#31070;&#32463;ODE&#20013;&#21319;&#32423;&#20026;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#25214;&#21040;&#20102;Transformer&#27169;&#22411;&#19982;&#31070;&#32463;ODE&#21450;&#20854;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#28982;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02362</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#23545;&#31216;&#24615;&#32479;&#19968;&#65306;Transformer, &#21069;&#39304;&#21644;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29289;&#29702;&#23398;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21407;&#29702;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21457;&#29616;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#20887;&#20313;&#21487;&#20197;&#35299;&#37322;&#20026;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#24182;&#35777;&#26126;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#20887;&#20313;&#22312;&#31070;&#32463;ODE&#20013;&#21319;&#32423;&#20026;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#25214;&#21040;&#20102;Transformer&#27169;&#22411;&#19982;&#31070;&#32463;ODE&#21450;&#20854;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#28982;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#36816;&#20316;&#65292;&#21253;&#25324;transformers&#65292;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38590;&#39064;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#23398;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21407;&#29702;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#20989;&#25968;&#35270;&#20026;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#65292;&#21457;&#29616;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#20887;&#20313;&#21487;&#20197;&#35299;&#37322;&#20026;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;ODE&#20013;&#25968;&#23398;&#24418;&#24335;&#21270;&#20102;&#21442;&#25968;&#20887;&#20313;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#30001;&#26102;&#31354;&#24494;&#20998;&#21516;&#32986;&#32473;&#20986;&#65292;&#36825;&#22312;&#29233;&#22240;&#26031;&#22374;&#30340;&#24341;&#21147;&#29702;&#35770;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;ODE&#35270;&#20026;&#36830;&#32493;&#29256;&#26412;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#20887;&#20313;&#30830;&#23454;&#22312;&#31070;&#32463;ODE&#20013;&#21319;&#32423;&#20026;&#24494;&#20998;&#21516;&#32986;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;Transformer&#27169;&#22411;&#65292;&#25214;&#21040;&#20102;&#19982;&#31070;&#32463;ODE&#21450;&#20854;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#28982;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The
&lt;/p&gt;</description></item><item><title>Pruner&#26159;&#19968;&#31181;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#23454;&#29616;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02361</link><description>&lt;p&gt;
Pruner:&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02361
&lt;/p&gt;
&lt;p&gt;
Pruner&#26159;&#19968;&#31181;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#23454;&#29616;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#65288;DLAs&#65289;&#19978;&#30340;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#23545;&#20110;&#26377;&#25928;&#30340;&#27169;&#22411;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#25628;&#32034;&#30340;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#65288;DLC&#65289;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30528;&#25628;&#32034;&#25928;&#29575;&#20302;&#21644;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#24046;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pruner&#65292;&#36981;&#24490;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21407;&#21017;&#26469;&#20998;&#23618;&#25552;&#21319;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#12290;Pruner&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#12290;&#21069;&#32773;&#20316;&#20026;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#21644;&#20844;&#24335;&#21270;&#30340;&#24615;&#33021;&#20998;&#26512;&#24037;&#20855;&#65292;&#24341;&#23548;&#25628;&#32034;&#31354;&#38388;&#30340;&#20462;&#21098;&#65292;&#32780;&#21518;&#32773;&#26681;&#25454;&#20851;&#38190;&#30340;&#25968;&#25454;&#27969;&#27169;&#24335;&#23454;&#29616;&#20102;&#23545;&#24352;&#37327;&#31243;&#24207;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#35777;&#26377;&#25928;&#30340;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model ($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22686;&#37327;&#25311;&#29275;&#39039;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#31209;&#19968;&#26356;&#26032;&#21644;&#22359;&#26356;&#26032;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#26080;&#26465;&#20214;&#25968;&#30340;&#23616;&#37096;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02359</link><description>&lt;p&gt;
&#20855;&#26377;&#26356;&#24555;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#22686;&#37327;&#25311;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incremental Quasi-Newton Methods with Faster Superlinear Convergence Rates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22686;&#37327;&#25311;&#29275;&#39039;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#31209;&#19968;&#26356;&#26032;&#21644;&#22359;&#26356;&#26032;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#26080;&#26465;&#20214;&#25968;&#30340;&#23616;&#37096;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26377;&#38480;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#20998;&#37327;&#20989;&#25968;&#37117;&#26159;&#24378;&#20984;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;Lipschitz&#36830;&#32493;&#30340;&#26799;&#24230;&#21644;Hessian&#30697;&#38453;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22686;&#37327;&#25311;&#29275;&#39039;&#26041;&#27861;&#22522;&#20110;BFGS&#26356;&#26032;&#65292;&#36798;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#26465;&#20214;&#25968;&#30340;&#23616;&#37096;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#25311;&#29275;&#39039;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#31216;&#31209;&#19968;&#26356;&#26032;&#32467;&#21512;&#21040;&#22686;&#37327;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#26465;&#20214;&#25968;&#30340;&#23616;&#37096;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;Hessian&#36817;&#20284;&#20540;&#19978;&#24212;&#29992;&#22359;&#26356;&#26032;&#26469;&#25552;&#21319;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#24555;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite-sum optimization problem, where each component function is strongly convex and has Lipschitz continuous gradient and Hessian. The recently proposed incremental quasi-Newton method is based on BFGS update and achieves a local superlinear convergence rate that is dependent on the condition number of the problem. This paper proposes a more efficient quasi-Newton method by incorporating the symmetric rank-1 update into the incremental framework, which results in the condition-number-free local superlinear convergence rate. Furthermore, we can boost our method by applying the block update on the Hessian approximation, which leads to an even faster local convergence rate. The numerical experiments show the proposed methods significantly outperform the baseline methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#34701;&#21512;&#20102;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#27169;&#24577;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#26681;&#22240;&#23450;&#20301;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02357</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#19982;&#26681;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Causal Structure Learning and Root Cause Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#34701;&#21512;&#20102;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#27169;&#24577;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#26681;&#22240;&#23450;&#20301;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#26681;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#23545;&#20110;&#36805;&#36895;&#24674;&#22797;&#26381;&#21153;&#12289;&#26368;&#23567;&#21270;&#25439;&#22833;&#20197;&#21450;&#30830;&#20445;&#22797;&#26434;&#31995;&#32479;&#30340;&#39034;&#21033;&#36816;&#34892;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;RCA&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#26500;&#24314;&#20381;&#36182;&#25110;&#22240;&#26524;&#22270;&#20197;&#36861;&#28335;&#26681;&#22240;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#19981;&#22815;&#29702;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mulan&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#22240;&#23450;&#20301;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#38024;&#23545;&#26085;&#24535;&#23450;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20419;&#36827;&#26085;&#24535;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#26085;&#24535;&#24207;&#21015;&#36716;&#25442;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#25506;&#32034;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#21462;&#27169;&#24577;&#19981;&#21464;&#21644;&#27169;&#24577;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#38190;&#32489;&#25928;
&lt;/p&gt;
&lt;p&gt;
Effective root cause analysis (RCA) is vital for swiftly restoring services, minimizing losses, and ensuring the smooth operation and management of complex systems. Previous data-driven RCA methods, particularly those employing causal discovery techniques, have primarily focused on constructing dependency or causal graphs for backtracking the root causes. However, these methods often fall short as they rely solely on data from a single modality, thereby resulting in suboptimal solutions. In this work, we propose Mulan, a unified multi-modal causal structure learning method for root cause localization. We leverage a log-tailored language model to facilitate log representation learning, converting log sequences into time-series data. To explore intricate relationships across different modalities, we propose a contrastive learning-based approach to extract modality-invariant and modality-specific representations within a shared latent space. Additionally, we introduce a novel key performa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#22330;&#26223;&#19979;&#30340;&#38750;&#20984;&#27714;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21152;&#36895;&#25928;&#26524;&#30340;&#38543;&#26426;&#20998;&#25955;&#19968;&#38454;&#31639;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02356</link><description>&lt;p&gt;
&#20998;&#25955;&#30340;&#38750;&#20984;&#27714;&#21644;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Decentralized Sum-of-Nonconvex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#22330;&#26223;&#19979;&#30340;&#38750;&#20984;&#27714;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21152;&#36895;&#25928;&#26524;&#30340;&#38543;&#26426;&#20998;&#25955;&#19968;&#38454;&#31639;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26368;&#23567;&#21270;&#38750;&#20984;&#20989;&#25968;&#30340;&#27714;&#21644;&#38382;&#39064;&#65292;&#21363;&#23558;&#38750;&#20984;&#32452;&#20998;&#30340;&#24179;&#22343;&#20540;&#20316;&#20026;&#20984;&#20989;&#25968;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#30340;&#38543;&#26426;&#31639;&#27861;&#21482;&#20851;&#27880;&#21333;&#21488;&#26426;&#22120;&#21644;&#38598;&#20013;&#24335;&#22330;&#26223;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#22330;&#26223;&#19979;&#30340;&#38750;&#20984;&#27714;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#35813;&#38382;&#39064;&#30340;PMGT-SVRG&#31639;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#20182;&#20204;&#26041;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#28982;&#32780;&#65292;PMGT-SVRG&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#26465;&#20214;&#25968;&#21576;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#26465;&#20214;&#25968;&#36739;&#24046;&#30340;&#38382;&#39064;&#20013;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#38543;&#26426;&#20998;&#25955;&#19968;&#38454;&#31639;&#27861;&#65292;&#23558;&#21152;&#36895;&#12289;&#26799;&#24230;&#36861;&#36394;&#21644;&#22810;&#19968;&#33268;&#24615;&#28151;&#21512;&#25216;&#26415;&#34701;&#20837;SVRG&#31639;&#27861;&#20013;&#12290;&#25152;&#25552;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#26465;&#20214;&#25968;&#21576;&#24179;&#26041;&#26681;&#20381;&#36182;&#20851;&#31995;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimization problem of minimizing the sum-of-nonconvex function, i.e., a convex function that is the average of nonconvex components. The existing stochastic algorithms for such a problem only focus on a single machine and the centralized scenario. In this paper, we study the sum-of-nonconvex optimization in the decentralized setting. We present a new theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the linear convergence of their approach. However, the convergence rate of the PMGT-SVRG algorithm has a linear dependency on the condition number, which is undesirable for the ill-conditioned problem. To remedy this issue, we propose an accelerated stochastic decentralized first-order algorithm by incorporating the techniques of acceleration, gradient tracking, and multi-consensus mixing into the SVRG algorithm. The convergence rate of the proposed method has a square-root dependency on the condition number. The numerical experiments validate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{Symbol}&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#26469;&#33258;&#21160;&#21457;&#29616;&#40657;&#30418;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;\textsc{Symbol}&#29983;&#25104;&#30340;&#20248;&#21270;&#22120;&#22312;&#36229;&#36234;&#29616;&#26377;&#22522;&#20934;&#32447;&#30340;&#21516;&#26102;&#65292;&#36824;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02355</link><description>&lt;p&gt;
Symbol:&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#29983;&#25104;&#28789;&#27963;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{Symbol}&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#26469;&#33258;&#21160;&#21457;&#29616;&#40657;&#30418;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;\textsc{Symbol}&#29983;&#25104;&#30340;&#20248;&#21270;&#22120;&#22312;&#36229;&#36234;&#29616;&#26377;&#22522;&#20934;&#32447;&#30340;&#21516;&#26102;&#65292;&#36824;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;MetaBBO&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20803;&#23398;&#20064;&#20256;&#32479;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#37197;&#32622;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#39044;&#23450;&#20041;&#25163;&#24037;&#20248;&#21270;&#22120;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{Symbol}&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#26469;&#20419;&#36827;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#33258;&#21160;&#21457;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21495;&#26041;&#31243;&#29983;&#25104;&#22120;(SEG)&#65292;&#20801;&#35768;&#20026;&#29305;&#23450;&#20219;&#21153;&#21644;&#20248;&#21270;&#27493;&#39588;&#21160;&#24577;&#29983;&#25104;&#38381;&#24335;&#20248;&#21270;&#35268;&#21017;&#12290;&#22312;\textsc{Symbol}&#20869;&#37096;&#65292;&#25105;&#20204;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#20803;&#23398;&#20064;SEG&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;\textsc{Symbol}&#29983;&#25104;&#30340;&#20248;&#21270;&#22120;&#19981;&#20165;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;BBO&#21644;MetaBBO&#22522;&#20934;&#32447;&#65292;&#32780;&#19988;&#22312;&#23436;&#20840;&#19981;&#21516;&#38382;&#39064;&#30340;&#20840;&#26032;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present \textsc{Symbol}, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning. Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps. Within \textsc{Symbol}, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently. Extensive experiments reveal that the optimizers generated by \textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#36741;&#21161;&#27169;&#22411;&#25429;&#25417;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39069;&#22806;&#30340;&#20449;&#24687;&#21015;&#26469;&#28508;&#22312;&#22320;&#22686;&#24378;&#30446;&#26631;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.02354</link><description>&lt;p&gt;
&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#28508;&#22312;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#30340;&#33539;&#24335;&#65306;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Paradigm for Potential Model Performance Improvement in Classification and Regression Problems. A Proof of Concept
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#36741;&#21161;&#27169;&#22411;&#25429;&#25417;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39069;&#22806;&#30340;&#20449;&#24687;&#21015;&#26469;&#28508;&#22312;&#22320;&#22686;&#24378;&#30446;&#26631;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#36741;&#21161;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#20449;&#24687;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#39069;&#22806;&#30340;&#20449;&#24687;&#21015;&#65292;&#20174;&#32780;&#28508;&#22312;&#22320;&#22686;&#24378;&#30446;&#26631;&#39044;&#27979;&#12290;&#25991;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#26696;&#20363;&#21644;&#30456;&#20851;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
A methodology that seeks to enhance model prediction performance is presented. The method involves generating multiple auxiliary models that capture relationships between attributes as a function of each other. Such information serves to generate additional informative columns in the dataset that can potentially enhance target prediction. A proof of case and related code is provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#19979;&#34892;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#32593;&#32476;&#30340;&#22810;&#22336;&#25509;&#20837;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#20013;&#21387;&#32553;&#20102;&#26032;&#22411;&#38543;&#26426;&#25509;&#20837;&#20449;&#20196;&#65288;Ce2RACH&#65289;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30340;&#39069;&#22806;&#20449;&#20196;&#28040;&#24687;&#20132;&#25442;&#26469;&#20943;&#36731;&#21355;&#26143;&#38388;&#24178;&#25200;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36798;36.65&#65285;&#30340;&#32593;&#32476;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.02350</link><description>&lt;p&gt;
&#24863;&#30693;&#24178;&#25200;&#30340;&#19979;&#34892;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#26032;&#22411;&#38543;&#26426;&#25509;&#20837;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Interference-Aware Emergent Random Access Protocol for Downlink LEO Satellite Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#19979;&#34892;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#32593;&#32476;&#30340;&#22810;&#22336;&#25509;&#20837;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#20013;&#21387;&#32553;&#20102;&#26032;&#22411;&#38543;&#26426;&#25509;&#20837;&#20449;&#20196;&#65288;Ce2RACH&#65289;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30340;&#39069;&#22806;&#20449;&#20196;&#28040;&#24687;&#20132;&#25442;&#26469;&#20943;&#36731;&#21355;&#26143;&#38388;&#24178;&#25200;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36798;36.65&#65285;&#30340;&#32593;&#32476;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#19979;&#34892;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#32593;&#32476;&#30340;&#22810;&#22336;&#25509;&#20837;&#21327;&#35758;&#12290;&#36890;&#36807;&#25913;&#36827;&#29616;&#26377;&#30340;&#23398;&#20064;&#21327;&#35758;&#65292;&#21363;&#26032;&#22411;&#38543;&#26426;&#25509;&#20837;&#20449;&#36947;&#65288;eRACH&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#34987;&#21629;&#21517;&#20026;&#38598;&#20013;&#21387;&#32553;&#30340;&#26032;&#22411;&#38543;&#26426;&#25509;&#20837;&#20449;&#20196;&#65288;Ce2RACH&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;MADRL&#35757;&#32451;&#36807;&#31243;&#20013;&#32852;&#21512;&#23398;&#20064;&#30340;&#39069;&#22806;&#20449;&#20196;&#28040;&#24687;&#20132;&#25442;&#26469;&#20943;&#36731;&#21355;&#26143;&#38388;&#24178;&#25200;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;eRACH&#30456;&#27604;&#65292;Ce2RACH&#30340;&#32593;&#32476;&#21534;&#21520;&#37327;&#39640;&#36798;36.65&#65285;&#65292;&#32780;&#20449;&#20196;&#28040;&#24687;&#30340;&#25104;&#26412;&#19982;&#29992;&#25143;&#25968;&#37327;&#32447;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a multi-agent deep reinforcement learning (MADRL) framework to train a multiple access protocol for downlink low earth orbit (LEO) satellite networks. By improving the existing learned protocol, emergent random access channel (eRACH), our proposed method, coined centralized and compressed emergent signaling for eRACH (Ce2RACH), can mitigate inter-satellite interference by exchanging additional signaling messages jointly learned through the MADRL training process. Simulations demonstrate that Ce2RACH achieves up to 36.65% higher network throughput compared to eRACH, while the cost of signaling messages increase linearly with the number of users.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#65292;&#22686;&#24378;&#20102;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#24494;&#35843;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.02347</link><description>&lt;p&gt;
Riemannian Preconditioned LoRA&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#65292;&#22686;&#24378;&#20102;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#24494;&#35843;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#26469;&#25552;&#21319;&#20854;&#20248;&#21270;&#27493;&#39588;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;$r\times r$&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;&#20854;&#20013;$r$&#26159;LoRA&#30340;&#31209;&#12290;&#36825;&#20010;&#39044;&#26465;&#20214;&#22120;&#23545;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20195;&#30721;&#21482;&#38656;&#35201;&#20570;&#20986;&#24456;&#23567;&#30340;&#25913;&#21464;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#23384;&#20648;&#21644;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#36807;&#31243;&#23545;&#20110;&#23398;&#20064;&#29575;&#31561;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#39044;&#26465;&#20214;&#22120;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#24494;&#35843;&#20004;&#23618;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38381;&#29615;&#26080;&#30417;&#30563;&#34920;&#31034;&#35299;&#32544;&#26041;&#27861;CL-Dis&#65292;&#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;Diff-AE&#65289;&#21644;&#946;-VAE&#20849;&#21516;&#25552;&#21462;&#35821;&#20041;&#35299;&#32544;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#34920;&#31034;&#35299;&#32544;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02346</link><description>&lt;p&gt;
&#38381;&#29615;&#26080;&#30417;&#30563;&#34920;&#31034;&#35299;&#32544;&#30340;&#946;-VAE&#33976;&#39311;&#19982;&#25193;&#25955;&#27010;&#29575;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Closed-Loop Unsupervised Representation Disentanglement with $\beta$-VAE Distillation and Diffusion Probabilistic Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38381;&#29615;&#26080;&#30417;&#30563;&#34920;&#31034;&#35299;&#32544;&#26041;&#27861;CL-Dis&#65292;&#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;Diff-AE&#65289;&#21644;&#946;-VAE&#20849;&#21516;&#25552;&#21462;&#35821;&#20041;&#35299;&#32544;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#34920;&#31034;&#35299;&#32544;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#35299;&#32544;&#21487;&#33021;&#26377;&#21161;&#20110;AI&#26681;&#26412;&#19978;&#29702;&#35299;&#29616;&#23454;&#19990;&#30028;&#65292;&#20174;&#32780;&#20351;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#21463;&#30410;&#12290;&#30446;&#21069;&#33267;&#23569;&#26377;&#19977;&#20010;&#26410;&#35299;&#20915;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#65288;i&#65289;&#36807;&#20110;&#20381;&#36182;&#26631;&#31614;&#27880;&#37322;&#21644;&#21512;&#25104;&#25968;&#25454;-&#23548;&#33268;&#22312;&#33258;&#28982;&#24773;&#26223;&#19979;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65307;&#65288;ii&#65289;&#21551;&#21457;&#24335;/&#25163;&#24037;&#21046;&#20316;&#30340;&#35299;&#32544;&#32422;&#26463;&#20351;&#24471;&#38590;&#20197;&#33258;&#36866;&#24212;&#22320;&#23454;&#29616;&#26368;&#20339;&#35757;&#32451;&#26435;&#34913;&#65307;&#65288;iii&#65289;&#32570;&#20047;&#21512;&#29702;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30495;&#23454;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;CL-Dis&#30340;&#38381;&#29615;&#26080;&#30417;&#30563;&#34920;&#31034;&#35299;&#32544;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;Diff-AE&#65289;&#20316;&#20026;&#39592;&#24178;&#65292;&#24182;&#20351;&#29992;&#946;-VAE&#20316;&#20026;&#21103;&#39550;&#39542;&#21592;&#26469;&#25552;&#21462;&#35821;&#20041;&#35299;&#32544;&#30340;&#34920;&#31034;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#21644;VAE&#27169;&#22411;&#30340;&#33391;&#22909;&#35299;&#32544;&#33021;&#21147;&#26159;&#20114;&#34917;&#30340;&#12290;&#20026;&#20102;&#21152;&#24378;&#35299;&#32544;&#65292;&#20351;&#29992;VAE&#28508;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02345</link><description>&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances - &#24212;&#29992;&#20110;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#27604;&#36739;&#30340;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#23398;&#12289;&#21307;&#23398;&#39046;&#22495;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#31561;&#21508;&#20010;&#39046;&#22495;&#65292;&#27604;&#36739;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36317;&#31163;&#65292;&#27604;&#22914;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#23545;&#20110;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#24050;&#32463;&#24341;&#21457;&#20102;&#27963;&#36291;&#30340;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29699;&#24418;&#27010;&#29575;&#27979;&#24230;&#30340;&#21464;&#20307;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#25105;&#20204;&#20180;&#32454;&#22788;&#29702;&#20102;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#21450;&#20854;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20174;&#36965;&#24863;&#21644;&#22788;&#29702;&#25928;&#29575;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both spe
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#35270;&#35273;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#22495;&#24182;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.02340</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#25552;&#31034;&#20013;&#23398;&#20064;&#35821;&#20041;&#20195;&#29702;&#65292;&#20026;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#35270;&#35273;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#22495;&#24182;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;(DML)&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20851;&#27880;&#30340;&#37325;&#28857;&#30446;&#26631;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#38598;&#20013;&#20110;&#23545;&#20256;&#32479;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#30001;&#20110;&#26368;&#36817;&#20174;&#26356;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#65292;&#23558;&#35813;&#27169;&#22411;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#22495;&#30340;DML&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;DML&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;(ViT)&#20013;&#30340;&#35270;&#35273;&#25552;&#31034;(VPT)&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;DML&#33539;&#20363;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#21644;ViT&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#20195;&#29702;&#20013;&#26469;&#20248;&#21270;&#27599;&#20010;&#31867;&#21035;&#30340;&#35270;&#35273;&#25552;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#36924;&#36817;&#26041;&#27861;&#22312;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#20248;&#20110;&#20195;&#34920;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models trained from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02339</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#21463;&#21040;&#22495;&#38388;&#24046;&#24322;&#30340;&#38480;&#21046;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#25972;&#20307;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25237;&#24433;&#32422;&#26463;&#65292;&#36825;&#20165;&#20165;&#30830;&#20445;&#20102;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#23545;&#40784;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270; (UAO) &#26694;&#26550;&#65292;&#23427;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;2D&#21040;3D&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#30456;&#24212;&#30340;3D&#23039;&#21183;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;3D&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#27979;&#35797;&#26102;&#30340;&#20248;&#21270;&#65292;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20165;&#20248;&#21270;&#23569;&#37327;&#20851;&#38190;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36866;&#24212;&#32593;&#32476;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#30340;&#30446;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02338</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Adaptation for Networking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36866;&#24212;&#32593;&#32476;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#30340;&#30446;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#35768;&#22810;&#32593;&#32476;&#20219;&#21153;&#37117;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#39044;&#27979;&#21644;&#31995;&#32479;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;DL&#30340;&#31639;&#27861;&#30340;&#35774;&#35745;&#21746;&#23398;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#24037;&#31243;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#32593;&#32476;&#20219;&#21153;&#25163;&#21160;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#27492;&#22806;&#65292;DNN&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#20998;&#24067;/&#29615;&#22659;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#30340;&#25512;&#21160;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;LLM&#29992;&#20110;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#25506;&#32034;&#26356;&#21487;&#25345;&#32493;&#30340;&#35774;&#35745;&#21746;&#23398;&#12290;&#20973;&#20511;&#28023;&#37327;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;LLM&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19988;&#26377;&#26395;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NetLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23558;LLM&#24212;&#29992;&#20110;&#35299;&#20915;&#32593;&#32476;&#38382;&#39064;&#30340;&#36866;&#24212;&#24615;&#26694;&#26550;&#12290;NetLLM&#35299;&#20915;&#20102;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.   Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve "one model for all" with even better performance and stronger generalization for various tasks. In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems. NetLLM addresses many practical challenges in L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02334</link><description>&lt;p&gt;
&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#38656;&#35201;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#21040;&#26368;&#36817;&#65292;&#20851;&#20110;&#28145;&#24230;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24402;&#32435;&#20559;&#35265;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#23545;&#20110;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#36731;&#24494;&#29305;&#24449;&#20132;&#20114;&#20551;&#35774;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#65292;&#31216;&#20026;AMFormer&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AMFormer&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#20248;&#20110;&#24378;&#23545;&#25163;&#12290;&#36825;&#24402;&#22240;&#20110;&#20854;&#24182;&#34892;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#20248;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#20855;&#26377;&#31639;&#26415;&#24037;&#31243;&#29305;&#24449;&#30340;&#25193;&#23637;&#31354;&#38388;&#20013;&#20998;&#31163;&#34920;&#26684;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20063;&#39564;&#35777;&#20102;AMFormer&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#21512;&#29702;&#24615;&#65292;&#34920;&#26126;&#23427;&#24050;&#32463;&#24314;&#31435;&#20102;&#24378;&#26377;&#21147;&#30340;&#24402;&#32435;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.02333</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Copyright Protection in Generative AI: A Technical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#25193;&#23637;&#20102;&#20854;&#21019;&#24314;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#20195;&#30721;&#31561;&#21512;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;Deep Generative Models&#65292;DGMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#39640;&#20445;&#30495;&#24230;&#21644;&#30495;&#23454;&#24615;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#20851;&#20110;&#22914;&#20309;&#26377;&#25928;&#20445;&#25252;DGMs&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#27861;&#24459;&#36777;&#35770;&#12290;&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#25552;&#20379;&#20102;&#29256;&#26435;&#20445;&#25252;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#26469;&#36827;&#34892;&#30740;&#31350;&#65306;&#19968;&#26159;&#19982;&#25968;&#25454;&#25152;&#26377;&#32773;&#25152;&#25345;&#26377;&#30340;&#28304;&#25968;&#25454;&#30456;&#20851;&#30340;&#29256;&#26435;&#65292;&#20108;&#26159;&#19982;&#27169;&#22411;&#26500;&#24314;&#32773;&#25152;&#32500;&#25252;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#20851;&#30340;&#29256;&#26435;&#12290;&#23545;&#20110;&#25968;&#25454;&#29256;&#26435;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#25152;&#26377;&#32773;&#22914;&#20309;&#20445;&#25252;&#20854;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#20405;&#29359;&#36825;&#20123;&#26435;&#21033;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;DGMs&#12290;&#23545;&#20110;&#27169;&#22411;&#29256;&#26435;&#65292;&#25105;&#20204;&#30340;&#35752;&#35770;&#24310;&#20280;&#21040;&#38450;&#27490;&#27169;&#22411;&#30423;&#31363;&#21644;&#35782;&#21035;&#29305;&#23450;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#22788;&#29702;&#36825;&#20123;&#29256;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight 
&lt;/p&gt;</description></item><item><title>Minusformer&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#23558;Transformer&#27169;&#22411;&#20013;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#25913;&#20026;&#20943;&#27861;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02332</link><description>&lt;p&gt;
Minusformer: &#36890;&#36807;&#28176;&#36827;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02332
&lt;/p&gt;
&lt;p&gt;
Minusformer&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#23558;Transformer&#27169;&#22411;&#20013;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#25913;&#20026;&#20943;&#27861;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26222;&#36941;&#23384;&#22312;&#30340;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#39044;&#27979;&#27169;&#22411;&#23481;&#26131;&#20005;&#37325;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#21435;&#20887;&#20313;&#30340;&#26041;&#27861;&#36880;&#27493;&#24674;&#22797;TS&#30340;&#20869;&#22312;&#20215;&#20540;&#20197;&#29992;&#20110;&#26410;&#26469;&#30340;&#26102;&#38388;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#36716;&#21464;&#20026;&#20943;&#27861;&#26469;&#25913;&#36827;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21407;&#27169;&#22411;&#30340;&#27599;&#20010;&#27169;&#22359;&#20013;&#21152;&#20837;&#19968;&#20010;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#65292;&#26500;&#24314;&#19968;&#26465;&#36890;&#24448;&#26368;&#32456;&#39044;&#27979;&#30340;&#39640;&#36895;&#20844;&#36335;&#12290;&#35813;&#20998;&#25903;&#20013;&#21518;&#32493;&#27169;&#22359;&#30340;&#36755;&#20986;&#23558;&#20943;&#21435;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#32467;&#26524;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#12290;&#36825;&#31181;&#35774;&#35745;&#20419;&#36827;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#30340;&#36880;&#27493;&#23398;&#20064;&#39537;&#21160;&#38544;&#24335;&#20998;&#35299;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#38887;&#24615;&#12290;&#30001;&#20110;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#32858;&#21512;&#37117;&#26159;&#20943;&#21495;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;Minusformer&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02328</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#21450;&#20854;&#22312;&#20998;&#25903;&#23450;&#30028;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design using neural networks with applications to branch-and-cut
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26159;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#19968;&#31867;&#31639;&#27861;&#20013;&#36873;&#25321;&#22312;&#26576;&#20010;&#65288;&#26410;&#30693;&#65289;&#38382;&#39064;&#23454;&#20363;&#20998;&#24067;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#30340;&#33539;&#20363;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24605;&#36335;&#65292;&#21363;&#19981;&#20165;&#20165;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#30340;&#21333;&#19968;&#31639;&#27861;&#65292;&#32780;&#26159;&#20801;&#35768;&#26681;&#25454;&#38382;&#39064;&#23454;&#20363;&#36873;&#25321;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38382;&#39064;&#23454;&#20363;&#26679;&#26412;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#38382;&#39064;&#23454;&#20363;&#26144;&#23556;&#21040;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#24605;&#36335;&#24418;&#24335;&#21270;&#65292;&#24182;&#26681;&#25454;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#24037;&#20316;&#65292;&#25512;&#23548;&#20986;&#20102;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#20005;&#26684;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#21040;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#30340;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#20197;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by introducing the idea where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm {\em for that instance}. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02325</link><description>&lt;p&gt;
&#21160;&#37327;&#22312;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#20013;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#20316;&#29992;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23545;&#27492;&#32570;&#20047;&#29702;&#35770;&#35299;&#37322;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;SGD&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#31243;&#24230;&#30001;&#23398;&#20064;&#29575;&#12289;&#25209;&#22823;&#23567;&#12289;&#21160;&#37327;&#22240;&#23376;&#12289;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#21450;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#30830;&#23450;&#12290;&#36825;&#19968;&#29702;&#35770;&#21457;&#29616;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#21160;&#37327;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21160;&#37327;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#20316;&#29992;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SGD&#21160;&#37327;&#24179;&#28369;&#29305;&#24615;&#30340;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;SGD&#21160;&#37327;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.02322</link><description>&lt;p&gt;
&#21160;&#24577;&#22686;&#37327;&#20248;&#21270;&#29992;&#20110;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dynamic Incremental Optimization for Best Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#25915;&#20987;&#36825;&#20010;&#38750;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#22522;&#20110;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#28508;&#22312;&#22320;&#20943;&#23569;&#20102;&#20887;&#20313;&#35745;&#31639;&#24182;&#25913;&#36827;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26377;&#22122;&#22768;&#22270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;GALClean&#65292;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#23454;&#29616;&#25968;&#25454;&#36873;&#25321;&#21644;&#22270;&#30340;&#28165;&#29702;&#65292;&#35299;&#20915;&#20102;&#36873;&#26631;&#35760;&#25968;&#25454;&#21644;&#33719;&#21462;&#24178;&#20928;&#22270;&#32467;&#26500;&#36825;&#20004;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.02321</link><description>&lt;p&gt;
&#26080;&#22122;&#22768;&#32467;&#26500;&#22270;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Graphs with Noisy Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26377;&#22122;&#22768;&#22270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;GALClean&#65292;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#23454;&#29616;&#25968;&#25454;&#36873;&#25321;&#21644;&#22270;&#30340;&#28165;&#29702;&#65292;&#35299;&#20915;&#20102;&#36873;&#26631;&#35760;&#25968;&#25454;&#21644;&#33719;&#21462;&#24178;&#20928;&#22270;&#32467;&#26500;&#36825;&#20004;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36275;&#22815;&#26631;&#35760;&#30340;&#33410;&#28857;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#22823;&#35268;&#27169;&#22270;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#20154;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#22270;&#19978;&#30340;&#20027;&#21160;&#23398;&#20064;&#19978;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#26368;&#22823;&#21270;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#20102;&#21487;&#38752;&#30340;&#22270;&#25299;&#25169;&#65292;&#32780;&#23454;&#38469;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#20986;&#29616;&#26377;&#22122;&#22768;&#30340;&#22270;&#12290;&#37492;&#20110;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#25104;&#21151;&#30340;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#22270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26159;&#38750;&#24120;&#24517;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36873;&#25321;&#26631;&#35760;&#25968;&#25454;&#21644;&#33719;&#21462;&#24178;&#20928;&#30340;&#22270;&#26159;&#20004;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65306;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38656;&#35201;&#24178;&#20928;&#30340;&#22270;&#32467;&#26500;&#65292;&#32780;&#28165;&#29702;&#22122;&#22768;&#22270;&#32467;&#26500;&#38656;&#35201;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;GALClean&#65292;&#35813;&#26694;&#26550;&#19987;&#38376;&#35774;&#35745;&#25104;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#22270;&#30340;&#28165;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have seen significant success in tasks such as node classification, largely contingent upon the availability of sufficient labeled nodes. Yet, the excessive cost of labeling large-scale graphs led to a focus on active learning on graphs, which aims for effective data selection to maximize downstream model performance. Notably, most existing methods assume reliable graph topology, while real-world scenarios often present noisy graphs. Given this, designing a successful active learning framework for noisy graphs is highly needed but challenging, as selecting data for labeling and obtaining a clean graph are two tasks naturally interdependent: selecting high-quality data requires clean graph structure while cleaning noisy graph structure requires sufficient labeled data. Considering the complexity mentioned above, we propose an active learning framework, GALClean, which has been specifically designed to adopt an iterative approach for conducting both data sele
&lt;/p&gt;</description></item><item><title>Spin&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22810;&#26041;&#35745;&#31639;(MPC)&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#20010;&#35745;&#31639;&#26041;&#21644;&#19981;&#35802;&#23454;&#22810;&#25968;&#23545;&#25239;&#35774;&#32622;&#12290;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20851;&#38190;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20248;&#21270;&#21327;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#38024;&#23545;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#30340;&#26032;&#39062;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.02320</link><description>&lt;p&gt;
Spin: &#19968;&#31181;&#20855;&#22791;GPU&#21152;&#36895;&#30340;&#39640;&#25928;&#23433;&#20840;&#35745;&#31639;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Spin: An Efficient Secure Computation Framework with GPU Acceleration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02320
&lt;/p&gt;
&lt;p&gt;
Spin&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22810;&#26041;&#35745;&#31639;(MPC)&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#20010;&#35745;&#31639;&#26041;&#21644;&#19981;&#35802;&#23454;&#22810;&#25968;&#23545;&#25239;&#35774;&#32622;&#12290;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20851;&#38190;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20248;&#21270;&#21327;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#38024;&#23545;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#30340;&#26032;&#39062;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#23545;&#20110;&#22810;&#26041;&#35745;&#31639;&#65288;MPC&#65289;&#26694;&#26550;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;Spin&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#20010;&#35745;&#31639;&#26041;&#21644;&#19981;&#35802;&#23454;&#22810;&#25968;&#23545;&#25239;&#35774;&#32622;&#30340;GPU&#21152;&#36895;&#30340;MPC&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20851;&#38190;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20248;&#21270;&#21327;&#35758;&#65292;&#20197;&#21450;&#38024;&#23545;Transformer&#27169;&#22411;&#30340;&#22522;&#26412;&#21333;&#20803;&#27880;&#24847;&#21147;&#30340;&#20960;&#31181;&#26032;&#39062;&#20248;&#21270;&#65292;&#20351;Spin&#33021;&#22815;&#22312;&#19981;&#29306;&#29298;&#23433;&#20840;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38750;&#24120;&#35268;CNN&#35757;&#32451;&#21644;Transformer&#25512;&#26029;&#12290;&#22312;&#21518;&#31471;&#23618;&#38754;&#65292;Spin&#21033;&#29992;GPU&#12289;CPU&#21644;RDMA&#21551;&#29992;&#30340;&#26234;&#33021;&#32593;&#32476;&#21345;&#36827;&#34892;&#21152;&#36895;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Spin&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#38754;&#27604;&#26368;&#20808;&#36827;&#25216;&#26415;&#24555;&#20004;&#20493;&#12290;&#23545;&#20110;&#20855;&#26377;1890&#19975;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#29305;&#23450;&#20248;&#21270;&#20351;Spin&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#29575;&#12289;&#26356;&#23569;&#30340;&#36890;&#20449;&#21644;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks. Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup. We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration. Comprehensive evaluations demonstrate that Spin can be up to $2\times$ faster than the state-of-the-art for deep neural network training. For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#21644;&#23376;&#38598;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#26469;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#23545;&#25968;&#34892;&#21015;&#24335;&#36317;&#31163;&#26469;&#34913;&#37327;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24402;&#19968;&#21270;&#30340;&#26435;&#37325;&#26799;&#24230;&#31354;&#38388;&#20013;&#65292;&#25552;&#20986;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#19982;&#25351;&#20196;&#36981;&#24490;&#24615;&#33021;&#30456;&#20851;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25351;&#23548;&#25968;&#25454;&#36873;&#25321;&#21644;&#20998;&#26512;&#25968;&#25454;&#38598;&#26500;&#24314;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.02318</link><description>&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#21644;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Diversity Measurement and Subset Selection for Instruction Tuning Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#21644;&#23376;&#38598;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#26469;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#23545;&#25968;&#34892;&#21015;&#24335;&#36317;&#31163;&#26469;&#34913;&#37327;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24402;&#19968;&#21270;&#30340;&#26435;&#37325;&#26799;&#24230;&#31354;&#38388;&#20013;&#65292;&#25552;&#20986;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#19982;&#25351;&#20196;&#36981;&#24490;&#24615;&#33021;&#30456;&#20851;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25351;&#23548;&#25968;&#25454;&#36873;&#25321;&#21644;&#20998;&#26512;&#25968;&#25454;&#38598;&#26500;&#24314;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#25968;&#25454;&#23376;&#38598;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#36981;&#24490;&#25351;&#20196;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#25968;&#25454;&#38598;&#26500;&#24314;&#20013;&#22810;&#26679;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22914;&#20219;&#21153;&#25968;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#26469;&#25429;&#25417;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#20197;&#36827;&#34892;&#23376;&#38598;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#23545;&#25968;&#34892;&#21015;&#24335;&#36317;&#31163;&#26469;&#34913;&#37327;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#21363;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#38598;&#19982;&#26368;&#22823;&#22810;&#26679;&#24615;&#21442;&#32771;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#30340;&#26435;&#37325;&#26799;&#24230;&#31354;&#38388;&#20013;&#25152;&#25552;&#20986;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#19982;&#19979;&#28216;&#25351;&#20196;&#36981;&#24490;&#24615;&#33021;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#25351;&#23548;&#20309;&#26102;&#26368;&#26377;&#24110;&#21161;&#22320;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#65292;&#24182;&#20998;&#26512;&#25968;&#25454;&#38598;&#26500;&#24314;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to select data subsets for the fine-tuning of large language models to more effectively follow instructions. Prior work has emphasized the importance of diversity in dataset curation but relied on heuristics such as the number of tasks. In this paper, we use determinantal point processes to capture the diversity and quality of instruction tuning datasets for subset selection. We propose to measure dataset diversity with log determinant distance that is the distance between the dataset of interest and a maximally diverse reference dataset. Our experiments demonstrate that the proposed diversity measure in the normalized weight gradient space is correlated with downstream instruction-following performance. Consequently, it can be used to inform when data selection is the most helpful and to analyze dataset curation strategies. We demonstrate the utility of our approach on various instruction tuning datasets.
&lt;/p&gt;</description></item><item><title>INViT&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#23884;&#22871;&#35774;&#35745;&#21644;&#19981;&#21464;&#30340;&#35270;&#22270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#25552;&#39640;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02317</link><description>&lt;p&gt;
INViT:&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#21487;&#27867;&#21270;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02317
&lt;/p&gt;
&lt;p&gt;
INViT&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#23884;&#22871;&#35774;&#35745;&#21644;&#19981;&#21464;&#30340;&#35270;&#22270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#25552;&#39640;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23398;&#20064;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#24555;&#36895;&#21551;&#21457;&#24335;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22810;&#25968;&#27714;&#35299;&#22120;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#25110;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#30340;&#20998;&#24067;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#65288;INViT&#65289;&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#24378;&#21046;&#19968;&#20010;&#23884;&#22871;&#35774;&#35745;&#20197;&#21450;&#19981;&#21464;&#30340;&#35270;&#22270;&#26469;&#20419;&#36827;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#24212;&#29992;&#20102;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;INViT&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02316</link><description>&lt;p&gt;
&#20320;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Certifiably Robust Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#20316;&#20026;&#40065;&#26834;&#20998;&#31867;&#30340;&#29983;&#25104;&#22120;&#20998;&#31867;&#22120;&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25193;&#25955;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#65292;&#36825;&#35753;&#25105;&#20204;&#24576;&#30097;&#23427;&#20204;&#26159;&#21542;&#20250;&#23481;&#26131;&#21463;&#21040;&#26410;&#26469;&#26356;&#24378;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#21629;&#21517;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#36825;&#20123;&#20998;&#24067;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBOs&#65289;&#65292;&#21033;&#29992;ELBO&#36817;&#20284;&#20284;&#28982;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#23450;&#29702;&#35745;&#31639;&#20998;&#31867;&#27010;&#29575;&#65292;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25512;&#24191;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;NDCs&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#35748;&#35777;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36798;&#21040;80%&#30340;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#8220;&#22270;&#20687;&#36234;&#29425;&#25552;&#31034;&#8221;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#26410;&#30693;&#25552;&#31034;&#21644;&#22270;&#20687;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#21333;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#30340;&#26041;&#27861;&#65292;&#23558;&#35813;&#36234;&#29425;&#26041;&#27861;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36739;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.02309</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Attack against Multimodal Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#8220;&#22270;&#20687;&#36234;&#29425;&#25552;&#31034;&#8221;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#26410;&#30693;&#25552;&#31034;&#21644;&#22270;&#20687;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#21333;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#30340;&#26041;&#27861;&#65292;&#23558;&#35813;&#36234;&#29425;&#26041;&#27861;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36739;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#26088;&#22312;&#24341;&#23548;MLLM&#29983;&#25104;&#23545;&#26377;&#23475;&#29992;&#25143;&#26597;&#35810;&#19981;&#24403;&#22238;&#24212;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#31639;&#27861;&#26469;&#25214;&#21040;&#8220;&#22270;&#20687;&#36234;&#29425;&#25552;&#31034;&#8221;&#65288;imgJP&#65289;&#65292;&#23454;&#29616;&#23545;MLLM&#22312;&#22810;&#20010;&#26410;&#30693;&#25552;&#31034;&#21644;&#22270;&#20687;&#19978;&#30340;&#36234;&#29425;&#65288;&#21363;&#25968;&#25454;&#36890;&#29992;&#23646;&#24615;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#65292;&#29983;&#25104;&#30340;imgJP&#21487;&#20197;&#20197;&#40657;&#30418;&#26041;&#24335;&#36716;&#31227;&#21040;&#21508;&#31181;&#27169;&#22411;&#19978;&#36827;&#34892;&#36234;&#29425;&#65292;&#21253;&#25324;MiniGPT-v2&#12289;LLaVA&#12289;InstructBLIP&#21644;mPLUG-Owl2&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;MLLM&#36234;&#29425;&#21644;LLM&#36234;&#29425;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;LLM&#36234;&#29425;&#65292;&#23637;&#31034;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;&#20195;&#30721;&#38142;&#25509;&#22312;&#36825;&#37324;&#12290;&#27880;&#24847;&#65306;&#19968;&#20123;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#21487;&#33021;&#23545;&#26576;&#20123;&#35835;&#32773;&#20855;&#26377;&#20882;&#29359;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \textbf{Warning: some content generated by language models may be offensive to some readers.}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02304</link><description>&lt;p&gt;
&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20174;&#22320;&#38663;&#24314;&#27169;&#21040;&#21307;&#23398;&#25104;&#20687;&#65292;&#23545;&#20110;&#39640;&#39057;&#27874;&#20256;&#25773;&#30340;&#39640;&#20445;&#30495;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#27874;&#20256;&#25773;&#27169;&#22411;&#20013;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#36275;&#22815;&#20934;&#30830;&#30340;&#32454;&#27714;&#35299;&#22120;&#36755;&#20986;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#31895;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#31283;&#23450;&#19988;&#24555;&#36895;&#30340;&#27714;&#35299;&#22120;&#36824;&#20801;&#35768;&#20351;&#29992;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;Parareal&#26469;&#25552;&#21462;&#21644;&#32416;&#27491;&#39640;&#39057;&#27874;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;Nguyen&#21644;Tsai&#65288;2023&#65289;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#31995;&#32479;&#65292;&#23558;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#26694;&#26550;&#20013;&#12290;&#22312;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#21644;Parareal&#26041;&#26696;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#21327;&#35843;&#30340;&#32467;&#26500;&#22312;&#19981;&#29306;&#29298;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#21644;&#27604;&#36739;&#20102;AI&#22686;&#24378;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#25216;&#26415;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20123;&#26032;&#30340;&#30001;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02299</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#30340;&#22238;&#39038;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Review and Comparison of AI Enhanced Side Channel Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#21644;&#27604;&#36739;&#20102;AI&#22686;&#24378;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#25216;&#26415;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20123;&#26032;&#30340;&#30001;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20391;&#20449;&#36947;&#20998;&#26512;&#65288;SCA&#65289;&#23545;&#29616;&#20195;&#35745;&#31639;&#31995;&#32479;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#26500;&#25104;&#20102;&#26126;&#30830;&#30340;&#23041;&#32961;&#12290;&#32477;&#22823;&#22810;&#25968;&#36890;&#20449;&#37117;&#26159;&#36890;&#36807;&#23494;&#30721;&#31639;&#27861;&#36827;&#34892;&#20445;&#25252;&#30340;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#23494;&#30721;&#23398;&#35282;&#24230;&#19978;&#24448;&#24448;&#26159;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#30340;&#23454;&#29616;&#20250;&#24341;&#20837;&#28431;&#27934;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#28431;&#27934;&#26469;&#36827;&#34892;SCA&#65292;&#24182;&#24674;&#22797;&#26426;&#23494;&#20449;&#24687;&#65292;&#22914;&#23494;&#38053;&#25110;&#20869;&#37096;&#29366;&#24577;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#25915;&#20987;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;SCA&#30340;&#23041;&#32961;&#20063;&#22823;&#22823;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20391;&#20449;&#36947;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#20854;&#32972;&#21518;&#30340;&#29702;&#35770;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#23558;&#25918;&#22312;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#30340;&#27169;&#22411;&#25915;&#20987;&#19978;&#65292;&#21516;&#26102;&#36824;&#23558;&#30740;&#31350;&#19968;&#20123;&#26032;&#20852;&#30340;&#30001;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#22914;&#38750;&#27169;&#22411;&#25915;&#20987;&#12289;&#20154;&#24037;&#36857;&#32447;&#29983;&#25104;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Side Channel Analysis (SCA) presents a clear threat to privacy and security in modern computing systems. The vast majority of communications are secured through cryptographic algorithms. These algorithms are often provably-secure from a cryptographical perspective, but their implementation on real hardware introduces vulnerabilities. Adversaries can exploit these vulnerabilities to conduct SCA and recover confidential information, such as secret keys or internal states. The threat of SCA has greatly increased as machine learning, and in particular deep learning, enhanced attacks become more common. In this work, we will examine the latest state-of-the-art deep learning techniques for side channel analysis, the theory behind them, and how they are conducted. Our focus will be on profiling attacks using deep learning techniques, but we will also examine some new and emerging methodologies enhanced by deep learning techniques, such as non-profiled attacks, artificial trace generation, and
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#21453;&#21521;&#25511;&#21046;&#31995;&#32479;&#20351;&#24471;&#32456;&#27490;&#29366;&#24577;&#23646;&#20110;&#30446;&#26631;&#38598;&#21512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25511;&#21046;&#20223;&#23556;&#31995;&#32479;&#19988;&#28385;&#36275;&#21487;&#25511;&#24615;&#26465;&#20214;&#26102;&#65292;&#25511;&#21046;&#31995;&#32479;&#33021;&#22815;&#23436;&#20840;&#36861;&#36394;&#27491;&#21521;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02297</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion-Based Control of Nonlinear Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02297
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#21453;&#21521;&#25511;&#21046;&#31995;&#32479;&#20351;&#24471;&#32456;&#27490;&#29366;&#24577;&#23646;&#20110;&#30446;&#26631;&#38598;&#21512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25511;&#21046;&#20223;&#23556;&#31995;&#32479;&#19988;&#28385;&#36275;&#21487;&#25511;&#24615;&#26465;&#20214;&#26102;&#65292;&#25511;&#21046;&#31995;&#32479;&#33021;&#22815;&#23436;&#20840;&#36861;&#36394;&#27491;&#21521;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;DDPMs&#26159;&#30446;&#21069;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#37319;&#26679;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23558;&#21453;&#39304;&#25511;&#21046;&#38382;&#39064;&#35270;&#20026;&#22312;&#25511;&#21046;&#31995;&#32479;&#32422;&#26463;&#19979;&#20174;&#30446;&#26631;&#38598;&#21512;&#20013;&#32472;&#21046;&#26679;&#26412;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;DDPMs&#30340;&#27491;&#21521;&#36807;&#31243;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#26500;&#24314;&#20174;&#30446;&#26631;&#38598;&#21512;&#36215;&#22987;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#23398;&#20064;&#22914;&#20309;&#21453;&#21521;&#25511;&#21046;&#21160;&#21147;&#31995;&#32479;&#65292;&#20351;&#24471;&#32456;&#27490;&#29366;&#24577;&#23646;&#20110;&#30446;&#26631;&#38598;&#21512;&#12290;&#23545;&#20110;&#27809;&#26377;&#28418;&#31227;&#30340;&#25511;&#21046;&#20223;&#23556;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#21487;&#25511;&#24615;&#30340;&#26446;&#25324;&#21495;&#26465;&#20214;&#25104;&#31435;&#26102;&#65292;&#25511;&#21046;&#31995;&#32479;&#33021;&#22815;&#23436;&#20840;&#36861;&#36394;&#27491;&#21521;&#36807;&#31243;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38750;&#32447;&#24615;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#29289;&#29702;&#23398;&#23454;&#39564;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#36229;&#20986;&#29702;&#35770;&#32467;&#26524;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach based on Denoising Diffusion Probabilistic Models (DDPMs) to control nonlinear dynamical systems. DDPMs are the state-of-art of generative models that have achieved success in a wide variety of sampling tasks. In our framework, we pose the feedback control problem as a generative task of drawing samples from a target set under control system constraints. The forward process of DDPMs constructs trajectories originating from a target set by adding noise. We learn to control a dynamical system in reverse such that the terminal state belongs to the target set. For control-affine systems without drift, we prove that the control system can exactly track the trajectory of the forward process in reverse, whenever the the Lie bracket based condition for controllability holds. We numerically study our approach on various nonlinear systems and verify our theoretical results. We also conduct numerical experiments for cases beyond our theoretical results on a physics-eng
&lt;/p&gt;</description></item><item><title>QuadratiK&#36719;&#20214;&#21253;&#26159;&#19968;&#20010;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25311;&#21512;&#24230;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#29699;&#24418;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02290</link><description>&lt;p&gt;
&#29699;&#24418;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#32858;&#31867;&#65306;R&#21644;Python&#20013;&#30340;QuadratiK&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package in R and Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02290
&lt;/p&gt;
&lt;p&gt;
QuadratiK&#36719;&#20214;&#21253;&#26159;&#19968;&#20010;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25311;&#21512;&#24230;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#29699;&#24418;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;QuadratiK&#36719;&#20214;&#21253;&#65292;&#35813;&#36719;&#20214;&#21253;&#21253;&#21547;&#20102;&#21019;&#26032;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#35813;&#36719;&#20214;&#21253;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#65292;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#36866;&#24212;&#24230;&#25311;&#21512;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#20108;&#27425;&#36317;&#31163;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#23454;&#29616;&#20102;&#21333;&#26679;&#26412;&#12289;&#21452;&#26679;&#26412;&#21644;k&#26679;&#26412;&#36866;&#24212;&#24230;&#25311;&#21512;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#25968;&#23398;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27010;&#29575;&#20998;&#24067;&#30340;&#25311;&#21512;&#24230;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#25193;&#23637;&#20102;&#21151;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#27850;&#26494;&#26680;&#23494;&#24230;&#30340;$d$&#32500;&#29699;&#19978;&#22343;&#21248;&#24615;&#27979;&#35797;&#65292;&#20197;&#21450;&#20174;&#27850;&#26494;&#26680;&#23494;&#24230;&#20013;&#29983;&#25104;&#38543;&#26426;&#26679;&#26412;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#36824;&#21253;&#25324;&#19968;&#31181;&#38024;&#23545;&#29699;&#24418;&#25968;&#25454;&#32780;&#29305;&#21035;&#37327;&#36523;&#23450;&#21046;&#30340;&#29420;&#29305;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#29699;&#38754;&#19978;&#22522;&#20110;&#27850;&#26494;&#26680;&#23494;&#24230;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#36824;&#21253;&#25324;&#20854;&#20182;&#22270;&#24418;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the QuadratiK package that incorporates innovative data analysis methodologies. The presented software, implemented in both R and Python, offers a comprehensive set of goodness-of-fit tests and clustering techniques using kernel-based quadratic distances, thereby bridging the gap between the statistical and machine learning literatures. Our software implements one, two and k-sample tests for goodness of fit, providing an efficient and mathematically sound way to assess the fit of probability distributions. Expanded capabilities of our software include supporting tests for uniformity on the $d$-dimensional Sphere based on Poisson kernel densities, and algorithms for generating random samples from Poisson kernel densities. Particularly noteworthy is the incorporation of a unique clustering algorithm specifically tailored for spherical data that leverages a mixture of Poisson-kernel-based densities on the sphere. Alongside this, our software includes additional graphical func
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02286</link><description>&lt;p&gt;
&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#30528;&#37325;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#21516;&#26102;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#19968;&#20123;&#22330;&#26223;&#19979;&#65292;&#22914;&#33258;&#20027;&#23548;&#33322;&#21644;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#21516;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#65288;MFARANet&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#26469;&#20445;&#35777;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#26469;&#24357;&#34917;&#27973;&#39592;&#24178;&#24341;&#36215;&#30340;&#27169;&#22411;&#23481;&#37327;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65288;MFAM&#65289;&#65292;&#23558;&#32534;&#30721;&#22120;&#20013;&#30340;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#21040;&#27599;&#20010;&#23610;&#24230;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#31354;&#38388;&#23545;&#40784;&#21644;&#22810;&#23610;&#24230;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#27969;&#30340;&#23545;&#40784;&#26469;&#24314;&#31435;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#65288;RAM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
&lt;/p&gt;</description></item><item><title>SynthDST&#26159;&#19968;&#20010;&#38024;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#35774;&#35745;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#21644;&#23545;&#35805;&#27169;&#24335;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#24182;&#20351;Join&#36830;&#36890;&#29575;&#25552;&#21319;4-5&#65285;.</title><link>https://arxiv.org/abs/2402.02285</link><description>&lt;p&gt;
SynthDST: &#23569;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#25152;&#38656;&#30340;&#20840;&#37096;&#26159;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02285
&lt;/p&gt;
&lt;p&gt;
SynthDST&#26159;&#19968;&#20010;&#38024;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#35774;&#35745;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#21644;&#23545;&#35805;&#27169;&#24335;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#24182;&#20351;Join&#36830;&#36890;&#29575;&#25552;&#21319;4-5&#65285;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#25104;&#20026;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#34920;&#29616;&#26368;&#22909;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#28041;&#21450;&#26816;&#32034;&#21644;&#28155;&#21152;&#31867;&#20284;&#30340;&#31034;&#20363;&#21040;&#25552;&#31034;&#20013;&#65292;&#38656;&#35201;&#35775;&#38382;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#33719;&#21462;&#36825;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#26377;&#26102;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#34429;&#28982;&#38646;&#26679;&#26412;&#23398;&#20064;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#26126;&#26174;&#33853;&#21518;&#12290;&#22240;&#27492;&#65292;&#8220;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20026;&#20219;&#20309;&#23545;&#35805;&#27169;&#24335;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65311;&#8221;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\method&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#19987;&#38376;&#38024;&#23545;DST&#65292;&#21033;&#29992;LLM&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#23545;&#35805;&#27169;&#24335;&#21644;&#19968;&#20123;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#65292;&#23601;&#33021;&#21512;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#12290;&#20351;&#29992;{\method}&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32467;&#26524;&#26174;&#31034;&#65292;Join&#36830;&#36890;&#29575;&#25552;&#21319;&#20102;4-5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Join
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02277</link><description>&lt;p&gt;
&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Bayesian Optimization via Exogenous Distribution Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#23558;&#30446;&#26631;&#21464;&#37327;&#26368;&#22823;&#21270;&#20316;&#20026;&#25805;&#20316;&#30446;&#26631;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#25913;&#21464;&#22240;&#26524;&#32467;&#26500;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#30828;&#24178;&#39044;&#65292;&#35201;&#20040;&#24341;&#20837;&#21160;&#20316;&#33410;&#28857;&#21040;&#20869;&#29983;&#21464;&#37327;&#20013;&#65292;&#20197;&#35843;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#25110;&#36890;&#36807;&#26399;&#26395;&#36827;&#34892;&#36793;&#32536;&#21270;&#12290;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;&#25552;&#39640;&#20102;&#36890;&#24120;&#36890;&#36807;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#22806;&#28304;&#20998;&#24067;&#23558;&#29616;&#26377;&#30340;CBO&#25193;&#23637;&#21040;&#36229;&#20986;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#19968;&#33324;&#22240;&#26524;&#26041;&#26696;&#12290;&#24674;&#22797;&#22806;&#28304;&#21464;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#22122;&#22768;&#25110;&#26410;&#35266;&#27979;&#21040;&#30340;&#38544;&#34255;&#21464;&#37327;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20808;&#39564;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CBO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
&lt;/p&gt;</description></item><item><title>SudokuSens&#26159;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;IoT&#24863;&#30693;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#27169;&#20223;&#23454;&#38469;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#23454;&#39564;&#37197;&#32622;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02275</link><description>&lt;p&gt;
SudokuSens: &#20351;&#29992;&#29983;&#25104;&#26041;&#27861;&#22686;&#24378;IoT&#24863;&#30693;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02275
&lt;/p&gt;
&lt;p&gt;
SudokuSens&#26159;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;IoT&#24863;&#30693;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#27169;&#20223;&#23454;&#38469;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#23454;&#39564;&#37197;&#32622;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SudokuSens&#65292;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#24212;&#29992;&#20013;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#27169;&#20223;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#23454;&#39564;&#37197;&#32622;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#26114;&#30340;IoT&#24212;&#29992;&#12290;&#35813;&#24037;&#20316;&#30340;&#21160;&#26426;&#26159;&#22240;&#20026;IoT&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32416;&#32544;&#20102;&#35266;&#23519;&#23545;&#35937;&#30340;&#29305;&#24449;&#20197;&#21450;&#21608;&#22260;&#29615;&#22659;&#30340;&#28151;&#26434;&#20869;&#22312;&#23646;&#24615;&#21644;&#21160;&#24577;&#29615;&#22659;&#24178;&#25200;&#12290;&#20026;&#20102;&#22312;IoT&#35757;&#32451;&#25968;&#25454;&#20013;&#21152;&#20837;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#19982;&#23545;&#35937;&#25968;&#37327;&#21644;&#21487;&#33021;&#36935;&#21040;&#30340;&#29615;&#22659;&#26465;&#20214;&#25104;&#20493;&#22686;&#21152;&#30340;&#35757;&#32451;&#29992;&#20363;&#30340;&#32452;&#21512;&#29190;&#28856;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22823;&#22823;&#20943;&#23569;&#20102;&#36825;&#20123;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these mult
&lt;/p&gt;</description></item><item><title>InceptionCapsule&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;Inception-ResNet&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#21021;&#22987;&#26435;&#37325;&#36873;&#25321;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02274</link><description>&lt;p&gt;
InceptionCapsule: &#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Inception-Resnet&#21644;CapsuleNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02274
&lt;/p&gt;
&lt;p&gt;
InceptionCapsule&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;Inception-ResNet&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#21021;&#22987;&#26435;&#37325;&#36873;&#25321;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21021;&#22987;&#26435;&#37325;&#30340;&#36873;&#25321;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#38543;&#26426;&#36873;&#25321;&#30340;&#26435;&#37325;&#21487;&#33021;&#20135;&#29983;&#19981;&#21516;&#30340;&#36755;&#20986;&#65292;&#22686;&#21152;&#36807;&#25311;&#21512;&#21644;&#27424;&#25311;&#21512;&#30340;&#27010;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#21521;&#37327;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#38656;&#35201;&#20016;&#23500;&#30340;&#21521;&#37327;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;InceptionCapsule&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;Inception-ResNet&#27169;&#22411;&#65292;&#36991;&#20813;&#38543;&#26426;&#36873;&#25321;&#26435;&#37325;&#65292;&#20174;ImageNet&#20013;&#33719;&#21462;&#21021;&#22987;&#26435;&#37325;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;Inception&#20013;&#38388;&#23618;&#30340;&#36755;&#20986;&#29983;&#25104;&#20016;&#23500;&#30340;&#21521;&#37327;&#12290;&#25552;&#21462;&#30340;&#21521;&#37327;&#34987;&#20256;&#36882;&#32473;&#35013;&#22791;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33014;&#22218;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#12290;&#20351;&#29992;Kvasir&#25968;&#25454;&#38598;&#21644;BUSI with GT&#25968;&#25454;&#38598;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#22312;5&#31867;&#20998;&#31867;&#19978;&#23454;&#29616;&#20102;97.62%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;8&#31867;&#20998;&#31867;&#19978;&#23454;&#29616;&#20102;94.30%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Initial weighting is significant in deep neural networks because the random selection of weights produces different outputs and increases the probability of overfitting and underfitting. On the other hand, vector-based approaches to extract vector features need rich vectors for more accurate classification. The InceptionCapsule approach is presented to alleviate these two problems. This approach uses transfer learning and the Inception-ResNet model to avoid random selection of weights, which takes initial weights from ImageNet. It also uses the output of Inception middle layers to generate rich vectors. Extracted vectors are given to a capsule network for learning, which is equipped with an attention technique. Kvasir data and BUSI with the GT dataset were used to evaluate this approach. This model was able to achieve 97.62 accuracies in 5-class classification and also achieved 94.30 accuracies in 8-class classification on Kvasir. In the BUSI with GT dataset, the proposed approach achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#21253;&#25324;&#22914;&#20309;&#26377;&#25928;&#22320;&#34701;&#20837;&#26032;&#29305;&#24449;&#12289;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#26032;&#30693;&#35782;&#21040;&#36798;&#30340;&#24418;&#24335;&#21644;&#26102;&#38388;&#23545;&#32435;&#20837;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26032;&#30693;&#35782;&#26102;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.02268</link><description>&lt;p&gt;
&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22522;&#30784;&#65292;&#36827;&#23637;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with New Knowledge: Fundamentals, Advances, and Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#21253;&#25324;&#22914;&#20309;&#26377;&#25928;&#22320;&#34701;&#20837;&#26032;&#29305;&#24449;&#12289;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#26032;&#30693;&#35782;&#21040;&#36798;&#30340;&#24418;&#24335;&#21644;&#26102;&#38388;&#23545;&#32435;&#20837;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26032;&#30693;&#35782;&#26102;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#26085;&#30410;&#37325;&#35270;&#30340;&#26102;&#20195;&#36805;&#36895;&#21457;&#23637;&#12290;&#27491;&#26159;&#36825;&#31181;&#24555;&#36895;&#21457;&#23637;&#36235;&#21183;&#65292;&#20197;&#21450;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#19981;&#26029;&#26032;&#38656;&#27714;&#30340;&#20986;&#29616;&#65292;&#20419;&#20351;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#65306;&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#23558;&#21508;&#31181;&#26032;&#30693;&#35782;&#26377;&#25928;&#22320;&#34701;&#20837;&#29616;&#26377;&#30340;FL&#31995;&#32479;&#20013;&#65292;&#24182;&#20351;&#36825;&#20123;&#31995;&#32479;&#38477;&#20302;&#25104;&#26412;&#65292;&#24310;&#38271;&#23551;&#21629;&#65292;&#24182;&#20419;&#36827;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23450;&#20041;&#20102;FL&#20013;&#26032;&#30693;&#35782;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#26032;&#29305;&#24449;&#12289;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;&#12290;&#23545;&#20110;&#27599;&#20010;&#26469;&#28304;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#21644;&#35752;&#35770;&#22914;&#20309;&#23558;&#26032;&#30693;&#35782;&#32435;&#20837;&#29616;&#26377;&#30340;FL&#31995;&#32479;&#65292;&#24182;&#30740;&#31350;&#26032;&#30693;&#35782;&#21040;&#36798;&#30340;&#24418;&#24335;&#21644;&#26102;&#38388;&#23545;&#32435;&#20837;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20840;&#38754;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26032;&#30693;&#35782;&#26102;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving distributed learning approach that is rapidly developing in an era where privacy protection is increasingly valued. It is this rapid development trend, along with the continuous emergence of new demands for FL in the real world, that prompts us to focus on a very important problem: Federated Learning with New Knowledge. The primary challenge here is to effectively incorporate various new knowledge into existing FL systems and evolve these systems to reduce costs, extend their lifespan, and facilitate sustainable development. In this paper, we systematically define the main sources of new knowledge in FL, including new features, tasks, models, and algorithms. For each source, we thoroughly analyze and discuss how to incorporate new knowledge into existing FL systems and examine the impact of the form and timing of new knowledge arrival on the incorporation process. Furthermore, we comprehensively discuss the potential future directions for
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>XTSFormer&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#30340;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02258</link><description>&lt;p&gt;
XTSFormer: &#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02258
&lt;/p&gt;
&lt;p&gt;
XTSFormer&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#30340;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#39044;&#27979;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#31867;&#22411;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#36830;&#32493;&#20107;&#20214;&#20043;&#38388;&#26102;&#38388;&#38388;&#38548;&#30340;&#19981;&#35268;&#21017;&#24615;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#65292;&#20197;&#21450;&#38271;&#20107;&#20214;&#24207;&#21015;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#20107;&#20214;&#20132;&#20114;&#30340;&#22810;&#23610;&#24230;&#29305;&#24615;&#65292;&#32780;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#20020;&#24202;&#20107;&#20214;&#25968;&#25454;&#65289;&#24456;&#24120;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#65288;XTSFormer&#65289;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#65288;FCPE&#65289;&#65292;&#33021;&#22815;&#28789;&#27963;&#25429;&#25417;&#26102;&#38388;&#30340;&#24490;&#29615;&#24615;&#36136;&#65292;&#20197;&#21450;&#19968;&#20010;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#12290;&#36825;&#20123;&#23610;&#24230;&#30001;&#33258;&#24213;&#21521;&#19978;&#30340;&#32858;&#31867;&#31639;&#27861;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event prediction aims to forecast the time and type of a future event based on a historical event sequence. Despite its significance, several challenges exist, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, and multi-scale event interactions, as well as the high computational costs for long event sequences. Existing neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data. Our model comprises two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism. These scales are determined by a bottom-up clustering algorithm. Extens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#24182;&#19988;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#30340;&#24778;&#35766;&#20272;&#35745;&#19982;&#33258;&#28982;&#20154;&#38405;&#35835;&#26102;&#38388;&#30340;&#36866;&#24212;&#24615;&#19979;&#38477;&#12290;&#32780;&#35789;&#39057;&#26159;&#35299;&#37322;&#36825;&#31181;&#36866;&#24212;&#24615;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#36807;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#20154;&#32676;&#20013;&#26368;&#19981;&#39057;&#32321;&#30340;&#35789;&#27719;&#65292;&#32780;&#36739;&#22823;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#20102;&#32597;&#35265;&#30340;&#35789;&#27719;&#65292;&#36825;&#35299;&#37322;&#20102;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23610;&#23544;&#23545;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02255</link><description>&lt;p&gt;
&#39057;&#29575;&#35299;&#37322;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#12289;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#24778;&#35766;&#31243;&#24230;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#21453;&#30456;&#20851;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#24182;&#19988;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#30340;&#24778;&#35766;&#20272;&#35745;&#19982;&#33258;&#28982;&#20154;&#38405;&#35835;&#26102;&#38388;&#30340;&#36866;&#24212;&#24615;&#19979;&#38477;&#12290;&#32780;&#35789;&#39057;&#26159;&#35299;&#37322;&#36825;&#31181;&#36866;&#24212;&#24615;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#36807;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#20154;&#32676;&#20013;&#26368;&#19981;&#39057;&#32321;&#30340;&#35789;&#27719;&#65292;&#32780;&#36739;&#22823;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#20102;&#32597;&#35265;&#30340;&#35789;&#27719;&#65292;&#36825;&#35299;&#37322;&#20102;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23610;&#23544;&#23545;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#24182;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#30340;&#24778;&#35766;&#20272;&#35745;&#19982;&#33258;&#28982;&#20154;&#38405;&#35835;&#26102;&#38388;&#30340;&#36866;&#24212;&#24615;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#20998;&#26512;&#26174;&#31034;&#65292;&#35789;&#39057;&#26159;&#36825;&#20004;&#20010;&#36235;&#21183;&#32972;&#21518;&#30340;&#20851;&#38190;&#35299;&#37322;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#26469;&#33258;&#22235;&#20010;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#22312;&#22235;&#20010;&#35821;&#26009;&#24211;&#19978;&#30340;&#27531;&#24046;&#35823;&#24046;&#26174;&#31034;&#65292;&#27169;&#22411;&#23610;&#23544;&#19982;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#21453;&#30456;&#20851;&#22312;&#26368;&#19981;&#39057;&#32321;&#30340;&#35789;&#27719;&#23376;&#38598;&#19978;&#26368;&#20026;&#26174;&#33879;&#65292;&#36825;&#26159;&#30001;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#36807;&#24230;&#20934;&#30830;&#30340;&#39044;&#27979;&#25152;&#25512;&#21160;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21160;&#24577;&#26174;&#31034;&#65292;&#22312;&#21518;&#26399;&#35757;&#32451;&#27493;&#39588;&#20013;&#65292;&#25152;&#26377;&#27169;&#22411;&#21464;&#20307;&#23398;&#20064;&#39044;&#27979;&#32597;&#35265;&#30340;&#35789;&#27719;&#65292;&#24182;&#19988;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#30340;&#39044;&#27979;&#26356;&#20026;&#20934;&#30830;&#65292;&#36825;&#35299;&#37322;&#20102;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23610;&#23544;&#23545;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#29305;&#24449;&#24402;&#22240;&#20998;&#26512;&#35777;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#21464;&#20307;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#32597;&#35265;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#33021;&#37327;&#25910;&#38598;&#26465;&#20214;&#19979;&#22810;&#28304;&#22810;&#20013;&#32487;&#23556;&#39057;&#33021;&#37327;&#25910;&#38598;&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#20013;&#32487;&#36873;&#25321;&#12289;&#35843;&#24230;&#21644;&#21151;&#29575;&#25511;&#21046;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#20998;&#21035;&#37319;&#29992;&#20256;&#32479;&#30340;2D&#21367;&#31215;&#22359;&#21644;Inception&#22359;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02254</link><description>&lt;p&gt;
&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#26080;&#32447;&#20379;&#33021;&#36890;&#20449;&#20013;&#20302;&#22797;&#26434;&#24230;&#20013;&#32487;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Learning based Low Complexity Relay Selection in Wireless Powered Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#33021;&#37327;&#25910;&#38598;&#26465;&#20214;&#19979;&#22810;&#28304;&#22810;&#20013;&#32487;&#23556;&#39057;&#33021;&#37327;&#25910;&#38598;&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#20013;&#32487;&#36873;&#25321;&#12289;&#35843;&#24230;&#21644;&#21151;&#29575;&#25511;&#21046;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#20998;&#21035;&#37319;&#29992;&#20256;&#32479;&#30340;2D&#21367;&#31215;&#22359;&#21644;Inception&#22359;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#33021;&#37327;&#25910;&#38598;&#32593;&#32476;&#26159;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36890;&#36807;&#20026;&#33021;&#37327;&#26377;&#38480;&#30340;&#35774;&#22791;&#25552;&#20379;&#21487;&#25511;&#21644;&#38271;&#36317;&#31163;&#30340;&#33021;&#37327;&#20256;&#36755;&#12290;&#20013;&#32487;&#22120;&#22312;&#33021;&#37327;&#25110;&#20449;&#24687;&#20256;&#36755;&#26041;&#38754;&#30340;&#24110;&#21161;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#33021;&#37327;&#25910;&#38598;&#26465;&#20214;&#19979;&#22810;&#28304;&#22810;&#20013;&#32487;&#23556;&#39057;&#33021;&#37327;&#25910;&#38598;&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#20013;&#32487;&#36873;&#25321;&#12289;&#35843;&#24230;&#21644;&#21151;&#29575;&#25511;&#21046;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#32473;&#23450;&#30340;&#20013;&#32487;&#36873;&#25321;&#33719;&#21462;&#20102;&#35843;&#24230;&#21644;&#21151;&#29575;&#25511;&#21046;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#28982;&#21518;&#65292;&#23558;&#20013;&#32487;&#36873;&#25321;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#26550;&#26500;&#12290;&#31532;&#19968;&#31181;&#26550;&#26500;&#37319;&#29992;&#20256;&#32479;&#30340;2D&#21367;&#31215;&#22359;&#65292;&#24182;&#20174;&#23618;&#20043;&#38388;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#30340;&#20248;&#21183;&#65307;&#31532;&#20108;&#31181;&#26550;&#26500;&#20351;&#29992;Inception&#22359;&#26367;&#25442;&#23427;&#20204;&#65292;&#20197;&#20943;&#23567;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#22823;&#23567;&#32780;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio Frequency Energy Harvesting (RF-EH) networks are key enablers of massive Internet-of-things by providing controllable and long-distance energy transfer to energy-limited devices. Relays, helping either energy or information transfer, have been demonstrated to significantly improve the performance of these networks. This paper studies the joint relay selection, scheduling, and power control problem in multiple-source-multiple-relay RF-EH networks under nonlinear EH conditions. We first obtain the optimal solution to the scheduling and power control problem for the given relay selection. Then, the relay selection problem is formulated as a classification problem, for which two convolutional neural network (CNN) based architectures are proposed. While the first architecture employs conventional 2D convolution blocks and benefits from skip connections between layers; the second architecture replaces them with inception blocks, to decrease trainable parameter size without sacrificing 
&lt;/p&gt;</description></item><item><title>&#22312;&#27604;&#36739;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#26102;&#65292;&#36890;&#36807;&#25910;&#38598;&#26356;&#22810;&#26679;&#26412;&#30340;&#21333;&#20010;&#26631;&#31614;&#32780;&#19981;&#26159;&#27719;&#24635;&#22810;&#20010;&#22122;&#22768;&#26631;&#31614;&#33021;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.02249</link><description>&lt;p&gt;
&#19981;&#35201;&#37325;&#22797;&#26631;&#35760;&#65306;&#22312;&#26377;&#38480;&#39044;&#31639;&#19979;&#27604;&#36739;&#20108;&#20803;&#20998;&#31867;&#22120;&#26102;&#65292;&#25968;&#37327;&#32988;&#36807;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02249
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#26102;&#65292;&#36890;&#36807;&#25910;&#38598;&#26356;&#22810;&#26679;&#26412;&#30340;&#21333;&#20010;&#26631;&#31614;&#32780;&#19981;&#26159;&#27719;&#24635;&#22810;&#20010;&#22122;&#22768;&#26631;&#31614;&#33021;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#38480;&#39044;&#31639;&#26469;&#27604;&#36739;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#22810;&#27425;&#25910;&#38598;&#21644;&#27719;&#24635;&#32473;&#23450;&#25968;&#25454;&#28857;&#30340;&#22810;&#20010;&#22122;&#22768;&#26631;&#31614;&#65292;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#24418;&#25104;&#19968;&#20010;&#19981;&#22826;&#22122;&#22768;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#19982;&#24120;&#35782;&#30456;&#21453;&#30340;&#23450;&#29702;&#12290;&#22914;&#26524;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;&#20998;&#31867;&#22120;&#20013;&#30340;&#36739;&#22909;&#32773;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#20570;&#27861;&#26159;&#23558;&#39044;&#31639;&#29992;&#20110;&#25910;&#38598;&#26356;&#22810;&#26679;&#26412;&#30340;&#21333;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26469;&#33258;&#20110;&#23545;Cram\'er&#23450;&#29702;&#30340;&#38750;&#24179;&#20961;&#24212;&#29992;&#65292;&#36825;&#26159;&#22823;&#20559;&#24046;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23427;&#20204;&#25512;&#32763;&#20102;&#19968;&#20123;&#21382;&#21490;&#19978;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#27604;Hoeffding&#30028;&#26356;&#20248;&#30340;&#26679;&#26412;&#22823;&#23567;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cram\'er's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound.
&lt;/p&gt;</description></item><item><title>ExTTNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#12290;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#33719;&#21462;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.92&#30340;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.02246</link><description>&lt;p&gt;
ExTTNet:&#19968;&#31181;&#29992;&#20110;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02246
&lt;/p&gt;
&lt;p&gt;
ExTTNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#12290;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#33719;&#21462;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.92&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;ExTTNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33258;&#21160;&#22320;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#20135;&#21697;&#34920;&#26684;&#12290;&#39318;&#20808;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#33719;&#21462;&#25991;&#26412;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#20102;Tesseract OCR&#24341;&#25806; [37]&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#22686;&#21152;&#29616;&#26377;&#29305;&#24449;&#30340;&#25968;&#37327;&#26469;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#26681;&#25454;&#27599;&#20010;OCR&#33719;&#24471;&#30340;&#25991;&#26412;&#26159;&#21542;&#26159;&#34920;&#26684;&#20803;&#32032;&#65292;&#36827;&#34892;&#26631;&#35760;&#22788;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#20102;&#22810;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;Nvidia RTX 3090&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#65292;&#32791;&#26102;162&#20998;&#38047;&#12290;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;F1&#20998;&#25968;&#20026;0.92&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#36335;&#38754;&#24322;&#24120;&#21306;&#22495;&#26816;&#27979;&#12290;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#20174;&#24322;&#26500;&#36755;&#20837;&#20013;&#20272;&#35745;&#27010;&#29575;&#29305;&#24449;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20960;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#26102;&#24615;&#33021;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02245</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20108;&#36827;&#21046;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02245
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#36335;&#38754;&#24322;&#24120;&#21306;&#22495;&#26816;&#27979;&#12290;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#20174;&#24322;&#26500;&#36755;&#20837;&#20013;&#20272;&#35745;&#27010;&#29575;&#29305;&#24449;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20960;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#26102;&#24615;&#33021;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#36335;&#38754;&#34920;&#38754;&#29366;&#20917;&#26816;&#27979;&#26088;&#22312;&#36890;&#36807;&#31639;&#27861;&#33258;&#21160;&#26816;&#27979;&#20195;&#34920;&#24322;&#24120;&#29366;&#24577;&#65288;&#22914;&#35010;&#32541;&#65289;&#30340;&#20687;&#32032;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30456;&#20851;&#39046;&#22495;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#30456;&#20851;&#35299;&#20915;&#26041;&#26696;&#24456;&#23569;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#36335;&#38754;&#24322;&#24120;&#21306;&#22495;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#29983;&#25104;&#22120;&#20174;&#24322;&#26500;&#36755;&#20837;&#20013;&#20272;&#35745;&#27010;&#29575;&#29305;&#24449;&#22270;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23558;&#20960;&#31181;&#27880;&#24847;&#26426;&#21046;&#32435;&#20837;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#35757;&#32451;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalous pavement surface conditions detection aims to detect pixels representing anomalous states, such as cracks, on pavement surface images automatically by algorithms. Recently, deep learning models have been intensively applied to related topics with outstanding performance. However, most existing deep learning-related solutions rarely achieve a stable performance on diverse datasets. To address this issue, in this work, we propose a deep learning framework based on conditional Generative Adversarial Networks for anomalous region detection on pavement images at the pixel level. In particular, the proposed framework is developed to enhance the generator's ability to estimate the probability feature map from heterogeneous inputs with two training stages and multiscale feature representation. Moreover, several attention mechanisms are incorporated into the proposed framework to mitigate the performance deterioration of model training on severely imbalanced datasets. We implement exp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.02244</link><description>&lt;p&gt;
&#36229;&#36234;&#26497;&#38480;&#65306;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#24322;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29702;&#35299;&#19978;&#19979;&#25991;&#12289;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#20005;&#26684;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#20026;&#20195;&#20215;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#25903;&#25345;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26368;&#36817;&#20026;&#25193;&#23637;LLMs&#24207;&#21015;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#38271;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#20462;&#25913;&#20301;&#32622;&#32534;&#30721;&#21644;&#20462;&#25913;&#27880;&#24847;&#26426;&#21046;&#31561;&#26550;&#26500;&#20462;&#25913;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#26356;&#38271;&#24207;&#21015;&#30340;&#22788;&#29702;&#65292;&#21516;&#26102;&#36991;&#20813;&#35745;&#31639;&#38656;&#27714;&#30340;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#30340;&#22810;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;LLMs&#30340;&#19981;&#21516;&#38454;&#27573;&#65288;&#21363;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#65289;&#20013;&#21033;&#29992;&#12290;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#24182;&#25552;&#21319;&#23545;&#38271;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;</title><link>https://arxiv.org/abs/2402.02242</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;PVMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;PVMs&#36798;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#65292;&#26631;&#20934;&#30340;&#20840;&#38754;&#24494;&#35843;&#33539;&#24335;&#30001;&#20110;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#65292;&#26088;&#22312;&#20197;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#35270;&#35273;PEFT&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;PEFT&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#28155;&#21152;&#30340;&#12289;&#22522;&#20110;&#37096;&#20998;&#30340;&#21644;&#22522;&#20110;&#32479;&#19968;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02239</link><description>&lt;p&gt;
&#20998;&#24067;&#32422;&#31616;&#65306;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#38477;&#32500;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#28508;&#22312;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#28041;&#21450;&#20351;&#29992;&#38477;&#32500;&#26041;&#27861;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#31354;&#38388;&#19978;&#65292;&#25110;&#23558;&#25968;&#25454;&#28857;&#32452;&#32455;&#25104;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26159;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#65292;&#32780;&#19981;&#33021;&#20445;&#35777;&#32858;&#31867;&#19982;&#38477;&#32500;&#30456;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#20351;&#29992;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#25105;&#20204;&#23558;&#32858;&#31867;&#21644;&#38477;&#32500;&#32479;&#19968;&#20026;&#19968;&#20010;&#31216;&#20026;&#20998;&#24067;&#32422;&#31616;&#30340;&#21333;&#19968;&#26694;&#26550;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#32858;&#31867;&#21644;&#38477;&#32500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. Traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. In practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. In this work, we offer a fresh perspective: that of distributions. Leveraging tools from optimal transport, particularly the Gromov-Wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. This allows us to jointly address clustering and dimensionality reduction with a single optimization problem. Through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20998;&#24067;&#24335;&#21644;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#19979;&#38477;&#26368;&#20026;&#20005;&#37325;&#12290;</title><link>https://arxiv.org/abs/2402.02230</link><description>&lt;p&gt;
&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20998;&#24067;&#24335;&#21644;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#19979;&#38477;&#26368;&#20026;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#20445;&#25252;&#23458;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#65292;&#36991;&#20813;&#22312;&#19981;&#21516;&#21442;&#19982;&#26041;&#20043;&#38388;&#20849;&#20139;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#21442;&#25968;&#26435;&#37325;&#65292;&#31169;&#23494;&#20449;&#24687;&#20173;&#28982;&#21487;&#33021;&#27844;&#38706;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#20110;&#23458;&#25143;&#25968;&#37327;&#21644;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#23545;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#35777;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#24335;&#21644;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#19979;&#38477;&#26368;&#20026;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving client's private data from being shared among different parties. Nevertheless, private information can still be divulged by analyzing uploaded parameter weights from clients. In this report, we showcase our empirical benchmark of the effect of the number of clients and the addition of differential privacy (DP) mechanisms on the performance of the model on different types of data. Our results show that non-i.i.d and small datasets have the highest decrease in performance in a distributed and differentially private setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02229</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Vanilla Bayesian Optimization Performs Great in High Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#32500;&#38382;&#39064;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#36719;&#32907;&#12290;&#21463;&#21040;&#32500;&#24230;&#22122;&#38899;&#30340;&#21050;&#28608;&#65292;&#35768;&#22810;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#30446;&#26631;&#24212;&#29992;&#21508;&#31181;&#31616;&#21270;&#20551;&#35774;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#23548;&#33268;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#19981;&#36866;&#29992;&#30340;&#36864;&#21270;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#24212;&#23545;&#36825;&#20123;&#36864;&#21270;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#20013;&#20856;&#22411;&#20808;&#39564;&#20551;&#35774;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23545;&#30446;&#26631;&#26045;&#21152;&#32467;&#26500;&#24615;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23558;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#21487;&#31649;&#29702;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#32500;&#24230;&#23545;&#39640;&#26031;&#36807;&#31243;&#38271;&#24230;&#20808;&#39564;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#8212;&#8212;&#25581;&#31034;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26126;&#30830;&#34920;&#26126;&#20854;&#25928;&#26524;&#36828;&#36828;&#36229;&#20986;&#20197;&#24448;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPreFL&#30340;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#36866;&#24212;&#20219;&#20309;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02225</link><description>&lt;p&gt;
&#37325;&#24605;&#20986;&#21457;&#28857;&#65306;&#36890;&#36807;&#21327;&#20316;&#39044;&#35757;&#32451;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPreFL&#30340;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#36866;&#24212;&#20219;&#20309;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#20174;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23454;&#35777;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20026;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#26377;&#30410;&#30340;&#21021;&#22987;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;CoPreFL&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#35774;&#35745;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20026;&#20219;&#20309;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#27169;&#20223;&#19979;&#28216;&#20998;&#24067;&#24335;&#22330;&#26223;&#30340;&#20803;&#23398;&#20064;&#36807;&#31243;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26410;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#12290;CoPreFL&#30340;&#39044;&#35757;&#32451;&#20248;&#21270;&#36807;&#31243;&#20063;&#22312;&#24179;&#22343;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#21021;&#22987;&#21270;&#26469;&#35299;&#20915;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#25361;&#25112;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#20219;&#20309;&#26410;&#30693;&#30340;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe
&lt;/p&gt;</description></item><item><title>&#22270;&#22522;&#30784;&#27169;&#22411;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.02216</link><description>&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02216
&lt;/p&gt;
&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;Graph Foundation Model&#65292;GFM&#65289;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;GFM&#12290;&#26500;&#24314;GFM&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#33021;&#22312;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#30340;&#22270;&#20043;&#38388;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#65292;&#21363;&#28508;&#34255;&#20110;&#22270;&#20013;&#30340;&#22522;&#26412;&#21487;&#36801;&#31227;&#21333;&#20803;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#20174;&#32593;&#32476;&#20998;&#26512;&#12289;&#29702;&#35770;&#22522;&#30784;&#21644;&#31283;&#23450;&#24615;&#31561;&#37325;&#35201;&#26041;&#38754;&#26469;&#24314;&#31435;&#22270;&#35789;&#27719;&#34920;&#12290;&#36825;&#31181;&#35789;&#27719;&#34920;&#30340;&#35270;&#35282;&#26377;&#21161;&#20110;&#25353;&#29031;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#23567;&#26031;&#22374;&#32435;&#26641;&#20043;&#38388;&#30340;&#26597;&#35810;&#20915;&#31574;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21407;&#21017;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#20102;&#38382;&#39064;&#21487;&#24471;&#21040;&#36739;&#22909;&#30340;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.02211</link><description>&lt;p&gt;
&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#23567;&#26031;&#22374;&#32435;&#26641;&#20043;&#38388;&#30340;&#26597;&#35810;&#20915;&#31574;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Query-decision Regression between Shortest Path and Minimum Steiner Tree
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#23567;&#26031;&#22374;&#32435;&#26641;&#20043;&#38388;&#30340;&#26597;&#35810;&#20915;&#31574;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21407;&#21017;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#20102;&#38382;&#39064;&#21487;&#24471;&#21040;&#36739;&#22909;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#26435;&#37325;&#30340;&#22270;&#65292;&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19968;&#20123;&#33410;&#28857;&#23376;&#38598;&#23545;&#24212;&#30340;&#26368;&#23567;&#26031;&#22374;&#32435;&#26641;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#19968;&#23545;&#33410;&#28857;&#30340;&#26368;&#30701;&#36335;&#24452;&#65311;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#19968;&#20010;&#22266;&#23450;&#30340;&#28508;&#22312;&#20915;&#31574;&#31995;&#32479;&#65288;&#20363;&#22914;&#21152;&#26435;&#22270;&#65289;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#21033;&#29992;&#19982;&#21478;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65288;&#20363;&#22914;&#26368;&#23567;&#26031;&#22374;&#32435;&#26641;&#38382;&#39064;&#65289;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65288;&#20363;&#22914;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#26679;&#19968;&#20010;&#21407;&#22411;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20855;&#26377;&#20219;&#21153;&#36716;&#31227;&#30340;&#26597;&#35810;&#20915;&#31574;&#22238;&#24402;&#8221;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#21644;&#26368;&#23567;&#26031;&#22374;&#32435;&#26641;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26500;&#24314;&#35780;&#20998;&#27169;&#22411;&#30340;&#21487;&#23454;&#29616;&#20551;&#35774;&#31354;&#38388;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21407;&#21017;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31867;&#38382;&#39064;&#21487;&#20197;&#22312;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#31243;&#24230;&#19978;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering a graph with unknown weights, can we find the shortest path for a pair of nodes if we know the minimal Steiner trees associated with some subset of nodes? That is, with respect to a fixed latent decision-making system (e.g., a weighted graph), we seek to solve one optimization problem (e.g., the shortest path problem) by leveraging information associated with another optimization problem (e.g., the minimal Steiner tree problem). In this paper, we study such a prototype problem called \textit{query-decision regression with task shifts}, focusing on the shortest path problem and the minimum Steiner tree problem. We provide theoretical insights regarding the design of realizable hypothesis spaces for building scoring models, and present two principled learning frameworks. Our experimental studies show that such problems can be solved to a decent extent with statistical significance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#24358;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#21608;&#26399;&#24615;&#21487;&#24179;&#38138;&#32441;&#29702;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20613;&#31435;&#21494;&#32423;&#25968;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#31354;&#38388;&#22352;&#26631;&#19978;&#30452;&#25509;&#35780;&#20272;&#32441;&#29702;&#65292;&#36991;&#20813;&#20102;&#25554;&#20540;&#12290;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#21644;&#27850;&#26494;&#26041;&#31243;&#65292;&#30830;&#20445;&#29983;&#25104;&#21487;&#24179;&#38138;&#30340;&#32441;&#29702;&#65292;&#24182;&#33021;&#39640;&#25928;&#37325;&#24314;&#39640;&#20998;&#36776;&#29575;&#32441;&#29702;&#12290;&#24212;&#29992;&#20110;&#21453;&#38191;&#40831;&#34920;&#38754;&#33021;&#22815;&#25552;&#20379;&#39640;&#35270;&#35273;&#20445;&#30495;&#24230;&#21644;&#38160;&#24230;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02208</link><description>&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21487;&#24179;&#38138;&#26448;&#36136;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representation of Tileable Material Textures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#24358;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#21608;&#26399;&#24615;&#21487;&#24179;&#38138;&#32441;&#29702;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20613;&#31435;&#21494;&#32423;&#25968;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#31354;&#38388;&#22352;&#26631;&#19978;&#30452;&#25509;&#35780;&#20272;&#32441;&#29702;&#65292;&#36991;&#20813;&#20102;&#25554;&#20540;&#12290;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#21644;&#27850;&#26494;&#26041;&#31243;&#65292;&#30830;&#20445;&#29983;&#25104;&#21487;&#24179;&#38138;&#30340;&#32441;&#29702;&#65292;&#24182;&#33021;&#39640;&#25928;&#37325;&#24314;&#39640;&#20998;&#36776;&#29575;&#32441;&#29702;&#12290;&#24212;&#29992;&#20110;&#21453;&#38191;&#40831;&#34920;&#38754;&#33021;&#22815;&#25552;&#20379;&#39640;&#35270;&#35273;&#20445;&#30495;&#24230;&#21644;&#38160;&#24230;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#27491;&#24358;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#21608;&#26399;&#24615;&#21487;&#24179;&#38138;&#32441;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#29992;&#25972;&#25968;&#39057;&#29575;&#21021;&#22987;&#21270;&#27491;&#24358;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#23618;&#65292;&#21033;&#29992;&#20613;&#31435;&#21494;&#32423;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#27491;&#24358;&#23618;&#30340;&#32452;&#21512;&#21482;&#29983;&#25104;&#20855;&#26377;&#21608;&#26399;P&#30340;&#25972;&#25968;&#39057;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20064;&#20102;&#21608;&#26399;&#27169;&#24335;&#30340;&#36830;&#32493;&#34920;&#31034;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#20219;&#24847;&#31354;&#38388;&#22352;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#26080;&#38656;&#25554;&#20540;&#12290;&#20026;&#20102;&#20351;&#32467;&#26524;&#27169;&#24335;&#21487;&#24179;&#38138;&#65292;&#25105;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#20102;&#22522;&#20110;&#27850;&#26494;&#26041;&#31243;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#32039;&#20945;&#19988;&#33021;&#22815;&#39640;&#25928;&#37325;&#24314;&#20855;&#26377;&#22810;&#20010;&#23618;&#27425;&#32454;&#33410;&#30340;&#39640;&#20998;&#36776;&#29575;&#32441;&#29702;&#65292;&#24182;&#20855;&#26377;&#39640;&#35270;&#35273;&#20445;&#30495;&#24230;&#21644;&#38160;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21453;&#38191;&#40831;&#34920;&#38754;&#39046;&#22495;&#20013;&#24212;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore sinusoidal neural networks to represent periodic tileable textures. Our approach leverages the Fourier series by initializing the first layer of a sinusoidal neural network with integer frequencies with a period $P$. We prove that the compositions of sinusoidal layers generate only integer frequencies with period $P$. As a result, our network learns a continuous representation of a periodic pattern, enabling direct evaluation at any spatial coordinate without the need for interpolation. To enforce the resulting pattern to be tileable, we add a regularization term, based on the Poisson equation, to the loss function. Our proposed neural implicit representation is compact and enables efficient reconstruction of high-resolution textures with high visual fidelity and sharpness across multiple levels of detail. We present applications of our approach in the domain of anti-aliased surface.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#23433;&#20840;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20182;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#26377;&#23475;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;VLGuard&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23558;&#35813;&#25968;&#25454;&#38598;&#38598;&#25104;&#21040;&#35270;&#35273;&#35821;&#35328;&#24494;&#35843;&#20013;&#25110;&#36827;&#34892;&#20107;&#21518;&#24494;&#35843;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#40784;VLLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#26377;&#30410;&#24615;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#29978;&#33267;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#23545;&#29616;&#26377;VLLMs&#36827;&#34892;&#23433;&#20840;&#27979;&#35797;&#12289;&#35757;&#32451;&#26032;&#27169;&#22411;&#25110;&#20445;&#25252;&#39044;&#35757;&#32451;VLLMs&#12290;</title><link>https://arxiv.org/abs/2402.02207</link><description>&lt;p&gt;
&#23433;&#20840;&#24494;&#35843;&#20960;&#20046;&#38646;&#25104;&#26412;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35270;&#35273;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#23433;&#20840;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20182;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#26377;&#23475;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;VLGuard&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23558;&#35813;&#25968;&#25454;&#38598;&#38598;&#25104;&#21040;&#35270;&#35273;&#35821;&#35328;&#24494;&#35843;&#20013;&#25110;&#36827;&#34892;&#20107;&#21518;&#24494;&#35843;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#40784;VLLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#26377;&#30410;&#24615;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#29978;&#33267;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#23545;&#29616;&#26377;VLLMs&#36827;&#34892;&#23433;&#20840;&#27979;&#35797;&#12289;&#35757;&#32451;&#26032;&#27169;&#22411;&#25110;&#20445;&#25252;&#39044;&#35757;&#32451;VLLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;&#35270;&#35273;&#35821;&#35328;&#25351;&#20196;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#26377;&#23475;&#25968;&#25454;&#65292;&#24182;&#19988;VLLM&#30340;&#24494;&#35843;&#20250;&#23548;&#33268;&#23545;&#24213;&#23618;LLM&#20043;&#21069;&#23398;&#20064;&#30340;&#23433;&#20840;&#23545;&#40784;&#24615;&#30340;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#26377;&#23475;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;VLGuard&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#35813;&#25968;&#25454;&#38598;&#38598;&#25104;&#21040;&#26631;&#20934;&#30340;&#35270;&#35273;&#35821;&#35328;&#24494;&#35843;&#20013;&#25110;&#23558;&#20854;&#29992;&#20110;&#20107;&#21518;&#24494;&#35843;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#40784;VLLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#31181;&#23545;&#40784;&#26159;&#22312;&#23545;&#27169;&#22411;&#30340;&#26377;&#30410;&#24615;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#29978;&#33267;&#26377;&#25152;&#25552;&#21319;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#23433;&#20840;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#25104;&#20026;&#23545;&#29616;&#26377;VLLMs&#36827;&#34892;&#23433;&#20840;&#27979;&#35797;&#65292;&#35757;&#32451;&#26032;&#27169;&#22411;&#25110;&#20445;&#25252;&#39044;&#35757;&#32451;VLLMs&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21327;&#21516;&#32534;&#25490;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#25506;&#32034;&#32452;&#21512;&#24211;&#20013;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38477;&#32500;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#25511;&#21046;&#22797;&#26434;&#21487;&#35266;&#27979;&#37327;&#30340;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02198</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21327;&#21516;&#32534;&#25490;&#36890;&#36807;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;&#25506;&#32034;&#32452;&#21512;&#24211;&#20013;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Multimodal Co-orchestration for Exploring Structure-Property Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21327;&#21516;&#32534;&#25490;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#25506;&#32034;&#32452;&#21512;&#24211;&#20013;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38477;&#32500;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#25511;&#21046;&#22797;&#26434;&#21487;&#35266;&#27979;&#37327;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21644;&#33258;&#20027;&#20202;&#22120;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#22810;&#27169;&#24577;&#24037;&#20855;&#30340;&#21327;&#21516;&#32534;&#25490;&#26426;&#20250;&#65292;&#36825;&#20123;&#24037;&#20855;&#37197;&#22791;&#20102;&#22810;&#31181;&#39034;&#24207;&#26816;&#27979;&#26041;&#27861;&#65292;&#25110;&#32773;&#22810;&#20010;&#34920;&#24449;&#24037;&#20855;&#26469;&#21516;&#26102;&#25506;&#32034;&#30456;&#21516;&#26679;&#21697;&#12290;&#36825;&#21487;&#20197;&#20197;&#32452;&#21512;&#24211;&#20026;&#20363;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#24037;&#20855;&#21516;&#26102;&#22312;&#22810;&#20010;&#20301;&#32622;&#25506;&#32034;&#65292;&#25110;&#32773;&#22312;&#33258;&#21160;&#21512;&#25104;&#31995;&#32479;&#20013;&#36827;&#34892;&#19979;&#28216;&#34920;&#24449;&#12290;&#22312;&#21327;&#21516;&#32534;&#25490;&#26041;&#27861;&#20013;&#65292;&#20174;&#19968;&#31181;&#27169;&#24577;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#24212;&#35813;&#21152;&#36895;&#20854;&#20182;&#27169;&#24577;&#30340;&#21457;&#29616;&#12290;&#30456;&#24212;&#22320;&#65292;&#21327;&#21516;&#32534;&#25490;&#20195;&#29702;&#24212;&#35813;&#26681;&#25454;&#39044;&#26399;&#30340;&#30693;&#35782;&#22686;&#30410;&#21644;&#27979;&#37327;&#25104;&#26412;&#26469;&#36873;&#25321;&#27979;&#37327;&#27169;&#24577;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#29992;&#20110;&#36827;&#34892;&#20855;&#26377;&#22797;&#26434;&#21487;&#35266;&#27979;&#37327;&#65288;&#22914;&#20809;&#35889;&#25110;&#22270;&#20687;&#65289;&#30340;&#27979;&#37327;&#30340;&#21327;&#21516;&#32534;&#25490;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38477;&#32500;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#25511;&#21046;&#28508;&#22312;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of automated and autonomous instrumentations brings forth an opportunity for the co-orchestration of multimodal tools, equipped with multiple sequential detection methods, or several characterization tools to explore identical samples. This can be exemplified by the combinatorial libraries that can be explored in multiple locations by multiple tools simultaneously, or downstream characterization in automated synthesis systems. In the co-orchestration approaches, information gained in one modality should accelerate the discovery of other modalities. Correspondingly, the orchestrating agent should select the measurement modality based on the anticipated knowledge gain and measurement cost. Here, we propose and implement a co-orchestration approach for conducting measurements with complex observables such as spectra or images. The method relies on combining dimensionality reduction by variational autoencoders with representation learning for control over the latent space 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.02196</link><description>&lt;p&gt;
&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#26679;&#26412;&#39640;&#25928;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02196
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#32858;&#31867;&#21644;&#24449;&#26381;"&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25171;&#30772;&#26679;&#26412;&#25928;&#29575;&#30340;&#29942;&#39048;&#12290;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#32858;&#31867;&#21487;&#20197;&#23454;&#29616;O(p)&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#36895;&#24230;&#65292;&#36825;&#26159;&#29702;&#35770;&#19978;&#21487;&#36798;&#21040;&#30340;&#26368;&#20339;&#20943;&#23569;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#22312;&#22266;&#23450;&#39044;&#31639;&#21644;&#22266;&#23450;&#31934;&#24230;&#30340;&#33539;&#24335;&#19979;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21508;&#31181;&#24120;&#35265;&#30340;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#22312;&#26080;&#38656;&#39640;&#31934;&#30830;&#24230;&#30456;&#20851;&#20272;&#35745;&#21644;&#31934;&#30830;&#32858;&#31867;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25913;&#36827;&#12290;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#22914;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#26080;&#31579;&#36873;&#29256;&#26412;&#30340;&#26041;&#27861;&#24778;&#20154;&#22320;&#36229;&#36807;&#20102;&#23436;&#20840;&#39034;&#24207;&#21270;&#30340;&#22522;&#20934;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#34920;&#26126;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#30456;&#20851;&#24615;&#65292;&#26159;&#32469;&#36807;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#26465;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose novel "clustering and conquer" procedures for the parallel large-scale ranking and selection (R&amp;S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency. In parallel computing environments, correlation-based clustering can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal reduction rate theoretically attainable. Our proposed framework is versatile, allowing for seamless integration of various prevalent R&amp;S methods under both fixed-budget and fixed-precision paradigms. It can achieve improvements without the necessity of highly accurate correlation estimation and precise clustering. In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency. This suggests that leveraging valuable structural information, such as correlation, is a viable path to bypassing the trad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;(CTRA)&#65292;&#29992;&#20110;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;CTRA&#36890;&#36807;&#23545;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#35299;&#20915;&#20102;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02190</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;(CTRA)&#65292;&#29992;&#20110;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;CTRA&#36890;&#36807;&#23545;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#35299;&#20915;&#20102;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#23547;&#25214;&#26368;&#20339;&#35299;&#26159;&#26368;&#24120;&#35265;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#30446;&#26631;&#20989;&#25968;&#21644;&#32422;&#26463;&#26465;&#20214;&#21482;&#26159;&#21407;&#22987;&#29616;&#23454;&#19990;&#30028;&#24773;&#20917;&#30340;&#36817;&#20284;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23547;&#25214;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#21644;&#32422;&#26463;&#20005;&#37325;&#24615;&#30340;&#21464;&#21270;&#25104;&#20026;&#33258;&#28982;&#30340;&#26041;&#21521;&#12290;&#36825;&#31181;&#31574;&#30053;&#25552;&#20379;&#20102;&#22312;&#21518;&#22788;&#29702;&#36807;&#31243;&#20013;&#36873;&#25321;&#21512;&#36866;&#35299;&#20915;&#26041;&#26696;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#36825;&#20123;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#27604;&#30830;&#23450;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#36830;&#32493;&#24352;&#37327;&#26494;&#24347;&#36864;&#28779; (CTRA) &#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;CTRA&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#26494;&#24347;&#26041;&#27861;&#65292;&#23558;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36716;&#25442;&#20026;&#36830;&#32493;&#24352;&#37327;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22810;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#25214;&#21040;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#21644;&#32422;&#26463;&#20005;&#37325;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the best solution is the most common objective in combinatorial optimization (CO) problems. However, a single solution may not be suitable in practical scenarios, as the objective functions and constraints are only approximations of original real-world situations. To tackle this, finding (i) "heterogeneous solutions", diverse solutions with distinct characteristics, and (ii) "penalty-diversified solutions", variations in constraint severity, are natural directions. This strategy provides the flexibility to select a suitable solution during post-processing. However, discovering these diverse solutions is more challenging than identifying a single solution. To overcome this challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning-based CO solvers. CTRA addresses various problems simultaneously by extending the continual relaxation approach, which transforms discrete decision variables into continual tensors. This method finds heterog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#24182;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GFlowNets&#20195;&#29702;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02186</link><description>&lt;p&gt;
&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Evolution Guided Generative Flow Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#24182;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GFlowNets&#20195;&#29702;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31867;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#25353;&#29031;&#22870;&#21169;&#27604;&#20363;&#23545;&#32452;&#21512;&#23545;&#35937;&#36827;&#34892;&#37319;&#26679;&#12290;GFlowNets&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#26159;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#26102;&#26377;&#25928;&#35757;&#32451;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#36825;&#26159;&#23545;GFlowNets&#35757;&#32451;&#30340;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#65288;EA&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20219;&#20309;GFlowNets&#35757;&#32451;&#30446;&#26631;&#30340;&#22522;&#30784;&#19978;&#24037;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;EA&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#20351;&#29992;&#23384;&#20648;&#30340;&#36712;&#36857;&#35757;&#32451;GFlowNets&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#29609;&#20855;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#36712;&#36857;&#21644;&#31232;&#30095;&#22870;&#21169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are a family of probabilistic generative models that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#22495;&#25512;&#33616;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#36741;&#21161;&#39046;&#22495;&#20013;&#28155;&#21152;&#25968;&#25454;&#26469;&#35299;&#20915;&#20919;&#21551;&#21160;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#26144;&#23556;&#27169;&#22359;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#30830;&#23450;&#20102;&#36328;&#22495;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02182</link><description>&lt;p&gt;
&#36328;&#22495;&#25512;&#33616;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Cross-domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#22495;&#25512;&#33616;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#36741;&#21161;&#39046;&#22495;&#20013;&#28155;&#21152;&#25968;&#25454;&#26469;&#35299;&#20915;&#20919;&#21551;&#21160;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#26144;&#23556;&#27169;&#22359;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#30830;&#23450;&#20102;&#36328;&#22495;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#65292;&#32473;&#19982;&#20919;&#21551;&#21160;&#29992;&#25143;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#32467;&#26524;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#35299;&#20915;&#30446;&#26631;&#39046;&#22495;&#20013;&#20919;&#21551;&#21160;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#36741;&#21161;&#39046;&#22495;&#28155;&#21152;&#25968;&#25454;&#12290;&#25214;&#21040;&#19968;&#31181;&#21512;&#36866;&#30340;&#26041;&#27861;&#20174;&#36741;&#21161;&#39046;&#22495;&#25552;&#21462;&#30693;&#35782;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#26159;&#36328;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#22312;&#29616;&#26377;&#30340;&#26041;&#27861;&#20013;&#65292;&#26144;&#23556;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36328;&#22495;&#25512;&#33616;&#27169;&#22411;&#65288;CDRs&#65289;&#12290;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#26144;&#23556;&#27169;&#22359;&#36215;&#30528;&#23558;&#25968;&#25454;&#20174;&#19968;&#20010;&#22495;&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;&#22495;&#30340;&#20316;&#29992;&#12290;&#23427;&#20027;&#35201;&#20915;&#23450;&#20102;&#26144;&#23556;&#26041;&#27861;CDRs&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#12290;&#23427;&#20204;&#28041;&#21450;&#20174;&#28155;&#21152;&#22122;&#22768;&#30340;&#26679;&#26412;&#20013;&#24674;&#22797;&#22270;&#20687;&#65292;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#20986;&#33394;&#24615;&#33021;&#30340;&#25968;&#25454;&#36716;&#25442;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is always a challenge for recommender systems to give high-quality outcomes to cold-start users. One potential solution to alleviate the data sparsity problem for cold-start users in the target domain is to add data from the auxiliary domain. Finding a proper way to extract knowledge from an auxiliary domain and transfer it into a target domain is one of the main objectives for cross-domain recommendation (CDR) research. Among the existing methods, mapping approach is a popular one to implement cross-domain recommendation models (CDRs). For models of this type, a mapping module plays the role of transforming data from one domain to another. It primarily determines the performance of mapping approach CDRs. Recently, diffusion probability models (DPMs) have achieved impressive success for image synthesis related tasks. They involve recovering images from noise-added samples, which can be viewed as a data transformation process with outstanding performance. To further enhance the perfo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;IPS&#65288;LIPS&#65289;&#30340;&#26032;&#30340;Slate Bandit OPE&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#24230;&#30340;Slate&#25277;&#35937;&#31354;&#38388;&#20013;&#23450;&#20041;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20248;&#21270;Slate&#25277;&#35937;&#26469;&#20943;&#23567;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.02171</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#25277;&#35937;&#30340;&#26041;&#24335;&#36827;&#34892;Slate Bandit&#31574;&#30053;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02171
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;IPS&#65288;LIPS&#65289;&#30340;&#26032;&#30340;Slate Bandit OPE&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#24230;&#30340;Slate&#25277;&#35937;&#31354;&#38388;&#20013;&#23450;&#20041;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20248;&#21270;Slate&#25277;&#35937;&#26469;&#20943;&#23567;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;Slate&#19978;&#19979;&#25991;&#24378;&#30423;&#38382;&#39064;&#20013;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#31574;&#30053;&#36873;&#25321;&#31216;&#20026;slates&#30340;&#22810;&#32500;&#21160;&#20316;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#33829;&#38144;&#20197;&#21450;&#21307;&#30103;&#24212;&#29992;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#21160;&#20316;&#31354;&#38388;&#22823;&#65292;&#20856;&#22411;&#30340;&#36870;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#23384;&#22312;&#36739;&#22823;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#26377;&#25928;&#30340;OPE&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20266;&#36870;&#65288;PI&#65289;&#20272;&#35745;&#22120;&#24050;&#34987;&#24341;&#20837;&#20197;&#20943;&#23567;&#26041;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#20551;&#35774;&#22870;&#21169;&#20989;&#25968;&#32447;&#24615;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#22240;&#20026;&#36825;&#20010;&#20551;&#35774;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#24456;&#38590;&#39564;&#35777;&#24182;&#19988;&#32463;&#24120;&#20250;&#34987;&#23454;&#36136;&#24615;&#36829;&#21453;&#12290;&#20026;&#20102;&#35299;&#20915;&#20043;&#21069;&#20272;&#35745;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;Slate Bandit OPE&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;&#28508;&#22312;IPS&#65288;LIPS&#65289;&#65292;&#23427;&#22312;&#20302;&#32500;&#24230;&#30340;Slate&#25277;&#35937;&#31354;&#38388;&#20013;&#23450;&#20041;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20248;&#21270;Slate&#25277;&#35937;&#26469;&#26368;&#23567;&#21270;LIPS&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By do
&lt;/p&gt;</description></item><item><title>DyExpert&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#24182;&#32467;&#21512;&#38142;&#25509;&#39044;&#27979;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02168</link><description>&lt;p&gt;
&#36328;&#22495;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#30340;&#19968;&#31181;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
One Graph Model for Cross-domain Dynamic Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02168
&lt;/p&gt;
&lt;p&gt;
DyExpert&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#24182;&#32467;&#21512;&#38142;&#25509;&#39044;&#27979;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DyExpert&#65292;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#26126;&#30830;&#22320;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#65292;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#36827;&#32780;&#36827;&#34892;&#29305;&#23450;&#27169;&#24335;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;DyExpert&#37319;&#29992;&#20102;&#35299;&#30721;&#22120;&#20248;&#21270;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#28436;&#21270;&#24314;&#27169;&#21644;&#38142;&#25509;&#39044;&#27979;&#30340;&#8220;&#26465;&#20214;&#38142;&#25509;&#29983;&#25104;&#8221;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24182;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;DyExpert&#22312;&#21253;&#21547;6&#30334;&#19975;&#20010;&#21160;&#24577;&#36793;&#30340;&#24191;&#27867;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20843;&#20010;&#26410;&#35757;&#32451;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;DyExpert&#22312;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#19982;&#30456;&#21516;&#35774;&#32622;&#19979;&#30340;&#20808;&#36827;&#22522;&#20934;&#30456;&#27604;&#65292;DyExpert&#22312;&#20843;&#20010;&#22270;&#19978;&#30340;&#24179;&#22343;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;11.40&#65285;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;&#22312;&#20845;&#20010;&#26410;&#35757;&#32451;&#30340;&#22270;&#19978;&#65292;&#23427;&#36229;&#36807;&#20102;&#20843;&#20010;&#20808;&#36827;&#22522;&#32447;&#30340;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EvaLLM&#30340;&#27010;&#24565;&#27169;&#22411;&#26632;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#37322;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21487;&#35270;&#21270;&#12290;&#23427;&#35299;&#20915;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35270;&#21270;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02167</link><description>&lt;p&gt;
Vi(E)va LLM&#65281;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#37322;&#29983;&#25104;AI&#21487;&#35270;&#21270;&#30340;&#27010;&#24565;&#27169;&#22411;&#26632;
&lt;/p&gt;
&lt;p&gt;
Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EvaLLM&#30340;&#27010;&#24565;&#27169;&#22411;&#26632;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#37322;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21487;&#35270;&#21270;&#12290;&#23427;&#35299;&#20915;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35270;&#21270;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#21487;&#35270;&#21270;&#26159;&#19968;&#39033;&#21476;&#32769;&#30340;&#20219;&#21153;&#65292;&#22810;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#21644;&#23454;&#36341;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#25104;&#20026;&#25903;&#25345;&#19982;&#21487;&#35270;&#21270;&#30456;&#20851;&#30340;&#29983;&#25104;&#20219;&#21153;&#30340;&#26377;&#36259;&#36873;&#25321;&#65292;&#24182;&#23637;&#31034;&#20102;&#21021;&#27493;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#25351;&#23548;LLM&#29983;&#25104;&#25152;&#38656;&#32467;&#26524;&#30340;&#22810;&#31181;&#26041;&#24335;&#65292;&#24341;&#23548;&#29983;&#25104;&#30340;&#19981;&#21516;&#35270;&#35282;&#65288;&#22522;&#20110;&#20195;&#30721;&#12289;&#22522;&#20110;&#22270;&#20687;&#12289;&#22522;&#20110;&#35821;&#27861;&#65289;&#65292;&#20197;&#21450;&#21363;&#20351;&#22312;&#21487;&#35270;&#21270;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23384;&#22312;&#30340;&#24187;&#35273;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#20351;&#29992;&#19981;&#22914;&#39044;&#26399;&#30340;&#37027;&#26679;&#21487;&#34892;&#12290;&#22312;&#31867;&#20284;&#20026;LLM&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#20513;&#35758;&#19979;&#65292;&#26412;&#25991;&#38024;&#23545;&#36890;&#36807;LLM&#23545;&#29983;&#25104;&#30340;&#21487;&#35270;&#21270;&#24314;&#27169;&#30340;&#35780;&#20272;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#35780;&#20272;&#27169;&#22411;&#26632;&#65292;EvaLLM&#65292;&#23427;&#23558;&#35780;&#20272;&#24037;&#20316;&#20998;&#35299;&#20026;&#20854;&#21407;&#23376;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23545;&#20854;&#24615;&#36136;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#27010;&#36848;&#20102;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic generation of visualizations is an old task that, through the years, has shown more and more interest from the research and practitioner communities. Recently, large language models (LLM) have become an interesting option for supporting generative tasks related to visualization, demonstrating initial promising results. At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected. Following similar initiatives for benchmarking LLMs, this paper copes with the problem of modeling the evaluation of a generated visualization through an LLM. We propose a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort in its atomic components, characterizes their nature, and provides an overview of how
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;Q&#23398;&#20064;&#30340;&#26368;&#20248;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#19988;&#31283;&#24577;&#30340;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#23384;&#22312;&#65292;&#35813;&#31574;&#30053;&#19982;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#36125;&#23572;&#26364;&#35823;&#24046;&#20197;&#33719;&#24471;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#26102;&#20351;&#29992;$L^{\infty}$-&#33539;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02165</link><description>&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;Q&#23398;&#20064;&#30340;&#26368;&#20248;&#21270;&#30740;&#31350;&#19982;&#36125;&#23572;&#26364;&#26080;&#31351;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;Q&#23398;&#20064;&#30340;&#26368;&#20248;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#19988;&#31283;&#24577;&#30340;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#23384;&#22312;&#65292;&#35813;&#31574;&#30053;&#19982;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#36125;&#23572;&#26364;&#35823;&#24046;&#20197;&#33719;&#24471;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#26102;&#20351;&#29992;$L^{\infty}$-&#33539;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#40065;&#26834;&#31574;&#30053;&#23545;&#20110;&#25269;&#24481;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26234;&#33021;&#20307;&#30340;&#25915;&#20987;&#25110;&#24178;&#25200;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29366;&#24577;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#26263;&#31034;&#20102;&#32570;&#20047;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#65288;ORP&#65289;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#36825;&#32473;&#35774;&#23450;&#20005;&#26684;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;ORP&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#20551;&#35774;&#65288;CAP&#65289;&#65292;&#35813;&#20551;&#35774;&#25351;&#20986;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#21160;&#20316;&#22312;&#24494;&#23567;&#25200;&#21160;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24471;&#21040;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#30340;&#25903;&#25345;&#12290;&#22312;CAP&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20851;&#38190;&#22320;&#35777;&#26126;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#19988;&#31283;&#24577;&#30340;ORP&#30340;&#23384;&#22312;&#65292;&#35813;ORP&#19982;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38416;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#36125;&#23572;&#26364;&#35823;&#24046;&#20197;&#33719;&#24471;ORP&#26102;&#65292;$L^{\infty}$-&#33539;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#28548;&#28165;&#20102;&#20808;&#21069;&#38024;&#23545;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#20351;&#29992;$L^{1}$-&#33539;&#25968;&#30340;DRL&#31639;&#27861;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#28608;&#21169;&#25105;&#20204;&#35757;&#32451;&#19968;&#31181;&#19968;&#33268;&#24615;&#23545;&#25239;&#40065;&#26834;&#28145;&#24230;Q&#32593;&#32476;&#65288;CA&#65289;
&lt;/p&gt;
&lt;p&gt;
Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CA
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65292;&#35813;&#25351;&#25968;&#26681;&#25454;&#29616;&#26377;&#30340;&#22522;&#30784;&#25351;&#25968;&#23450;&#20041;&#65292;&#24182;&#29992;&#20110;&#26816;&#27979;&#27425;&#20248;&#32858;&#31867;&#25968;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25351;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02162</link><description>&lt;p&gt;
&#19968;&#20010;&#36125;&#21494;&#26031;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Bayesian cluster validity index
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65292;&#35813;&#25351;&#25968;&#26681;&#25454;&#29616;&#26377;&#30340;&#22522;&#30784;&#25351;&#25968;&#23450;&#20041;&#65292;&#24182;&#29992;&#20110;&#26816;&#27979;&#27425;&#20248;&#32858;&#31867;&#25968;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25351;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#32858;&#31867;&#31639;&#27861;&#26102;&#65292;&#36873;&#25321;&#32858;&#31867;&#25968;&#26159;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65288;CVIs&#65289;&#12290;&#22823;&#22810;&#25968;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#37117;&#34987;&#23450;&#20041;&#20026;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#38544;&#34255;&#30340;&#26368;&#20248;&#32858;&#31867;&#25968;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#26377;&#26102;&#24182;&#19981;&#26399;&#26395;&#33719;&#24471;&#26368;&#20248;&#32858;&#31867;&#25968;&#65292;&#32780;&#26159;&#26356;&#36866;&#21512;&#20182;&#20204;&#24212;&#29992;&#30340;&#27425;&#20248;&#32858;&#31867;&#25968;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#22522;&#30784;&#25351;&#25968;&#30340;&#36125;&#21494;&#26031;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65288;BCVI&#65289;&#12290;&#35813;&#25351;&#25968;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#25110;&#24191;&#20041;&#29380;&#21033;&#20811;&#38647;&#20808;&#39564;&#23450;&#20041;&#65292;&#24471;&#21040;&#30456;&#21516;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#22522;&#20110;Wiroonsri&#25351;&#25968;&#65288;WI&#65289;&#21644;Wiroonsri-Preedasawakul&#25351;&#25968;&#65288;WP&#65289;&#20316;&#20026;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#30340;&#22522;&#30784;&#25351;&#25968;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;BCVI&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#30340;&#32467;&#26524;&#19982;&#21407;&#22987;&#30340;&#22522;&#30784;&#25351;&#25968;&#20197;&#21450;&#19968;&#20123;&#20854;&#20182;&#23384;&#22312;&#30340;CVIs&#65288;&#21253;&#25324;Davies and Bouldin (DB)&#65292;Starczewski (STR)&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the number of clusters is one of the key processes when applying clustering algorithms. To fulfill this task, various cluster validity indices (CVIs) have been introduced. Most of the cluster validity indices are defined to detect the optimal number of clusters hidden in a dataset. However, users sometimes do not expect to get the optimal number of groups but a secondary one which is more reasonable for their applications. This has motivated us to introduce a Bayesian cluster validity index (BCVI) based on existing underlying indices. This index is defined based on either Dirichlet or Generalized Dirichlet priors which result in the same posterior distribution. Our BCVI is then tested based on the Wiroonsri index (WI), and the Wiroonsri-Preedasawakul index (WP) as underlying indices for hard and soft clustering, respectively. We compare their outcomes with the original underlying indices, as well as a few more existing CVIs including Davies and Bouldin (DB), Starczewski (STR)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36234;&#37326;&#33258;&#21160;&#39550;&#39542;&#20998;&#21106;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#38750;&#31283;&#20581;&#29305;&#24449;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02154</link><description>&lt;p&gt;
&#35780;&#20272;&#36234;&#37326;&#33258;&#21160;&#39550;&#39542;&#20998;&#21106;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#25239;&#24178;&#25200;&#24615;&#65306;&#20197;&#25968;&#25454;&#38598;&#20026;&#20013;&#24515;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Off-Road Autonomous Driving Segmentation against Adversarial Attacks: A Dataset-Centric analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36234;&#37326;&#33258;&#21160;&#39550;&#39542;&#20998;&#21106;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#38750;&#31283;&#20581;&#29305;&#24449;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#36234;&#37326;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#65292;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#23545;&#25239;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;&#23613;&#31649;&#22312;&#36890;&#29992;&#26465;&#20214;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#29978;&#33267;&#26159;&#36739;&#23567;&#30340;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26368;&#32456;&#23548;&#33268;&#39640;&#32622;&#20449;&#24230;&#30340;&#19981;&#20934;&#30830;&#39044;&#27979;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#37325;&#28857;&#25918;&#22312;&#36890;&#36807;&#20462;&#25913;&#26550;&#26500;&#24182;&#20351;&#29992;&#22024;&#26434;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#26469;&#20351;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#65292;&#20294;&#21364;&#27809;&#26377;&#25506;&#32034;&#25968;&#25454;&#38598;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#36234;&#37326;&#25968;&#25454;&#38598;&#20013;&#30340;&#38750;&#31283;&#20581;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#23545;&#19981;&#21516;&#20998;&#21106;&#32593;&#32476;&#26550;&#26500;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21482;&#21253;&#21547;&#31283;&#20581;&#29305;&#24449;&#65292;&#24182;&#22312;&#35813;&#31283;&#20581;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32593;&#32476;&#12290;&#25105;&#20204;&#23545;&#30740;&#31350;&#32467;&#26524;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#36825;&#20123;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the vulnerability of semantic segmentation models to adversarial input perturbations, in the domain of off- road autonomous driving. Despite good performance in generic conditions, the state-of-the-art classifiers are often susceptible to (even) small perturbations, ultimately resulting in inaccurate predic- tions with high confidence. Prior research has directed their focus on making models more robust by modifying the architecture and training with noisy input images, but has not explored the influence of datasets in adversarial attacks. Our study aims to address this gap by examining the impact of non-robust features in off-road datasets and comparing the effects of adversarial attacks on different seg- mentation network architectures. To enable this, a robust dataset is created consisting of only robust features and training the net- works on this robustified dataset. We present both qualitative and quantitative analysis of our findings, which have important
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02152</link><description>&lt;p&gt;
&#35770;&#25991;&#39064;&#30446;&#65306;&#20026;&#20160;&#20040;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#20027;&#23548;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#65307;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#22788;&#20110;&#19968;&#31181;&#22855;&#29305;&#30340;&#22659;&#22320;&#12290;&#23613;&#31649;&#22312;&#36890;&#36807;A/B&#27979;&#35797;&#26469;&#34913;&#37327;&#24615;&#33021;&#26041;&#38754;&#26377;&#19968;&#20010;&#38750;&#24120;&#20005;&#26684;&#30340;&#21327;&#35758;&#65292;&#20294;&#25214;&#21040;&#35201;&#27979;&#35797;&#30340;&#8220;B&#8221;&#30340;&#26368;&#20339;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#38024;&#23545;&#24615;&#33021;&#65292;&#32780;&#26159;&#38024;&#23545;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;A/B&#27979;&#35797;&#30340;&#25104;&#21151;&#25110;&#22833;&#36133;&#23436;&#20840;&#21462;&#20915;&#20110;&#25152;&#25552;&#20986;&#30340;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#19982;&#24615;&#33021;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27809;&#26377;&#21407;&#21017;&#21487;&#20197;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#30830;&#23450;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#26356;&#22909;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#20204;&#25720;&#19981;&#30528;&#22836;&#33041;&#12290;&#26412;&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#36136;&#30097;&#36825;&#31181;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#65292;&#24182;&#20027;&#24352;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#23454;&#38469;&#19978;&#26377;&#28508;&#21147;&#35299;&#38145;&#20248;&#21270;&#22870;&#21169;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#20256;&#25773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#26041;&#27861;&#31561;&#20215;&#20110;&#23545;&#32473;&#23450;&#25193;&#25955;&#22122;&#22768;&#22270;&#20687;&#30340;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36817;&#20284;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21363;&#25554;&#21363;&#29992;&#30340;&#21518;&#39564;&#21327;&#26041;&#24046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#21453;&#21521;&#21327;&#26041;&#24046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02149</link><description>&lt;p&gt;
&#25913;&#36827;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#20256;&#25773;&#27169;&#22411;&#29992;&#20110;&#36870;&#38382;&#39064;&#65292;&#20351;&#29992;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#20256;&#25773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#26041;&#27861;&#31561;&#20215;&#20110;&#23545;&#32473;&#23450;&#25193;&#25955;&#22122;&#22768;&#22270;&#20687;&#30340;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36817;&#20284;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21363;&#25554;&#21363;&#29992;&#30340;&#21518;&#39564;&#21327;&#26041;&#24046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#21453;&#21521;&#21327;&#26041;&#24046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20256;&#25773;&#27169;&#22411;&#20026;&#22024;&#26434;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#20026;&#29305;&#23450;&#30340;&#36870;&#38382;&#39064;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#20174;&#26465;&#20214;&#25277;&#26679;&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#36817;&#20284;&#21518;&#39564;&#22343;&#20540;&#30340;&#35282;&#24230;&#65292;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#38646;&#26679;&#26412;&#26041;&#27861;&#30340;&#31532;&#19968;&#20010;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26368;&#36817;&#30340;&#26041;&#27861;&#31561;&#20215;&#20110;&#23545;&#32473;&#23450;&#25193;&#25955;&#22122;&#22768;&#22270;&#20687;&#30340;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36817;&#20284;&#65292;&#21807;&#19968;&#30340;&#24046;&#21035;&#26159;&#21508;&#21521;&#21516;&#24615;&#21518;&#39564;&#21327;&#26041;&#24046;&#30340;&#25163;&#24037;&#35774;&#35745;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#36890;&#29992;&#21363;&#25554;&#21363;&#29992;&#21518;&#39564;&#21327;&#26041;&#24046;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#21453;&#21521;&#21327;&#26041;&#24046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent diffusion models provide a promising zero-shot solution to noisy linear inverse problems without retraining for specific inverse problems. In this paper, we propose the first unified interpretation for existing zero-shot methods from the perspective of approximating the conditional posterior mean for the reverse diffusion process of conditional sampling. We reveal that recent methods are equivalent to making isotropic Gaussian approximations to intractable posterior distributions over clean images given diffused noisy images, with the only difference in the handcrafted design of isotropic posterior covariances. Inspired by this finding, we propose a general plug-and-play posterior covariance optimization based on maximum likelihood estimation to improve recent methods. To achieve optimal posterior covariance without retraining, we provide general solutions based on two approaches specifically designed to leverage pre-trained models with and without reverse covariances. Experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#28145;&#24230;&#38598;&#25104;&#26862;&#26519;&#26041;&#27861;&#22312;&#20174;AOD&#25968;&#25454;&#20272;&#35745;PM2.5&#27987;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#30456;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#31561;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.02139</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#38598;&#25104;&#26862;&#26519;&#23454;&#29616;&#22522;&#20110;MODIS MAIAC AOD&#30340;PM2.5&#39640;&#20998;&#36776;&#29575;&#21046;&#22270;&#20110;&#20234;&#26391;&#24503;&#40657;&#20848;
&lt;/p&gt;
&lt;p&gt;
Using Deep Ensemble Forest for High Resolution Mapping of PM2.5 from MODIS MAIAC AOD in Tehran, Iran
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#28145;&#24230;&#38598;&#25104;&#26862;&#26519;&#26041;&#27861;&#22312;&#20174;AOD&#25968;&#25454;&#20272;&#35745;PM2.5&#27987;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#30456;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#31561;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27745;&#26579;&#28304;&#22797;&#26434;&#19988;&#22320;&#38754;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#31449;&#25968;&#37327;&#19981;&#36275;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#32454;&#39063;&#31890;&#29289;&#65288;PM2.5&#65289;&#27987;&#24230;&#21046;&#22270;&#26159;&#24503;&#40657;&#20848;&#24066;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#21355;&#26143;&#27668;&#28342;&#33014;&#20809;&#23398;&#28145;&#24230;&#65288;AOD&#65289;&#25968;&#25454;&#26469;&#21046;&#22270;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25991;&#29486;&#20013;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23637;&#31034;&#20102;&#20854;&#20174;AOD&#25968;&#25454;&#20272;&#35745;PM2.5&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35299;&#20915;&#20174;&#21355;&#26143;AOD&#25968;&#25454;&#20272;&#35745;PM2.5&#30340;&#38382;&#39064;&#19978;&#23384;&#22312;&#19968;&#20123;&#24369;&#28857;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#28145;&#24230;&#38598;&#25104;&#26862;&#26519;&#26041;&#27861;&#22312;&#20174;AOD&#25968;&#25454;&#20272;&#35745;PM2.5&#27987;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#38598;&#25104;&#26862;&#26519;&#26041;&#27861;&#65288;R2 = 0.74&#65289;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;R2 = 0.67&#65289;&#20197;&#21450;&#32463;&#20856;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22914;&#38543;&#26426;&#26862;&#26519;&#65288;R2 = 0.68&#65289;&#20855;&#26377;&#26356;&#39640;&#30340;PM2.5&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
High resolution mapping of PM2.5 concentration over Tehran city is challenging because of the complicated behavior of numerous sources of pollution and the insufficient number of ground air quality monitoring stations. Alternatively, high resolution satellite Aerosol Optical Depth (AOD) data can be employed for high resolution mapping of PM2.5. For this purpose, different data-driven methods have been used in the literature. Recently, deep learning methods have demonstrated their ability to estimate PM2.5 from AOD data. However, these methods have several weaknesses in solving the problem of estimating PM2.5 from satellite AOD data. In this paper, the potential of the deep ensemble forest method for estimating the PM2.5 concentration from AOD data was evaluated. The results showed that the deep ensemble forest method with R2 = 0.74 gives a higher accuracy of PM2.5 estimation than deep learning methods (R2 = 0.67) as well as classic data-driven methods such as random forest (R2 = 0.68).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EvoFlow&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#36827;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#32452;&#21512;&#12290;EvoFlow&#36890;&#36807;&#22686;&#24378;&#24037;&#20316;&#27969;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#24615;&#38598;&#25104;&#31574;&#30053;&#65292;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#38656;&#27714;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EvoFlow&#22312;&#22810;&#20010;AutoML&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02124</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#36827;&#21270;&#26041;&#27861;&#29992;&#20110;&#20855;&#26377;&#29305;&#23450;&#39046;&#22495;&#25805;&#20316;&#21644;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Grammar-based evolutionary approach for automated workflow composition with domain-specific operators and ensemble diversity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EvoFlow&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#36827;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#32452;&#21512;&#12290;EvoFlow&#36890;&#36807;&#22686;&#24378;&#24037;&#20316;&#27969;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#24615;&#38598;&#25104;&#31574;&#30053;&#65292;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#38656;&#27714;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EvoFlow&#22312;&#22810;&#20010;AutoML&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#21644;&#26032;&#39062;&#35265;&#35299;&#30340;&#36807;&#31243;&#28041;&#21450;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#27493;&#39588;&#12290;&#22312;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#20013;&#65292;&#37325;&#28857;&#30740;&#31350;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#36873;&#25321;&#31639;&#27861;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#20219;&#21153;&#12290;AutoML&#20013;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26159;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#32452;&#21512;&#65288;AWC&#65289;&#12290;AWC&#26088;&#22312;&#35782;&#21035;&#26368;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24207;&#21015;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AWC&#26041;&#27861;&#22312;&#32452;&#21512;&#24037;&#20316;&#27969;&#20013;&#30340;&#31639;&#27861;&#25968;&#37327;&#21644;&#26041;&#24335;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EvoFlow&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#36827;&#21270;&#26041;&#27861;&#29992;&#20110;AWC&#12290;EvoFlow&#22686;&#24378;&#20102;&#35774;&#35745;&#24037;&#20316;&#27969;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#20204;&#29305;&#23450;&#38656;&#27714;&#30340;&#31639;&#27861;&#12290;EvoFlow&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#21019;&#26032;&#29305;&#24615;&#33073;&#39062;&#32780;&#20986;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#34920;&#31034;&#26041;&#27861;&#26469;&#25551;&#36848;&#24037;&#20316;&#27969;&#65292;&#20801;&#35768;&#20174;&#39044;&#20808;&#23450;&#20041;&#30340;&#39046;&#22495;&#29305;&#23450;&#25805;&#20316;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#31639;&#27861;&#12290;&#20854;&#27425;&#65292;&#23427;&#24341;&#20837;&#22810;&#26679;&#24615;&#38598;&#25104;&#31574;&#30053;&#26469;&#40723;&#21169;&#24037;&#20316;&#27969;&#20013;&#19981;&#21516;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EvoFlow&#22312;&#22810;&#20010;AutoML&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of extracting valuable and novel insights from raw data involves a series of complex steps. In the realm of Automated Machine Learning (AutoML), a significant research focus is on automating aspects of this process, specifically tasks like selecting algorithms and optimising their hyper-parameters. A particularly challenging task in AutoML is automatic workflow composition (AWC). AWC aims to identify the most effective sequence of data preprocessing and ML algorithms, coupled with their best hyper-parameters, for a specific dataset. However, existing AWC methods are limited in how many and in what ways they can combine algorithms within a workflow.   Addressing this gap, this paper introduces EvoFlow, a grammar-based evolutionary approach for AWC. EvoFlow enhances the flexibility in designing workflow structures, empowering practitioners to select algorithms that best fit their specific requirements. EvoFlow stands out by integrating two innovative features. First, it emplo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#30340;SAR-&#20809;&#23398;&#25968;&#25454;&#65292;&#21487;&#20197;&#22686;&#24378;&#20892;&#20316;&#29289;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02121</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#30340;SAR-&#20809;&#23398;&#25968;&#25454;&#26469;&#25552;&#39640;&#20892;&#20316;&#29289;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing crop classification accuracy by synthetic SAR-Optical data generation using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02121
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#30340;SAR-&#20809;&#23398;&#25968;&#25454;&#65292;&#21487;&#20197;&#22686;&#24378;&#20892;&#20316;&#29289;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#20316;&#29289;&#20998;&#31867;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#24050;&#25104;&#20026;&#36817;&#20960;&#21313;&#24180;&#26469;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#34701;&#21512;SAR&#21644;&#20809;&#23398;&#24433;&#20687;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#36825;&#20250;&#23545;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#20892;&#19994;&#22320;&#21306;&#65292;&#20027;&#35201;&#20316;&#29289;&#36890;&#24120;&#21253;&#25324;&#19968;&#20004;&#31181;&#29305;&#23450;&#31867;&#22411;&#65292;&#32780;&#20854;&#20182;&#20316;&#29289;&#21017;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#25910;&#38598;&#35757;&#32451;&#26679;&#26412;&#20197;&#21019;&#24314;&#20892;&#20135;&#21697;&#22320;&#22270;&#26102;&#65292;&#20027;&#35201;&#20316;&#29289;&#30340;&#26679;&#26412;&#20805;&#35029;&#65292;&#24418;&#25104;&#22823;&#37096;&#20998;&#31867;&#21035;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#20316;&#29289;&#30340;&#26679;&#26412;&#24456;&#23569;&#65292;&#20195;&#34920;&#30528;&#23569;&#25968;&#31867;&#21035;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#20811;&#26381;&#20256;&#32479;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#24369;&#28857;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crop classification using remote sensing data has emerged as a prominent research area in recent decades. Studies have demonstrated that fusing SAR and optical images can significantly enhance the accuracy of classification. However, a major challenge in this field is the limited availability of training data, which adversely affects the performance of classifiers. In agricultural regions, the dominant crops typically consist of one or two specific types, while other crops are scarce. Consequently, when collecting training samples to create a map of agricultural products, there is an abundance of samples from the dominant crops, forming the majority classes. Conversely, samples from other crops are scarce, representing the minority classes. Addressing this issue requires overcoming several challenges and weaknesses associated with traditional data generation methods. These methods have been employed to tackle the imbalanced nature of the training data. Nevertheless, they still face lim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25237;&#24433;&#26080;&#20851;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#20013;&#30340;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#20013;&#24515;&#21270;&#21644;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#37117;&#33021;&#36798;&#21040;O(\sqrt{B})&#30340;&#36951;&#25022;&#30028;&#65292;&#23545;&#20110;&#24310;&#36831;&#35774;&#32622;&#20013;&#30340;OCO&#38382;&#39064;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02114</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#20013;&#22788;&#29702;&#24310;&#36831;&#21453;&#39304;&#30340;&#25237;&#24433;&#26080;&#20851;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Handling Delayed Feedback in Distributed Online Optimization : A Projection-Free Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25237;&#24433;&#26080;&#20851;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#20013;&#30340;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#20013;&#24515;&#21270;&#21644;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#37117;&#33021;&#36798;&#21040;O(\sqrt{B})&#30340;&#36951;&#25022;&#30028;&#65292;&#23545;&#20110;&#24310;&#36831;&#35774;&#32622;&#20013;&#30340;OCO&#38382;&#39064;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26412;&#22320;&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#36793;&#32536;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#65292;&#38656;&#35201;&#31616;&#21333;&#65288;&#20197;&#20415;&#26412;&#22320;&#35774;&#22791;&#21487;&#20197;&#25191;&#34892;&#65289;&#21644;&#21487;&#38752;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#19981;&#30830;&#23450;&#24615;&#21644;&#32593;&#32476;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24310;&#36831;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23545;&#25239;&#24615;&#24310;&#36831;&#21453;&#39304;&#19979;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25237;&#24433;&#26080;&#20851;&#31639;&#27861;&#65292;&#29992;&#20110;&#20013;&#24515;&#21270;&#21644;&#20998;&#24067;&#24335;&#35774;&#32622;&#65292;&#24182;&#20180;&#32454;&#35774;&#35745;&#20197;&#23454;&#29616;O(\sqrt{B})&#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;B&#26159;&#24310;&#36831;&#24635;&#21644;&#65292;&#23545;&#20110;&#24310;&#36831;&#35774;&#32622;&#20013;&#30340;OCO&#38382;&#39064;&#26469;&#35828;&#26159;&#26368;&#20248;&#30340;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#25237;&#24433;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#30340;&#27604;&#36739;&#26469;&#23454;&#39564;&#39564;&#35777;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning at the edges has become increasingly important as large quantities of data are continually generated locally. Among others, this paradigm requires algorithms that are simple (so that they can be executed by local devices), robust (again uncertainty as data are continually generated), and reliable in a distributed manner under network issues, especially delays. In this study, we investigate the problem of online convex optimization under adversarial delayed feedback. We propose two projection-free algorithms for centralised and distributed settings in which they are carefully designed to achieve a regret bound of O(\sqrt{B}) where B is the sum of delay, which is optimal for the OCO problem in the delay setting while still being projection-free. We provide an extensive theoretical study and experimentally validate the performance of our algorithms by comparing them with existing ones on real-world problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02111</link><description>&lt;p&gt;
&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#65306;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#23601;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;(MLMC)&#26469;&#25552;&#39640;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#22810;&#27493;&#21069;&#30651;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26222;&#36890;&#33945;&#29305;&#21345;&#27931;&#30340;&#22797;&#26434;&#24230;&#22312;&#23884;&#22871;&#25805;&#20316;&#20013;&#20250;&#38477;&#20302;&#65292;&#32780;MLMC&#33021;&#22815;&#20197;&#35268;&#33539;&#33945;&#29305;&#21345;&#27931;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#32500;&#24230;&#21644;&#24179;&#28369;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#19968;&#27493;&#21644;&#20004;&#27493;&#21069;&#30651;&#37319;&#38598;&#20989;&#25968;&#30340;&#36817;&#20284;&#25913;&#36827;&#65292;&#20294;&#27491;&#22914;&#25105;&#20204;&#25152;&#35752;&#35770;&#30340;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#31181;&#26041;&#38754;&#26159;&#21487;&#25512;&#24191;&#30340;&#65292;&#21253;&#25324;&#36229;&#36234;BO&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;MLMC&#22312;BO&#20013;&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#22312;&#36825;&#37324;&#33719;&#21462;&#65306;https://github.com/Shangda-Yang/MLMCBO&#12290;
&lt;/p&gt;
&lt;p&gt;
We leverage multilevel Monte Carlo (MLMC) to improve the performance of multi-step look-ahead Bayesian optimization (BO) methods that involve nested expectations and maximizations. The complexity rate of naive Monte Carlo degrades for nested operations, whereas MLMC is capable of achieving the canonical Monte Carlo convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. Our theoretical study focuses on the approximation improvements for one- and two-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of BO. Findings are verified numerically and the benefits of MLMC for BO are illustrated on several benchmark examples. Code is available here https://github.com/Shangda-Yang/MLMCBO.
&lt;/p&gt;</description></item><item><title>&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#20026;&#22810;&#39046;&#22495;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.02110</link><description>&lt;p&gt;
&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#65306;&#22312;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#30340;&#22810;&#39046;&#22495;&#20027;&#21160;&#23398;&#20064;&#20013;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02110
&lt;/p&gt;
&lt;p&gt;
&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#20026;&#22810;&#39046;&#22495;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#37327;&#26368;&#22823;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#65292;&#20174;&#32780;&#22312;&#22266;&#23450;&#30340;&#26631;&#27880;&#39044;&#31639;&#20869;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;AL&#26041;&#27861;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#24773;&#20917;&#65292;&#21363;&#25152;&#26377;&#25968;&#25454;&#37117;&#26469;&#33258;&#21516;&#19968;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#21516;&#19968;&#25968;&#25454;&#38598;&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#24448;&#24448;&#28041;&#21450;&#22810;&#20010;&#39046;&#22495;&#12290;&#20363;&#22914;&#65292;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#65292;&#36890;&#24120;&#24076;&#26395;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#22312;&#19981;&#21516;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#32972;&#26223;&#65289;&#20013;&#24037;&#20316;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#27599;&#20010;&#29615;&#22659;&#30340;&#22270;&#20687;&#26500;&#25104;&#19968;&#20010;&#39046;&#22495;&#12290;&#36825;&#31181;&#22810;&#39046;&#22495;AL&#35774;&#32622;&#23545;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#65288;1&#65289;&#22312;&#20998;&#37197;&#26631;&#27880;&#39044;&#31639;&#26102;&#24573;&#35270;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#65288;2&#65289;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#65292;&#31216;&#20026;&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#65292;&#29992;&#20110;&#22810;&#39046;&#22495;AL&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#24335;&#22320;&#32771;&#34385;&#20102;&#38382;&#39064;&#20013;&#30340;&#39046;&#22495;&#23618;&#32423;&#21644;&#23454;&#20363;&#23618;&#32423;&#20449;&#24687;&#65307;CAL&#39318;&#20808;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to improve model performance within a fixed labeling budget by choosing the most informative data points to label. Existing AL focuses on the single-domain setting, where all data come from the same domain (e.g., the same dataset). However, many real-world tasks often involve multiple domains. For example, in visual recognition, it is often desirable to train an image classifier that works across different environments (e.g., different backgrounds), where images from each environment constitute one domain. Such a multi-domain AL setting is challenging for prior methods because they (1) ignore the similarity among different domains when assigning labeling budget and (2) fail to handle distribution shift of data across different domains. In this paper, we propose the first general method, dubbed composite active learning (CAL), for multi-domain AL. Our approach explicitly considers the domain-level and instance-level information in the problem; CAL first assigns
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;Agda&#29983;&#24577;&#31995;&#32479;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#36182;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#30340;&#35777;&#26126;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#32780;&#38750;&#21629;&#21517;&#21407;&#21017;&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#20381;&#36182;&#31867;&#22411;&#31243;&#24207;&#65292;&#24182;&#22312;&#21069;&#25552;&#36873;&#25321;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02104</link><description>&lt;p&gt;
&#23398;&#20064;&#20381;&#36182;&#31867;&#22411;&#30340;&#32467;&#26500;&#24863;&#30693;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Structure-Aware Representations of Dependent Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;Agda&#29983;&#24577;&#31995;&#32479;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#36182;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#30340;&#35777;&#26126;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#32780;&#38750;&#21629;&#21517;&#21407;&#21017;&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#20381;&#36182;&#31867;&#22411;&#31243;&#24207;&#65292;&#24182;&#22312;&#21069;&#25552;&#36873;&#25321;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Agda&#26159;&#19968;&#31181;&#20381;&#36182;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#21644;&#35777;&#26126;&#21161;&#25163;&#65292;&#22312;&#35777;&#26126;&#24418;&#24335;&#21270;&#21644;&#32534;&#31243;&#35821;&#35328;&#29702;&#35770;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#23558;Agda&#29983;&#24577;&#31995;&#32479;&#25193;&#23637;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#21453;&#36807;&#26469;&#20351;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#20351;&#29992;Agda&#30456;&#20851;&#36164;&#28304;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#21457;&#24067;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Agda&#31243;&#24207;&#35777;&#26126;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26082;&#35814;&#23613;&#21448;&#24191;&#27867;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#36825;&#26159;&#39318;&#20010;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#65292;&#35814;&#32454;&#23637;&#31034;&#20102;&#20122;&#22411;&#32423;&#21035;&#30340;&#35777;&#26126;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#32780;&#19981;&#26159;&#21629;&#21517;&#21407;&#21017;&#20934;&#30830;&#34920;&#31034;&#20381;&#36182;&#31867;&#22411;&#31243;&#24207;&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#21069;&#25552;&#36873;&#25321;&#35774;&#32622;&#20013;&#23454;&#20363;&#21270;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26550;&#26500;&#65292;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory. This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners. We introduce and release a novel dataset of Agda program-proofs that is elaborate and extensive enough to support various machine learning applications -- the first of its kind. Leveraging the dataset's ultra-high resolution, detailing proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles. We instantiate and evaluate our architecture in a premise selection setup, where it achieves strong initial results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;VLMs&#65292;&#27169;&#22411;&#30830;&#23454;&#20250;&#20445;&#30041;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#25991;&#26412;&#38543;&#26426;&#21270;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#35760;&#24518;&#29616;&#35937;&#32780;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.02103</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
D\'ej\`a Vu Memorization in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02103
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;VLMs&#65292;&#27169;&#22411;&#30830;&#23454;&#20250;&#20445;&#30041;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#25991;&#26412;&#38543;&#26426;&#21270;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#35760;&#24518;&#29616;&#35937;&#32780;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#20855;&#26377;&#35832;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20250;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20063;&#23545;&#27867;&#21270;&#26377;&#30528;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;VLMs&#20013;&#35760;&#24518;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;&#12290;&#23545;&#20110;&#22312;&#22270;&#20687;-&#26631;&#39064;&#23545;&#19978;&#35757;&#32451;&#30340;VLMs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30830;&#23454;&#20445;&#30041;&#20102;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#36229;&#20986;&#20102;&#20174;&#30456;&#20851;&#24615;&#25110;&#22270;&#20687;&#26631;&#39064;&#20013;&#21487;&#20197;&#25512;&#26029;&#20986;&#30340;&#33539;&#30068;&#12290;&#25105;&#20204;&#22312;&#26679;&#26412;&#21644;&#24635;&#20307;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;&#65292;&#24182;&#23637;&#31034;&#20102;OpenCLIP&#22312;&#22810;&#36798;5000&#19975;&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#19978;&#35757;&#32451;&#26102;&#30340;&#26174;&#33879;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25991;&#26412;&#38543;&#26426;&#21270;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#35760;&#24518;&#65292;&#21516;&#26102;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#20102;&#36866;&#24230;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\'ej\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\'ej\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#21457;&#29616;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#38750;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.02099</link><description>&lt;p&gt;
&#20998;&#26512;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02099
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#21457;&#29616;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#38750;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20284;&#20046;&#26174;&#31034;&#20986;&#22312;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#35774;&#32622;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#38646;-shot&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#31243;&#24230;&#34920;&#31034;&#36136;&#30097;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#23454;&#20363;&#65292;&#25361;&#25112;&#20102;&#39640;&#38646;-shot&#24615;&#33021;&#22312;&#30446;&#26631;&#20219;&#21153;&#20013;&#21453;&#26144;&#39640;&#36328;&#35821;&#35328;&#33021;&#21147;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#19981;&#38656;&#35201;&#36716;&#31227;&#23454;&#38469;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#34987;&#24573;&#35270;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;QK&#29305;&#24449;&#20540;&#35889;&#30340;&#38598;&#20013;&#23450;&#20301;&#29616;&#35937;&#26469;&#35299;&#20915;&#19981;&#21516;&#35266;&#28857;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.02098</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#32593;&#32476;&#22312;QK&#29305;&#24449;&#20540;&#35889;&#38598;&#20013;&#26102;&#36827;&#34892;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Self-attention Networks Localize When QK-eigenspectrum Concentrates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;QK&#29305;&#24449;&#20540;&#35889;&#30340;&#38598;&#20013;&#23450;&#20301;&#29616;&#35937;&#26469;&#35299;&#20915;&#19981;&#21516;&#35266;&#28857;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#12290;&#23427;&#20855;&#26377;&#36866;&#24212;&#24615;&#36873;&#25321;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#27880;&#24847;&#21147;&#23450;&#20301;&#30340;&#31243;&#24230;&#26469;&#23454;&#29616;&#65292;&#36825;&#34987;&#24456;&#22810;&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#26159;&#24378;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#22522;&#30784;&#65292;&#20294;&#20063;&#22797;&#26434;&#21270;&#20102;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#20027;&#35201;&#26377;&#20004;&#31181;&#35266;&#28857;&#23558;&#27880;&#24847;&#21147;&#23450;&#20301;&#19982;&#27169;&#22411;&#24615;&#33021;&#32852;&#31995;&#36215;&#26469;&#12290;&#19968;&#31181;&#35266;&#28857;&#26159;&#31209;&#22349;&#32553;&#65292;&#21363;&#33258;&#27880;&#24847;&#22359;&#23884;&#20837;&#30340;&#26631;&#35760;&#22312;&#19981;&#21516;&#30340;&#26631;&#35760;&#20043;&#38388;&#21464;&#24471;&#38750;&#24120;&#30456;&#20284;&#65292;&#23548;&#33268;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#38477;&#20302;&#12290;&#21478;&#19968;&#31181;&#35266;&#28857;&#26159;&#29109;&#22349;&#32553;&#65292;&#21363;&#27880;&#24847;&#27010;&#29575;&#25509;&#36817;&#38750;&#22343;&#21248;&#19988;&#29109;&#20302;&#65292;&#20351;&#24471;&#23398;&#20064;&#21160;&#21147;&#23398;&#26356;&#23481;&#26131;&#38519;&#20837;&#24179;&#21488;&#26399;&#12290;&#36825;&#20004;&#31181;&#22833;&#25928;&#27169;&#24335;&#20284;&#20046;&#30456;&#20114;&#30683;&#30462;&#65292;&#22240;&#20026;&#31209;&#21644;&#29109;&#22349;&#32553;&#20998;&#21035;&#19982;&#22343;&#21248;&#21644;&#38750;&#22343;&#21248;&#27880;&#24847;&#21147;&#30456;&#20851;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;QK&#29305;&#24449;&#20540;&#35889;&#30340;&#38598;&#20013;&#23450;&#20301;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02097</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#39062;&#24615;&#20849;&#20139;&#35299;&#20915;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#21327;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#26159;&#20840;&#23616;&#29366;&#24577;&#30340;&#26032;&#39062;&#24615;&#19981;&#21487;&#29992;&#65292;&#32780;&#23616;&#37096;&#35266;&#23519;&#30340;&#26032;&#39062;&#24615;&#23384;&#22312;&#20559;&#24046;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#26234;&#33021;&#20307;&#22914;&#20309;&#21327;&#35843;&#22320;&#36827;&#34892;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;MACE&#12290;&#36890;&#36807;&#20165;&#20256;&#25773;&#23616;&#37096;&#26032;&#39062;&#24615;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#32771;&#34385;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#32047;&#35745;&#26032;&#39062;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#20869;&#22312;&#22238;&#25253;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25506;&#32034;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#19977;&#31181;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#25200;&#21160;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36755;&#20837;&#32500;&#24230;&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#26063;&#36890;&#29992;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#19968;&#33268;&#22320;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#20854;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02095</link><description>&lt;p&gt;
&#30524;&#35265;&#26410;&#24517;&#20026;&#23454;&#65306;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Seeing is not always believing: The Space of Harmless Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02095
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#25200;&#21160;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36755;&#20837;&#32500;&#24230;&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#26063;&#36890;&#29992;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#19968;&#33268;&#22320;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#20854;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#21363;&#25200;&#21160;&#20250;&#20351;&#32593;&#32476;&#36755;&#20986;&#23436;&#20840;&#19981;&#21464;&#12290;&#26080;&#35770;&#36825;&#20123;&#25200;&#21160;&#22312;&#24212;&#29992;&#20110;&#22270;&#20687;&#26102;&#30340;&#22823;&#23567;&#22914;&#20309;&#65292;&#21482;&#35201;&#23427;&#20204;&#20301;&#20110;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#20869;&#65292;&#23601;&#19981;&#20250;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32593;&#32476;&#20013;&#30340;&#20219;&#20309;&#32447;&#24615;&#23618;&#65292;&#36755;&#20837;&#32500;&#24230;$n$&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;$m$&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36830;&#32493;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#20854;&#32500;&#24230;&#20026;$(n-m)$&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#26063;&#19968;&#33268;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#30340;&#36890;&#29992;&#25200;&#21160;&#65292;&#32780;&#19981;&#35770;&#23427;&#20204;&#30340;&#22823;&#23567;&#22914;&#20309;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#23475;&#25200;&#21160;&#22312;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#20351;&#29992;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#34987;&#20154;&#31867;&#25429;&#25417;&#21040;&#30340;&#37325;&#35201;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of deep neural networks, we expose the existence of a harmless perturbation space, where perturbations leave the network output entirely unaltered. Perturbations within this harmless perturbation space, regardless of their magnitude when applied to images, exhibit no impact on the network's outputs of the original images. Specifically, given any linear layer within the network, where the input dimension $n$ exceeds the output dimension $m$, we demonstrate the existence of a continuous harmless perturbation subspace with a dimension of $(n-m)$. Inspired by this, we solve for a family of general perturbations that consistently influence the network output, irrespective of their magnitudes. With these theoretical findings, we explore the application of harmless perturbations for privacy-preserving data usage. Our work reveals the difference between DNNs and human perception that the significant perturbations captured by humans may not affect the recognition of DNNs. As a re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#22122;&#22768;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20943;&#23569;&#35823;&#23548;&#24182;&#20174;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#24212;&#23545;&#22122;&#22768;&#26679;&#26412;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02081</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#25193;&#25955;&#65306;&#20174;&#24102;&#22122;&#22768;&#26679;&#26412;&#23398;&#20064;&#28508;&#22312;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Diffusion: Learning the Underlying Distribution from Noisy Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#22122;&#22768;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20943;&#23569;&#35823;&#23548;&#24182;&#20174;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#24212;&#23545;&#22122;&#22768;&#26679;&#26412;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#23384;&#22312;&#22122;&#22768;&#26679;&#26412;&#26102;&#24456;&#33030;&#24369;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#37327;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#29615;&#22659;&#19981;&#20687;&#22270;&#20687;&#21512;&#25104;&#37027;&#26679;&#24178;&#20928;&#12290;&#21463;&#21040;&#25105;&#20204;&#23545;&#22122;&#22768;&#26679;&#26412;&#19982;&#24178;&#20928;&#26679;&#26412;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20998;&#24067;&#24046;&#24322;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39118;&#38505;&#25935;&#24863;&#30340;SDE&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#39118;&#38505;&#65288;&#21363;&#25968;&#25454;&#8220;&#33039;&#20081;&#24230;&#8221;&#65289;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#29992;&#20110;&#35843;&#25972;&#22122;&#22768;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20943;&#23569;&#35823;&#23548;&#24182;&#20174;&#20013;&#33719;&#21462;&#20449;&#24687;&#12290;&#39118;&#38505;&#25935;&#24863;&#30340;SDE&#30340;&#26368;&#20248;&#34920;&#36798;&#24335;&#21462;&#20915;&#20110;&#29305;&#23450;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#26368;&#23567;&#21270;&#39640;&#26031;&#21644;&#19968;&#33324;&#38750;&#39640;&#26031;&#25200;&#21160;&#35823;&#23548;&#30340;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#22914;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#65289;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
While achieving remarkable performances, we show that diffusion models are fragile to the presence of noisy samples, limiting their potential in the vast amount of settings where, unlike image synthesis, we are not blessed with clean data. Motivated by our finding that such fragility originates from the distribution gaps between noisy and clean samples along the diffusion process, we introduce risk-sensitive SDE, a stochastic differential equation that is parameterized by the risk (i.e., data "dirtiness") to adjust the distributions of noisy samples, reducing misguidance while benefiting from their contained information. The optimal expression for risk-sensitive SDE depends on the specific noise distribution, and we derive its parameterizations that minimize the misguidance of noisy samples for both Gaussian and general non-Gaussian perturbations. We conduct extensive experiments on both synthetic and real-world datasets (e.g., medical time series), showing that our model effectively r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26080;&#38597;&#21508;&#24067;&#36870;&#21521;&#20256;&#25773;&#26041;&#27861;&#26469;&#35757;&#32451;&#38544;&#24335;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#21435;&#27169;&#31946;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#30340;&#21069;&#39304;&#32593;&#32476;&#21644;&#29616;&#26377;&#30340;&#38544;&#24335;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02065</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#38597;&#21508;&#24067;&#36870;&#21521;&#20256;&#25773;&#35757;&#32451;&#38544;&#24335;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#21435;&#27169;&#31946;
&lt;/p&gt;
&lt;p&gt;
Training Implicit Networks for Image Deblurring using Jacobian-Free Backpropagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26080;&#38597;&#21508;&#24067;&#36870;&#21521;&#20256;&#25773;&#26041;&#27861;&#26469;&#35757;&#32451;&#38544;&#24335;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#21435;&#27169;&#31946;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#30340;&#21069;&#39304;&#32593;&#32476;&#21644;&#29616;&#26377;&#30340;&#38544;&#24335;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24212;&#29992;&#38544;&#24335;&#32593;&#32476;&#35299;&#20915;&#25104;&#20687;&#21453;&#38382;&#39064;&#26041;&#38754;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#19982;&#21069;&#39304;&#32593;&#32476;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#29978;&#33267;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#38544;&#24335;&#32593;&#32476;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#21482;&#38656;&#35201;&#22266;&#23450;&#20869;&#23384;&#65292;&#32780;&#19981;&#21463;&#23618;&#25968;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#23481;&#26131;&#35757;&#32451;&#12290;&#26799;&#24230;&#35745;&#31639;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#38656;&#35201;&#36890;&#36807;&#22266;&#23450;&#28857;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#30001;&#22266;&#23450;&#28857;&#36845;&#20195;&#20013;&#30340;&#29305;&#24449;&#25968;&#20915;&#23450;&#22823;&#23567;&#30340;&#22823;&#22411;&#32447;&#24615;&#26041;&#31243;&#32452;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#38597;&#21508;&#24067;&#36870;&#21521;&#20256;&#25773;&#65288;JFB&#65289;&#65292;&#22312;&#22270;&#20687;&#21435;&#27169;&#31946;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;JFB&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#19982;&#20248;&#21270;&#26041;&#26696;&#12289;&#26368;&#20808;&#36827;&#30340;&#21069;&#39304;&#32593;&#32476;&#21644;&#29616;&#26377;&#30340;&#38544;&#24335;&#32593;&#32476;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts in applying implicit networks to solve inverse problems in imaging have achieved competitive or even superior results when compared to feedforward networks. These implicit networks only require constant memory during backpropagation, regardless of the number of layers. However, they are not necessarily easy to train. Gradient calculations are computationally expensive because they require backpropagating through a fixed point. In particular, this process requires solving a large linear system whose size is determined by the number of features in the fixed point iteration. This paper explores a recently proposed method, Jacobian-free Backpropagation (JFB), a backpropagation scheme that circumvents such calculation, in the context of image deblurring problems. Our results show that JFB is comparable against fine-tuned optimization schemes, state-of-the-art (SOTA) feedforward networks, and existing implicit networks at a reduced computational cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21069;&#30651;&#35299;&#30721;&#30340;&#31934;&#30830;&#12289;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#25442;&#27599;&#27493;&#25805;&#20316;&#25968;&#20197;&#20943;&#23569;&#24635;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#21152;&#36895;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#23427;&#19981;&#38656;&#35201;&#36741;&#21161;&#27169;&#22411;&#25110;&#25968;&#25454;&#23384;&#20648;&#65292;&#24182;&#19988;&#19982;&#24182;&#21457;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#65292;&#21069;&#30651;&#35299;&#30721;&#21487;&#23558;&#33258;&#22238;&#24402;&#35299;&#30721;&#21152;&#36895;1.8&#20493;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;GPU&#19978;&#23454;&#29616;&#24378;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02057</link><description>&lt;p&gt;
&#25171;&#30772;LLM&#25512;&#29702;&#30340;&#39034;&#24207;&#20381;&#36182;&#65306;&#20351;&#29992;&#21069;&#30651;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Break the Sequential Dependency of LLM Inference Using Lookahead Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21069;&#30651;&#35299;&#30721;&#30340;&#31934;&#30830;&#12289;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#25442;&#27599;&#27493;&#25805;&#20316;&#25968;&#20197;&#20943;&#23569;&#24635;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#21152;&#36895;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#23427;&#19981;&#38656;&#35201;&#36741;&#21161;&#27169;&#22411;&#25110;&#25968;&#25454;&#23384;&#20648;&#65292;&#24182;&#19988;&#19982;&#24182;&#21457;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#65292;&#21069;&#30651;&#35299;&#30721;&#21487;&#23558;&#33258;&#22238;&#24402;&#35299;&#30721;&#21152;&#36895;1.8&#20493;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;GPU&#19978;&#23454;&#29616;&#24378;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#21463;&#21040;&#20869;&#23384;&#24102;&#23485;&#38480;&#21046;&#65292;&#23548;&#33268;&#24310;&#36831;&#36739;&#39640;&#65292;&#24182;&#19988;&#28010;&#36153;&#20102;&#29616;&#20195;&#21152;&#36895;&#22120;&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#21152;&#36895;LLM&#35299;&#30721;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#33609;&#31295;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#25512;&#27979;&#35299;&#30721;&#65289;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#19981;&#26131;&#33719;&#21462;&#19988;&#26080;&#27861;&#25512;&#24191;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21069;&#30651;&#35299;&#30721;&#65292;&#19968;&#31181;&#31934;&#30830;&#30340;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#21152;&#36895;LLM&#35299;&#30721;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#27169;&#22411;&#25110;&#25968;&#25454;&#23384;&#20648;&#12290;&#23427;&#20801;&#35768;&#20132;&#25442;&#27599;&#27493;log&#65288;FLOPs&#65289;&#20197;&#20943;&#23569;&#24635;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#22312;&#21333;&#20010;&#25110;&#22810;&#20010;&#29616;&#20195;&#21152;&#36895;&#22120;&#19978;&#26356;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#19982;&#24182;&#21457;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#20363;&#22914;FlashAttention&#65289;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#21069;&#30651;&#35299;&#30721;&#23454;&#29616;&#21487;&#20197;&#22312;MT-bench&#19978;&#21152;&#36895;&#33258;&#22238;&#24402;&#35299;&#30721;1.8&#20493;&#65292;&#24182;&#22312;&#22810;&#20010;GPU&#19978;&#23454;&#29616;&#24378;&#25193;&#23637;&#24615;&#65292;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#19978;&#21152;&#36895;4&#20493;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/hao-ai-lab/LookaheadDecoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#29702;&#35770;&#21407;&#21017;&#25903;&#25345;&#30340;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;(Variance Alignment Score&#65292;VAS)&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#30340;VAS&#26469;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.02055</link><description>&lt;p&gt;
&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;: &#19968;&#31181;&#31616;&#21333;&#20294;&#38590;&#20197;&#36229;&#36234;&#30340;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#29702;&#35770;&#21407;&#21017;&#25903;&#25345;&#30340;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;(Variance Alignment Score&#65292;VAS)&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#30340;VAS&#26469;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#36873;&#25321;&#24050;&#32463;&#25104;&#20026;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22024;&#26434;&#30340;&#32593;&#32476;&#25277;&#26679;&#25968;&#25454;&#38598;&#19978;&#12290;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#31574;&#30053;&#26159;&#20026;&#27599;&#20010;&#26679;&#26412;&#20998;&#37197;&#36136;&#37327;&#20998;&#25968;&#65292;&#22914;CLIP&#30456;&#20284;&#24230;&#65292;&#24182;&#20445;&#30041;&#20855;&#26377;&#26368;&#39640;&#20998;&#25968;&#30340;&#25968;&#25454;&#23545;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#20998;&#24067;&#26159;&#26080;&#30693;&#30340;&#65292;&#22987;&#32456;&#26080;&#27861;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#29702;&#35770;&#21407;&#21017;&#25903;&#25345;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;(Variance Alignment Score&#65292;VAS)&#65292;&#23427;&#30340;&#24418;&#24335;&#26159;$\langle \Sigma_{\text{test}}, \Sigma_i\rangle$&#12290;&#36825;&#37324;&#65292;$\Sigma_{\text{test}}$&#34920;&#31034;&#25105;&#20204;&#24076;&#26395;&#23545;&#40784;&#30340;&#30446;&#26631;&#65288;&#20132;&#21449;&#65289;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#33021;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;$\Sigma_i$&#34920;&#31034;&#31532;$i$&#20010;&#26679;&#26412;&#30340;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#24352;&#37327;&#31215;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#24635;&#30340;VAS&#12290;&#25105;&#20204;&#22312;&#31616;&#21270;&#30340;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#20010;&#26041;&#27861;&#30340;&#29702;&#35770;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here, $\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.02054</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Neural Scaling Laws on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21464;&#25442;&#22120;&#65289;&#24050;&#25104;&#20026;&#21033;&#29992;&#21508;&#31181;&#31867;&#22411;&#22270;&#30340;&#30693;&#35782;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#32553;&#25918;&#29305;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#65292;&#23545;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26469;&#23454;&#29616;&#22823;&#22411;&#22270;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#32034;&#20102;&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#36825;&#20123;&#23450;&#24459;&#22312;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#25551;&#36848;&#32553;&#25918;&#34892;&#20026;&#30340;&#20844;&#24335;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#30830;&#23450;&#20102;&#36807;&#25311;&#21512;&#21487;&#33021;&#26159;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#32553;&#25918;&#34892;&#20026;&#65292;&#36825;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#35266;&#23519;&#32467;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25105;&#20204;&#24314;&#35758;&#22270;&#25968;&#37327;&#26080;&#27861;&#26377;&#25928;&#34913;&#37327;&#22270;&#25968;&#25454;&#37327;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23380;&#38592;&#20132;&#37197;&#27010;&#24565;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20113;&#29615;&#22659;&#23433;&#20840;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02052</link><description>&lt;p&gt;
&#20351;&#29992;&#23380;&#38592;&#20132;&#37197;&#27010;&#24565;&#36827;&#34892;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Feature Selection using the concept of Peafowl Mating in IDS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23380;&#38592;&#20132;&#37197;&#27010;&#24565;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20113;&#29615;&#22659;&#23433;&#20840;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#20114;&#32852;&#32593;&#30340;&#26381;&#21153;&#65292;&#20381;&#36182;&#20110;&#20849;&#20139;&#35745;&#31639;&#36164;&#28304;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36866;&#29992;&#24615;&#12290;&#20113;&#35745;&#31639;&#25552;&#20379;&#22522;&#30784;&#35774;&#26045;&#12289;&#24179;&#21488;&#21644;&#36719;&#20214;&#26381;&#21153;&#12290;&#35813;&#25216;&#26415;&#30340;&#27969;&#34892;&#26159;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12289;&#39640;&#27700;&#24179;&#30340;&#35745;&#31639;&#33021;&#21147;&#12289;&#20302;&#25104;&#26412;&#30340;&#26381;&#21153;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#20113;&#29615;&#22659;&#20013;&#65292;&#25968;&#25454;&#30340;&#21487;&#33719;&#21462;&#24615;&#21644;&#24320;&#25918;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#26816;&#27979;&#25915;&#20987;&#24182;&#30830;&#20445;&#20449;&#24687;&#23433;&#20840;&#65292;&#20351;&#29992;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33268;&#32780;&#39640;&#25928;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#22312;&#20113;&#29615;&#22659;&#20013;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#20445;&#35777;&#27700;&#24179;&#12290;&#26412;&#25991;&#23558;&#23380;&#38592;&#30340;&#20132;&#37197;&#34892;&#20026;&#32435;&#20837;&#21040;&#19968;&#20010;&#20248;&#21270;&#31639;&#27861;&#20013;&#65292;&#35813;&#31639;&#27861;&#36827;&#32780;&#29992;&#20316;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#29992;&#20110;&#20943;&#23567;&#20113;&#25968;&#25454;&#30340;&#24040;&#22823;&#22823;&#23567;&#65292;&#20351;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#33021;&#22815;&#27491;&#24120;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud computing has high applicability as an Internet based service that relies on sharing computing resources. Cloud computing provides services that are Infrastructure based, Platform based and Software based. The popularity of this technology is due to its superb performance, high level of computing ability, low cost of services, scalability, availability and flexibility. The obtainability and openness of data in cloud environment make it vulnerable to the world of cyber-attacks. To detect the attacks Intrusion Detection System is used, that can identify the attacks and ensure information security. Such a coherent and proficient Intrusion Detection System is proposed in this paper to achieve higher certainty levels regarding safety in cloud environment. In this paper, the mating behavior of peafowl is incorporated into an optimization algorithm which in turn is used as a feature selection algorithm. The algorithm is used to reduce the huge size of cloud data so that the IDS can work
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#21270;&#20026;&#38750;&#32447;&#24615;&#22495;&#24182;&#21033;&#29992;&#23398;&#20064;&#26426;&#21046;&#26500;&#24314;&#33258;&#34920;&#31034;&#30697;&#38453;&#65292;&#21516;&#26102;&#24341;&#20837;&#23616;&#37096;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;&#20197;&#22686;&#24378;&#20998;&#32452;&#25928;&#24212;&#65292;&#25913;&#21892;&#32858;&#31867;&#32467;&#26524;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#20445;&#35777;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21069;&#25552;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02051</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Nonlinear subspace clustering by functional link neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#21270;&#20026;&#38750;&#32447;&#24615;&#22495;&#24182;&#21033;&#29992;&#23398;&#20064;&#26426;&#21046;&#26500;&#24314;&#33258;&#34920;&#31034;&#30697;&#38453;&#65292;&#21516;&#26102;&#24341;&#20837;&#23616;&#37096;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;&#20197;&#22686;&#24378;&#20998;&#32452;&#25928;&#24212;&#65292;&#25913;&#21892;&#32858;&#31867;&#32467;&#26524;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#20445;&#35777;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21069;&#25552;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#27604;&#19968;&#20123;&#20808;&#36827;&#30340;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#38656;&#35201;&#36827;&#34892;&#24179;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#21270;&#20026;&#38750;&#32447;&#24615;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#26426;&#21046;&#21033;&#29992;&#26144;&#23556;&#26679;&#26412;&#26500;&#24314;&#33258;&#34920;&#31034;&#30697;&#38453;&#12290;&#30001;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#30340;&#21516;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;&#20197;&#22686;&#24378;&#20998;&#32452;&#25928;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#32858;&#31867;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20984;&#32452;&#21512;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#26696;&#65292;&#23427;&#32467;&#21512;&#20102;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear subspace clustering based on a feed-forward neural network has been demonstrated to provide better clustering accuracy than some advanced subspace clustering algorithms. While this approach demonstrates impressive outcomes, it involves a balance between effectiveness and computational cost. In this study, we employ a functional link neural network to transform data samples into a nonlinear domain. Subsequently, we acquire a self-representation matrix through a learning mechanism that builds upon the mapped samples. As the functional link neural network is a single-layer neural network, our proposed method achieves high computational efficiency while ensuring desirable clustering performance. By incorporating the local similarity regularization to enhance the grouping effect, our proposed method further improves the quality of the clustering results. Additionally, we introduce a convex combination subspace clustering scheme, which combining a linear subspace clustering method 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02047</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Quality and Trust in LLM-generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#38169;&#12290;&#29992;&#25143;&#38656;&#35201;&#21487;&#38752;&#30340;&#25351;&#31034;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#20449;&#65292;&#20174;&#32780;&#21487;&#20197;&#20570;&#20986;&#29702;&#24615;&#20915;&#31574;&#26159;&#21542;&#20351;&#29992;&#35813;&#36755;&#20986;&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#23558;&#36755;&#20986;&#19982;&#32622;&#20449;&#24230;&#30456;&#20851;&#32852;&#65307;&#22914;&#26524;&#32622;&#20449;&#24230;&#19982;&#27491;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#24378;&#30456;&#20851;&#65292;&#21017;&#31216;&#35813;&#27169;&#22411;&#20026;&#33391;&#22909;&#26657;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39640;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#23433;&#20840;&#25509;&#21463;&#65292;&#20302;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#25298;&#32477;&#12290;&#26657;&#20934;&#36804;&#20170;&#20027;&#35201;&#22312;&#38750;&#29983;&#25104;&#24615;&#65288;&#20363;&#22914;&#20998;&#31867;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#24456;&#23481;&#26131;&#20986;&#38169;&#65306;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#30693;&#36947;&#20309;&#26102;&#30452;&#25509;&#20351;&#29992;&#12289;&#32463;&#36807;&#20180;&#32454;&#23457;&#26597;&#21518;&#20351;&#29992;&#25110;&#20002;&#24323;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#65292;&#26657;&#20934;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#27010;&#24565;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#27492;&#26657;&#20934;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected.   Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;&#22312;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;Turbo LVQ&#21644;multi-means LVQ&#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;&#25628;&#32034;&#24615;&#33021;28%&#21644;27%&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LVQ&#21450;&#20854;&#26032;&#21464;&#20307;&#20351;&#24471;&#21521;&#37327;&#25628;&#32034;&#21464;&#24471;&#26497;&#24555;&#12290;</title><link>https://arxiv.org/abs/2402.02044</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24335;&#21521;&#37327;&#25628;&#32034;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Locally-Adaptive Quantization for Streaming Vector Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;&#22312;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;Turbo LVQ&#21644;multi-means LVQ&#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;&#25628;&#32034;&#24615;&#33021;28%&#21644;27%&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LVQ&#21450;&#20854;&#26032;&#21464;&#20307;&#20351;&#24471;&#21521;&#37327;&#25628;&#32034;&#21464;&#24471;&#26497;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#21521;&#37327;&#38598;&#21512;&#20013;&#25214;&#21040;&#19982;&#32473;&#23450;&#26597;&#35810;&#26368;&#30456;&#20284;&#30340;&#21521;&#37327;&#23884;&#20837;&#19968;&#30452;&#26159;&#26080;&#25968;&#23454;&#38469;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23601;&#26159;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#20043;&#19968;&#12290;&#23545;&#20110;&#35768;&#22810;&#36825;&#20123;&#24212;&#29992;&#65292;&#25968;&#25454;&#24211;&#36890;&#36807;&#25554;&#20837;&#26032;&#25968;&#25454;&#21644;&#21024;&#38500;&#36807;&#26102;&#25968;&#25454;&#32780;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#38382;&#39064;&#34987;&#31216;&#20026;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#12290;&#34429;&#28982;&#23616;&#37096;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#65288;LVQ&#65289;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#38750;&#28436;&#21270;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#25628;&#32034;&#24615;&#33021;&#65292;&#20294;&#20854;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#23578;&#26410;&#30830;&#23450;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LVQ&#22312;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;LVQ&#30340;&#25913;&#36827;&#65306;Turbo LVQ&#21644;multi-means LVQ&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;&#20854;&#25628;&#32034;&#24615;&#33021;&#39640;&#36798;28%&#21644;27%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LVQ&#21450;&#20854;&#26032;&#21464;&#20307;&#20351;&#24471;&#21521;&#37327;&#25628;&#32034;&#21464;&#24471;&#26497;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieving the most similar vector embeddings to a given query among a massive collection of vectors has long been a key component of countless real-world applications. The recently introduced Retrieval-Augmented Generation is one of the most prominent examples. For many of these applications, the database evolves over time by inserting new data and removing outdated data. In these cases, the retrieval problem is known as streaming similarity search. While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector compression method, yields state-of-the-art search performance for non-evolving databases, its usefulness in the streaming setting has not been yet established. In this work, we study LVQ in streaming similarity search. In support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and multi-means LVQ that boost its search performance by up to 28% and 27%, respectively. Our studies show that LVQ and its new variants enable blazing fast vector search,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20214;&#24335;&#24494;&#22411;AI&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#26234;&#33021;&#21644;&#36873;&#25321;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#20256;&#36755;&#38598;&#25104;&#21040;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#12290;&#36890;&#36807;&#25918;&#32622;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38752;&#36817;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20256;&#36755;&#30340;&#26234;&#33021;&#25511;&#21046;&#65292;&#31579;&#36873;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#20256;&#36755;&#39057;&#29575;&#20002;&#24323;&#26080;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#20248;&#21270;&#36817;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#23454;&#26102;&#20256;&#24863;&#22120;&#25511;&#21046;&#12290;&#23450;&#21046;&#35757;&#32451;&#36807;&#31243;&#21644;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#8220;&#25042;&#24816;&#8221;&#20256;&#24863;&#22120;&#20572;&#29992;&#31574;&#30053;&#25552;&#21319;&#26694;&#26550;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#20854;&#20182;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#21487;&#19982;&#20043;&#20849;&#23384;&#12290;</title><link>https://arxiv.org/abs/2402.02043</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#21644;&#36873;&#25321;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#20256;&#36755;&#30340;&#25554;&#20214;&#24335;&#24494;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02043
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20214;&#24335;&#24494;&#22411;AI&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#26234;&#33021;&#21644;&#36873;&#25321;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#20256;&#36755;&#38598;&#25104;&#21040;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#12290;&#36890;&#36807;&#25918;&#32622;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38752;&#36817;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20256;&#36755;&#30340;&#26234;&#33021;&#25511;&#21046;&#65292;&#31579;&#36873;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#20256;&#36755;&#39057;&#29575;&#20002;&#24323;&#26080;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#20248;&#21270;&#36817;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#23454;&#26102;&#20256;&#24863;&#22120;&#25511;&#21046;&#12290;&#23450;&#21046;&#35757;&#32451;&#36807;&#31243;&#21644;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#8220;&#25042;&#24816;&#8221;&#20256;&#24863;&#22120;&#20572;&#29992;&#31574;&#30053;&#25552;&#21319;&#26694;&#26550;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#20854;&#20182;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#21487;&#19982;&#20043;&#20849;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20294;&#24403;&#21069;&#24863;&#30693;&#31995;&#32479;&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26234;&#33021;&#65292;&#23548;&#33268;&#25968;&#25454;&#20135;&#29983;&#37327;&#24040;&#22823;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24863;&#30693;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25918;&#32622;&#22312;&#20256;&#24863;&#22120;&#38468;&#36817;&#65292;&#20026;&#24863;&#30693;&#26694;&#26550;&#25552;&#20379;&#26234;&#33021;&#25968;&#25454;&#20256;&#36755;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#21450;&#26102;&#21453;&#39304;&#32473;&#24863;&#30693;&#31995;&#32479;&#65292;&#21482;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#35843;&#33410;&#25968;&#25454;&#20256;&#36755;&#30340;&#39057;&#29575;&#20002;&#24323;&#26080;&#20851;&#20449;&#24687;&#12290;&#36817;&#20256;&#24863;&#22120;&#27169;&#22411;&#32463;&#36807;&#37327;&#21270;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#23454;&#26102;&#20256;&#24863;&#22120;&#25511;&#21046;&#12290;&#20026;&#20102;&#25552;&#21319;&#26694;&#26550;&#24615;&#33021;&#65292;&#23450;&#21046;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#8220;&#25042;&#24816;&#8221;&#20256;&#24863;&#22120;&#20572;&#29992;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#19982;&#20854;&#20182;&#29289;&#32852;&#32593;&#26694;&#26550;&#27491;&#20132;&#65292;&#21487;&#20197;&#20445;&#35777;&#20849;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications in the Internet of Things (IoT) utilize machine learning to analyze sensor-generated data. However, a major challenge lies in the lack of targeted intelligence in current sensing systems, leading to vast data generation and increased computational and communication costs. To address this challenge, we propose a novel sensing module to equip sensing frameworks with intelligent data transmission capabilities by integrating a highly efficient machine learning model placed near the sensor. This model provides prompt feedback for the sensing system to transmit only valuable data while discarding irrelevant information by regulating the frequency of data transmission. The near-sensor model is quantized and optimized for real-time sensor control. To enhance the framework's performance, the training process is customized and a "lazy" sensor deactivation strategy utilizing temporal information is introduced. The suggested method is orthogonal to other IoT frameworks and can be cons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;</title><link>https://arxiv.org/abs/2402.02042</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDP&#36827;&#34892;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#39046;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#30740;&#31350;&#20855;&#26377;&#36890;&#29992;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#24179;&#22343;&#22238;&#25253;CMDP&#30340;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#30830;&#20445;&#20302;&#36951;&#25022;&#20445;&#35777;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#19978;&#20855;&#26377; $\tilde{\mathcal{O}}({T}^{3/4})$ &#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02041</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#23494;&#24230;&#27604;&#20272;&#35745;(DRE)&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22240;DRE&#30340;&#25439;&#22833;&#20989;&#25968;&#32780;&#20986;&#29616;&#20102;&#20248;&#21270;&#38382;&#39064;&#65306;KL&#25955;&#24230;&#38656;&#35201;&#22823;&#26679;&#26412;&#65292;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#65292;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#26377;&#20559;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25216;&#26415;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;DRE&#30340;&#26679;&#26412;&#35201;&#27714;&#65292;&#20197;$L_1$&#35823;&#24046;&#30340;&#19978;&#30028;&#32852;&#31995;&#36215;&#26469;&#65292;&#35813;&#19978;&#30028;&#23558;&#39640;&#32500;&#24230;DRE&#20219;&#21153;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#20316;&#20026;&#19968;&#20010;&#20849;&#21516;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#35299;&#37322;&#23376;&#22270;&#30340;&#20195;&#29702;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19982;&#21487;&#35299;&#37322;&#23376;&#22270;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02036</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#24335;&#20195;&#29702;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Interpreting Graph Neural Networks with In-Distributed Proxies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#35299;&#37322;&#23376;&#22270;&#30340;&#20195;&#29702;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19982;&#21487;&#35299;&#37322;&#23376;&#22270;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22270;&#25968;&#25454;&#22788;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#20851;&#38190;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#37096;&#32626;GNN&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#38656;&#35201;&#29992;&#25143;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#33021;&#22815;&#35299;&#37322;&#20854;&#21407;&#22240;&#12290;&#35299;&#37322;GNN&#30340;&#27969;&#34892;&#33539;&#24335;&#26159;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#19982;&#21407;&#22987;&#22270;&#30340;&#26631;&#31614;&#26469;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#23376;&#22270;&#12290;&#30001;&#20110;&#35757;&#32451;&#38598;&#20013;&#21407;&#22987;&#22270;&#19982;&#21487;&#35299;&#37322;&#23376;&#22270;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23548;&#33268;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#23376;&#22270;&#30340;&#26631;&#31614;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30456;&#31526;&#30340;&#21487;&#35299;&#37322;&#23376;&#22270;&#30340;&#20195;&#29702;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#29983;&#25104;&#22120;&#29983;&#25104;&#20195;&#29702;&#22270;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#22522;&#20110;&#20449;&#24687;&#35770;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#22270;&#19981;&#20165;&#36981;&#24490;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#32780;&#19988;&#20415;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become a building block in graph data processing, with wide applications in critical domains. The growing needs to deploy GNNs in high-stakes applications necessitate explainability for users in the decision-making processes. A popular paradigm for the explainability of GNNs is to identify explainable subgraphs by comparing their labels with the ones of original graphs. This task is challenging due to the substantial distributional shift from the original graphs in the training set to the set of explainable subgraphs, which prevents accurate prediction of labels with the subgraphs. To address it, in this paper, we propose a novel method that generates proxy graphs for explainable subgraphs that are in the distribution of training data. We introduce a parametric method that employs graph generators to produce proxy graphs. A new training objective based on information theory is designed to ensure that proxy graphs not only adhere to the distribution of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65292;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.02034</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#38024;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65292;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#29992;&#26041;&#27861;&#26088;&#22312;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;/&#25110;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#65292;&#32780;&#21453;&#21521;&#24037;&#31243;&#26041;&#27861;&#36890;&#24120;&#26126;&#30830;&#20551;&#35774;&#20854;&#20013;&#19968;&#31181;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#34987;&#38450;&#23432;&#30340;DNN&#30340;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65307;&#23427;&#21487;&#20197;&#22312;&#21518;&#35757;&#32451;&#26102;&#25805;&#20316;&#65288;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#65289;&#65307;&#23545;&#20110;&#19981;&#21516;&#30340;&#23884;&#20837;&#26426;&#21046;&#65288;&#21363;&#36890;&#29992;&#30340;&#65289;&#38750;&#24120;&#26377;&#25928;&#65307;&#24182;&#19988;&#20855;&#26377;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#27492;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#19981;&#21516;&#25915;&#20987;&#36827;&#34892;&#20102;&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of defenses have been proposed against backdoors attacks on deep neural network (DNN) classifiers. Universal methods seek to reliably detect and/or mitigate backdoors irrespective of the incorporation mechanism used by the attacker, while reverse-engineering methods often explicitly assume one. In this paper, we describe a new detector that: relies on internal feature map of the defended DNN to detect and reverse-engineer the backdoor and identify its target class; can operate post-training (without access to the training dataset); is highly effective for various incorporation mechanisms (i.e., is universal); and which has low computational overhead and so is scalable. Our detection approach is evaluated for different attacks on a benchmark CIFAR-10 image classifier.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#29702;&#35770;&#19982;&#35774;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#39640;&#40065;&#26834;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02032</link><description>&lt;p&gt;
RobustTSF: &#20851;&#20110;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#29702;&#35770;&#19982;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#29702;&#35770;&#19982;&#35774;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#39640;&#40065;&#26834;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37325;&#35201;&#32780;&#21069;&#27839;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25216;&#26415;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#24322;&#24120;&#20540;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#20026;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21487;&#33021;&#34987;&#27745;&#26579;&#12290;&#22914;&#26524;&#30452;&#25509;&#20351;&#29992;&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#35757;&#32451;&#65292;&#39044;&#27979;&#27169;&#22411;&#23558;&#34920;&#29616;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#20174;&#21463;&#27745;&#26579;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#40065;&#26834;&#30340;&#39044;&#27979;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#19977;&#31181;&#24322;&#24120;&#36827;&#34892;&#20102;&#32479;&#35745;&#23450;&#20041;&#65292;&#28982;&#21518;&#22312;&#36825;&#20123;&#24322;&#24120;&#23384;&#22312;&#26102;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#20998;&#26512;&#20102;&#25439;&#22833;&#40065;&#26834;&#24615;&#21644;&#26679;&#26412;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#40065;&#26834;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/haochenglouis/RobustTSF&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is an important and forefront task in many real-world applications. However, most of time series forecasting techniques assume that the training data is clean without anomalies. This assumption is unrealistic since the collected time series data can be contaminated in practice. The forecasting model will be inferior if it is directly trained by time series with anomalies. Thus it is essential to develop methods to automatically learn a robust forecasting model from the contaminated data. In this paper, we first statistically define three types of anomalies, then theoretically and experimentally analyze the loss robustness and sample robustness when these anomalies exist. Based on our analyses, we propose a simple and efficient algorithm to learn a robust forecasting model. Extensive experiments show that our method is highly robust and outperforms all existing approaches. The code is available at https://github.com/haochenglouis/RobustTSF.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#36924;&#30495;&#24230;&#30340;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23450;&#21046;&#30340;&#22810;&#36924;&#30495;&#24230;&#33258;&#32534;&#30721;&#22120;&#23558;&#20855;&#26377;&#19981;&#21516;&#36924;&#30495;&#24230;&#27700;&#24179;&#30340;&#25968;&#25454;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.02031</link><description>&lt;p&gt;
&#22810;&#32423;&#36924;&#30495;&#30340;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity physics constrained neural networks for dynamical systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#36924;&#30495;&#24230;&#30340;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23450;&#21046;&#30340;&#22810;&#36924;&#30495;&#24230;&#33258;&#32534;&#30721;&#22120;&#23558;&#20855;&#26377;&#19981;&#21516;&#36924;&#30495;&#24230;&#27700;&#24179;&#30340;&#25968;&#25454;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34987;&#29992;&#26469;&#22686;&#24378;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#30456;&#27604;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21253;&#21547;&#29289;&#29702;&#32422;&#26463;&#25439;&#22833;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#32500;&#31995;&#32479;&#32780;&#35328;&#12290;&#20107;&#23454;&#19978;&#65292;&#20256;&#32479;&#30340;&#29289;&#29702;&#32422;&#26463;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#19968;&#36924;&#30495;&#24230;&#25968;&#25454;&#65292;&#38656;&#35201;&#22312;&#39640;&#32500;&#22330;&#20013;&#35780;&#20272;&#29289;&#29702;&#32422;&#26463;&#65292;&#36825;&#24341;&#20837;&#20102;&#35745;&#31639;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#36755;&#20837;&#22823;&#23567;&#65292;&#20351;&#29992;&#22810;&#36924;&#30495;&#24230;&#30340;&#35757;&#32451;&#25968;&#25454;&#20063;&#20250;&#21464;&#24471;&#32321;&#29712;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;MSPCNN&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#30340;&#22810;&#36924;&#30495;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#20855;&#26377;&#19981;&#21516;&#36924;&#30495;&#24230;&#27700;&#24179;&#30340;&#25968;&#25454;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained neural networks are commonly employed to enhance prediction robustness compared to purely data-driven models, achieved through the inclusion of physical constraint losses during the model training process. However, one of the major challenges of physics-constrained neural networks consists of the training complexity especially for high-dimensional systems. In fact, conventional physics-constrained models rely on singular-fidelity data necessitating the assessment of physical constraints within high-dimensional fields, which introduces computational difficulties. Furthermore, due to the fixed input size of the neural networks, employing multi-fidelity training data can also be cumbersome. In this paper, we propose the Multi-Scale Physics-Constrained Neural Network (MSPCNN), which offers a novel methodology for incorporating data with different levels of fidelity into a unified latent space through a customised multi-fidelity autoencoder. Additionally, multiple decode
&lt;/p&gt;</description></item><item><title>ScribFormer&#26159;&#19968;&#31181;&#26032;&#30340;CNN-Transformer&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#28034;&#40486;&#30417;&#30563;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#34701;&#21512;CNN&#23398;&#20064;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;Transformer&#33719;&#24471;&#30340;&#20840;&#23616;&#34920;&#31034;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#28034;&#40486;&#27880;&#37322;&#20013;&#23398;&#20064;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02029</link><description>&lt;p&gt;
ScribFormer: Transformer&#20351;&#24471;&#22522;&#20110;&#28034;&#40486;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;CNN&#24037;&#20316;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02029
&lt;/p&gt;
&lt;p&gt;
ScribFormer&#26159;&#19968;&#31181;&#26032;&#30340;CNN-Transformer&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#28034;&#40486;&#30417;&#30563;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#34701;&#21512;CNN&#23398;&#20064;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;Transformer&#33719;&#24471;&#30340;&#20840;&#23616;&#34920;&#31034;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#28034;&#40486;&#27880;&#37322;&#20013;&#23398;&#20064;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#28034;&#40486;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20855;&#26377;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;CNN&#26694;&#26550;&#12290;&#23613;&#31649;&#36825;&#20010;&#26694;&#26550;&#26377;&#22810;&#20010;&#22909;&#22788;&#65292;&#20294;&#26159;&#30001;&#20110;&#21367;&#31215;&#23618;&#21482;&#33021;&#25429;&#25417;&#20855;&#26377;&#23616;&#37096;&#24863;&#21463;&#37326;&#30340;&#23567;&#33539;&#22260;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#24456;&#38590;&#20174;&#28034;&#40486;&#27880;&#37322;&#25552;&#20379;&#30340;&#26377;&#38480;&#20449;&#24687;&#20013;&#23398;&#20064;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CNN-Transformer&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#28034;&#40486;&#30417;&#30563;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#31216;&#20026;ScribFormer&#12290;&#25152;&#25552;&#20986;&#30340;ScribFormer&#27169;&#22411;&#20855;&#26377;&#19977;&#20010;&#20998;&#25903;&#32467;&#26500;&#65292;&#21363;CNN&#20998;&#25903;&#65292;Transformer&#20998;&#25903;&#21644;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#31867;&#28608;&#27963;&#22270;&#65288;ACAM&#65289;&#20998;&#25903;&#30340;&#28151;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CNN&#20998;&#25903;&#19982;Transformer&#20998;&#25903;&#21512;&#20316;&#65292;&#23558;&#20174;CNN&#23398;&#20064;&#30340;&#23616;&#37096;&#29305;&#24449;&#19982;&#20174;Transformer&#33719;&#24471;&#30340;&#20840;&#23616;&#34920;&#31034;&#30456;&#34701;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#29616;&#26377;&#28034;&#40486;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent scribble-supervised segmentation methods commonly adopt a CNN framework with an encoder-decoder architecture. Despite its multiple benefits, this framework generally can only capture small-range feature dependency for the convolutional layer with the local receptive field, which makes it difficult to learn global shape information from the limited information provided by scribble annotations. To address this issue, this paper proposes a new CNN-Transformer hybrid solution for scribble-supervised medical image segmentation called ScribFormer. The proposed ScribFormer model has a triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer branch, and an attention-guided class activation map (ACAM) branch. Specifically, the CNN branch collaborates with the Transformer branch to fuse the local features learned from CNN with the global representations obtained from Transformer, which can effectively overcome limitations of existing scribble-supervised segmentation m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;UE&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20813;&#21463;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26410;&#25480;&#26435;&#35757;&#32451;&#12290;&#36890;&#36807;&#36873;&#25321;&#24615;&#24212;&#29992;&#26368;&#23567;&#21270;&#35823;&#24046;&#22122;&#22768;&#65292;&#25105;&#20204;&#20351;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#27573;&#23545;DNN&#27169;&#22411;&#19981;&#21487;&#23398;&#20064;&#65292;&#21516;&#26102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#19981;&#21487;&#23519;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.02028</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21487;&#23398;&#20064;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Examples For Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;UE&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20813;&#21463;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26410;&#25480;&#26435;&#35757;&#32451;&#12290;&#36890;&#36807;&#36873;&#25321;&#24615;&#24212;&#29992;&#26368;&#23567;&#21270;&#35823;&#24046;&#22122;&#22768;&#65292;&#25105;&#20204;&#20351;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#27573;&#23545;DNN&#27169;&#22411;&#19981;&#21487;&#23398;&#20064;&#65292;&#21516;&#26102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#19981;&#21487;&#23519;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#23398;&#20064;&#30340;&#26679;&#26412;&#65288;UEs&#65289;&#26159;&#25351;&#32463;&#36807;&#20462;&#25913;&#20351;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#26159;&#36890;&#36807;&#28155;&#21152;&#26368;&#23567;&#21270;&#35823;&#24046;&#30340;&#22122;&#22768;&#29983;&#25104;&#30340;&#65292;&#36825;&#20123;&#22122;&#22768;&#21487;&#20197;&#27450;&#39575;DNN&#27169;&#22411;&#35748;&#20026;&#20174;&#25968;&#25454;&#20013;&#27809;&#26377;&#20219;&#20309;&#65288;&#26080;&#35823;&#24046;&#65289;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#12290;UE&#30340;&#27010;&#24565;&#34987;&#25552;&#20986;&#20316;&#20026;&#23545;&#20010;&#20154;&#25968;&#25454;&#30340;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#30340;&#23545;&#31574;&#12290;&#34429;&#28982;UE&#22312;&#22270;&#20687;&#19978;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22914;&#20309;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21019;&#24314;&#26377;&#25928;&#30340;UE&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;UE&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20445;&#25252;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20813;&#21463;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26410;&#25480;&#26435;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#26368;&#23567;&#21270;&#35823;&#24046;&#22122;&#22768;&#65292;&#21487;&#20197;\emph{&#36873;&#25321;&#24615;&#22320;}&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#29305;&#23450;&#27573;&#33853;&#65292;&#20351;&#23427;&#20204;&#23545;DNN&#27169;&#22411;&#19981;&#21487;&#23398;&#20064;&#65292;&#21516;&#26102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#26469;&#35828;&#19981;&#21487;&#23519;&#35273;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;UE&#29983;&#25104;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlearnable examples (UEs) refer to training samples modified to be unlearnable to Deep Neural Networks (DNNs). These examples are usually generated by adding error-minimizing noises that can fool a DNN model into believing that there is nothing (no error) to learn from the data. The concept of UE has been proposed as a countermeasure against unauthorized data exploitation on personal data. While UE has been extensively studied on images, it is unclear how to craft effective UEs for time series data. In this work, we introduce the first UE generation method to protect time series data from unauthorized training by deep learning models. To this end, we propose a new form of error-minimizing noise that can be \emph{selectively} applied to specific segments of time series, rendering them unlearnable to DNN models while remaining imperceptible to human observers. Through extensive experiments on a wide range of time series datasets, we demonstrate that the proposed UE generation method is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02025</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Constraint Formulations in Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#23433;&#20840;RL&#25104;&#20026;&#19968;&#31181;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#23433;&#20840;&#20248;&#21270;&#20195;&#29702;&#31574;&#30053;&#30340;&#22522;&#26412;&#32780;&#24378;&#22823;&#30340;&#33539;&#20363;&#12290;&#22522;&#20110;&#32422;&#26463;&#20934;&#21017;&#30340;&#23433;&#20840;RL&#26041;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#22312;RL&#20013;&#23454;&#29616;&#23433;&#20840;&#24615;&#30340;&#23581;&#35797;&#28608;&#22686;&#65292;&#20294;&#30001;&#20110;&#32422;&#26463;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35752;&#35770;&#24456;&#23569;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#24615;&#20102;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20195;&#34920;&#24615;&#32422;&#26463;&#24418;&#24335;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#20197;&#21450;&#38024;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#30340;&#31934;&#36873;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25581;&#31034;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02023</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#39044;&#27979;&#30001;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24615;&#32780;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#28369;&#21160;&#31383;&#21475;&#26469;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#37096;&#20998;&#22312;&#30701;&#31383;&#21475;&#20869;&#34987;&#25429;&#25417;&#21040;&#30340;&#38271;&#26399;&#21464;&#21270;&#65288;&#21363;&#22806;&#31383;&#21475;&#21464;&#21270;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32858;&#28966;&#38271;&#26399;&#21464;&#21270;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#25439;&#22833;&#23558;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#26500;&#24314;&#27491;&#36127;&#23545;&#12290;&#24403;&#19982;&#25105;&#20204;&#30340;&#20998;&#35299;&#32593;&#32476;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, es
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#26159;&#36739;&#22909;&#30340;&#36873;&#25321;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02021</link><description>&lt;p&gt;
ECG&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in ECG Diagnosis: Is It Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#26159;&#36739;&#22909;&#30340;&#36873;&#25321;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#21463;&#21040;&#22823;&#35268;&#27169;&#12289;&#26631;&#35760;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#21033;&#29992;&#20174;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36801;&#31227;&#23398;&#20064;&#22987;&#32456;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#26222;&#36941;&#20551;&#35774;&#20174;&#26410;&#34987;&#31995;&#32479;&#39564;&#35777;&#36807;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#26631;&#31614;&#24515;&#30005;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#24494;&#35843;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#24191;&#27867;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#26469;&#39564;&#35777;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#23454;&#65292;&#23545;&#20110;&#23567;&#22411;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#24494;&#35843;&#26159;&#26356;&#22909;&#30340;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#23613;&#31649;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#26469;&#36814;&#22836;&#36214;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of deep learning in ECG diagnosis is often hindered by the scarcity of large, well-labeled datasets in real-world scenarios, leading to the use of transfer learning to leverage features learned from larger datasets. Yet the prevailing assumption that transfer learning consistently outperforms training from scratch has never been systematically validated. In this study, we conduct the first extensive empirical study on the effectiveness of transfer learning in multi-label ECG classification, by investigating comparing the fine-tuning performance with that of training from scratch, covering a variety of ECG datasets and deep neural networks. We confirm that fine-tuning is the preferable choice for small downstream datasets; however, when the dataset is sufficiently large, training from scratch can achieve comparable performance, albeit requiring a longer training time to catch up. Furthermore, we find that transfer learning exhibits better compatibility with convolutional ne
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;</title><link>https://arxiv.org/abs/2402.02018</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#36229;&#32423;&#35745;&#31639;&#30740;&#31350;&#21644;LLMs&#30340;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Position Paper: The Landscape and Challenges of HPC Research and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02018
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#29305;&#21035;&#26159;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#22343;&#23637;&#29616;&#20986;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#20219;&#21153;&#20013;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#26426;&#26500;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#26041;&#38754;&#25237;&#20837;&#20102;&#22823;&#37327;&#36164;&#28304;&#65292;&#36798;&#21040;&#25110;&#31361;&#30772;&#20102;&#36229;&#32423;&#35745;&#31639;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#23558;&#36825;&#20123;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#35843;&#25972;&#21644;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#23558;&#20250;&#38750;&#24120;&#26377;&#30410;&#12290;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#25105;&#20204;&#19978;&#36848;&#35266;&#28857;&#30340;&#29702;&#30001;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#24819;&#27861;&#22312;HPC&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20215;&#20540;&#22686;&#24378;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;RCSL&#30340;&#31283;&#23450;&#24615;&#19982;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#30340;&#36830;&#25509;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21160;&#24577;&#22320;&#26681;&#25454;&#36712;&#36857;&#22238;&#25253;&#23558;&#20215;&#20540;&#24110;&#21161;&#27880;&#20837;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#36712;&#36857;&#22238;&#25253;&#65292;&#25512;&#21160;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.02017</link><description>&lt;p&gt;
&#26080;&#38656;&#22870;&#21169;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Value-Aided Conditional Supervised Learning for Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20215;&#20540;&#22686;&#24378;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;RCSL&#30340;&#31283;&#23450;&#24615;&#19982;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#30340;&#36830;&#25509;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21160;&#24577;&#22320;&#26681;&#25454;&#36712;&#36857;&#22238;&#25253;&#23558;&#20215;&#20540;&#24110;&#21161;&#27880;&#20837;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#36712;&#36857;&#22238;&#25253;&#65292;&#25512;&#21160;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#22522;&#20110;&#22238;&#25253;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#65288;RCSL&#65289;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#27599;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20123;&#23454;&#38469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#22686;&#24378;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#65288;VCS&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;RCSL&#30340;&#31283;&#23450;&#24615;&#19982;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#30340;&#36830;&#25509;&#33021;&#21147;&#26377;&#25928;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#31070;&#32463;&#20999;&#32447;&#26680;&#20998;&#26512;&#65292;VCS&#21487;&#20197;&#21160;&#24577;&#22320;&#26681;&#25454;&#36712;&#36857;&#22238;&#25253;&#23558;&#20215;&#20540;&#24110;&#21161;&#27880;&#20837;RCSL&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20197;&#21306;&#20998;&#20215;&#20540;&#20989;&#25968;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#31283;&#23450;&#36830;&#25509;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;VCS&#19981;&#20165;&#26174;&#33879;&#20248;&#20110;RCSL&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#20102;&#25110;&#32463;&#24120;&#36229;&#36807;&#26368;&#39640;&#30340;&#36712;&#36857;&#22238;&#25253;&#12290;&#36825;&#19968;&#31361;&#30772;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#65292;&#25512;&#21160;&#20102;&#21487;&#23454;&#29616;&#30340;&#26497;&#38480;&#65292;&#24182;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has seen notable advancements through return-conditioned supervised learning (RCSL) and value-based methods, yet each approach comes with its own set of practical challenges. Addressing these, we propose Value-Aided Conditional Supervised Learning (VCS), a method that effectively synergizes the stability of RCSL with the stitching ability of value-based methods. Based on the Neural Tangent Kernel analysis to discern instances where value function may not lead to stable stitching, VCS injects the value aid into the RCSL's loss function dynamically according to the trajectory return. Our empirical studies reveal that VCS not only significantly outperforms both RCSL and value-based methods but also consistently achieves, or often surpasses, the highest trajectory returns across diverse offline RL benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the limits of what can be achieved and fostering further innovations.
&lt;/p&gt;</description></item><item><title>GenFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#20803;&#38543;&#26426;&#36807;&#31243;&#12290;&#23427;&#33021;&#20445;&#30041;&#30446;&#26631;&#32479;&#35745;&#29305;&#24615;&#65292;&#21253;&#25324;&#36793;&#38469;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#20013;&#36817;&#20284;&#25429;&#25417;&#21040;&#20854;&#20182;&#26399;&#26395;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#24212;&#29992;&#20110;&#39118;&#36895;&#25968;&#25454;&#27169;&#25311;&#30340;&#23454;&#39564;&#20013;&#65292;GenFormer&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#39118;&#38505;&#31649;&#29702;&#30340;&#36229;&#36234;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02010</link><description>&lt;p&gt;
GenFormer: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#22810;&#20803;&#38543;&#26426;&#36807;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GenFormer: A Deep-Learning-Based Approach for Generating Multivariate Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02010
&lt;/p&gt;
&lt;p&gt;
GenFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#20803;&#38543;&#26426;&#36807;&#31243;&#12290;&#23427;&#33021;&#20445;&#30041;&#30446;&#26631;&#32479;&#35745;&#29305;&#24615;&#65292;&#21253;&#25324;&#36793;&#38469;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#20013;&#36817;&#20284;&#25429;&#25417;&#21040;&#20854;&#20182;&#26399;&#26395;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#24212;&#29992;&#20110;&#39118;&#36895;&#25968;&#25454;&#27169;&#25311;&#30340;&#23454;&#39564;&#20013;&#65292;GenFormer&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#39118;&#38505;&#31649;&#29702;&#30340;&#36229;&#36234;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29983;&#25104;&#22120;&#23545;&#20110;&#29983;&#25104;&#20445;&#25345;&#30446;&#26631;&#32479;&#35745;&#29305;&#24615;&#30340;&#21512;&#25104;&#23454;&#29616;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenFormer&#65292;&#19968;&#20010;&#29992;&#20110;&#26102;&#31354;&#22810;&#20803;&#38543;&#26426;&#36807;&#31243;&#30340;&#38543;&#26426;&#29983;&#25104;&#22120;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#23558;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24207;&#21015;&#26144;&#23556;&#21040;&#26102;&#38388;&#24207;&#21015;&#20540;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;GenFormer&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#20445;&#30041;&#20102;&#30446;&#26631;&#36793;&#38469;&#20998;&#24067;&#65292;&#24182;&#22312;&#28041;&#21450;&#22823;&#37327;&#31354;&#38388;&#20301;&#32622;&#21644;&#38271;&#26102;&#38388;&#27169;&#25311;&#30340;&#25361;&#25112;&#24615;&#24212;&#29992;&#20013;&#36817;&#20284;&#25429;&#25417;&#21040;&#20854;&#20182;&#26399;&#26395;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;GenFormer&#27169;&#22411;&#24212;&#29992;&#20110;&#22312;&#20315;&#32599;&#37324;&#36798;&#24030;&#30340;&#21508;&#20010;&#31449;&#28857;&#27169;&#25311;&#21512;&#25104;&#39118;&#36895;&#25968;&#25454;&#65292;&#20197;&#35745;&#31639;&#39118;&#38505;&#31649;&#29702;&#30340;&#36229;&#36234;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic generators are essential to produce synthetic realizations that preserve target statistical properties. We propose GenFormer, a stochastic generator for spatio-temporal multivariate stochastic processes. It is constructed using a Transformer-based deep learning model that learns a mapping between a Markov state sequence and time series values. The synthetic data generated by the GenFormer model preserves the target marginal distributions and approximately captures other desired statistical properties even in challenging applications involving a large number of spatial locations and a long simulation horizon. The GenFormer model is applied to simulate synthetic wind speed data at various stations in Florida to calculate exceedance probabilities for risk management.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#26041;&#27861;&#65292;&#26681;&#25454;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20811;&#26381;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.02009</link><description>&lt;p&gt;
&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#40065;&#26834;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Task Learning with Excess Risks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#26041;&#27861;&#65292;&#26681;&#25454;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20811;&#26381;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#36890;&#36807;&#20248;&#21270;&#25152;&#26377;&#20219;&#21153;&#25439;&#22833;&#30340;&#20984;&#32452;&#21512;&#26469;&#32771;&#34385;&#20026;&#22810;&#20010;&#20219;&#21153;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26041;&#26696;&#65292;&#26681;&#25454;&#21508;&#33258;&#30340;&#25439;&#22833;&#21160;&#24577;&#35843;&#25972;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20248;&#20808;&#32771;&#34385;&#22256;&#38590;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#20250;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36807;&#22810;&#30340;&#26435;&#37325;&#24448;&#24448;&#34987;&#20998;&#37197;&#32473;&#20855;&#26377;&#30456;&#23545;&#36739;&#22823;&#36125;&#21494;&#26031;&#26368;&#20248;&#35823;&#24046;&#30340;&#22122;&#22768;&#20219;&#21153;&#65292;&#20174;&#32780;&#25513;&#30422;&#20854;&#20182;&#20219;&#21153;&#24182;&#23548;&#33268;&#25972;&#20307;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36807;&#22810;&#39118;&#38505;&#30340;&#20219;&#21153;&#24179;&#34913;&#26041;&#27861;&#65292;&#36890;&#36807;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;ExcessMTL&#23558;&#26356;&#39640;&#30340;&#26435;&#37325;&#20998;&#37197;&#32473;&#36739;&#24046;&#35757;&#32451;&#30340;&#36317;&#31163;&#25910;&#25947;&#36739;&#36828;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20272;&#35745;&#36807;&#22810;&#39118;&#38505;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#29366;&#24577;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21333;&#31867;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.02007</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#31867;&#20998;&#31867;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Understanding Time Series Anomaly State Detection through One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#29366;&#24577;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21333;&#31867;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21457;&#29616;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#35802;&#28982;&#65292;&#36825;&#19982;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#20154;&#20204;&#20851;&#24515;&#30340;&#26159;&#65306;&#20551;&#35774;&#32473;&#23450;&#19968;&#20010;&#26631;&#20934;&#26102;&#38388;&#24207;&#21015;&#65292;&#22914;&#20309;&#21028;&#26029;&#21478;&#19968;&#20010;&#27979;&#35797;&#26102;&#38388;&#24207;&#21015;&#26159;&#21542;&#20559;&#31163;&#26631;&#20934;&#26102;&#38388;&#24207;&#21015;&#65292;&#36825;&#26356;&#31867;&#20284;&#20110;&#35752;&#35770;&#30340;&#21333;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;OCC&#37325;&#26032;&#29702;&#35299;&#21644;&#23450;&#20041;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#29366;&#24577;&#26816;&#27979;&#38382;&#39064;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#21644;&#20551;&#35774;&#26816;&#39564;&#20005;&#26684;&#23450;&#20041;&#20102;&#8220;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#29366;&#24577;&#26816;&#27979;&#38382;&#39064;&#8221;&#21450;&#20854;&#23545;&#24212;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#26500;&#24314;&#30456;&#24212;&#38382;&#39064;&#30340;&#20154;&#24037;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;38&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#24182;&#32416;&#27491;&#20102;&#20854;&#20013;&#19968;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, research on time series anomaly detection has mainly focused on finding outliers within a given time series. Admittedly, this is consistent with some practical problems, but in other practical application scenarios, people are concerned about: assuming a standard time series is given, how to judge whether another test time series deviates from the standard time series, which is more similar to the problem discussed in one-class classification (OCC). Therefore, in this article, we try to re-understand and define the time series anomaly detection problem through OCC, which we call 'time series anomaly state detection problem'. We first use stochastic processes and hypothesis testing to strictly define the 'time series anomaly state detection problem', and its corresponding anomalies. Then, we use the time series classification dataset to construct an artificial dataset corresponding to the problem. We compile 38 anomaly detection algorithms and correct some of the algori
&lt;/p&gt;</description></item><item><title>PresAIse&#26159;&#19968;&#31181;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38598;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#25968;&#25454;&#38480;&#21046;&#12289;&#24314;&#35758;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#19982;&#19994;&#21153;&#29992;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#38556;&#30861;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02006</link><description>&lt;p&gt;
PresAIse&#65292;&#19968;&#31181;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
PresAIse, An Enterprises Prescriptive AI Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02006
&lt;/p&gt;
&lt;p&gt;
PresAIse&#26159;&#19968;&#31181;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38598;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#25968;&#25454;&#38480;&#21046;&#12289;&#24314;&#35758;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#19982;&#19994;&#21153;&#29992;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#38556;&#30861;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#20195;&#34920;&#20102;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#19968;&#27425;&#36716;&#22411;&#24615;&#21464;&#38761;&#65292;&#25552;&#20379;&#22240;&#26524;&#27934;&#23519;&#21644;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20225;&#19994;&#37319;&#29992;&#24120;&#24120;&#38754;&#20020;&#20960;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35266;&#23519;&#25968;&#25454;&#30340;&#38480;&#21046;&#20351;&#24471;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#25104;&#20026;&#20102;&#33391;&#22909;&#20915;&#31574;&#21046;&#23450;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#20854;&#27425;&#65292;&#24314;&#35758;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#20225;&#19994;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#31532;&#19977;&#20010;&#25361;&#25112;&#26159;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#19994;&#21153;&#29992;&#25143;&#20043;&#38388;&#30340;&#38548;&#31163;&#65292;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#21512;&#20316;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;IBM&#30740;&#31350;&#30340;&#19968;&#39033;&#20513;&#35758;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#22871;&#39044;&#27979;&#24615; AI &#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#30740;&#31350;&#35770;&#25991;&#30340;&#35265;&#35299;&#65292;&#35299;&#20915;&#26041;&#26696;&#22871;&#20214;&#21253;&#25324;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#20197;&#21450;&#36890;&#36807;&#23545;&#35805;&#26694;&#26550;&#26469;&#24357;&#21512;&#27807;&#36890;&#38548;&#38402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the interpretability of recommendations, which is crucial for enterprise decision-making settings. The third challenge is the silos between data scientists and business users, hindering effective collaboration. This paper outlines an initiative from IBM Research, aiming to address some of these challenges by offering a suite of prescriptive AI solutions. Leveraging insights from various research papers, the solution suite includes scalable causal inference methods, interpretable decision-making approaches, and the integration of large language models (LLMs) to bridge communication gaps via a conversati
&lt;/p&gt;</description></item><item><title>TIGT&#26159;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#26032;&#22411;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#21306;&#20998;&#22270;&#21516;&#26500;&#24615;&#30340;&#33021;&#21147;&#21644;&#25552;&#39640;&#22270;&#24418;&#21464;&#25442;&#22120;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#21516;&#26500;&#24615;&#30340;&#26816;&#27979;&#21644;&#25972;&#20307;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.02005</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Topology-Informed Graph Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02005
&lt;/p&gt;
&lt;p&gt;
TIGT&#26159;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#26032;&#22411;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#21306;&#20998;&#22270;&#21516;&#26500;&#24615;&#30340;&#33021;&#21147;&#21644;&#25552;&#39640;&#22270;&#24418;&#21464;&#25442;&#22120;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#21516;&#26500;&#24615;&#30340;&#26816;&#27979;&#21644;&#25972;&#20307;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#24418;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#38598;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#22686;&#24378;&#22270;&#24418;&#21464;&#25442;&#22120;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22686;&#24378;&#21306;&#20998;&#22270;&#30340;&#21516;&#26500;&#24615;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#36825;&#22312;&#25552;&#39640;&#23427;&#20204;&#30340;&#39044;&#27979;&#24615;&#33021;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#24418;&#22120;&#8212;&#8212;&#8220;&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;TIGT&#65289;&#8221;&#65292;&#23427;&#22686;&#24378;&#20102;&#26816;&#27979;&#22270;&#21516;&#26500;&#24615;&#30340;&#21306;&#20998;&#33021;&#21147;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;TIGT&#30001;&#22235;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#24490;&#29615;&#23376;&#22270;&#30340;&#38750;&#21516;&#26500;&#21367;&#19978;&#30340;&#25299;&#25169;&#20301;&#32622;&#23884;&#20837;&#23618;&#65292;&#20197;&#30830;&#20445;&#21807;&#19968;&#30340;&#22270;&#34920;&#31034;&#65307;&#19968;&#20010;&#21452;&#36335;&#24452;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#25299;&#25169;&#29305;&#24449;&#65307;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#65307;&#21644;&#19968;&#20010;&#22270;&#20449;&#24687;&#23618;&#65292;&#29992;&#20110;&#37325;&#26032;&#26657;&#20934;&#36890;&#36947;&#32423;&#30340;&#22270;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for be
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#32467;&#26500;&#21387;&#32553;&#65288;GC&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;GC&#30340;&#24418;&#24335;&#21270;&#23450;&#20041;&#65292;&#24182;&#23558;&#29616;&#26377;&#26041;&#27861;&#25353;&#29031;&#30446;&#26631;&#21644;&#20844;&#24335;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#32508;&#36848;&#36824;&#21253;&#25324;&#20102;&#23545;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.02000</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#32467;&#26500;&#21387;&#32553;&#65288;GC&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;GC&#30340;&#24418;&#24335;&#21270;&#23450;&#20041;&#65292;&#24182;&#23558;&#29616;&#26377;&#26041;&#27861;&#25353;&#29031;&#30446;&#26631;&#21644;&#20844;&#24335;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#32508;&#36848;&#36824;&#21253;&#25324;&#20102;&#23545;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#30340;&#20998;&#26512;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#36164;&#28304;&#38656;&#27714;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22270;&#32467;&#26500;&#21387;&#32553;&#65288;GC&#65289;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#20197;&#24212;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#22270;&#25968;&#25454;&#12290;GC&#30340;&#21160;&#26426;&#26159;&#23558;&#22823;&#22270;&#32553;&#23567;&#20026;&#36739;&#23567;&#30340;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#19979;&#28216;&#20219;&#21153;&#25152;&#38656;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;GC&#65292;&#24182;&#23558;&#20854;&#21306;&#21035;&#20110;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GC&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#26681;&#25454;&#30446;&#26631;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65292;&#24182;&#23558;&#29983;&#25104;&#21387;&#32553;&#22270;&#30340;&#20844;&#24335;&#20998;&#31867;&#20026;&#20462;&#25913;&#21407;&#22987;&#22270;&#25110;&#21512;&#25104;&#20840;&#26032;&#22270;&#20004;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#36824;&#21253;&#25324;&#23545;&#35813;&#39046;&#22495;&#20013;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35752;&#35770;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#27010;&#36848;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#31616;&#26126;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analytics on large-scale graphs have posed significant challenges to computational efficiency and resource requirements. Recently, Graph condensation (GC) has emerged as a solution to address challenges arising from the escalating volume of graph data. The motivation of GC is to reduce the scale of large graphs to smaller ones while preserving essential information for downstream tasks. For a better understanding of GC and to distinguish it from other related topics, we present a formal definition of GC and establish a taxonomy that systematically categorizes existing methods into three types based on its objective, and classify the formulations to generate the condensed graphs into two categories as modifying the original graphs or synthetic completely new ones. Moreover, our survey includes a comprehensive analysis of datasets and evaluation metrics in this field. Finally, we conclude by addressing challenges and limitations, outlining future directions, and offering concise guidelin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#32500;&#24230;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#20302;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#36866;&#24212;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.01999</link><description>&lt;p&gt;
&#36793;&#32536;&#19978;&#22522;&#20110;&#26032;&#39062;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#32500;&#24230;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#20302;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#36866;&#24212;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#22312;&#32447;&#21644;&#31163;&#32447;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#19981;&#33021;&#26377;&#25928;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;&#32447;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#24120;&#24120;&#26114;&#36149;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#22312;&#32447;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#32447;&#24615;&#36229;&#32500;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#38750;&#32447;&#24615;&#20302;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34987;&#26144;&#23556;&#21040;&#39640;&#32500;&#65288;&#36229;&#32500;&#24230;&#65289;&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;TSF-HD&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20849;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#36229;&#32500;&#24230;&#26144;&#23556;&#21644;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#22120;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;TSF-HD&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#38477;&#20302;&#20102;&#25512;&#26029;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, both online and offline deep learning models have been developed for time series forecasting. However, offline deep forecasting models fail to adapt effectively to changes in time-series data, while online deep forecasting models are often expensive and have complex training procedures. In this paper, we reframe the online nonlinear time-series forecasting problem as one of linear hyperdimensional time-series forecasting. Nonlinear low-dimensional time-series data is mapped to high-dimensional (hyperdimensional) spaces for linear hyperdimensional prediction, allowing fast, efficient and lightweight online time-series forecasting. Our framework, TSF-HD, adapts to time-series distribution shifts using a novel co-training framework for its hyperdimensional mapping and its linear hyperdimensional predictor. TSF-HD is shown to outperform the state of the art, while having reduced inference latency, for both short-term and long-term time series forecasting. Our code is publi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01995</link><description>&lt;p&gt;
&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#65306;&#31532;&#19968;&#27425;&#36817;&#20284;&#31639;&#27861;&#65292;&#20855;&#26377;&#20840;&#32622;&#20449;&#21306;&#38388;&#38598;&#25104;&#30340;&#23398;&#20064;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#65292;&#23558;&#26377;&#38480;&#30340;&#27835;&#30103;&#39044;&#31639;&#20998;&#37197;&#21040;&#21487;&#29992;&#30340;&#39118;&#38505;&#26102;&#38388;&#19978;&#26159;&#20943;&#23569;&#29992;&#25143;&#30130;&#21171;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#30693;&#30340;&#23454;&#38469;&#39118;&#38505;&#26102;&#38388;&#25968;&#37327;&#65292;&#36825;&#19968;&#31574;&#30053;&#36935;&#21040;&#20102;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#24341;&#20837;&#36817;&#20284;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#31454;&#20105;&#27604;&#20998;&#26512;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#39564;&#21644;HeartSteps&#31227;&#21160;&#24212;&#29992;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital health, the strategy of allocating a limited treatment budget across available risk times is crucial to reduce user fatigue. This strategy, however, encounters a significant obstacle due to the unknown actual number of risk times, a factor not adequately addressed by existing methods lacking theoretical guarantees. This paper introduces, for the first time, the online uniform risk times sampling problem within the approximation algorithm framework. We propose two online approximation algorithms for this problem, one with and one without learning augmentation, and provide rigorous theoretical performance guarantees for them using competitive ratio analysis. We assess the performance of our algorithms using both synthetic experiments and a real-world case study on HeartSteps mobile applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01987</link><description>&lt;p&gt;
&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;RSV&#30149;&#20363;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Transfer Learning for RSV Case Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#26102;&#65292;&#20250;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#26631;&#27880;&#20449;&#24687;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#39044;&#27979;&#20307;&#31215;&#33258;&#36866;&#24212;&#21152;&#26435;&#65288;PVAW&#65289;&#12290;PVAW&#22312;&#25972;&#21512;&#27169;&#22411;&#20013;&#21019;&#36896;&#24615;&#22320;&#23454;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#36129;&#29486;&#24230;&#33258;&#21160;&#35843;&#25972;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#23398;&#20013;&#24515;&#25910;&#38598;&#30340;&#22810;&#20010;&#23395;&#33410;&#30340;&#21628;&#21560;&#36947;&#21512;&#32990;&#30149;&#27602;&#65288;RSV&#65289;&#25968;&#25454;&#19978;&#24212;&#29992;PVAW&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#22312;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#37325;&#21551;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#35813;&#25216;&#26415;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#24076;&#26395;&#33021;&#21551;&#21457;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.01981</link><description>&lt;p&gt;
&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38646;-shot&#35782;&#21035;&#21644;&#38477;&#20302;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#37325;&#21551;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#35813;&#25216;&#26415;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#24076;&#26395;&#33021;&#21551;&#21457;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20063;&#23481;&#26131;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#36825;&#22312;&#27809;&#26377;&#21487;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#38646;-shot&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#21051;&#26495;&#21360;&#35937;&#12290;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;&#35299;&#37322;&#33258;&#25105;&#21435;&#20559;&#24046;&#21644;&#37325;&#21551;&#33258;&#25105;&#21435;&#20559;&#24046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#25105;&#21435;&#20559;&#24046;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#21482;&#20381;&#36182;&#20110;LLM&#33258;&#36523;&#21644;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#20854;&#20013;&#35299;&#37322;&#27491;&#30830;&#22320;&#35782;&#21035;&#20986;&#26080;&#25928;&#30340;&#20551;&#35774;&#65292;&#32780;&#37325;&#21551;&#21017;&#20135;&#29983;&#20102;&#26368;&#22823;&#30340;&#20559;&#35265;&#20943;&#23569;&#25928;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#24320;&#21551;&#23545;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#65292;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#22411;&#30340;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01975</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#65292;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#22411;&#30340;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#30001;&#20854;&#21407;&#23376;&#12289;&#21407;&#23376;&#23646;&#24615;&#21644;&#20998;&#23376;&#30340;&#20849;&#20215;&#38190;&#32452;&#25104;&#12290;&#20998;&#23376;&#30340;3D&#65288;&#20960;&#20309;&#65289;&#34920;&#31034;&#31216;&#20026;&#26500;&#22411;&#65292;&#30001;&#20854;&#21407;&#23376;&#31867;&#22411;&#21644;&#31515;&#21345;&#23572;&#22352;&#26631;&#32452;&#25104;&#12290;&#27599;&#20010;&#26500;&#22411;&#37117;&#20855;&#26377;&#28508;&#22312;&#33021;&#37327;&#65292;&#33021;&#37327;&#36234;&#20302;&#65292;&#20854;&#22312;&#33258;&#28982;&#30028;&#20013;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#36234;&#22823;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#21482;&#32771;&#34385;2D&#20998;&#23376;&#22270;&#65292;&#35201;&#20040;&#21482;&#32771;&#34385;3D&#26500;&#22411;&#32467;&#26500;&#34920;&#31034;&#12290;&#21463;&#21040;&#26368;&#36817;&#20851;&#20110;&#22312;2D&#22270;&#34920;&#31034;&#21644;&#26500;&#22411;&#38598;&#21512;&#20013;&#20351;&#29992;&#38598;&#25104;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#26032;&#22411;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#29992;&#20110;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A molecule's 2D representation consists of its atoms, their attributes, and the molecule's covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose E(3)-invariant molecular conformer aggregation networks. The method integrates a molecule's 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D--3D aggregation mechanism based on a differentiable solver for the \emph{Fused Gromov-Wasserstein Barycenter} problem and the use of an efficient online conformer generation method based 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#39640;&#25928;&#25554;&#20214;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#24322;&#36136;&#22240;&#26524;&#23545;&#27604;&#65292;&#24182;&#35299;&#20915;&#20102;&#20854;&#20182;&#23398;&#20064;&#31574;&#30053;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#20154;&#21475;&#39118;&#38505;&#20989;&#25968;&#30340;&#39640;&#25928;&#25554;&#20214;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01972</link><description>&lt;p&gt;
&#32452;&#21512;T-learning&#21644;DR-learning&#65306;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#22240;&#26524;&#23545;&#27604;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Combining T-learning and DR-learning: a framework for oracle-efficient estimation of causal contrasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01972
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#39640;&#25928;&#25554;&#20214;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#24322;&#36136;&#22240;&#26524;&#23545;&#27604;&#65292;&#24182;&#35299;&#20915;&#20102;&#20854;&#20182;&#23398;&#20064;&#31574;&#30053;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#20154;&#21475;&#39118;&#38505;&#20989;&#25968;&#30340;&#39640;&#25928;&#25554;&#20214;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#25554;&#20214;&#65288;EP&#65289;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#24322;&#36136;&#22240;&#26524;&#23545;&#27604;&#30340;&#26032;&#26694;&#26550;&#65292;&#20363;&#22914;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#21644;&#26465;&#20214;&#30456;&#23545;&#39118;&#38505;&#12290; EP&#23398;&#20064;&#26694;&#26550;&#20139;&#26377;&#19982;Neyman&#27491;&#20132;&#23398;&#20064;&#31574;&#30053;&#65288;&#22914;DR-learning&#21644;R-learning&#65289;&#30456;&#21516;&#30340;oracle&#25928;&#29575;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#20027;&#35201;&#32570;&#28857;&#65292;&#21253;&#25324;&#65288;i&#65289;&#23454;&#38469;&#36866;&#29992;&#24615;&#21487;&#33021;&#21463;&#21040;&#25439;&#22833;&#20989;&#25968;&#38750;&#20984;&#24615;&#30340;&#38459;&#30861;&#65307; &#65288;ii&#65289;&#23427;&#20204;&#21487;&#33021;&#22240;&#36829;&#21453;&#30028;&#38480;&#30340;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#21644;&#20266;&#32467;&#26524;&#32780;&#23548;&#33268;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#24046;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#32570;&#28857;&#65292;EP&#23398;&#20064;&#32773;&#26500;&#24314;&#20102;&#22240;&#26524;&#23545;&#27604;&#30340;&#20154;&#21475;&#39118;&#38505;&#20989;&#25968;&#30340;&#39640;&#25928;&#25554;&#20214;&#20272;&#35745;&#22120;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;T-learning&#31561;&#25554;&#20214;&#20272;&#35745;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#29305;&#24615;&#12290;&#22312;&#21512;&#29702;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;EP&#23398;&#20064;&#32773;&#20855;&#26377;oracle&#25928;&#29575;&#65292;&#34920;&#29616;&#20986;&#28176;&#36817;&#31561;&#20215;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce efficient plug-in (EP) learning, a novel framework for the estimation of heterogeneous causal contrasts, such as the conditional average treatment effect and conditional relative risk. The EP-learning framework enjoys the same oracle-efficiency as Neyman-orthogonal learning strategies, such as DR-learning and R-learning, while addressing some of their primary drawbacks, including that (i) their practical applicability can be hindered by loss function non-convexity; and (ii) they may suffer from poor performance and instability due to inverse probability weighting and pseudo-outcomes that violate bounds. To avoid these drawbacks, EP-learner constructs an efficient plug-in estimator of the population risk function for the causal contrast, thereby inheriting the stability and robustness properties of plug-in estimation strategies like T-learning. Under reasonable conditions, EP-learners based on empirical risk minimization are oracle-efficient, exhibiting asymptotic equivalen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#34562;&#31389;&#35206;&#30422;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#29420;&#31435;&#25910;&#38598;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#30495;&#23454;&#20540;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#21644;&#39640;&#25928;&#31283;&#20581;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;CatBoost&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01969</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation-Enhanced Data Augmentation for Machine Learning Pathloss Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#34562;&#31389;&#35206;&#30422;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#29420;&#31435;&#25910;&#38598;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#30495;&#23454;&#20540;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#21644;&#39640;&#25928;&#31283;&#20581;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;CatBoost&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23545;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#20250;&#38477;&#20302;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20174;&#34562;&#31389;&#35206;&#30422;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#29420;&#31435;&#25910;&#38598;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#65288;&#21253;&#25324;&#20892;&#22330;&#12289;&#19992;&#38517;&#22320;&#24102;&#21644;&#20303;&#23429;&#21306;&#65289;&#24320;&#23637;&#24191;&#27867;&#30340;&#27979;&#37327;&#27963;&#21160;&#26469;&#25910;&#38598;&#12290;&#36825;&#31181;&#20840;&#38754;&#30340;&#25968;&#25454;&#25910;&#38598;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#32452;&#20449;&#36947;&#29305;&#24449;&#65292;&#21253;&#25324;&#20174;LiDAR&#25968;&#25454;&#38598;&#20013;&#23548;&#20986;&#30340;&#22320;&#29702;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;CatBoost&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#27169;&#25311;&#25968;&#25454;&#30340;&#38598;&#25104;&#26174;&#33879;&#25913;&#21892;&#20102;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) offers a promising solution to pathloss prediction. However, its effectiveness can be degraded by the limited availability of data. To alleviate these challenges, this paper introduces a novel simulation-enhanced data augmentation method for ML pathloss prediction. Our method integrates synthetic data generated from a cellular coverage simulator and independently collected real-world datasets. These datasets were collected through an extensive measurement campaign in different environments, including farms, hilly terrains, and residential areas. This comprehensive data collection provides vital ground truth for model training. A set of channel features was engineered, including geographical attributes derived from LiDAR datasets. These features were then used to train our prediction model, incorporating the highly efficient and robust gradient boosting ML algorithm, CatBoost. The integration of synthetic data, as demonstrated in our study, significantly improves t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.01968</link><description>&lt;p&gt;
&#23545;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#35843;&#26597;&#65306;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#20852;&#20027;&#39064;&#30340;&#20852;&#36215;&#65292;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#20852;&#36259;&#27491;&#22312;&#22686;&#21152;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#33879;&#25104;&#23601;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#33258;&#20027;&#20195;&#29702;&#20013;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#20351;&#36825;&#20123;&#20195;&#29702;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#23548;&#33322;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#22788;&#29702;&#21160;&#24577;&#24773;&#20917;&#26102;&#65292;&#19978;&#19979;&#25991;&#24847;&#35782;&#25104;&#20026;&#24378;&#21270;&#22810;agent&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;agent&#31995;&#32479;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#27010;&#36848;&#22914;&#20309;&#23558;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#19982;&#22810;agent&#31995;&#32479;&#38598;&#25104;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#26368;&#20808;&#36827;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20419;&#36827;&#36825;&#20123;&#31995;&#32479;&#20043;&#38388;&#38598;&#25104;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810; agent &#31995;&#32479;&#30340;&#29305;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36807;&#31243;&#26469;&#24314;&#27169;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;agent&#31995;&#32479;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#28176;&#36817;&#35774;&#32622;&#19979;&#30340;&#31934;&#30830;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#21644;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01965</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20248;&#21270;&#23545;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#28176;&#36817;&#35774;&#32622;&#19979;&#30340;&#31934;&#30830;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#21644;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#20013;&#21464;&#24471;&#24191;&#27867;&#20351;&#29992;&#12290;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#33073;&#39062;&#32780;&#20986;&#65292;&#38656;&#35201;&#20272;&#35745;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#25968;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20998;&#25968;&#21305;&#37197;&#21644;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#37325;&#26032;&#26500;&#24314;&#20026;&#20984;&#20248;&#21270;&#30340;&#24418;&#24335;&#65292;&#26469;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25193;&#25955;&#29702;&#35770;&#20027;&#35201;&#26159;&#28176;&#36817;&#30340;&#65292;&#20294;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#65292;&#24182;&#24314;&#31435;&#20102;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#28176;&#36817;&#35774;&#32622;&#20013;&#23398;&#20064;&#21040;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. Though existing diffusion theory is mainly asymptotic, we characterize the exact predicted score function and establish the convergence result for neural network-based diffusion models with finite data. This work contributes to understanding what neural network-based diffusion model learns in non-asymptotic settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#26102;&#24577;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#21521;&#26368;&#36817;&#37319;&#26679;&#31574;&#30053;&#21644;GPU&#21487;&#25191;&#34892;&#30340;&#22823;&#23567;&#21463;&#38480;&#21704;&#24076;&#34920;&#23454;&#29616;&#20102;&#23545;&#26597;&#35810;&#30340;&#24555;&#36895;&#21709;&#24212;&#21644;&#26368;&#23567;&#21270;&#25512;&#29702;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.01964</link><description>&lt;p&gt;
&#19981;&#38656;&#22238;&#39038;&#65306;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#26102;&#24577;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Need to Look Back: An Efficient and Scalable Approach for Temporal Network Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#26102;&#24577;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#21521;&#26368;&#36817;&#37319;&#26679;&#31574;&#30053;&#21644;GPU&#21487;&#25191;&#34892;&#30340;&#22823;&#23567;&#21463;&#38480;&#21704;&#24076;&#34920;&#23454;&#29616;&#20102;&#23545;&#26597;&#35810;&#30340;&#24555;&#36895;&#21709;&#24212;&#21644;&#26368;&#23567;&#21270;&#25512;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;TGRL&#65289;&#23545;&#20110;&#24314;&#27169;&#23454;&#38469;&#32593;&#32476;&#20013;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;TGRL&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#21644;&#25512;&#29702;&#24310;&#36831;&#36739;&#39640;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#36827;&#34892;&#27169;&#22411;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#22238;&#28335;&#27599;&#20010;&#33410;&#28857;&#30340;&#20132;&#20114;&#21382;&#21490;&#26469;&#36827;&#34892;&#26102;&#24577;&#37051;&#23621;&#30340;&#20302;&#25928;&#37319;&#26679;&#25152;&#33268;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;TGRL&#26694;&#26550;&#65292;&#21517;&#20026;No-Looking-Back&#65288;NLB&#65289;&#12290;NLB&#37319;&#29992;&#20102;&#8220;&#21069;&#21521;&#26368;&#36817;&#37319;&#26679;&#8221;&#31574;&#30053;&#65292;&#32469;&#36807;&#20102;&#22238;&#28335;&#21382;&#21490;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#30340;GPU&#21487;&#25191;&#34892;&#30340;&#22823;&#23567;&#21463;&#38480;&#21704;&#24076;&#34920;&#35760;&#24405;&#19979;&#37319;&#26679;&#21518;&#30340;&#26368;&#36817;&#20132;&#20114;&#65292;&#23454;&#29616;&#23545;&#26597;&#35810;&#30340;&#24555;&#36895;&#21709;&#24212;&#21644;&#26368;&#23567;&#21270;&#25512;&#29702;&#24310;&#36831;&#12290;&#35813;&#21704;&#24076;&#34920;&#30340;&#32500;&#25252;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;$O(1)$&#12290;NLB&#19982;GPU&#22788;&#29702;&#23436;&#20840;&#20860;&#23481;&#65292;&#26368;&#22823;&#21270;&#20102;&#21487;&#32534;&#31243;&#24615;&#12289;&#24182;&#34892;&#24615;&#21644;&#33021;&#25928;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Temporal graph representation learning (TGRL) is crucial for modeling complex, dynamic systems in real-world networks. Traditional TGRL methods, though effective, suffer from high computational demands and inference latency. This is mainly induced by their inefficient sampling of temporal neighbors by backtracking the interaction history of each node when making model inference. This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB). NLB employs a "forward recent sampling" strategy, which bypasses the need for backtracking historical interactions. This strategy is implemented using a GPU-executable size-constrained hash table for each node, recording down-sampled recent interactions, which enables rapid response to queries with minimal inference latency. The maintenance of this hash table is highly efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing, maximizing programmability, parallelism, and power efficiency. Empirical evaluations de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01963</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26631;&#31614;&#24816;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#26631;&#31614;&#35789;&#27719;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#20256;&#32479;k&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#28436;&#21270;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#22823;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;MEDLINE&#29983;&#29289;&#21307;&#23398;&#25991;&#26723;&#38598;&#21512;&#30340;&#22823;&#37096;&#20998;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#35813;&#38598;&#21512;&#20351;&#29992;&#21307;&#23398;&#20027;&#39064;&#35789;&#65288;MeSH&#65289;&#35789;&#27719;&#34920;&#20316;&#20026;&#25511;&#21046;&#35789;&#27719;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#25991;&#26723;&#34920;&#31034;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a multi-label lazy learning approach to deal with automatic semantic indexing in large document collections in the presence of complex and structured label vocabularies with high inter-label correlation. The proposed method is an evolution of the traditional k-Nearest Neighbors algorithm which uses a large autoencoder trained to map the large label space to a reduced size latent space and to regenerate the predicted labels from this latent space. We have evaluated our proposal in a large portion of the MEDLINE biomedical document collection which uses the Medical Subject Headings (MeSH) thesaurus as a controlled vocabulary. In our experiments we propose and evaluate several document representation approaches and different label autoencoder configurations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#20301;&#25968;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#33021;&#22815;&#22312;&#20989;&#25968;&#23450;&#20041;&#22495;&#19978;&#21516;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#38656;&#20998;&#24067;&#20551;&#35774;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;2D Darcy&#27969;&#21160;&#21644;3D&#36710;&#36742;&#34920;&#38754;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01960</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21512;&#39044;&#27979;&#23454;&#29616;&#36816;&#31639;&#22120;&#23398;&#20064;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01960
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#20301;&#25968;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#33021;&#22815;&#22312;&#20989;&#25968;&#23450;&#20041;&#22495;&#19978;&#21516;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#38656;&#20998;&#24067;&#20551;&#35774;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;2D Darcy&#27969;&#21160;&#21644;3D&#36710;&#36742;&#34920;&#38754;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31639;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#65292;&#20854;&#20013;&#24456;&#22810;&#24212;&#29992;&#38656;&#35201;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#30001;&#20110;&#36816;&#31639;&#22120;&#23398;&#20064;&#30340;&#36755;&#20986;&#26159;&#36830;&#32493;&#20989;&#25968;&#65292;&#22312;&#25972;&#20010;&#23450;&#20041;&#22495;&#19978;&#21516;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#32771;&#34385;&#19968;&#20010;&#28857;&#30340;&#26657;&#20934;&#65292;&#25110;&#32773;&#38024;&#23545;&#19968;&#20010;&#26631;&#37327;&#20989;&#25968;&#36827;&#34892;&#26657;&#20934;&#65292;&#25110;&#32773;&#20570;&#20986;&#24378;&#22823;&#30340;&#20551;&#35774;&#65292;&#27604;&#22914;&#20551;&#35774;&#39640;&#26031;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#30340;&#20998;&#20301;&#25968;&#31070;&#32463;&#36816;&#31639;&#22120;,&#19968;&#31181;&#26080;&#20998;&#24067;&#12289;&#26377;&#38480;&#26679;&#26412;&#30340;&#20989;&#25968;&#26657;&#20934;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#26657;&#20934;&#20445;&#35777;&#65292;&#21363;&#35206;&#30422;&#29575;&#65292;&#20854;&#23450;&#20041;&#20026;&#20989;&#25968;&#23450;&#20041;&#22495;&#20869;&#30495;&#23454;&#20540;&#20301;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#29699;&#20869;&#30340;&#39044;&#26399;&#30334;&#20998;&#27604;&#12290;&#22312;&#19968;&#20010;2D Darcy&#27969;&#21160;&#21644;&#19968;&#20010;3D&#36710;&#36742;&#34920;&#38754;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;&#26657;&#20934;&#30340;&#35206;&#30422;&#29575;&#21644;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operator learning has been increasingly adopted in scientific and engineering applications, many of which require calibrated uncertainty quantification. Since the output of operator learning is a continuous function, quantifying uncertainty simultaneously at all points in the domain is challenging. Current methods consider calibration at a single point or over one scalar function or make strong assumptions such as Gaussianity. We propose a risk-controlling quantile neural operator, a distribution-free, finite-sample functional calibration conformal prediction method. We provide a theoretical calibration guarantee on the coverage rate, defined as the expected percentage of points on the function domain whose true value lies within the predicted uncertainty ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure prediction tasks validate our theoretical results, demonstrating calibrated coverage and efficient uncertainty bands outperforming baseline methods. In particula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;OPSurv&#65292;&#36890;&#36807;&#27491;&#20132;&#22810;&#39033;&#24335;&#31215;&#20998;&#31639;&#27861;&#25552;&#20379;&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#24615;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#21021;&#22987;&#38646;&#26465;&#20214;&#21644;&#27491;&#20132;&#22810;&#39033;&#24335;&#30340;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#27599;&#20010;&#39118;&#38505;&#20107;&#20214;&#30340;&#21151;&#33021;&#36817;&#20284;&#31995;&#25968;&#30340;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#27861;&#26500;&#24314;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01955</link><description>&lt;p&gt;
OPSurv&#65306;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#27491;&#20132;&#22810;&#39033;&#24335;&#31215;&#20998;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;OPSurv&#65292;&#36890;&#36807;&#27491;&#20132;&#22810;&#39033;&#24335;&#31215;&#20998;&#31639;&#27861;&#25552;&#20379;&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#24615;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#21021;&#22987;&#38646;&#26465;&#20214;&#21644;&#27491;&#20132;&#22810;&#39033;&#24335;&#30340;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#27599;&#20010;&#39118;&#38505;&#20107;&#20214;&#30340;&#21151;&#33021;&#36817;&#20284;&#31995;&#25968;&#30340;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#27861;&#26500;&#24314;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#20013;&#21333;&#19968;&#39118;&#38505;&#21644;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#30340;Orthogonal Polynomials Quadrature Algorithm for Survival Analysis&#65288;OPSurv&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#24615;&#36755;&#20986;&#12290;OPSurv&#21033;&#29992;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#21021;&#22987;&#38646;&#26465;&#20214;&#21644;&#20351;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#23545;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#21807;&#19968;&#20998;&#35299;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39118;&#38505;&#20107;&#20214;&#23398;&#20064;&#21151;&#33021;&#36817;&#20284;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#27861;&#26500;&#24314;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23545;&#25239;&#36807;&#24230;&#25311;&#21512;&#65292;&#23588;&#20854;&#22312;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25511;&#21046;&#24615;&#12290;&#26412;&#25991;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;OPSurv&#30340;&#32463;&#39564;&#39564;&#35777;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#31361;&#20986;&#20102;&#20854;&#20316;&#20026;&#31454;&#20105;&#39118;&#38505;&#29983;&#23384;&#20998;&#26512;&#30340;&#19968;&#39033;&#37325;&#35201;&#36827;&#23637;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Orthogonal Polynomials Quadrature Algorithm for Survival Analysis (OPSurv), a new method providing time-continuous functional outputs for both single and competing risks scenarios in survival analysis. OPSurv utilizes the initial zero condition of the Cumulative Incidence function and a unique decomposition of probability densities using orthogonal polynomials, allowing it to learn functional approximation coefficients for each risk event and construct Cumulative Incidence Function estimates via Gauss--Legendre quadrature. This approach effectively counters overfitting, particularly in competing risks scenarios, enhancing model expressiveness and control. The paper further details empirical validations and theoretical justifications of OPSurv, highlighting its robust performance as an advancement in survival analysis with competing risks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PC-Winter&#30340;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35780;&#20272;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#22270;&#24418;&#32467;&#26500;&#20197;&#21450;&#35745;&#31639;&#25361;&#25112;&#65292;PC-Winter&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01943</link><description>&lt;p&gt;
&#26377;&#25928;&#22270;&#24418;&#25968;&#25454;&#20272;&#20540;&#30340;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Precedence-Constrained Winter Value for Effective Graph Data Valuation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01943
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PC-Winter&#30340;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35780;&#20272;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#22270;&#24418;&#32467;&#26500;&#20197;&#21450;&#35745;&#31639;&#25361;&#25112;&#65292;PC-Winter&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#37327;&#21270;&#25968;&#25454;&#30340;&#20215;&#20540;&#12289;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#21644;&#30830;&#23450;&#20844;&#24179;&#34917;&#20607;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#22312;&#35780;&#20272;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#30340;&#20215;&#20540;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#22270;&#24418;&#25968;&#25454;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22270;&#24418;&#25968;&#25454;&#20272;&#20540;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20215;&#20540;&#20272;&#35745;&#25104;&#26412;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#24418;&#25968;&#25454;&#20272;&#20540;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;(Precedence-Constrained Winter, PC-Winter)&#65292;&#20197;&#32771;&#34385;&#22797;&#26434;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#23454;&#29616;&#23545;PC-Winter&#30340;&#39640;&#25928;&#36817;&#20284;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;PC-Winter&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is essential for quantifying data's worth, aiding in assessing data quality and determining fair compensation. While existing data valuation methods have proven effective in evaluating the value of Euclidean data, they face limitations when applied to the increasingly popular graph-structured data. Particularly, graph data valuation introduces unique challenges, primarily stemming from the intricate dependencies among nodes and the exponential growth in value estimation costs. To address the challenging problem of graph data valuation, we put forth an innovative solution, Precedence-Constrained Winter (PC-Winter) Value, to account for the complex graph structure. Furthermore, we develop a variety of strategies to address the computational challenges and enable efficient approximation of PC-Winter. Extensive experiments demonstrate the effectiveness of PC-Winter across diverse datasets and tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#23383;&#24494;&#27169;&#22411;&#22312;&#20934;&#30830;&#21644;&#23433;&#20840;&#20132;&#26131;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#25968;&#23383;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.01931</link><description>&lt;p&gt;
&#20934;&#30830;&#21644;&#23433;&#20840;&#20132;&#26131;&#30340;&#25968;&#23383;&#24494;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Digits micro-model for accurate and secure transactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#23383;&#24494;&#27169;&#22411;&#22312;&#20934;&#30830;&#21644;&#23433;&#20840;&#20132;&#26131;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#25968;&#23383;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#34987;&#29992;&#20110;&#25552;&#21319;&#21628;&#21483;&#32773;&#20307;&#39564;&#65292;&#36890;&#36807;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#23454;&#29616;&#39640;&#25928;&#30452;&#35266;&#30340;&#20114;&#21160;&#12290;ASR&#31995;&#32479;&#30340;&#22686;&#21152;&#20351;&#29992;&#35201;&#27714;&#36825;&#20123;&#31995;&#32479;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;&#30446;&#21069;&#20027;&#35201;&#30340;ASR&#27169;&#22411;&#29992;&#20110;&#25968;&#23383;&#25968;&#25454;&#25910;&#38598;&#26159;&#22823;&#22411;&#36890;&#29992;&#21830;&#29992;&#27169;&#22411;- Google&#35821;&#38899;&#36716;&#25991;&#23383;&#65288;STT&#65289;&#25110;&#20122;&#39532;&#36874;&#36716;&#24405;-&#25110;&#24320;&#28304;&#65288;OpenAI&#30340;Whisper&#65289;&#12290;&#36825;&#20123;ASR&#27169;&#22411;&#36890;&#36807;&#25968;&#21313;&#19975;&#23567;&#26102;&#30340;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36816;&#34892;&#12290;&#23613;&#31649;&#26368;&#36817;&#22823;&#22411;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23567;&#22411;&#19987;&#38376;&#30340;&#8220;&#24494;&#8221;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#36825;&#26679;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#20110;80&#20998;&#38047;&#30340;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#33391;&#22909;&#20110;&#25968;&#23383;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#20013;&#65292;&#19982;Whisper&#25110;Google STT&#31561;&#36890;&#29992;&#27169;&#22411;&#31454;&#20105;&#65292;&#21516;&#26102;&#20351;&#29992;&#33267;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#20869;&#23384;&#36164;&#28304;&#12290;&#21478;&#22806;&#65292;&#19981;&#21516;&#20110;&#26356;&#22823;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#23383;&#24494;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) systems are used in the financial domain to enhance the caller experience by enabling natural language understanding and facilitating efficient and intuitive interactions. Increasing use of ASR systems requires that such systems exhibit very low error rates. The predominant ASR models to collect numeric data are large, general-purpose commercial models -- Google Speech-to-text (STT), or Amazon Transcribe -- or open source (OpenAI's Whisper). Such ASR models are trained on hundreds of thousands of hours of audio data and require considerable resources to run. Despite recent progress large speech recognition models, we highlight the potential of smaller, specialized "micro" models. Such light models can be trained perform well on number recognition specific tasks, competing with general models like Whisper or Google STT while using less than 80 minutes of training time and occupying at least an order of less memory resources. Also, unlike larger speech 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.01930</link><description>&lt;p&gt;
&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;
&lt;/p&gt;
&lt;p&gt;
Reducing Optimism Bias in Incomplete Cooperative Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#21327;&#21516;&#20915;&#31574;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#25351;&#23450;&#19968;&#20010;&#21512;&#20316;&#21338;&#24328;&#38656;&#35201;&#20026;&#25351;&#25968;&#22810;&#20010;&#32852;&#30431;&#20998;&#37197;&#20215;&#20540;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#19968;&#20010;&#32852;&#30431;&#20215;&#20540;&#21487;&#33021;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#19981;&#20844;&#24320;&#26576;&#20123;&#32852;&#30431;&#30340;&#20215;&#20540;&#20250;&#24341;&#20837;&#20851;&#20110;&#20010;&#20307;&#23545;&#38598;&#20307;&#22823;&#32852;&#30431;&#30340;&#36129;&#29486;&#30340;&#27169;&#31946;&#24615;&#12290;&#36825;&#31181;&#27169;&#31946;&#24615;&#32463;&#24120;&#23548;&#33268;&#29609;&#23478;&#25345;&#26377;&#36807;&#20110;&#20048;&#35266;&#30340;&#26399;&#26395;&#65292;&#20854;&#28304;&#20110;&#20869;&#22312;&#20559;&#35265;&#25110;&#25112;&#30053;&#32771;&#34385;&#65292;&#36827;&#32780;&#24120;&#24120;&#23548;&#33268;&#38598;&#20307;&#35201;&#27714;&#36229;&#36807;&#23454;&#38469;&#30340;&#22823;&#32852;&#30431;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#22320;&#32553;&#23567;&#21512;&#20316;&#21338;&#24328;&#20013;&#29609;&#23478;&#26399;&#26395;&#19982;&#21487;&#23454;&#29616;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative game theory has diverse applications in contemporary artificial intelligence, including domains like interpretable machine learning, resource allocation, and collaborative decision-making. However, specifying a cooperative game entails assigning values to exponentially many coalitions, and obtaining even a single value can be resource-intensive in practice. Yet simply leaving certain coalition values undisclosed introduces ambiguity regarding individual contributions to the collective grand coalition. This ambiguity often leads to players holding overly optimistic expectations, stemming from either inherent biases or strategic considerations, frequently resulting in collective claims exceeding the actual grand coalition value. In this paper, we present a framework aimed at optimizing the sequence for revealing coalition values, with the overarching goal of efficiently closing the gap between players' expectations and achievable outcomes in cooperative games. Our contributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01929</link><description>&lt;p&gt;
&#26679;&#26412;&#12289;&#20272;&#35745;&#12289;&#32858;&#21512;&#65306;&#22240;&#26524;&#21457;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample, estimate, aggregate: A recipe for causal discovery foundation models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#30740;&#31350;&#12289;&#25351;&#23548;&#20915;&#31574;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#20351;&#23427;&#20204;&#21464;&#24471;&#32531;&#24930;&#12289;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#24182;&#19988;&#33030;&#24369;&#12290;&#21463;&#22522;&#30784;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#29992;&#20110;&#22788;&#29702;&#22312;&#36739;&#23567;&#30340;&#21464;&#37327;&#23376;&#38598;&#19978;&#36816;&#34892;&#30340;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#22312;&#23567;&#38382;&#39064;&#19978;&#35745;&#31639;&#36895;&#24230;&#24555;&#65292;&#23545;&#65288;&#36793;&#38469;&#65289;&#25968;&#25454;&#32467;&#26500;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#19988;&#23427;&#20204;&#30340;&#36755;&#20986;&#32467;&#26500;&#20316;&#20026;&#23545;&#35937;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65292;&#24182;&#19988;&#25552;&#20379;&#27604;&#29616;&#26377;&#27169;&#22411;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#23545;&#24378;&#20581;&#24615;&#30340;&#32771;&#34385;&#12290;&#35813;&#35843;&#26597;&#35752;&#35770;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.01928</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24378;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations in Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#23545;&#24378;&#20581;&#24615;&#30340;&#32771;&#34385;&#12290;&#35813;&#35843;&#26597;&#35752;&#35770;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#34987;&#35748;&#20026;&#38750;&#24120;&#36866;&#21512;&#20026;&#21463;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#30340;&#23545;&#35937;&#25552;&#20379;&#31639;&#27861;&#19978;&#30340;&#34917;&#25937;&#25514;&#26045;&#12290;&#23613;&#31649;CEs&#23545;&#21463;&#24433;&#21709;&#30340;&#20010;&#20307;&#26377;&#30410;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#33719;&#21462;CEs&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#20851;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#24378;&#20581;&#24615;&#21487;&#33021;&#20250;&#25439;&#23475;CEs&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#37319;&#21462;&#25216;&#26415;&#26469;&#20943;&#36731;&#36825;&#20010;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24378;&#20581;CEs&#36825;&#19968;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#24182;&#23545;&#23427;&#20204;&#32771;&#34385;&#30340;&#24378;&#20581;&#24615;&#24418;&#24335;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations (CEs) are advocated as being ideally suited to providing algorithmic recourse for subjects affected by the predictions of machine learning models. While CEs can be beneficial to affected individuals, recent work has exposed severe issues related to the robustness of state-of-the-art methods for obtaining CEs. Since a lack of robustness may compromise the validity of CEs, techniques to mitigate this risk are in order. In this survey, we review works in the rapidly growing area of robust CEs and perform an in-depth analysis of the forms of robustness they consider. We also discuss existing solutions and their limitations, providing a solid foundation for future developments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01922</link><description>&lt;p&gt;
&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning from Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#30528;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#24369;&#30417;&#30563;&#30340;&#21508;&#31181;&#22330;&#26223;&#21644;&#30001;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;GLWS&#65289;&#12290;GLWS&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#30340;&#20844;&#24335;&#65292;&#28789;&#27963;&#22320;&#36866;&#24212;&#20102;&#21508;&#31181;&#24369;&#30417;&#30563;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#12289;&#32858;&#21512;&#32479;&#35745;&#12289;&#25104;&#23545;&#35266;&#23519;&#21644;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;NFA&#65289;&#20197;&#21450;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;EM&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#24120;&#25152;&#38656;&#30340;&#20108;&#27425;&#25110;&#38454;&#20056;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;&#22240;&#27492;&#65292;&#20174;&#20219;&#24847;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20102;&#23545;&#23427;&#20204;&#36827;&#34892;NFA&#24314;&#27169;&#12290;GLWS&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;+
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01920</link><description>&lt;p&gt;
&#23545;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#30340;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Preference Poisoning Attacks on Reward Model Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01920
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20004;&#20004;&#27604;&#36739;&#20013;&#23398;&#20064;&#25928;&#29992;&#25110;&#22870;&#21169;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#26412;&#36136;&#19978;&#38656;&#35201;&#20174;&#20154;&#20204;&#37027;&#37324;&#25910;&#38598;&#20559;&#22909;&#20449;&#24687;&#65292;&#32780;&#21453;&#39304;&#36890;&#24120;&#26159;&#21311;&#21517;&#25552;&#20379;&#30340;&#12290;&#30001;&#20110;&#20559;&#22909;&#26159;&#20027;&#35266;&#30340;&#65292;&#27809;&#26377;&#21487;&#20197;&#27604;&#36739;&#30340;&#40644;&#37329;&#26631;&#20934;&#65307;&#28982;&#32780;&#65292;&#23545;&#20559;&#22909;&#23398;&#20064;&#30340;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20381;&#36182;&#24615;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#20542;&#21521;&#20110;&#25197;&#26354;&#20197;&#36798;&#21040;&#20854;&#30446;&#30340;&#32780;&#37319;&#38598;&#30340;&#25968;&#25454;&#21019;&#36896;&#20102;&#24378;&#28872;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#31181;&#23041;&#32961;&#27169;&#22411;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#31181;&#28431;&#27934;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#65292;&#20197;&#20419;&#36827;&#25110;&#36140;&#20302;&#30446;&#26631;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#29992;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#31639;&#27861;&#26041;&#27861;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#26799;&#24230;&#26694;&#26550;&#21644;&#20960;&#31181;&#21464;&#31181;&#30340;&#25353;&#36317;&#31163;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31867;&#26368;&#20339;&#25915;&#20987;&#22312;&#25104;&#21151;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#25551;&#36848;&#31526;&#20998;&#37197;&#26041;&#27861;&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01916</link><description>&lt;p&gt;
CoLe&#21644;LYS&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#34920;&#24449;&#20998;&#37197;&#30340;&#30456;&#20284;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01916
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#25551;&#36848;&#31526;&#20998;&#37197;&#26041;&#27861;&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;&#20102;BioASQ&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#25351;&#26631;&#25361;&#25112;&#36187;&#30340;MESINESP&#20219;&#21153;&#12290;&#21442;&#19982;&#30340;&#31995;&#32479;&#20165;&#20351;&#29992;&#20102;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;IBECS/LILACS&#25991;&#26723;&#20013;&#25552;&#21462;&#32034;&#24341;&#26415;&#35821;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#23384;&#20648;&#22312;Apache Lucene&#32034;&#24341;&#20013;&#12290;&#36825;&#20123;&#32034;&#24341;&#34920;&#31034;&#20351;&#29992;&#35201;&#27880;&#37322;&#30340;&#25991;&#31456;&#20869;&#23481;&#36827;&#34892;&#26597;&#35810;&#65292;&#24182;&#20174;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21019;&#24314;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#20840;&#38598;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20855;&#26377;&#39640;&#20849;&#29616;&#24471;&#20998;&#30340;DeCS&#26631;&#31614;&#37197;&#23545;&#24182;&#21019;&#24314;&#20803;&#26631;&#31614;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20010;&#20154;&#36164;&#26009;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#23448;&#26041;&#36816;&#34892;&#20013;&#33719;&#24471;&#30340;&#32467;&#26524;&#20284;&#20046;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe our participation in the MESINESP Task of the BioASQ biomedical semantic indexing challenge. The participating system follows an approach based solely on conventional information retrieval tools. We have evaluated various alternatives for extracting index terms from IBECS/LILACS documents in order to be stored in an Apache Lucene index. Those indexed representations are queried using the contents of the article to be annotated and a ranked list of candidate labels is created from the retrieved documents. We also have evaluated a sort of limited Label Powerset approach which creates meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an alternative method based on label profile matching. Results obtained in official runs seem to confirm the suitability of this approach for languages like Spanish.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#28608;&#27963;&#23494;&#24230;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861; DEFT&#12290;&#30740;&#31350;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23384;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#25439;&#22833;&#26469;&#20419;&#36827;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#27969;&#30340;PEFT&#25216;&#26415;&#65292;&#21253;&#25324;QLoRA&#12289;LoRA&#12289;Adapter&#12289;Prompt/Prefix Tuning&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01911</link><description>&lt;p&gt;
&#20174;PEFT&#21040;DEFT&#65306;&#29992;&#20110;&#20943;&#23569;&#21464;&#21387;&#22120;&#20013;&#28608;&#27963;&#23494;&#24230;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#28608;&#27963;&#23494;&#24230;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861; DEFT&#12290;&#30740;&#31350;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23384;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#25439;&#22833;&#26469;&#20419;&#36827;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#27969;&#30340;PEFT&#25216;&#26415;&#65292;&#21253;&#25324;QLoRA&#12289;LoRA&#12289;Adapter&#12289;Prompt/Prefix Tuning&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#30340;&#20107;&#23454;&#19978;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#20316;&#20026;&#26377;&#25928;&#36866;&#24212;PLMs&#30340;&#25163;&#27573;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#20013;&#22810;&#23618;&#24863;&#30693;&#65288;MLP&#65289;&#27169;&#22359;&#30340;&#20013;&#38388;&#36755;&#20986;&#20013;&#23384;&#22312;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#12290;&#20302;&#28608;&#27963;&#23494;&#24230;&#33021;&#22815;&#22312;&#25903;&#25345;&#31232;&#30095;&#24863;&#30693;&#30828;&#20214;&#19978;&#23454;&#29616;&#39640;&#25928;&#27169;&#22411;&#25512;&#26029;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#22312;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23494;&#24230;&#25439;&#22833;&#65292;&#40723;&#21169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65288;&#31561;&#20215;&#20110;&#26356;&#20302;&#30340;&#28608;&#27963;&#23494;&#24230;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#21253;&#25324;QLoRA&#12289;LoRA&#12289;Adapter&#12289;Prompt/Prefix Tuning&#22312;&#20869;&#30340;&#20027;&#27969;PEFT&#25216;&#26415;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20419;&#36827;&#22312;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perception (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter, Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse downstream tasks. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.01909</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Catastrophic Inheritance of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#22768;&#31216;&#20855;&#26377;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#20854;&#20182;&#21508;&#20010;&#23398;&#31185;&#20013;&#30340;&#31070;&#31192;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#28508;&#21147;&#25552;&#20986;&#20102;&#26497;&#22823;&#20851;&#20999;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;LFMs&#20013;&#26681;&#28145;&#33922;&#22266;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#21253;&#25324;&#21463;&#25439;&#12289;&#38271;&#23614;&#12289;&#26377;&#22122;&#38899;&#12289;&#36229;&#20986;&#20998;&#24067;&#31561;&#26679;&#26412;&#12290;&#36825;&#31181;&#32487;&#25215;&#21487;&#33021;&#23545;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#65292;&#22914;&#20559;&#35265;&#12289;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12289;&#24615;&#33021;&#19979;&#38477;&#12289;&#23433;&#20840;&#28431;&#27934;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#20215;&#20540;&#35823;&#24046;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#38382;&#39064;&#32972;&#21518;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26469;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#21253;&#25324;&#26469;&#33258;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#36866;&#24212;&#30340;&#32487;&#25215;&#20869;&#23481;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inher
&lt;/p&gt;</description></item><item><title>EBV&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#12289;&#26377;&#25928;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25366;&#25496;&#21644;&#39044;&#27979;&#34588;&#34562;&#30340;&#28201;&#24230;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.01902</link><description>&lt;p&gt;
EBV: &#22522;&#20110;&#21407;&#29702;&#25366;&#25496;&#21644;&#39044;&#27979;&#34588;&#34562;&#26102;&#38388;&#24207;&#21015;&#30340;&#30005;&#23376;&#34588;&#34562;&#20861;&#21307;
&lt;/p&gt;
&lt;p&gt;
EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting of Honeybee Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01902
&lt;/p&gt;
&lt;p&gt;
EBV&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#12289;&#26377;&#25928;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25366;&#25496;&#21644;&#39044;&#27979;&#34588;&#34562;&#30340;&#28201;&#24230;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#34562;&#23545;&#20256;&#31881;&#21644;&#39135;&#29289;&#29983;&#20135;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35832;&#22810;&#22240;&#32032;&#20013;&#65292;&#26497;&#31471;&#28201;&#24230;&#65288;&#20363;&#22914;&#27668;&#20505;&#21464;&#21270;&#65289;&#23545;&#34588;&#34562;&#20581;&#24247;&#29305;&#21035;&#21361;&#38505;&#12290;&#39044;&#27979;&#36825;&#31181;&#26497;&#31471;&#24773;&#20917;&#23558;&#20351;&#20859;&#34562;&#20154;&#21487;&#20197;&#37319;&#21462;&#26089;&#26399;&#39044;&#38450;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#32473;&#23450;&#26469;&#33258;&#34562;&#31665;&#30340;&#20256;&#24863;&#22120;&#65288;&#28201;&#24230;&#65289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25105;&#20204;&#22914;&#20309;&#25214;&#21040;&#27169;&#24335;&#24182;&#36827;&#34892;&#39044;&#27979;&#65311;&#39044;&#27979;&#23545;&#20110;&#21457;&#29616;&#24847;&#22806;&#34892;&#20026;&#24182;&#21521;&#20859;&#34562;&#20154;&#21592;&#21457;&#20986;&#35686;&#21578;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#65311;ARIMA&#12289;RNNs&#36824;&#26159;&#20854;&#20182;&#19968;&#20123;&#26041;&#27861;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;EBV&#65288;&#30005;&#23376;&#34588;&#34562;&#20861;&#21307;&#65289;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#26377;&#30410;&#29305;&#24615;&#65306;(i) &#22522;&#20110;&#21407;&#29702;&#65306;&#23427;&#22522;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#25193;&#25955;&#26041;&#31243;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#21453;&#39304;&#29615;&#25511;&#21046;&#22120;&#65307;(ii) &#26377;&#25928;&#65306;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#36816;&#34892;&#33391;&#22909;&#65307;(iii) &#21487;&#35299;&#37322;&#65306;&#23427;&#21482;&#38656;&#35201;&#19968;&#20123;&#20859;&#34562;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#21644;&#20449;&#20219;&#30340;&#21442;&#25968;&#65288;&#20363;&#22914;&#34588;&#34562;&#24378;&#24230;&#65289;&#65307;(iv) &#21487;&#25193;&#23637;&#65306;&#23427;&#21487;&#20197;&#36866;&#24212;&#22823;&#35268;&#27169;&#25968;&#25454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honeybees are vital for pollination and food production. Among many factors, extreme temperature (e.g., due to climate change) is particularly dangerous for bee health. Anticipating such extremities would allow beekeepers to take early preventive action. Thus, given sensor (temperature) time series data from beehives, how can we find patterns and do forecasting? Forecasting is crucial as it helps spot unexpected behavior and thus issue warnings to the beekeepers. In that case, what are the right models for forecasting? ARIMA, RNNs, or something else?   We propose the EBV (Electronic Bee-Veterinarian) method, which has the following desirable properties: (i) principled: it is based on a) diffusion equations from physics and b) control theory for feedback-loop controllers; (ii) effective: it works well on multiple, real-world time sequences, (iii) explainable: it needs only a handful of parameters (e.g., bee strength) that beekeepers can easily understand and trust, and (iv) scalable: it
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#65288;EBRM&#65289;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#12290;&#22312;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#19979;&#65292;&#24314;&#31435;&#20102;EBRM&#20272;&#35745;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.01900</link><description>&lt;p&gt;
&#20351;&#29992;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Distributional Off-policy Evaluation with Bellman Residual Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#65288;EBRM&#65289;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#12290;&#22312;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#19979;&#65292;&#24314;&#31435;&#20102;EBRM&#20272;&#35745;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#23427;&#26159;&#35768;&#22810;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#65288;&#20381;&#36182;&#20110;&#26368;&#22823;&#20540;-&#25193;&#23637;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#22914;&#26368;&#22823;&#20540;Wasserstein&#36317;&#31163;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#37327;&#21270;&#20998;&#24067;&#24335;Bellman&#27531;&#24046;&#30340;&#26399;&#26395;-&#25193;&#23637;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#21487;&#20197;&#19978;&#30028;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#30340;&#26399;&#26395;&#35823;&#24046;&#12290;&#22522;&#20110;&#36825;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#24615;&#36136;&#65292;&#36890;&#36807;&#23558;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#26694;&#26550;&#25512;&#24191;&#21040;DRL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#65288;EBRM&#65289;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;EBRM&#20272;&#35745;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27493;&#24341;&#23548;&#36807;&#31243;&#30340;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#20197;&#23454;&#29616;&#22810;&#27493;&#25193;&#23637;&#12290;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#27493;&#38271;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distributional off-policy evaluation which serves as the foundation of many distributional reinforcement learning (DRL) algorithms. In contrast to most existing works (that rely on supremum-extended statistical distances such as supremum-Wasserstein distance), we study the expectation-extended statistical distance for quantifying the distributional Bellman residuals and show that it can upper bound the expected error of estimating the return distribution. Based on this appealing property, by extending the framework of Bellman residual minimization to DRL, we propose a method called Energy Bellman Residual Minimizer (EBRM) to estimate the return distribution. We establish a finite-sample error bound for the EBRM estimator under the realizability assumption. Furthermore, we introduce a variant of our method based on a multi-step bootstrapping procedure to enable multi-step extension. By selecting an appropriate step level, we obtain a better error bound for thi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;&#22312;NeSy&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26631;&#27880;&#21644;&#25163;&#21160;&#24037;&#31243;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01889</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01889
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;&#22312;NeSy&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26631;&#27880;&#21644;&#25163;&#21160;&#24037;&#31243;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#26377;&#26395;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#37096;&#32626;&#65292;&#22240;&#20026;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#25216;&#26415;&#25552;&#20379;&#20102;&#27491;&#24335;&#30340;&#34892;&#20026;&#20445;&#35777;&#12290;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#26377;&#25928;&#22320;&#25972;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#35745;&#31639;&#65292;&#20197;&#20415;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#29616;&#26377;&#30340;&#39034;&#24207;&#35757;&#32451;&#31070;&#32463;&#21644;&#31526;&#21495;&#32452;&#20214;&#30340;&#27969;&#27700;&#32447;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#65292;&#32780;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#30340;&#32452;&#21512;&#29190;&#28856;&#26041;&#38754;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#38544;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;NeSy&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#26631;&#27880;&#21644;&#20154;&#24037;&#24037;&#31243;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;NeSyGPT&#65292;&#23427;&#36890;&#36807;&#24494;&#35843;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26469;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#31526;&#21495;&#29305;&#24449;&#65292;&#28982;&#21518;&#23398;&#20064;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#31572;&#26696;&#38598;&#31243;&#24207;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#35780;&#20272;&#34920;&#26126;&#65292;NeSyGPT&#20855;&#26377;... (&#21097;&#20313;&#37096;&#20998;&#35831;&#33258;&#34892;&#32763;&#35793;)
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI systems, as interpretable symbolic techniques provide formal behaviour guarantees. The challenge is how to effectively integrate neural and symbolic computation, to enable learning and reasoning from raw data. Existing pipelines that train the neural and symbolic components sequentially require extensive labelling, whereas end-to-end approaches are limited in terms of scalability, due to the combinatorial explosion in the symbol grounding problem. In this paper, we leverage the implicit knowledge within foundation models to enhance the performance in NeSy tasks, whilst reducing the amount of data labelling and manual engineering. We introduce a new architecture, called NeSyGPT, which fine-tunes a vision-language foundation model to extract symbolic features from raw data, before learning a highly expressive answer set program to solve a downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;f-&#39046;&#22495;&#24046;&#24322;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#21644;&#24341;&#20837;&#32553;&#25918;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#30340;KL&#32467;&#26524;&#65292;&#23558;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#36890;&#36807;&#23450;&#20301;&#25216;&#26415;&#24320;&#21457;&#20102;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01887</link><description>&lt;p&gt;
&#22522;&#20110;f-&#25955;&#24230;&#21407;&#29702;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65306;&#19968;&#20010;&#25913;&#36827;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
On f-Divergence Principled Domain Adaptation: An Improved Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;f-&#39046;&#22495;&#24046;&#24322;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#21644;&#24341;&#20837;&#32553;&#25918;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#30340;KL&#32467;&#26524;&#65292;&#23558;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#36890;&#36807;&#23450;&#20301;&#25216;&#26415;&#24320;&#21457;&#20102;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;Acuna&#31561;&#20154;&#65288;2021&#24180;&#65289;&#25552;&#20986;&#30340;UDA&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#23545;&#20854;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#24046;&#24322;&#24230;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;f-&#39046;&#22495;&#24046;&#24322;&#65288;f-DD&#65289;&#12290;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#24182;&#24341;&#20837;&#19968;&#20010;&#32553;&#25918;&#21442;&#25968;&#65292;f-DD&#20135;&#29983;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#22522;&#20110;KL&#30340;&#32467;&#26524;&#65292;&#24182;&#24357;&#21512;&#20102;Acuna&#31561;&#20154;&#65288;2021&#24180;&#65289;&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21033;&#29992;&#23450;&#20301;&#25216;&#26415;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed by Acuna et al. (2021) by refining their f-divergence-based discrepancy and additionally introducing a new measure, f-domain discrepancy (f-DD). By removing the absolute value function and incorporating a scaling parameter, f-DD yields novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Leveraging a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of f-DD-based domain learning algorithms over previous works in popular UDA benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;IRLEED&#65292;&#23427;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#21644;&#24322;&#36136;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;IRLEED&#36890;&#36807;&#32467;&#21512;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#21644;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20339;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01886</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning by Estimating Expertise of Demonstrators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;IRLEED&#65292;&#23427;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#21644;&#24322;&#36136;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;IRLEED&#36890;&#36807;&#32467;&#21512;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#21644;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20339;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#21033;&#29992;&#27425;&#20248;&#21644;&#24322;&#36136;&#30340;&#28436;&#31034;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24615;&#36136;&#21508;&#19981;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#23558;&#36825;&#20123;&#25968;&#25454;&#38598;&#35270;&#20026;&#21516;&#36136;&#30340;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;&#27425;&#20248;&#28436;&#31034;&#30340;&#32570;&#38519;&#12290;&#20808;&#21069;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#22914;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#23376;&#38598;&#12289;&#32622;&#20449;&#24230;&#25490;&#21517;&#25110;&#26126;&#30830;&#30340;&#29615;&#22659;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IRLEED&#65288;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#21069;&#23545;&#28436;&#31034;&#32773;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#20102;&#35299;&#12290;IRLEED&#36890;&#36807;&#23558;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#19982;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#26469;&#22788;&#29702;&#22870;&#21169;&#20559;&#24046;&#21644;&#34892;&#21160;&#26041;&#24046;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Imitation Learning (IL), utilizing suboptimal and heterogeneous demonstrations presents a substantial challenge due to the varied nature of real-world data. However, standard IL algorithms consider these datasets as homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators. Previous approaches to this issue typically rely on impractical assumptions like high-quality data subsets, confidence rankings, or explicit environmental knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, a novel framework that overcomes these hurdles without prior knowledge of demonstrator expertise. IRLEED enhances existing Inverse Reinforcement Learning (IRL) algorithms by combining a general model for demonstrator suboptimality to address reward bias and action variance, with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations. Experiments in both online and offline I
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.01881</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Agent for Hyper-Parameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01881
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12289;&#22823;&#37327;&#23454;&#39564;&#20197;&#21450;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#12290;&#23613;&#31649;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35797;&#39564;&#25928;&#29575;&#12289;&#35774;&#32622;&#22797;&#26434;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#33258;&#21160;&#21270;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#31216;&#20026;AgentHPO&#65288;LLM Agent-based Hyperparameter Optimization&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AgentHPO&#33258;&#20027;&#22788;&#29702;&#20219;&#21153;&#20449;&#24687;&#65292;&#26681;&#25454;&#21382;&#21490;&#35797;&#39564;&#23545;&#29305;&#23450;&#36229;&#21442;&#25968;&#65288;HPs&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;AutoML&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#20248;&#21270;&#36807;&#31243;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#24182;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01879</link><description>&lt;p&gt;
$\sigma$-zero: &#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$-&#33539;&#25968;&#23545;&#25239;&#26679;&#26412;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#22522;&#20110;&#26799;&#24230;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#25915;&#20987;&#32771;&#34385;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#32422;&#26463;&#26469;&#21046;&#36896;&#36755;&#20837;&#25200;&#21160;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#20102;&#31232;&#30095;&#30340;$\ell_1$&#21644;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#22312;&#38750;&#20984;&#19988;&#38750;&#21487;&#24494;&#32422;&#26463;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26159;&#30740;&#31350;&#26368;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#35780;&#20272;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#25581;&#31034;&#22312;&#26356;&#20256;&#32479;&#30340;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#25915;&#20987;&#20013;&#26410;&#33021;&#27979;&#35797;&#20986;&#30340;&#24369;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#65292;&#31216;&#20026;$\sigma$-zero&#65292;&#23427;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#19968;&#20010;&#29305;&#27530;&#21487;&#24494;&#36817;&#20284;&#26469;&#20419;&#36827;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#21160;&#24577;&#35843;&#25972;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#25200;&#21160;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01878</link><description>&lt;p&gt;
LiPO: &#36890;&#36807;&#23398;&#20064;&#25490;&#24207;&#36827;&#34892;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
LiPO: Listwise Preference Optimization through Learning-to-Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#26159;&#25511;&#21046;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34892;&#20026;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;DPO&#21644;SLiC&#65292;&#25104;&#20026;&#20256;&#32479;&#30340;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#24037;&#21453;&#39304;&#36890;&#24120;&#20197;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#30340;&#26684;&#24335;&#25552;&#20379;&#65292;&#20197;&#25674;&#38144;&#38405;&#35835;&#25552;&#31034;&#30340;&#25104;&#26412;&#12290;&#22810;&#20010;&#21709;&#24212;&#20063;&#21487;&#20197;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#25110;AI&#21453;&#39304;&#36827;&#34892;&#25490;&#24207;&#12290;&#32570;&#23569;&#20851;&#20110;&#30452;&#25509;&#36866;&#24212;&#21709;&#24212;&#21015;&#34920;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;&#65288;LiPO&#65289;&#26694;&#26550;&#65292;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#31574;&#30053;&#21487;&#20197;&#20174;&#19968;&#20010;&#25490;&#21517;&#21015;&#34920;&#20013;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#34892;&#21709;&#24212;&#12290;&#36825;&#31181;&#35266;&#28857;&#19982;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#24418;&#25104;&#26126;&#30830;&#30340;&#32852;&#31995;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20559;&#22909;&#20248;&#21270;&#24037;&#20316;&#21487;&#20197;&#26144;&#23556;&#21040;&#29616;&#26377;&#30340;&#25490;&#21517;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
&lt;/p&gt;</description></item><item><title>&#31227;&#21160;&#35797;&#34915;&#38388;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#35774;&#22791;&#20869;&#25193;&#25955;&#27169;&#22411;&#30340;&#34394;&#25311;&#35797;&#31359;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#26381;&#35013;&#25918;&#32622;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#29992;&#25143;&#23450;&#21046;&#12290;&#23427;&#20026;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20225;&#19994;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26381;&#21153;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#34394;&#25311;&#35797;&#31359;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.01877</link><description>&lt;p&gt;
&#31227;&#21160;&#35797;&#34915;&#38388;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#22791;&#20869;&#34394;&#25311;&#35797;&#31359;
&lt;/p&gt;
&lt;p&gt;
Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01877
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#35797;&#34915;&#38388;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#35774;&#22791;&#20869;&#25193;&#25955;&#27169;&#22411;&#30340;&#34394;&#25311;&#35797;&#31359;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#26381;&#35013;&#25918;&#32622;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#29992;&#25143;&#23450;&#21046;&#12290;&#23427;&#20026;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20225;&#19994;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26381;&#21153;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#34394;&#25311;&#35797;&#31359;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#65292;&#38656;&#35201;&#20132;&#20114;&#24335;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#26469;&#36827;&#34892;&#34394;&#25311;&#35797;&#31359;&#34915;&#26381;&#12290;&#20256;&#32479;&#30340;&#35797;&#31359;&#26041;&#27861;&#22312;&#36866;&#24212;&#19981;&#21516;&#30340;&#32972;&#26223;&#12289;&#23039;&#21183;&#21644;&#20027;&#20307;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#34429;&#28982;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#32780;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#31227;&#21160;&#30028;&#38754;&#20132;&#20184;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#32500;&#24230;&#21644;&#38544;&#31169;&#38382;&#39064;&#22823;&#37096;&#20998;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31227;&#21160;&#35797;&#34915;&#38388;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#35774;&#22791;&#20869;&#25193;&#25955;&#30340;&#34394;&#25311;&#35797;&#31359;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#39640;&#36136;&#37327;&#26381;&#35013;&#25918;&#32622;&#21644;&#31227;&#21160;&#35774;&#22791;&#30340;&#27169;&#22411;&#21387;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#27969;&#31243;&#21644;&#30028;&#38754;&#35774;&#35745;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#21644;&#29992;&#25143;&#23450;&#21046;&#12290;&#19968;&#20010;&#20351;&#29992;&#22330;&#26223;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#22914;&#20309;&#20026;&#39038;&#23458;&#25552;&#20379;&#26080;&#32541;&#12289;&#20114;&#21160;&#30340;&#34394;&#25311;&#35797;&#31359;&#20307;&#39564;&#65292;&#24182;&#20026;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20225;&#19994;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing digital landscape of fashion e-commerce calls for interactive and user-friendly interfaces for virtually trying on clothes. Traditional try-on methods grapple with challenges in adapting to diverse backgrounds, poses, and subjects. While newer methods, utilizing the recent advances of diffusion models, have achieved higher-quality image generation, the human-centered dimensions of mobile interface delivery and privacy concerns remain largely unexplored. We present Mobile Fitting Room, the first on-device diffusion-based virtual try-on system. To address multiple inter-related technical challenges such as high-quality garment placement and model compression for mobile devices, we present a novel technical pipeline and an interface design that enables privacy preservation and user customization. A usage scenario highlights how our tool can provide a seamless, interactive virtual try-on experience for customers and provide a valuable service for fashion e-commerce businesses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;FPGA&#19978;&#36827;&#34892;&#39640;&#24615;&#33021;&#21943;&#27880;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#21644;&#39640;&#25928;&#30340;&#30828;&#20214;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#65292;&#20026;&#39640;&#20142;&#24230;&#38454;&#27573;&#26631;&#35760;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#21021;&#22987;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.01876</link><description>&lt;p&gt;
&#38598;&#21512;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#29992;&#20110;HL-LHC&#30340;&#36229;&#24555;&#36895;&#21943;&#27880;&#20998;&#31867;&#22312;FPGAs&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;FPGA&#19978;&#36827;&#34892;&#39640;&#24615;&#33021;&#21943;&#27880;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#21644;&#39640;&#25928;&#30340;&#30828;&#20214;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#65292;&#20026;&#39640;&#20142;&#24230;&#38454;&#27573;&#26631;&#35760;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#21021;&#22987;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21487;&#32534;&#31243;&#36923;&#36753;&#38376;&#38453;&#21015;&#19978;&#36827;&#34892;&#20934;&#30830;&#30340;&#21943;&#27880;&#39118;&#21619;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#24310;&#36831;&#21644;&#36164;&#28304;&#28040;&#32791;&#22914;&#20309;&#38543;&#30528;&#36755;&#20837;&#22823;&#23567;&#21644;&#31639;&#27861;&#36873;&#25321;&#32780;&#21464;&#21270;&#12290;&#36825;&#20123;&#26550;&#26500;&#20026;&#22312;CERN LHC&#30340;&#39640;&#20142;&#24230;&#38454;&#27573;&#26631;&#35760;&#25152;&#33021;&#20351;&#29992;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#35774;&#35745;&#12290;&#39640;&#20142;&#24230;&#21319;&#32423;&#23558;&#23548;&#33268;&#36136;&#23376;-&#36136;&#23376;&#30896;&#25758;&#30340;&#30636;&#26102;&#20142;&#24230;&#22686;&#21152;&#20116;&#20493;&#65292;&#36827;&#32780;&#20135;&#29983;&#26356;&#39640;&#30340;&#25968;&#25454;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#20363;&#22914;&#21943;&#27880;&#32452;&#25104;&#37096;&#20998;&#30340;&#21487;&#29992;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#21644;&#39640;&#25928;&#30340;&#30828;&#20214;&#23454;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#26434;&#26550;&#26500;&#65288;&#22914;&#28145;&#24230;&#38598;&#21512;&#21644;&#20132;&#20114;&#32593;&#32476;&#65289;&#30340;&#25512;&#26029;&#26102;&#38388;&#21487;&#20197;&#36798;&#21040;O&#65288;100&#65289;ns&#65292;&#24182;&#19988;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study various machine learning based algorithms for performing accurate jet flavor classification on field-programmable gate arrays and demonstrate how latency and resource consumption scale with the input size and choice of algorithm. These architectures provide an initial design for models that could be used for tagging at the CERN LHC during its high-luminosity phase. The high-luminosity upgrade will lead to a five-fold increase in its instantaneous luminosity for proton-proton collisions and, in turn, higher data volume and complexity, such as the availability of jet constituents. Through quantization-aware training and efficient hardware implementations, we show that O(100) ns inference of complex architectures such as deep sets and interaction networks is feasible at a low computational resource cost.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#23457;&#35270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.01874</link><description>&lt;p&gt;
RL/LLM&#20998;&#31867;&#26641;&#65306;&#22238;&#39038;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01874
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#23457;&#35270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#31867;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#20998;&#31867;&#26041;&#27861;&#22522;&#20110;&#36825;&#20004;&#31181;&#27169;&#22411;&#31867;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#31532;&#19968;&#31867;&#26159;RL4LLM&#65292;&#21253;&#25324;&#21033;&#29992;RL&#25913;&#36827;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#20851;&#20219;&#21153;&#19978;LLM&#24615;&#33021;&#30340;&#30740;&#31350;&#12290;L4LLM&#20998;&#20026;&#20004;&#20010;&#23376;&#31867;&#65292;&#21462;&#20915;&#20110;RL&#26159;&#30452;&#25509;&#24494;&#35843;&#29616;&#26377;LLM&#36824;&#26159;&#25913;&#36827;LLM&#30340;&#25552;&#31034;&#12290;&#22312;&#31532;&#20108;&#31867;LLM4RL&#20013;&#65292;LLM&#36741;&#21161;&#35757;&#32451;&#19968;&#20010;&#19982;&#33258;&#28982;&#35821;&#35328;&#26080;&#20851;&#30340;RL&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26681;&#25454;LLM&#36741;&#21161;&#25110;&#26367;&#20195;RL&#35757;&#32451;&#26694;&#26550;&#30340;&#32452;&#20214;&#65288;&#22870;&#21169;&#22609;&#36896;&#12289;&#30446;&#26631;&#29983;&#25104;&#21644;&#31574;&#30053;&#20989;&#25968;&#65289;&#23545;LLM4RL&#36827;&#34892;&#20102;&#32454;&#20998;&#12290;&#26368;&#21518;&#65292;&#22312;&#31532;&#19977;&#31867;RL+LLM&#20013;&#65292;&#19968;&#20010;LLM&#21644;&#19968;&#20010;RL&#20195;&#29702;&#34987;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde
&lt;/p&gt;</description></item><item><title>APIServe&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#20010;&#39640;&#25928;&#24037;&#20855;&#65292;&#23427;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001; API &#35843;&#29992;&#24341;&#36215;&#30340; GPU &#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#25552;&#39640;&#20102;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01869</link><description>&lt;p&gt;
APIServe: &#39640;&#25928;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;API&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
APIServe: Efficient API Support for Large-Language Model Inferencing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01869
&lt;/p&gt;
&lt;p&gt;
APIServe&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#20010;&#39640;&#25928;&#24037;&#20855;&#65292;&#23427;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001; API &#35843;&#29992;&#24341;&#36215;&#30340; GPU &#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#25552;&#39640;&#20102;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#21644;API&#38598;&#25104;&#65292;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20197;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#24515;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLM&#25512;&#29702;&#31995;&#32479;&#26159;&#20026;&#29420;&#31435;&#30340;LLM&#35774;&#35745;&#30340;&#12290;&#23427;&#20204;&#23558;API&#35843;&#29992;&#35270;&#20026;&#26032;&#35831;&#27714;&#65292;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#37325;&#26032;&#35745;&#31639;&#24050;&#32463;&#35745;&#31639;&#36807;&#30340;&#19978;&#19979;&#25991;&#65292;&#36825;&#21344;&#20102;&#24635;&#27169;&#22411;&#21069;&#21521;&#26102;&#38388;&#30340;37-40%&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;APIServe&#65292;&#36825;&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;LLM&#25512;&#29702;&#26694;&#26550;&#12290;APIServe&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001;API&#35843;&#29992;&#24341;&#36215;&#30340;GPU&#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#23558;&#33410;&#30465;&#30340;&#20869;&#23384;&#29992;&#20110;&#26381;&#21153;&#26356;&#22810;&#30340;&#35831;&#27714;&#12290;&#19982;&#29616;&#26377;&#30340;LLM&#25512;&#29702;&#31995;&#32479;&#30456;&#27604;&#65292;APIServe&#23558;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#25552;&#21319;&#20102;1.6&#20493;&#65292;&#27599;&#31186;&#23436;&#25104;&#30340;&#35831;&#27714;&#22686;&#21152;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are increasingly integrated with external tools and APIs like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat API calls as new requests, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents APIServe, the first LLM inference framework targeting API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API calls and dedicates saved memory for serving more requests. APISERVE improves the overall serving throughput by 1.6x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;PINNs&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;NNCG&#24182;&#20248;&#21270;&#20102;PINN&#24615;&#33021;&#65292;&#20026;&#35757;&#32451;PINNs&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#21644;&#26356;&#24378;&#22823;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01868</link><description>&lt;p&gt;
&#35757;&#32451;PINNs&#30340;&#25361;&#25112;&#65306;&#20174;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#35282;&#24230;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Challenges in Training PINNs: A Loss Landscape Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;PINNs&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;NNCG&#24182;&#20248;&#21270;&#20102;PINN&#24615;&#33021;&#65292;&#20026;&#35757;&#32451;PINNs&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#21644;&#26356;&#24378;&#22823;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26368;&#23567;&#21270;PINN&#25439;&#22833;&#20989;&#25968;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#27531;&#24046;&#39033;&#20013;&#30340;&#24494;&#20998;&#31639;&#23376;&#24341;&#36215;&#30340;&#30149;&#24577;&#26465;&#20214;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;Adam&#12289;L-BFGS&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;Adam+L-BFGS&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;Adam+L-BFGS&#26356;&#20248;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;NysNewton-CG&#65288;NNCG&#65289;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#30149;&#24577;&#24494;&#20998;&#31639;&#23376;&#19982;PINN&#25439;&#22833;&#20013;&#30340;&#30149;&#24577;&#26465;&#20214;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#32467;&#21512;&#19968;&#38454;&#21644;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35757;&#32451;PINNs&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#21644;&#26356;&#24378;&#22823;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;PINNs&#22312;&#35299;&#20915;&#22256;&#38590;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#26631;&#27880;&#20989;&#25968;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#32467;&#26500;&#65292;&#36890;&#36807;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#25552;&#39640;&#20102;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#27969;&#31243;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01867</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#20013;&#36827;&#34892;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01867
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#26631;&#27880;&#20989;&#25968;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#32467;&#26500;&#65292;&#36890;&#36807;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#25552;&#39640;&#20102;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#27969;&#31243;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#65288;PromptedWS&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#24369;&#30417;&#30563;&#26694;&#26550;&#20013;&#30340;&#26631;&#27880;&#20989;&#25968;&#65288;LFs&#65289;&#65292;&#20197;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;LLMs&#22312;&#24490;&#29615;&#20013;&#30340;&#20351;&#29992;&#65292;&#20197;&#35299;&#20915;&#24369;&#30417;&#30563;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23398;&#20064;&#30417;&#30563;&#28304;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35810;&#38382;LLM&#36825;&#20123;&#25552;&#31034;&#24335;LFs&#26377;&#22810;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#30340;&#26680;&#24515;&#26159;&#26631;&#27880;&#20989;&#25968;&#31227;&#38500;&#65288;LaRe&#65289;&#21644;&#30456;&#20851;&#32467;&#26500;&#29983;&#25104;&#65288;CosGen&#65289;&#12290;&#19982;&#20043;&#21069;&#20174;&#24369;&#26631;&#31614;&#20013;&#23398;&#20064;&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;LFs&#20869;&#22312;&#19988;&#19981;&#22826;&#20381;&#36182;&#25968;&#25454;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#22914;&#20309;&#25913;&#36827;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.01865</link><description>&lt;p&gt;
&#25105;&#30340;&#27169;&#22411;&#20250;&#24536;&#35760;&#20160;&#20040;&#65311;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20013;&#30340;&#34987;&#36951;&#24536;&#23454;&#20363;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20250;&#20986;&#29616;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#23558;&#27169;&#22411;&#26356;&#26032;&#20026;&#32416;&#27491;&#38169;&#35823;&#23454;&#20363;&#65292;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#22312;&#25351;&#23548;&#24494;&#35843;&#25110;&#19978;&#28216;&#35757;&#32451;&#38454;&#27573;&#20013;&#23398;&#21040;&#30340;&#23454;&#20363;&#19978;&#20986;&#29616;&#38169;&#35823;&#12290;&#38543;&#26426;&#37325;&#25773;&#19978;&#28216;&#25968;&#25454;&#30340;&#25928;&#26524;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#24448;&#24448;&#20276;&#38543;&#30528;&#36739;&#39640;&#30340;&#26041;&#24046;&#21644;&#36739;&#24046;&#30340;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#30001;&#20110;&#27169;&#22411;&#26356;&#26032;&#32780;&#36951;&#24536;&#30340;&#19978;&#28216;&#23454;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#30340;&#23454;&#20363;&#21644;&#30456;&#24212;&#34987;&#36951;&#24536;&#30340;&#19978;&#28216;&#39044;&#35757;&#32451;&#23454;&#20363;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#30340;&#21464;&#21270;&#31867;&#20284;&#20110;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
&lt;/p&gt;</description></item><item><title>DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01863</link><description>&lt;p&gt;
DFML&#65306;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DFML: Decentralized Federated Mutual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01863
&lt;/p&gt;
&lt;p&gt;
DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#35774;&#22791;&#39046;&#22495;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#21644;&#23481;&#26131;&#21463;&#21040;&#21333;&#28857;&#25925;&#38556;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35774;&#22791;&#22266;&#26377;&#22320;&#34920;&#29616;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#24037;&#20316;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#27492;&#24322;&#36136;&#24615;&#19988;&#19981;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#25110;&#20551;&#23450;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#30340;&#20998;&#25955;&#24335;FL&#65288;DFL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#65288;DFML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#26080;&#26381;&#21153;&#22120;&#30340;&#65292;&#25903;&#25345;&#38750;&#38480;&#21046;&#24615;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20381;&#36182;&#20844;&#20849;&#25968;&#25454;&#12290;DFML&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#24182;&#24490;&#29615;&#25913;&#21464;&#30417;&#30563;&#21644;&#25552;&#21462;&#20449;&#21495;&#30340;&#25968;&#37327;&#26469;&#26377;&#25928;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DFML&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#20840;&#23616;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26222;&#36941;&#23384;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of real-world devices, centralized servers in Federated Learning (FL) present challenges including communication bottlenecks and susceptibility to a single point of failure. Additionally, contemporary devices inherently exhibit model and data heterogeneity. Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such heterogeneity without imposing architectural restrictions or assuming the availability of public data. To address these issues, we propose a Decentralized Federated Mutual Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous models, and avoids reliance on public data. DFML effectively handles model and data heterogeneity through mutual learning, which distills knowledge between clients, and cyclically varying the amount of supervision and distillation signals. Extensive experimental results demonstrate consistent effectiveness of DFML in both convergence speed and global accuracy, outperforming prevalent b
&lt;/p&gt;</description></item><item><title>FedPFT &#20351;&#29992;&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#25552;&#39640;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#32622;&#20013;&#26174;&#31034;&#20986;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01862</link><description>&lt;p&gt;
&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#65306;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parametric Feature Transfer: One-shot Federated Learning with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01862
&lt;/p&gt;
&lt;p&gt;
FedPFT &#20351;&#29992;&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#25552;&#39640;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#32622;&#20013;&#26174;&#31034;&#20986;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#22312;&#19968;&#36718;&#36890;&#20449;&#20013;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#21516;&#26102;&#25439;&#22833;&#20102;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FedPFT&#65288;&#24102;&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#26469;&#25552;&#39640;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#27599;&#20010;&#23458;&#25143;&#31471;&#21442;&#25968;&#27169;&#22411;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#29305;&#24449;&#36827;&#34892;&#36801;&#31227;&#12290;&#38543;&#21518;&#65292;&#27599;&#20010;&#21442;&#25968;&#27169;&#22411;&#34987;&#29992;&#26469;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#22836;&#30340;&#21512;&#25104;&#29305;&#24449;&#12290;&#22312;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedPFT&#22312;&#38598;&#20013;&#21644;&#20998;&#25955;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#20197;&#21450;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#20219;&#21153;&#36716;&#31227;&#31561;&#22810;&#26679;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#32622;&#20013;&#65292;&#22686;&#24378;&#20102;&#36890;&#20449;-&#20934;&#30830;&#24615;&#30340;&#36793;&#30028;&#65292;&#25913;&#36827;&#20102;&#26368;&#39640;&#36798;20.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;Foundation&#27169;&#22411;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#24212;&#23545;&#31574;&#30053;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01857</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;Foundation&#27169;&#22411;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;Foundation&#27169;&#22411;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#24212;&#23545;&#31574;&#30053;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#21487;&#29992;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#21464;&#21270;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#38480;&#21046;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#23558;Foundation&#27169;&#22411;&#65288;FM&#65289;&#38598;&#25104;&#21040;FL&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#22686;&#21152;&#25968;&#25454;&#20016;&#23500;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38598;&#25104;&#24341;&#20837;&#20102;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26032;&#38382;&#39064;&#65292;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;FM-FL&#38598;&#25104;&#23545;&#36825;&#20123;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#21462;&#33293;&#65292;&#25581;&#31034;&#20102;&#35813;&#38598;&#25104;&#24341;&#20837;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26631;&#20934;&#21644;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37492;&#23450;&#20102;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20123;&#21069;&#26223;&#26041;&#21521;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle the performance and scalability of the models. The integration of Foundation Models (FMs) into FL presents a compelling solution to these issues, with the potential to enhance data richness and reduce computational demands through pre-training and data augmentation. However, this incorporation introduces novel issues in terms of robustness, privacy, and fairness, which have not been sufficiently addressed in the existing research. We make a preliminary investigation into this field by systematically evaluating the implications of FM-FL integration across these dimensions. We analyze the trade-offs involved, uncover the threats and issues introduced by this integration, and propose a set of criteria and strategies for navigating these challenges. Furthermore, we identify po
&lt;/p&gt;</description></item><item><title>SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01855</link><description>&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#31471;&#21040;&#31471;&#31070;&#32463;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01855
&lt;/p&gt;
&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#36890;&#24120;&#36890;&#36807;&#26368;&#20248;&#25554;&#20540;(Optimal Interpolation&#65292;OI)&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27169;&#22411;&#25110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25968;&#25454;&#21516;&#21270;&#25216;&#26415;&#26469;&#22788;&#29702;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;(Spatio-temporal Partial Differential Equations&#65292;SPDE)&#21644;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;(Gaussian Markov Random Fields&#65292;GMRF)&#20043;&#38388;&#30340;&#32852;&#31995;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#22788;&#29702;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#22823;&#25968;&#25454;&#38598;&#21644;&#29289;&#29702;&#35825;&#23548;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#26368;&#26032;&#36827;&#23637;&#20063;&#20351;&#24471;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#23884;&#20837;&#25968;&#25454;&#21516;&#21270;&#21464;&#20998;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#12290;&#37325;&#24314;&#20219;&#21153;&#34987;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#22312;&#21464;&#20998;&#20869;&#37096;&#25104;&#26412;&#20013;&#30340;&#20808;&#39564;&#23398;&#20064;&#38382;&#39064;&#21644;&#21518;&#32773;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#65306;&#20808;&#39564;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#37117;&#34987;&#34920;&#31034;&#20026;&#20855;&#26377;&#33258;&#21160;&#24494;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#20123;&#30495;&#23454;&#20540;&#21644;&#37325;&#24314;&#20540;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#23558;&#24223;&#29289;&#25910;&#38598;&#35268;&#21010;&#19987;&#23478;&#30693;&#35782;&#34701;&#20837;&#36866;&#24212;&#24615;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21046;&#23450;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#21644;&#20559;&#22909;&#21028;&#26029;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#20248;&#21270;&#30340;&#24223;&#29289;&#25910;&#38598;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.01849</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#23558;&#24223;&#29289;&#25910;&#38598;&#35268;&#21010;&#19987;&#23478;&#30693;&#35782;&#34701;&#20837;&#36866;&#24212;&#24615;&#20989;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
Capturing waste collection planning expert knowledge in a fitness function through preference learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#23558;&#24223;&#29289;&#25910;&#38598;&#35268;&#21010;&#19987;&#23478;&#30693;&#35782;&#34701;&#20837;&#36866;&#24212;&#24615;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21046;&#23450;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#21644;&#20559;&#22909;&#21028;&#26029;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#20248;&#21270;&#30340;&#24223;&#29289;&#25910;&#38598;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;COGERSA&#24223;&#29289;&#25910;&#38598;&#27969;&#31243;&#36827;&#34892;&#30740;&#31350;&#12290;&#30446;&#21069;&#65292;&#19987;&#23478;&#20204;&#36890;&#36807;&#35797;&#38169;&#26426;&#21046;&#25163;&#21160;&#35774;&#35745;&#35813;&#27969;&#31243;&#12290;&#20294;&#35813;&#27969;&#31243;&#24182;&#19981;&#26159;&#20840;&#23616;&#20248;&#21270;&#30340;&#65292;&#22240;&#20026;&#23427;&#26159;&#26681;&#25454;&#22996;&#21592;&#20250;&#38656;&#27714;&#36880;&#27493;&#21644;&#23616;&#37096;&#22320;&#26500;&#24314;&#30340;&#12290;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#36890;&#24120;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#19968;&#20010;&#36866;&#24212;&#24615;&#20989;&#25968;&#26469;&#35780;&#20272;&#36335;&#32447;&#35268;&#21010;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#21363;&#20351;&#19987;&#23478;&#20063;&#26080;&#27861;&#30452;&#25509;&#25552;&#20986;&#19968;&#20010;&#36866;&#24212;&#24615;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20559;&#22909;&#26694;&#26550;&#24314;&#31435;&#19968;&#20010;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#19987;&#23478;&#30693;&#35782;&#21644;&#19987;&#19994;&#33021;&#21147;&#12290;&#26681;&#25454;&#19987;&#23478;&#30340;&#24847;&#35265;&#65292;&#31934;&#24515;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#21644;&#20559;&#22909;&#21028;&#26029;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20204;&#30340;&#21487;&#21152;&#24615;&#23646;&#24615;&#20351;&#20219;&#21153;&#26356;&#21152;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#36335;&#32447;&#32780;&#19981;&#26159;&#36335;&#32447;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper copes with the COGERSA waste collection process. Up to now, experts have been manually designed the process using a trial and error mechanism. This process is not globally optimized, since it has been progressively and locally built as council demands appear. Planning optimization algorithms usually solve it, but they need a fitness function to evaluate a route planning quality. The drawback is that even experts are not able to propose one in a straightforward way due to the complexity of the process. Hence, the goal of this paper is to build a fitness function though a preference framework, taking advantage of the available expert knowledge and expertise. Several key performance indicators together with preference judgments are carefully established according to the experts for learning a promising fitness function. Particularly, the additivity property of them makes the task be much more affordable, since it allows to work with routes rather than with route plannings. Besi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#24179;&#21488;&#20013;&#19982;&#24178;&#25200;&#36827;&#34892;&#30340;&#23454;&#39564;&#12290;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#32773;&#20998;&#37197;&#19981;&#21516;&#30340;&#33218;&#32473;&#27599;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#26681;&#25454;&#21333;&#20803;&#20043;&#38388;&#30340;&#31354;&#38388;&#36317;&#31163;&#21644;&#23545;&#25163;&#36873;&#25321;&#30340;&#21305;&#37197;&#20989;&#25968;&#26469;&#20915;&#23450;&#27599;&#20010;&#21333;&#20803;&#22312;&#27599;&#36718;&#30340;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36716;&#25442;&#25919;&#31574;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#39044;&#26399;&#36951;&#25022;&#65292;&#20294;&#20219;&#20309;&#36716;&#25442;&#25919;&#31574;&#37117;&#20250;&#36973;&#21463;&#19968;&#23450;&#30340;&#36951;&#25022;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.01845</link><description>&lt;p&gt;
&#20855;&#26377;&#24178;&#25200;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Armed Bandits with Interference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#24179;&#21488;&#20013;&#19982;&#24178;&#25200;&#36827;&#34892;&#30340;&#23454;&#39564;&#12290;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#32773;&#20998;&#37197;&#19981;&#21516;&#30340;&#33218;&#32473;&#27599;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#26681;&#25454;&#21333;&#20803;&#20043;&#38388;&#30340;&#31354;&#38388;&#36317;&#31163;&#21644;&#23545;&#25163;&#36873;&#25321;&#30340;&#21305;&#37197;&#20989;&#25968;&#26469;&#20915;&#23450;&#27599;&#20010;&#21333;&#20803;&#22312;&#27599;&#36718;&#30340;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36716;&#25442;&#25919;&#31574;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#39044;&#26399;&#36951;&#25022;&#65292;&#20294;&#20219;&#20309;&#36716;&#25442;&#25919;&#31574;&#37117;&#20250;&#36973;&#21463;&#19968;&#23450;&#30340;&#36951;&#25022;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#22312;&#32447;&#24179;&#21488;&#19978;&#65292;&#19982;&#24178;&#25200;&#36827;&#34892;&#23454;&#39564;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20197;&#24448;&#26377;&#20851;&#24178;&#25200;&#23454;&#39564;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25919;&#31574;&#30340;&#26368;&#32456;&#36755;&#20986;&#19978;&#65292;&#32780;&#23545;&#20110;&#32047;&#35745;&#24615;&#33021;&#21017;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#20855;&#26377;&#24178;&#25200;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#8221;&#65288;MABI&#65289;&#38382;&#39064;&#65292;&#22312;&#26102;&#38388;&#27573;&#20026;T&#36718;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#20026;N&#20010;&#23454;&#39564;&#21333;&#20803;&#20013;&#30340;&#27599;&#20010;&#20998;&#37197;&#19968;&#20010;&#33218;&#12290;&#27599;&#20010;&#21333;&#20803;&#22312;&#27599;&#19968;&#36718;&#30340;&#22238;&#25253;&#21462;&#20915;&#20110;&#8220;&#25152;&#26377;&#8221;&#21333;&#20803;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#32780;&#21333;&#20803;&#20043;&#38388;&#30340;&#31354;&#38388;&#36317;&#31163;&#20250;&#23548;&#33268;&#21333;&#20803;&#30340;&#24433;&#21709;&#21147;&#36880;&#28176;&#34928;&#20943;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#36890;&#29992;&#35774;&#32622;&#65292;&#20854;&#20013;&#22238;&#25253;&#20989;&#25968;&#30001;&#23545;&#25163;&#36873;&#25321;&#65292;&#24182;&#19988;&#22312;&#36718;&#27425;&#21644;&#21333;&#20803;&#20043;&#38388;&#21487;&#20197;&#20219;&#24847;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#36716;&#25442;&#25919;&#31574;&#33021;&#22815;&#23545;&#26368;&#20339;&#22266;&#23450;&#33218;&#25919;&#31574;&#23454;&#29616;&#26368;&#20248;&#30340;&#8220;&#39044;&#26399;&#8221;&#36951;&#25022;&#65292;&#36951;&#25022;&#20540;&#20026;$O(\sqrt T)$&#12290;&#28982;&#32780;&#65292;&#20219;&#20309;&#19968;&#20010;&#36716;&#25442;&#25919;&#31574;&#30340;&#36951;&#25022;&#65288;&#20316;&#20026;&#19968;&#20010;&#38543;&#26426;&#21464;&#37327;&#65289;&#37117;&#20250;&#36973;&#21463;&#19968;&#23450;&#30340;&#36951;&#25022;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimentation with interference poses a significant challenge in contemporary online platforms. Prior research on experimentation with interference has concentrated on the final output of a policy. The cumulative performance, while equally crucial, is less well understood. To address this gap, we introduce the problem of {\em Multi-armed Bandits with Interference} (MABI), where the learner assigns an arm to each of $N$ experimental units over a time horizon of $T$ rounds. The reward of each unit in each round depends on the treatments of {\em all} units, where the influence of a unit decays in the spatial distance between units. Furthermore, we employ a general setup wherein the reward functions are chosen by an adversary and may vary arbitrarily across rounds and units. We first show that switchback policies achieve an optimal {\em expected} regret $\tilde O(\sqrt T)$ against the best fixed-arm policy. Nonetheless, the regret (as a random variable) for any switchback policy suffers 
&lt;/p&gt;</description></item><item><title>SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.01832</link><description>&lt;p&gt;
SynthCLIP: &#25105;&#20204;&#20934;&#22791;&#22909;&#24320;&#22987;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#35757;&#32451;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01832
&lt;/p&gt;
&lt;p&gt;
SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SynthCLIP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#19982;&#20043;&#21069;&#20381;&#36182;&#30495;&#23454;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#21306;&#21035;&#12290;&#20511;&#21161;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#35268;&#27169;&#30340;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#26631;&#39064;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#65292;SynthCLIP&#23454;&#29616;&#20102;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;SynthCI-30M&#65292;&#19968;&#20010;&#32431;&#31929;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3000&#19975;&#24352;&#24102;&#26631;&#39064;&#30340;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#24050;&#32463;&#22312;https://github.com/hammoudhasan/SynthCLIP&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
&lt;/p&gt;</description></item><item><title>Audio Flamingo&#26159;&#19968;&#31181;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#36798;&#21040;&#20102;&#26368;&#20248;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01831</link><description>&lt;p&gt;
Audio Flamingo: &#19968;&#31181;&#20855;&#22791;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01831
&lt;/p&gt;
&lt;p&gt;
Audio Flamingo&#26159;&#19968;&#31181;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#36798;&#21040;&#20102;&#26368;&#20248;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#29702;&#35299;&#38899;&#39057;&#8212;&#8212;&#21253;&#25324;&#38750;&#35821;&#38899;&#22768;&#38899;&#21644;&#38750;&#35328;&#35821;&#30340;&#35821;&#38899;&#8212;&#8212;&#23545;LLMs&#30340;&#22810;&#26679;&#21270;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Audio Flamingo&#30340;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#35757;&#32451;&#25216;&#26415;&#12289;&#26550;&#26500;&#35774;&#35745;&#21644;&#25968;&#25454;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#22791;&#36825;&#20123;&#21151;&#33021;&#12290;&#24191;&#27867;&#30340;&#38899;&#39057;&#29702;&#35299;&#20219;&#21153;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20248;&#25104;&#32489;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01830</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#21516;&#34892;&#35780;&#23457;&#26041;&#27861;&#65306;&#24320;&#25918;&#29615;&#22659;&#19979;LLMs&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38598;&#20013;&#20110;&#22312;&#19968;&#20123;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#23553;&#38381;&#29615;&#22659;&#21644;&#29305;&#23450;&#39046;&#22495;&#22522;&#20934;&#19978;&#27979;&#35797;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#33258;&#21160;&#34913;&#37327;LLMs&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#22788;&#20110;&#21516;&#19968;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#22238;&#31572;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#24182;&#20114;&#30456;&#35780;&#20272;&#65292;&#27599;&#20010;LLM&#30340;&#21709;&#24212;&#24471;&#20998;&#30001;&#20854;&#20182;&#21311;&#21517;&#30340;LLMs&#20849;&#21516;&#20915;&#23450;&#12290;&#20026;&#20102;&#33719;&#21462;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#26469;&#35843;&#25972;&#26368;&#32456;&#25490;&#24207;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27599;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32972;&#21518;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#27604;&#20302;&#23618;&#27425;&#30340;LLM&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#32780;&#39640;&#23618;&#27425;&#30340;LLM&#20063;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;ATP&#32467;&#21512;&#20301;&#28857;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01829</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;ATP&#32467;&#21512;&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;ATP&#32467;&#21512;&#20301;&#28857;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#39044;&#27979;&#22522;&#22240;&#20013;&#30340;ATP-&#34507;&#30333;&#32467;&#21512;&#20301;&#28857;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#36153;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#23454;&#39564;&#23460;&#23454;&#39564;&#36827;&#34892;&#12290;&#22810;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#25506;&#32034;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#26469;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;ATP-&#34507;&#30333;&#32467;&#21512;&#20301;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20027;&#35201;&#20351;&#29992;PSSMs&#21644;&#20960;&#20010;&#21333;&#35789;&#23884;&#20837;&#20316;&#20026;&#29305;&#24449;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;2D CNN&#21644;LightGBM&#20998;&#31867;&#22120;&#20316;&#20026;&#25105;&#20204;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;MP3Vec&#21644;BERT&#27169;&#22411;&#20063;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting ATP-Protein Binding sites in genes is of great significance in the field of Biology and Medicine. The majority of research in this field has been conducted through time- and resource-intensive 'wet experiments' in laboratories. Over the years, researchers have been investigating computational methods computational methods to accomplish the same goals, utilising the strength of advanced Deep Learning and NLP algorithms. In this paper, we propose to develop methods to classify ATP-Protein binding sites. We conducted various experiments mainly using PSSMs and several word embeddings as features. We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms. The MP3Vec and BERT models have also been subjected to testing in our study. The outcomes of our experiments demonstrated improvement over the state-of-the-art benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#22768;&#23398;&#35789;&#34955;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#26512;&#33258;&#28982;&#21475;&#35821;&#35821;&#35328;&#30340;&#38750;&#35789;&#27719;&#22768;&#23398;&#23646;&#24615;&#65292;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#35786;&#26029;&#35760;&#24518;&#38556;&#30861;&#30151;&#29366;&#65292;&#20174;&#32780;&#25552;&#21069;&#35782;&#21035;&#35748;&#30693;&#33021;&#21147;&#19979;&#38477;&#30340;&#26089;&#26399;&#36857;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.01824</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#22768;&#23398;&#35789;&#34955;&#27169;&#22411;&#35782;&#21035;&#36890;&#36807;&#21475;&#35821;&#35821;&#35328;&#30340;&#35748;&#30693;&#33021;&#21147;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Identification of Cognitive Decline from Spoken Language through Feature Selection and the Bag of Acoustic Words Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#22768;&#23398;&#35789;&#34955;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#26512;&#33258;&#28982;&#21475;&#35821;&#35821;&#35328;&#30340;&#38750;&#35789;&#27719;&#22768;&#23398;&#23646;&#24615;&#65292;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#35786;&#26029;&#35760;&#24518;&#38556;&#30861;&#30151;&#29366;&#65292;&#20174;&#32780;&#25552;&#21069;&#35782;&#21035;&#35748;&#30693;&#33021;&#21147;&#19979;&#38477;&#30340;&#26089;&#26399;&#36857;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#38556;&#30861;&#26159;&#32769;&#24180;&#20154;&#21151;&#33021;&#21644;&#26085;&#24120;&#27963;&#21160;&#34928;&#36864;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#30830;&#35748;&#30142;&#30149;&#12289;&#24320;&#22987;&#33647;&#29289;&#27835;&#30103;&#20197;&#20943;&#24930;&#20854;&#36827;&#31243;&#65292;&#24182;&#24320;&#23637;&#26088;&#22312;&#32500;&#25252;&#21644;&#24247;&#22797;&#35748;&#30693;&#33021;&#21147;&#30340;&#32844;&#19994;&#27835;&#30103;&#38656;&#35201;&#21307;&#23398;&#35786;&#26029;&#12290;&#26089;&#26399;&#35782;&#21035;&#35760;&#24518;&#38556;&#30861;&#30151;&#29366;&#65292;&#29305;&#21035;&#26159;&#35748;&#30693;&#33021;&#21147;&#19979;&#38477;&#65292;&#23545;&#20445;&#38556;&#20154;&#32676;&#30340;&#24184;&#31119;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#19982;&#35821;&#38899;&#20135;&#29983;&#30456;&#20851;&#30340;&#29305;&#24449;&#24050;&#30693;&#19982;&#35828;&#35805;&#32773;&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#21464;&#21270;&#26377;&#20851;&#12290;&#20020;&#24202;&#29615;&#22659;&#20013;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#35821;&#38899;&#27979;&#35797;&#23548;&#33268;&#23545;&#33258;&#28982;&#21475;&#35821;&#35821;&#35328;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24320;&#21457;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#38656;&#35201;&#24555;&#36895;&#12289;&#32463;&#27982;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#24555;&#36895;&#35786;&#26029;&#30142;&#30149;&#26102;&#65292;&#38750;&#35789;&#27719;&#20294;&#22768;&#23398;&#23646;&#24615;&#30340;&#21475;&#35821;&#35821;&#35328;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Memory disorders are a central factor in the decline of functioning and daily activities in elderly individuals. The confirmation of the illness, initiation of medication to slow its progression, and the commencement of occupational therapy aimed at maintaining and rehabilitating cognitive abilities require a medical diagnosis. The early identification of symptoms of memory disorders, especially the decline in cognitive abilities, plays a significant role in ensuring the well-being of populations. Features related to speech production are known to connect with the speaker's cognitive ability and changes. The lack of standardized speech tests in clinical settings has led to a growing emphasis on developing automatic machine learning techniques for analyzing naturally spoken language. Non-lexical but acoustic properties of spoken language have proven useful when fast, cost-effective, and scalable solutions are needed for the rapid diagnosis of a disease. The work presents an approach rel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.01821</link><description>&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#35299;&#37322;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Ecologically rational meta-learned inference explains human category learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#24615;&#26159;&#25351;&#20154;&#31867;&#20316;&#20026;&#36866;&#24212;&#29615;&#22659;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23450;&#20041;&#21738;&#20123;&#20219;&#21153;&#22312;&#29983;&#24577;&#19978;&#26159;&#26377;&#25928;&#30340;&#20197;&#21450;&#20026;&#36825;&#20123;&#20219;&#21153;&#24314;&#31435;&#21512;&#29702;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#31867;&#21035;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#21512;&#29702;&#24615;&#20027;&#20307;&#26469;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#12290;ERMI&#22312;&#20004;&#20010;&#19981;&#21516;&#23454;&#39564;&#20013;&#20197;&#23450;&#37327;&#26041;&#24335;&#27604;&#20854;&#20182;&#19971;&#20010;&#35748;&#30693;&#27169;&#22411;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#23450;&#24615;&#19978;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#21305;&#37197;&#65306;&#65288;1&#65289;&#23427;&#21457;&#29616;&#20102;&#19982;&#20154;&#31867;&#21457;&#29616;&#22256;&#38590;&#30340;&#30456;&#21516;&#20219;&#21153;&#65292;&#65288;2&#65289;&#23427;&#21464;&#24471;&#26356;&#20381;&#36182;&#20110;&#22522;&#20110;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy 
&lt;/p&gt;</description></item><item><title>LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01817</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#35268;&#21010;&#65292;&#20294;&#21487;&#20197;&#22312;LLM-Modulo&#26694;&#26550;&#20013;&#24110;&#21161;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01817
&lt;/p&gt;
&lt;p&gt;
LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#24785;&#12290;&#19968;&#26041;&#38754;&#26377;&#20154;&#36807;&#20110;&#20048;&#35266;&#22320;&#22768;&#31216;&#21482;&#38656;&#27491;&#30830;&#25552;&#31034;&#25110;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;LLMs&#23601;&#33021;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20063;&#26377;&#20154;&#36807;&#20110;&#24754;&#35266;&#22320;&#35748;&#20026;LLMs&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#20165;&#33021;&#20316;&#20026;&#38382;&#39064;&#35268;&#33539;&#30340;&#31616;&#21333;&#32763;&#35793;&#22120;&#65292;&#24182;&#23558;&#38382;&#39064;&#20132;&#32473;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#31181;&#26497;&#31471;&#35266;&#28857;&#37117;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#22238;&#24402;LLMs&#26412;&#36523;&#19981;&#33021;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65288;&#27605;&#31455;&#36825;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65289;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#35823;&#35299;&#21407;&#22240;&#36827;&#34892;&#20102;&#19968;&#20123;&#38416;&#36848;&#12290;&#25105;&#20204;&#36824;&#23558;&#36777;&#31216;LLMs&#24212;&#35813;&#34987;&#35270;&#20026;&#20855;&#26377;&#26356;&#26377;&#24847;&#20041;&#30340;&#35282;&#33394;&#30340;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#65292;&#33021;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#26356;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01812</link><description>&lt;p&gt;
&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#34701;&#20837;&#21040;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Distilling LLMs' Decomposition Abilities into Compact Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#22823;&#23567;&#24102;&#26469;&#20102;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#24182;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#23450;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32039;&#20945;&#27169;&#22411;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#22521;&#35757;&#65292;&#20294;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#24448;&#24448;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#33021;&#21147;&#30340;&#36827;&#27493;&#65292;&#25552;&#20379;&#21453;&#39304;&#24182;&#29983;&#25104;&#19987;&#38376;&#29992;&#20110;&#35757;&#32451;&#32039;&#20945;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#30001;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#24378;&#35843;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20449;&#29992;&#35780;&#20998;&#20013;&#24212;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#25928;&#26524;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.01811</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#22312;&#20844;&#24179;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Distributionally Robust Optimisation Approach to Fair Credit Scoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20449;&#29992;&#35780;&#20998;&#20013;&#24212;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#25928;&#26524;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#29992;&#35780;&#20998;&#34987;&#27431;&#27954;&#22996;&#21592;&#20250;&#21644;&#32654;&#22269;&#24635;&#32479;&#21150;&#20844;&#23460;&#24402;&#20026;&#39640;&#39118;&#38505;&#20998;&#31867;&#20219;&#21153;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;&#22522;&#20110;&#21487;&#33021;&#20559;&#21521;&#26576;&#20123;&#32676;&#20307;&#30340;&#27169;&#22411;&#36827;&#34892;&#36151;&#27454;&#25209;&#20934;&#20915;&#31574;&#21487;&#33021;&#36896;&#25104;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36817;&#26399;&#30340;&#20449;&#29992;&#35780;&#20998;&#30740;&#31350;&#32771;&#34385;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#25552;&#20986;&#30340;&#19968;&#31995;&#21015;&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#20998;&#31867;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#23545;&#24453;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20844;&#24179;&#24615;&#30340;&#23450;&#20041;&#25110;&#23454;&#26045;&#26041;&#27861;&#21508;&#26377;&#19981;&#21516;&#65292;&#36825;&#20123;&#25216;&#26415;&#22823;&#22810;&#24573;&#35270;&#20102;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#35757;&#32451;&#38598;&#20013;&#26377;&#25928;&#32416;&#27491;&#19981;&#20844;&#24179;&#23545;&#24453;&#65292;&#20294;&#22312;&#29983;&#25104;&#26679;&#26412;&#22806;&#30340;&#20998;&#31867;&#26102;&#20250;&#20877;&#27425;&#20135;&#29983;&#19981;&#20844;&#24179;&#23545;&#24453;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#22914;&#20309;&#23558;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;(DRO)&#26041;&#27861;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#65292;&#24182;&#20026;&#27492;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#25928;&#26524;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit scoring has been catalogued by the European Commission and the Executive Office of the US President as a high-risk classification task, a key concern being the potential harms of making loan approval decisions based on models that would be biased against certain groups. To address this concern, recent credit scoring research has considered a range of fairness-enhancing techniques put forward by the machine learning community to reduce bias and unfair treatment in classification systems. While the definition of fairness or the approach they follow to impose it may vary, most of these techniques, however, disregard the robustness of the results. This can create situations where unfair treatment is effectively corrected in the training set, but when producing out-of-sample classifications, unfair treatment is incurred again. Instead, in this paper, we will investigate how to apply Distributionally Robust Optimisation (DRO) methods to credit scoring, thereby empirically evaluating h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01810</link><description>&lt;p&gt;
&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Misspecification uncertainties in near-deterministic regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25439;&#22833;&#26159;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#30340;&#40065;&#26834;PAC-Bayes&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#26368;&#23567;&#21270;&#34987;&#35748;&#20026;&#24573;&#30053;&#20102;&#38169;&#35823;&#35268;&#33539;&#21270;&#65292;&#21363;&#27169;&#22411;&#19981;&#33021;&#23436;&#20840;&#22797;&#21046;&#35266;&#27979;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#22823;&#25968;&#25454;&#25110;&#27424;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#23545;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#33879;&#20302;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#36817;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#35268;&#33539;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36825;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24191;&#27867;&#30456;&#20851;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#21518;&#39564;&#20998;&#24067;&#24517;&#39035;&#35206;&#30422;&#27599;&#20010;&#35757;&#32451;&#28857;&#65292;&#20197;&#36991;&#20813;&#21457;&#25955;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#31526;&#21512;&#36825;&#20010;&#32422;&#26463;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#36825;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#39069;&#22806;&#24320;&#38144;&#26368;&#23567;&#12290;&#36825;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#27169;&#22411;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
&lt;/p&gt;</description></item><item><title>PhenoLinker&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#22411;-&#22522;&#22240;&#20851;&#32852;&#39044;&#27979;&#21644;&#35299;&#37322;&#65292;&#20026;&#21457;&#29616;&#26032;&#30340;&#20851;&#32852;&#21644;&#29702;&#35299;&#20154;&#31867;&#36951;&#20256;&#21464;&#24322;&#30340;&#21518;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01809</link><description>&lt;p&gt;
PhenoLinker: &#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#22411;-&#22522;&#22240;&#36830;&#25509;&#39044;&#27979;&#21644;&#35299;&#37322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PhenoLinker: Phenotype-Gene Link Prediction and Explanation using Heterogeneous Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01809
&lt;/p&gt;
&lt;p&gt;
PhenoLinker&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#22411;-&#22522;&#22240;&#20851;&#32852;&#39044;&#27979;&#21644;&#35299;&#37322;&#65292;&#20026;&#21457;&#29616;&#26032;&#30340;&#20851;&#32852;&#21644;&#29702;&#35299;&#20154;&#31867;&#36951;&#20256;&#21464;&#24322;&#30340;&#21518;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32473;&#23450;&#30340;&#20154;&#31867;&#34920;&#22411;&#19982;&#36951;&#20256;&#21464;&#24322;&#30456;&#20851;&#32852;&#20173;&#28982;&#26159;&#29983;&#29289;&#23398;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PhenoLinker&#30340;&#26032;&#31995;&#32479;&#65292;&#33021;&#22815;&#20351;&#29992;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#21644;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#27169;&#22411;&#26469;&#20026;&#34920;&#22411;-&#22522;&#22240;&#20851;&#31995;&#20998;&#37197;&#19968;&#20010;&#24471;&#20998;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#21457;&#29616;&#26032;&#30340;&#20851;&#32852;&#65292;&#24182;&#29702;&#35299;&#20154;&#31867;&#36951;&#20256;&#21464;&#24322;&#30340;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The association of a given human phenotype to a genetic variant remains a critical challenge for biology. We present a novel system called PhenoLinker capable of associating a score to a phenotype-gene relationship by using heterogeneous information networks and a convolutional neural network-based model for graphs, which can provide an explanation for the predictions. This system can aid in the discovery of new associations and in the understanding of the consequences of human genetic variation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#65292;&#20801;&#35768;&#27169;&#22411;&#30340;&#20080;&#21334;&#24182;&#36890;&#36807;&#36866;&#24403;&#23450;&#20215;&#21644;&#28608;&#21169;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#23454;&#29616;&#26368;&#22823;&#20132;&#26131;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01802</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;
&lt;/p&gt;
&lt;p&gt;
An Auction-based Marketplace for Model Trading in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#65292;&#20801;&#35768;&#27169;&#22411;&#30340;&#20080;&#21334;&#24182;&#36890;&#36807;&#36866;&#24403;&#23450;&#20215;&#21644;&#28608;&#21169;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#23454;&#29616;&#26368;&#22823;&#20132;&#26131;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36234;&#26469;&#36234;&#34987;&#35748;&#35782;&#21040;&#22312;&#20351;&#29992;&#26412;&#22320;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#20849;&#20139;&#25968;&#25454;&#30340;&#36866;&#24403;&#20272;&#20540;&#20173;&#26410;&#24471;&#21040;&#36275;&#22815;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;FL&#35774;&#24819;&#20026;&#19968;&#20010;&#27169;&#22411;&#20132;&#26131;&#30340;&#24066;&#22330;&#65292;&#23458;&#25143;&#26082;&#26159;&#20080;&#23478;&#20063;&#26159;&#21334;&#23478;&#65292;&#21442;&#19982;&#27169;&#22411;&#20132;&#26131;&#12290;&#36825;&#20010;FL&#24066;&#22330;&#20801;&#35768;&#23458;&#25143;&#36890;&#36807;&#20986;&#21806;&#33258;&#24049;&#30340;&#27169;&#22411;&#36186;&#21462;&#36135;&#24065;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#36141;&#20080;&#20182;&#20154;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#26412;&#22320;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#22522;&#20110;&#24615;&#33021;&#22686;&#30410;&#30340;&#36866;&#24403;&#23450;&#20215;&#12290;&#28608;&#21169;&#26426;&#21046;&#34987;&#35774;&#35745;&#20986;&#26469;&#40723;&#21169;&#23458;&#25143;&#30495;&#23454;&#22320;&#25581;&#31034;&#20182;&#20204;&#23545;&#27169;&#22411;&#30340;&#20215;&#20540;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#29992;&#20110;&#24066;&#22330;&#36816;&#33829;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24066;&#22330;&#29366;&#20917;&#19979;&#30340;&#26368;&#22823;&#20132;&#26131;&#37327;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FL&#24066;&#22330;&#21487;&#20197;&#23454;&#29616;&#39640;&#20132;&#26131;&#25910;&#30410;&#21644;&#20844;&#24179;&#30340;&#27169;&#22411;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is increasingly recognized for its efficacy in training models using locally distributed data. However, the proper valuation of shared data in this collaborative process remains insufficiently addressed. In this work, we frame FL as a marketplace of models, where clients act as both buyers and sellers, engaging in model trading. This FL market allows clients to gain monetary reward by selling their own models and improve local model performance through the purchase of others' models. We propose an auction-based solution to ensure proper pricing based on performance gain. Incentive mechanisms are designed to encourage clients to truthfully reveal their model valuations. Furthermore, we introduce a reinforcement learning (RL) framework for marketing operations, aiming to achieve maximum trading volumes under the dynamic and evolving market status. Experimental results on four datasets demonstrate that the proposed FL market can achieve high trading revenue and fai
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01799</link><description>&lt;p&gt;
&#26356;&#24555;&#26356;&#36731;&#30340;LLMs&#65306;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#27169;&#22411;&#21387;&#32553;&#21644;&#31995;&#32479;&#32423;&#20248;&#21270;&#26041;&#27861;&#26041;&#38754;&#30340;&#36827;&#23637;&#26088;&#22312;&#22686;&#24378;LLM&#25512;&#29702;&#25928;&#26524;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#24378;&#35843;&#20102;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21387;&#32553;&#25216;&#26415;&#65292;&#20026;&#22312;&#32479;&#19968;&#29615;&#22659;&#20013;&#39640;&#25928;&#37096;&#32626;LLM&#25552;&#20379;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#35777;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#25913;&#21892;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;https://github.com/nyunAI/Faster-LLM-Survey&#21457;&#24067;&#20102;&#29992;&#20110;&#22797;&#29616;&#26412;&#25991;&#32467;&#26524;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#37325;&#23614;&#26799;&#24230;&#30340;&#26032;&#22411;&#21387;&#32553;&#26041;&#26696;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#23558;&#26799;&#24230;&#25130;&#26029;&#21644;&#37327;&#21270;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20248;&#21270;&#20851;&#38190;&#21442;&#25968;&#30340;&#20540;&#26469;&#20943;&#23567;&#37327;&#21270;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.01798</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#37327;&#21270;&#31574;&#30053;&#29992;&#20110;&#31649;&#29702;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#37325;&#23614;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Improved Quantization Strategies for Managing Heavy-tailed Gradients in Distributed Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#37325;&#23614;&#26799;&#24230;&#30340;&#26032;&#22411;&#21387;&#32553;&#26041;&#26696;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#23558;&#26799;&#24230;&#25130;&#26029;&#21644;&#37327;&#21270;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20248;&#21270;&#20851;&#38190;&#21442;&#25968;&#30340;&#20540;&#26469;&#20943;&#23567;&#37327;&#21270;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#35299;&#20915;&#36890;&#20449;&#25928;&#29575;&#25361;&#25112;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#35266;&#23519;&#21040;&#26799;&#24230;&#20998;&#24067;&#21576;&#37325;&#23614;&#20998;&#24067;&#65292;&#31163;&#32676;&#20540;&#26174;&#33879;&#24433;&#21709;&#21387;&#32553;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;&#29616;&#26377;&#30340;&#21442;&#25968;&#37327;&#21270;&#26041;&#27861;&#22312;&#24573;&#30053;&#37325;&#23614;&#29305;&#24449;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#37325;&#23614;&#26799;&#24230;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#26799;&#24230;&#25130;&#26029;&#19982;&#37327;&#21270;&#24039;&#22937;&#32467;&#21512;&#12290;&#35813;&#26041;&#26696;&#24039;&#22937;&#22320;&#23454;&#29616;&#22312;&#19968;&#20010;&#36890;&#20449;&#21463;&#38480;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#36981;&#24490;&#24130;&#24459;&#20998;&#24067;&#30340;&#37325;&#23614;&#26799;&#24230;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#30001;&#37327;&#21270;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#20174;&#32780;&#30830;&#23450;&#20004;&#20010;&#20851;&#38190;&#21442;&#25968;&#30340;&#26368;&#20248;&#20540;&#65306;&#25130;&#26029;&#38408;&#20540;&#21644;&#37327;&#21270;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient compression has surfaced as a key technique to address the challenge of communication efficiency in distributed learning. In distributed deep learning, however, it is observed that gradient distributions are heavy-tailed, with outliers significantly influencing the design of compression strategies. Existing parameter quantization methods experience performance degradation when this heavy-tailed feature is ignored. In this paper, we introduce a novel compression scheme specifically engineered for heavy-tailed gradients, which effectively combines gradient truncation with quantization. This scheme is adeptly implemented within a communication-limited distributed Stochastic Gradient Descent (SGD) framework. We consider a general family of heavy-tail gradients that follow a power-law distribution, we aim to minimize the error resulting from quantization, thereby determining optimal values for two critical parameters: the truncation threshold and the quantization density. We provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#23398;&#20064;&#40065;&#26834;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#22909;&#22320;&#36924;&#36817;&#20102;0-1&#25439;&#22833;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#20984;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#19982;&#26631;&#20934;SVMs&#30456;&#31454;&#20105;&#65292;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.01797</link><description>&lt;p&gt;
&#40065;&#26834;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#38181;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust support vector machines via conic optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#23398;&#20064;&#40065;&#26834;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#22909;&#22320;&#36924;&#36817;&#20102;0-1&#25439;&#22833;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#20984;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#19982;&#26631;&#20934;SVMs&#30456;&#31454;&#20105;&#65292;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#23398;&#20064;&#40065;&#26834;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#38382;&#39064;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#65292;&#20856;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#38128;&#38142;&#25439;&#22833;&#65292;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#24322;&#24120;&#20540;&#38750;&#24120;&#25935;&#24863;&#65292;&#22312;&#35813;&#35774;&#32622;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;0-1&#25439;&#22833;&#25110;&#36866;&#24403;&#30340;&#38750;&#20984;&#36924;&#36817;&#21487;&#20197;&#24471;&#21040;&#40065;&#26834;&#30340;&#20272;&#35745;&#65292;&#20294;&#20195;&#20215;&#26159;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#25216;&#26415;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#36873;&#25321;&#30456;&#27604;&#26356;&#22909;&#22320;&#36924;&#36817;&#20102;0-1&#25439;&#22833;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#20984;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#35745;&#31639;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#26080;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#19982;&#24102;&#38128;&#38142;&#25439;&#22833;&#30340;&#26631;&#20934;SVMs&#30456;&#31454;&#20105;&#65292;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning support vector machines robust to uncertainty. It has been established in the literature that typical loss functions, including the hinge loss, are sensible to data perturbations and outliers, thus performing poorly in the setting considered. In contrast, using the 0-1 loss or a suitable non-convex approximation results in robust estimators, at the expense of large computational costs. In this paper we use mixed-integer optimization techniques to derive a new loss function that better approximates the 0-1 loss compared with existing alternatives, while preserving the convexity of the learning problem. In our computational results, we show that the proposed estimator is competitive with the standard SVMs with the hinge loss in outlier-free regimes and better in the presence of outliers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#21457;&#29616;&#36873;&#25321;&#36866;&#24403;&#30340;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#19988;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01796</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#23618;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring transfer learning for pathological speech feature prediction: Impact of layer selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#21457;&#29616;&#36873;&#25321;&#36866;&#24403;&#30340;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#19988;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#23545;&#20020;&#24202;&#35821;&#38899;&#36827;&#34892;&#33258;&#21160;&#23458;&#35266;&#35780;&#20272;&#65292;&#24182;&#20419;&#36827;&#35821;&#38899;&#38556;&#30861;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#39044;&#27979;&#30149;&#29702;&#35821;&#38899;&#23384;&#22312;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#23618;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#36873;&#25321;&#26368;&#20339;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65288;&#24179;&#22343;&#24179;&#34913;&#20934;&#30830;&#29575;&#22686;&#21152;12.4%&#65289;&#65292;&#23613;&#31649;&#26368;&#20339;&#23618;&#22240;&#39044;&#27979;&#29305;&#24449;&#32780;&#24322;&#65292;&#24182;&#19988;&#24182;&#19981;&#24635;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#27867;&#21270;&#33391;&#22909;&#12290;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#22312;&#20998;&#24067;&#20869;&#19982;&#24179;&#22343;&#26368;&#20339;&#23618;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is interest in leveraging AI to conduct automatic, objective assessments of clinical speech, in turn facilitating diagnosis and treatment of speech disorders. We explore transfer learning, focusing on the impact of layer selection, for the downstream task of predicting the presence of pathological speech. We find that selecting an optimal layer offers large performance improvements (12.4% average increase in balanced accuracy), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution and has better generalization for out-of-distribution data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#21069;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#27979;&#35797;&#21644;&#35780;&#20272;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.01795</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23569;&#26679;&#26412;&#22330;&#26223;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Scenario Testing for Autonomous Vehicles Based on Neighborhood Coverage and Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#21069;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#27979;&#35797;&#21644;&#35780;&#20272;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#21069;&#65292;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#36827;&#34892;&#27979;&#35797;&#21644;&#35780;&#20272;&#20854;&#23433;&#20840;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27979;&#35797;&#25104;&#26412;&#25110;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#38024;&#23545;&#29305;&#23450;AV&#27169;&#22411;&#30340;&#21487;&#25509;&#21463;&#27979;&#35797;&#25104;&#26412;&#36890;&#24120;&#20250;&#34987;&#26497;&#38480;&#21046;&#22320;&#38477;&#20302;&#12290;&#29616;&#26377;&#30340;&#27979;&#35797;&#26041;&#27861;&#20005;&#26684;&#38480;&#21046;&#30340;&#27979;&#35797;&#25968;&#30446;&#20250;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#30340;&#26174;&#33879;&#19981;&#30830;&#23450;&#24615;&#25110;&#25361;&#25112;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#8220;&#23569;&#26679;&#26412;&#27979;&#35797;&#65288;FST&#65289;&#8221;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;FST&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#23567;&#35268;&#27169;&#27979;&#35797;&#22330;&#26223;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20248;&#21270;&#22330;&#26223;&#21033;&#29992;&#65292;&#25105;&#20204;&#23558;FST&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#23547;&#25214;&#19968;&#20010;&#23567;&#30340;&#22330;&#26223;&#38598;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20449;&#24687;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#65288;SMs&#65289;&#65292;&#25105;&#20204;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#22330;&#26223;&#38598;&#21512;&#21644;&#20854;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing and evaluating the safety performance of autonomous vehicles (AVs) is essential before the large-scale deployment. Practically, the acceptable cost of testing specific AV model can be restricted within an extremely small limit because of testing cost or time. With existing testing methods, the limitations imposed by strictly restricted testing numbers often result in significant uncertainties or challenges in quantifying testing results. In this paper, we formulate this problem for the first time the "few-shot testing" (FST) problem and propose a systematic FST framework to address this challenge. To alleviate the considerable uncertainty inherent in a small testing scenario set and optimize scenario utilization, we frame the FST problem as an optimization problem and search for a small scenario set based on neighborhood coverage and similarity. By leveraging the prior information on surrogate models (SMs), we dynamically adjust the testing scenario set and the contribution of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26550;&#26500;&#65288;QC-GAN&#65289;&#65292;&#36890;&#36807;&#22312;&#25163;&#20889;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#26469;&#25913;&#36827;&#20256;&#32479;GAN&#12290;&#36825;&#20010;&#26550;&#26500;&#22312;&#25910;&#25947;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#21442;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#20102;&#37327;&#23376;&#30005;&#36335;&#30340;&#32416;&#32544;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01791</link><description>&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22686;&#24378;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Variational Quantum Circuits Enhanced Generative Adversarial Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26550;&#26500;&#65288;QC-GAN&#65289;&#65292;&#36890;&#36807;&#22312;&#25163;&#20889;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#26469;&#25913;&#36827;&#20256;&#32479;GAN&#12290;&#36825;&#20010;&#26550;&#26500;&#22312;&#25910;&#25947;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#21442;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#20102;&#37327;&#23376;&#30005;&#36335;&#30340;&#32416;&#32544;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#35757;&#32451;GAN&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;GAN&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26550;&#26500;&#65288;QC-GAN&#65289;&#12290;&#36890;&#36807;&#22312;&#25163;&#20889;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#20351;&#29992;MindSpore Quantum&#19982;&#20256;&#32479;GAN&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#25968;&#20540;&#26816;&#39564;&#12290;QC-GAN&#30340;&#29983;&#25104;&#22120;&#30001;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;&#19968;&#23618;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#21028;&#21035;&#22120;&#30001;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#12290;&#20511;&#21161;&#37327;&#23376;&#30005;&#36335;&#30340;&#32416;&#32544;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#26550;&#26500;&#22312;&#25910;&#25947;&#26102;&#27604;&#20256;&#32479;GAN&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65288;Frechet Inception Distance&#65289;&#65292;&#24182;&#19988;&#35757;&#32451;&#21442;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#26356;&#23569;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial network (GAN) is one of the widely-adopted machine-learning frameworks for a wide range of applications such as generating high-quality images, video, and audio contents. However, training a GAN could become computationally expensive for large neural networks. In this work, we propose a hybrid quantum-classical architecture for improving GAN (denoted as QC-GAN). The performance was examed numerically by benchmarking with a classical GAN using MindSpore Quantum on the task of hand-written image generation. The generator of the QC-GAN consists of a quantum variational circuit together with a one-layer neural network, and the discriminator consists of a traditional neural network. Leveraging the entangling and expressive power of quantum circuits, our hybrid architecture achieved better performance (Frechet Inception Distance) than the classical GAN, with much fewer training parameters and number of iterations for convergence. We have also demonstrated the superiori
&lt;/p&gt;</description></item><item><title>&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#24352;&#37327;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21644;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#35299;&#21644;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26041;&#27861;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.01790</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30340;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#30340;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An introduction to graphical tensor notation for mechanistic interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01790
&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#24352;&#37327;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21644;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#35299;&#21644;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26041;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#24352;&#37327;&#32447;&#24615;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#28304;&#33258;&#29289;&#29702;&#23398;&#12290;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20960;&#20046;&#23436;&#20840;&#30001;&#24352;&#37327;&#25805;&#20316;&#32452;&#25104;&#65292;&#22240;&#27492;&#29702;&#35299;&#24352;&#37327;&#25805;&#20316;&#23545;&#20110;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#23588;&#20854;&#26159;&#22312;&#35797;&#22270;&#21453;&#21521;&#24037;&#31243;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#31639;&#27861;&#20197;&#29702;&#35299;&#20854;&#34892;&#20026;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#65292;&#36825;&#20010;&#39046;&#22495;&#34987;&#31216;&#20026;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#24352;&#37327;&#38388;&#36827;&#34892;&#30340;&#25805;&#20316;&#24448;&#24448;&#35753;&#20154;&#28151;&#28102;&#65292;&#24182;&#19988;&#24456;&#38590;&#25235;&#20303;&#25972;&#20307;&#32467;&#26500;&#65292;&#20294;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#20351;&#24471;&#24555;&#36895;&#35299;&#26512;&#21644;&#21457;&#29616;&#26377;&#36259;&#30340;&#31561;&#20215;&#20851;&#31995;&#26356;&#21152;&#23481;&#26131;&#12290;&#26412;&#25991;&#30340;&#21069;&#21322;&#37096;&#20998;&#20171;&#32461;&#20102;&#36825;&#31181;&#31526;&#21495;&#21270;&#26041;&#27861;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20123;&#20998;&#35299;&#26041;&#27861;&#65288;SVD&#65292;CP&#65292;Tucker&#21644;&#24352;&#37327;&#32593;&#32476;&#20998;&#35299;&#65289;&#65292;&#21518;&#21322;&#37096;&#20998;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#29992;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26041;&#27861;&#65292;&#22823;&#33268;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely follow
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01787</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21361;&#23475;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Harm Amplification in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01787
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#29992;&#25143;&#36755;&#20837;&#30475;&#20284;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21361;&#23475;&#25918;&#22823;&#65292;&#23427;&#27604;&#23545;&#25239;&#25552;&#31034;&#26356;&#20855;&#28508;&#22312;&#39118;&#38505;&#65292;&#20351;&#29992;&#25143;&#26080;&#24847;&#38388;&#36973;&#21463;&#20260;&#23475;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#36129;&#29486;&#20110;&#24320;&#21457;&#29992;&#20110;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#22330;&#26223;&#65292;&#21253;&#25324;&#37327;&#21270;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#19981;&#21516;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#24037;&#20855;&#21435;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
&lt;/p&gt;</description></item><item><title>COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.01786</link><description>&lt;p&gt;
COA-GPT&#65306;&#29992;&#20110;&#20891;&#20107;&#34892;&#21160;&#20013;&#21152;&#36895;&#34892;&#21160;&#26041;&#26696;&#24320;&#21457;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01786
&lt;/p&gt;
&lt;p&gt;
COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#34892;&#21160;&#20013;&#34892;&#21160;&#26041;&#26696;&#65288;COAs&#65289;&#30340;&#24320;&#21457;&#20256;&#32479;&#19978;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;COA-GPT&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;COAs&#30340;&#26032;&#31639;&#27861;&#12290;COA-GPT&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#21040;LLMs&#20013;&#65292;&#20801;&#35768;&#25351;&#25381;&#23448;&#36755;&#20837;&#20219;&#21153;&#20449;&#24687;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#26684;&#24335;&#65289;&#65292;&#24182;&#33719;&#24471;&#19982;&#25112;&#30053;&#23545;&#40784;&#30340;COAs&#20197;&#20379;&#23457;&#26597;&#21644;&#25209;&#20934;&#12290;&#29420;&#29305;&#30340;&#26159;&#65292;COA-GPT&#19981;&#20165;&#21152;&#36895;&#20102;COA&#30340;&#24320;&#21457;&#65292;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#21021;&#22987;COAs&#65292;&#36824;&#33021;&#26681;&#25454;&#25351;&#25381;&#23448;&#30340;&#21453;&#39304;&#23454;&#26102;&#31934;&#32454;&#21270;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;&#12298;&#26143;&#38469;&#20105;&#38712;II&#12299;&#28216;&#25103;&#30340;&#20891;&#20107;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;COA-GPT&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;COA-GPT&#22312;&#26356;&#24555;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;&#30340;COAs&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#26550;&#26500;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26631;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20102;&#30452;&#25509;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#30740;&#31350;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.01785</link><description>&lt;p&gt;
DoubleMLDeep: &#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#22240;&#26524;&#25928;&#24212;&#36827;&#34892;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DoubleMLDeep: Estimation of Causal Effects with Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#26550;&#26500;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26631;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20102;&#30452;&#25509;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#30740;&#31350;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21363;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#20110;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#37096;&#20998;&#32447;&#24615;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#35770;&#25991;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35780;&#20272;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#24182;&#19982;&#26631;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#31361;&#20986;&#20102;&#30452;&#25509;&#22312;&#22240;&#26524;&#30740;&#31350;&#20013;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#32463;&#27982;&#23398;&#12289;&#24066;&#22330;&#33829;&#38144;&#12289;&#37329;&#34701;&#12289;&#21307;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20182;&#20204;&#24076;&#26395;&#20351;&#29992;&#38750;&#20256;&#32479;&#25968;&#25454;&#20272;&#35745;&#22240;&#26524;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#31995;&#32479;&#35774;&#35745;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.01783</link><description>&lt;p&gt;
&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Label Classification of Online Vaccine Concerns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#31995;&#32479;&#35774;&#35745;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#20851;&#27880;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30446;&#26631;&#65292;&#21487;&#20197;&#22312;COVID-19&#22823;&#27969;&#34892;&#20013;&#24555;&#36895;&#21464;&#21270;&#12290;&#36890;&#36807;&#35782;&#21035;&#30123;&#33495;&#20851;&#27880;&#21644;&#38169;&#35823;&#20449;&#24687;&#30340;&#38271;&#26399;&#36235;&#21183;&#65292;&#21487;&#20197;&#24110;&#21161;&#20844;&#20849;&#21355;&#29983;&#21162;&#21147;&#22312;&#36164;&#28304;&#25110;&#20449;&#24687;&#23459;&#20256;&#19978;&#36827;&#34892;&#25112;&#30053;&#24615;&#20998;&#37197;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#22312;&#22312;&#32447;&#35752;&#35770;&#20013;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#30340;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#23454;&#26102;&#30417;&#25511;&#22312;&#32447;&#26469;&#28304;&#38656;&#35201;&#22823;&#35268;&#27169;&#25512;&#29702;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20197;&#20026;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#30340;&#31995;&#32479;&#35774;&#35745;&#36873;&#25321;&#25552;&#20379;&#20449;&#24687;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;&#23545;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;LLM&#22810;&#27425;&#36827;&#34892;&#20998;&#31867;&#65292;&#27599;&#27425;&#36890;&#36807;&#24067;&#23572;&#38382;&#39064;&#21028;&#26029;&#25991;&#26412;&#26159;&#21542;&#25552;&#21040;&#30123;&#33495;&#20851;&#27880;&#65292;&#25928;&#26524;&#26368;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#33021;&#22815;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01782</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spiking Neural Network Learning Methods with Varying Locality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20449;&#24687;&#22312;SNN&#20013;&#20197;&#33033;&#20914;&#24418;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#26426;&#21046;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33033;&#20914;&#26426;&#21046;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#35757;&#32451;SNN&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29983;&#29289;&#23398;&#19978;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#23616;&#37096;&#24615;&#30340;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#22312;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but comes with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, this research examines the implicitly recurrent nature of SNNs and investigat
&lt;/p&gt;</description></item><item><title>&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01781</link><description>&lt;p&gt;
&#24403;&#22522;&#20934;&#25104;&#20026;&#30446;&#26631;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25490;&#34892;&#27036;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01781
&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#20934;&#25490;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25490;&#34892;&#27036;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#23454;&#36341;&#32773;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#12290;&#36890;&#24120;&#65292;&#21457;&#24067;&#30340;&#25490;&#34892;&#27036;&#25490;&#21517;&#34987;&#30452;&#25509;&#25509;&#21463; - &#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#65288;&#28508;&#22312;&#26114;&#36149;&#30340;&#65289;&#38169;&#35823;&#12290;&#22312;&#29616;&#26377;&#30340;&#25490;&#34892;&#27036;&#19979;&#65292;LLM&#30340;&#30456;&#23545;&#24615;&#33021;&#23545;&#65288;&#36890;&#24120;&#24494;&#23567;&#30340;&#65289;&#32454;&#33410;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#27969;&#34892;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22522;&#20934;&#65288;&#20363;&#22914;MMLU&#65289;&#65292;&#23545;&#22522;&#20934;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#22914;&#25913;&#21464;&#36873;&#39033;&#39034;&#24207;&#25110;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#65292;&#20250;&#23548;&#33268;&#25490;&#21517;&#21464;&#21270;&#36798;&#21040;8&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#25200;&#21160;&#31867;&#21035;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#24182;&#30830;&#23450;&#36825;&#19968;&#34892;&#20026;&#30340;&#26469;&#28304;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#20248;&#21270;&#30340;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#36827;&#34892;&#31572;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20381;&#36182;&#31616;&#21333;&#22522;&#20934;&#35780;&#20272;&#30340;&#39118;&#38505;&#65292;&#24182;&#20026;&#26356;&#20581;&#22766;&#30340;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#25351;&#23548;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01779</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play image restoration with Stochastic deNOising REgularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#31639;&#27861;&#26159;&#19968;&#31867;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#22270;&#20687;&#21453;&#28436;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22270;&#20687;&#24674;&#22797;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#23569;&#22122;&#38899;&#30340;&#22270;&#20687;&#19978;&#30340;&#19968;&#31181;&#38750;&#26631;&#20934;&#30340;&#21435;&#22122;&#22120;&#20351;&#29992;&#26041;&#27861;&#65292;&#36825;&#19982;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#26368;&#26032;&#31639;&#27861;&#30456;&#30683;&#30462;&#65292;&#22312;&#36825;&#20123;&#31639;&#27861;&#20013;&#65292;&#21435;&#22122;&#22120;&#20165;&#24212;&#29992;&#20110;&#37325;&#26032;&#21152;&#22122;&#30340;&#22270;&#20687;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PnP&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#65292;&#23427;&#20165;&#22312;&#22122;&#22768;&#27700;&#24179;&#36866;&#24403;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#12290;&#23427;&#22522;&#20110;&#26174;&#24335;&#30340;&#38543;&#26426;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31181;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#21450;&#20854;&#36864;&#28779;&#25193;&#23637;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Matlab&#23454;&#29616;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#24314;&#27169;&#12289;&#24378;&#22823;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#19977;&#20010;&#21333;&#35789;&#36827;&#34892;&#27491;&#30830;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.01778</link><description>&lt;p&gt;
&#35821;&#38899;&#35782;&#21035;&#23548;&#35770;
&lt;/p&gt;
&lt;p&gt;
Introduction to speech recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01778
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Matlab&#23454;&#29616;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#24314;&#27169;&#12289;&#24378;&#22823;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#19977;&#20010;&#21333;&#35789;&#36827;&#34892;&#27491;&#30830;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21253;&#21547;&#20102;&#20351;&#29992;Matlab&#36827;&#34892;&#35762;&#24231;&#21644;&#23454;&#36341;&#23454;&#39564;&#30340;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#33021;&#22815;&#27491;&#30830;&#20998;&#31867;&#19977;&#20010;&#21333;&#35789;&#65288;one&#65292;two&#21644;three&#65289;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#20102;&#19968;&#20010;&#38750;&#24120;&#23567;&#30340;&#25968;&#25454;&#24211;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#26679;&#30340;&#24615;&#33021;&#65292;&#23427;&#20351;&#29992;&#20102;&#29305;&#23450;&#30340;&#35821;&#38899;&#24314;&#27169;&#65292;&#24378;&#22823;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#65288;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#21644;Dijkstra&#31639;&#27861;&#65289;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#65288;&#26368;&#36817;&#37051;&#65289;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document contains lectures and practical experimentations using Matlab and implementing a system which is actually correctly classifying three words (one, two and three) with the help of a very small database. To achieve this performance, it uses speech modeling specificities, powerful computer algorithms (dynamic time warping and Dijktra's algorithm) and machine learning (nearest neighbor). This document introduces also some machine learning evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20855;&#26377;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36716;&#31227;&#25928;&#26524;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#24847;&#22806;&#22320;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#20135;&#29983;&#27491;&#21521;&#36716;&#31227;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01772</link><description>&lt;p&gt;
&#35299;&#24320;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#21644;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20855;&#26377;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36716;&#31227;&#25928;&#26524;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#24847;&#22806;&#22320;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#20135;&#29983;&#27491;&#21521;&#36716;&#31227;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;(MMT)&#22312;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#19968;&#23545;&#22810;&#32763;&#35793;&#30456;&#27604;&#20110;&#22810;&#23545;&#19968;&#32763;&#35793;&#30340;&#25913;&#36827;&#20165;&#26377;&#24494;&#23567;&#29978;&#33267;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#24322;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#19968;&#23545;&#22810;MT&#20013;&#65292;&#27491;&#21521;&#36716;&#31227;&#22312;&#30446;&#26631;&#31471;&#30340;&#20316;&#29992;&#31243;&#24230;&#22914;&#20309;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#35821;&#26009;&#24211;&#22823;&#23567;&#36825;&#20004;&#20010;&#32500;&#24230;&#21464;&#21270;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#65292;&#20197;&#23637;&#31034;&#30693;&#35782;&#36716;&#31227;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27491;&#21521;&#36716;&#31227;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#20351;&#20027;&#35201;&#35821;&#35328;&#23545;&#21463;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#20986;&#20046;&#24847;&#26009;&#22320;&#20351;&#20027;&#35201;&#35821;&#35328;&#23545;&#21463;&#30410;&#65292;&#21363;&#20351;&#27491;&#21521;&#36716;&#31227;&#26368;&#23567;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation (MMT) benefits from knowledge transfer across different language pairs. However, improvements in one-to-many translation compared to many-to-one translation are only marginal and sometimes even negligible. This performance discrepancy raises the question of to what extent positive transfer plays a role on the target-side for one-to-many MT. In this paper, we conduct a large-scale study that varies the auxiliary target side languages along two dimensions, i.e., linguistic similarity and corpus size, to show the dynamic impact of knowledge transfer on the main language pairs. We show that linguistically similar auxiliary target languages exhibit strong ability to transfer positive knowledge. With an increasing size of similar target languages, the positive transfer is further enhanced to benefit the main language pairs. Meanwhile, we find distant auxiliary target languages can also unexpectedly benefit main language pairs, even with minimal positive trans
&lt;/p&gt;</description></item><item><title>BlackMamba&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;Mamba SSM&#21644;MoE&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23427;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.01771</link><description>&lt;p&gt;
BlackMamba: &#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BlackMamba: Mixture of Experts for State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01771
&lt;/p&gt;
&lt;p&gt;
BlackMamba&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;Mamba SSM&#21644;MoE&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23427;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;transformer&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#65292;&#20854;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;SSM&#27169;&#22411;Mamba&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22788;&#29702;&#38271;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#22312;&#26174;&#33879;&#38477;&#20302;&#25512;&#26029;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20063;&#22686;&#21152;&#20102;&#26356;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BlackMamba&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23558;Mamba SSM&#19982;MoE&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#20004;&#32773;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35777;&#26126;BlackMamba&#22312;Mamba&#21644;transformer&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#25512;&#26029;&#21644;&#35757;&#32451;FLOPs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#30340;300B&#26631;&#35760;&#19978;&#20840;&#38754;&#35757;&#32451;&#24182;&#24320;&#28304;&#20102;340M/1.5B&#21644;630M/2.8B&#30340;BlackMamba&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BlackMamba&#32487;&#25215;&#24182;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65288;EPINNs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24378;&#32806;&#21512;&#21644;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#21160;&#24577;&#27850;&#26494;-&#32435;&#23425;&#26031;&#29305;-&#26222;&#26391;&#20811;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#33258;&#36866;&#24212;&#25439;&#22833;&#26435;&#37325;&#21644;&#37319;&#29992;&#37325;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#35299;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01768</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21160;&#24577;&#27850;&#26494;-&#32435;&#23425;&#26031;&#29305;-&#26222;&#26391;&#20811;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enriched Physics-informed Neural Networks for Dynamic Poisson-Nernst-Planck Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65288;EPINNs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24378;&#32806;&#21512;&#21644;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#21160;&#24577;&#27850;&#26494;-&#32435;&#23425;&#26031;&#29305;-&#26222;&#26391;&#20811;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#33258;&#36866;&#24212;&#25439;&#22833;&#26435;&#37325;&#21644;&#37319;&#29992;&#37325;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#35299;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32593;&#26684;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#22686;&#24378;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;EPINNs&#65289;&#65292;&#29992;&#20110;&#27714;&#35299;&#20855;&#26377;&#24378;&#32806;&#21512;&#21644;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#21160;&#24577;&#27850;&#26494;-&#32435;&#23425;&#26031;&#29305;-&#26222;&#26391;&#20811;&#65288;PNP&#65289;&#26041;&#31243;&#12290;EPINNs&#23558;&#20256;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#26412;&#26694;&#26550;&#65292;&#24182;&#21152;&#20837;&#20102;&#33258;&#36866;&#24212;&#25439;&#22833;&#26435;&#37325;&#26469;&#24179;&#34913;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#26681;&#25454;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#26356;&#26032;&#21442;&#25968;&#33258;&#21160;&#20998;&#37197;&#25439;&#22833;&#30340;&#26435;&#37325;&#12290;&#22312;EPINNs&#20013;&#37319;&#29992;&#37325;&#37319;&#26679;&#31574;&#30053;&#26469;&#21152;&#36895;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;GPU&#24182;&#34892;&#35745;&#31639;&#25216;&#26415;&#26469;&#21152;&#36895;&#27714;&#35299;&#36807;&#31243;&#12290;&#36890;&#36807;&#22235;&#20010;&#31034;&#20363;&#26469;&#35777;&#26126;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#27861;&#30456;&#27604;&#65292;&#26032;&#26041;&#27861;&#22312;&#35299;&#20915;&#36825;&#31181;&#32806;&#21512;&#38750;&#32447;&#24615;&#31995;&#32479;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;EPINNs&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a meshless deep learning algorithm, enriched physics-informed neural networks (EPINNs), to solve dynamic Poisson-Nernst-Planck (PNP) equations with strong coupling and nonlinear characteristics. The EPINNs takes the traditional physics-informed neural networks as the foundation framework, and adds the adaptive loss weight to balance the loss functions, which automatically assigns the weights of losses by updating the parameters in each iteration based on the maximum likelihood estimate. The resampling strategy is employed in the EPINNs to accelerate the convergence of loss function. Meanwhile, the GPU parallel computing technique is adopted to accelerate the solving process. Four examples are provided to demonstrate the validity and effectiveness of the proposed method. Numerical results indicate that the new method has better applicability than traditional numerical methods in solving such coupled nonlinear systems. More importantly, the EPINNs is more accurate, st
&lt;/p&gt;</description></item><item><title>HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01767</link><description>&lt;p&gt;
HiQA&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#30340;&#20998;&#23618;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;RAG&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01767
&lt;/p&gt;
&lt;p&gt;
HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36805;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#34917;&#20805;&#25991;&#26723;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#23398;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#20943;&#36731;&#20102;&#24187;&#35273;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#22823;&#37327;&#26080;&#27861;&#21306;&#20998;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26377;&#38480;&#65292;&#32473;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MDQA&#65289;&#26694;&#26550;&#65292;&#23558;&#32423;&#32852;&#30340;&#20803;&#25968;&#25454;&#25972;&#21512;&#21040;&#20869;&#23481;&#20013;&#65292;&#21516;&#26102;&#20855;&#22791;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;MasQA&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#30740;&#31350;MDQA&#12290;&#26368;&#21518;&#65292;HiQA&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01763</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#19978;&#21521;&#37327;&#25968;&#25454;&#24211;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Large Language Models Meet Vector Databases: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#22312;&#20154;&#31867;&#25991;&#23383;&#22788;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#21253;&#25324;&#24187;&#35273;&#12289;&#20559;&#35265;&#12289;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#20197;&#21450;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#32500;&#25252;&#30340;&#39640;&#25104;&#26412;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#32780;&#21478;&#19968;&#31181;&#26085;&#30410;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#21521;&#37327;&#25968;&#25454;&#24211;&#21017;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25797;&#38271;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#25628;&#32034;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#23427;&#20204;&#26174;&#33879;&#22686;&#24378;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#29420;&#29305;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01761</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking Interpretability in the Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01761
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#39046;&#22495;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#30340;&#25512;&#21160;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20026;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#26426;&#20250;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#20351;&#24471;LLMs&#33021;&#22815;&#25193;&#23637;&#32473;&#20154;&#31867;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19978;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#30340;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#27604;&#22914;&#34394;&#26500;&#30340;&#35299;&#37322;&#21644;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#35780;&#20272;&#26032;&#20852;LLM&#35299;&#37322;&#39046;&#22495;&#30340;&#29616;&#26377;&#26041;&#27861;&#65288;&#21253;&#25324;&#35299;&#37322;LLM&#21644;&#20351;&#29992;LLM&#36827;&#34892;&#35299;&#37322;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;LLMs&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#23545;LLMs&#26412;&#36523;&#30340;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#23637;&#31034;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01759</link><description>&lt;p&gt;
&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65306;&#29992;&#20110;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Systematic Literature Review: Computational Approaches for Humour Style Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#23637;&#31034;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21508;&#31181;&#24189;&#40664;&#39118;&#26684;&#23545;&#20110;&#29702;&#35299;&#24189;&#40664;&#30340;&#22810;&#38754;&#24615;&#21450;&#20854;&#22312;&#24515;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#29702;&#35299;&#25581;&#31034;&#20102;&#20381;&#25454;&#25152;&#37319;&#29992;&#30340;&#39118;&#26684;&#65292;&#24189;&#40664;&#21487;&#20197;&#23545;&#20010;&#20154;&#30340;&#20581;&#24247;&#21644;&#20154;&#38469;&#20851;&#31995;&#20135;&#29983;&#27835;&#30103;&#25110;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#19987;&#38376;&#30740;&#31350;&#22522;&#20110;&#35745;&#31639;&#30340;&#24189;&#40664;&#39118;&#26684;&#20998;&#26512;&#30340;&#30740;&#31350;&#20173;&#28982;&#27604;&#36739;&#23569;&#35265;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#20108;&#20803;&#24189;&#40664;&#21644;&#35773;&#21050;&#35782;&#21035;&#26041;&#38754;&#65292;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#24212;&#29992;&#20110;&#36825;&#20123;&#30456;&#20851;&#20219;&#21153;&#30340;&#35745;&#31639;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#24189;&#40664;&#39118;&#26684;&#20998;&#26512;&#30340;&#22522;&#26412;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#26377;&#25928;&#22320;&#24341;&#23548;&#24189;&#40664;&#30740;&#31350;&#30340;&#22797;&#26434;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#30830;&#23450;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding various humour styles is essential for comprehending the multifaceted nature of humour and its impact on fields such as psychology and artificial intelligence. This understanding has revealed that humour, depending on the style employed, can either have therapeutic or detrimental effects on an individual's health and relationships. Although studies dedicated exclusively to computational-based humour style analysis remain somewhat rare, an expansive body of research thrives within related task, particularly binary humour and sarcasm recognition. In this systematic literature review (SLR), we survey the landscape of computational techniques applied to these related tasks and also uncover their fundamental relevance to humour style analysis. Through this study, we unveil common approaches, illuminate various datasets and evaluation metrics, and effectively navigate the complex terrain of humour research. Our efforts determine potential research gaps and outlined promising di
&lt;/p&gt;</description></item><item><title>SpecDiff-GAN &#26159;&#19968;&#31181;&#22522;&#20110; HiFi-GAN &#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#36890;&#36807;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#26469;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35889;&#24418;&#29366;&#22122;&#22768;&#20998;&#24067;&#20351;&#37492;&#21035;&#22120;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#27169;&#22411;&#22312;&#35821;&#38899;&#21644;&#38899;&#20048;&#21512;&#25104;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.01753</link><description>&lt;p&gt;
SpecDiff-GAN&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#21644;&#38899;&#20048;&#21512;&#25104;&#30340;&#35889;&#24418;&#29366;&#22122;&#22768;&#25193;&#25955;GAN
&lt;/p&gt;
&lt;p&gt;
SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and Music Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01753
&lt;/p&gt;
&lt;p&gt;
SpecDiff-GAN &#26159;&#19968;&#31181;&#22522;&#20110; HiFi-GAN &#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#36890;&#36807;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#26469;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35889;&#24418;&#29366;&#22122;&#22768;&#20998;&#24067;&#20351;&#37492;&#21035;&#22120;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#27169;&#22411;&#22312;&#35821;&#38899;&#21644;&#38899;&#20048;&#21512;&#25104;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#20449;&#21495;&#65292;&#21516;&#26102;&#30830;&#20445;&#24555;&#36895;&#37319;&#26679;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#35757;&#32451;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#22349;&#32553;&#21644;&#21457;&#25955;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;HiFi-GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#8212;&#8212;SpecDiff-GAN&#65292;&#35813;&#27169;&#22411;&#26368;&#21021;&#29992;&#20110;&#20174;mel&#39057;&#35889;&#22270;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#26469;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#35813;&#36807;&#31243;&#22312;&#36755;&#20837;&#37492;&#21035;&#22120;&#20043;&#21069;&#21521;&#30495;&#23454;&#26679;&#26412;&#21644;&#20266;&#36896;&#26679;&#26412;&#27880;&#20837;&#26469;&#33258;&#39640;&#26031;&#20998;&#24067;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#35889;&#24418;&#29366;&#22122;&#22768;&#20998;&#24067;&#26469;&#20351;&#37492;&#21035;&#22120;&#30340;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#21644;&#38899;&#20048;&#21512;&#25104;&#26041;&#38754;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#19982;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38899;&#39057;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial network (GAN) models can synthesize highquality audio signals while ensuring fast sample generation. However, they are difficult to train and are prone to several issues including mode collapse and divergence. In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN, which was initially devised for speech synthesis from mel spectrogram. In our model, the training stability is enhanced by means of a forward diffusion process which consists in injecting noise from a Gaussian distribution to both real and fake samples before inputting them to the discriminator. We further improve the model by exploiting a spectrally-shaped noise distribution with the aim to make the discriminator's task more challenging. We then show the merits of our proposed model for speech and music synthesis on several datasets. Our experiments confirm that our model compares favorably in audio quality and efficiency compared to several baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#26469;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#20197;&#21450;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01752</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#35782;&#21035;&#36785;&#39554;&#35328;&#35770;&#21644;&#20551;&#28040;&#24687;&#65292;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;
&lt;/p&gt;
&lt;p&gt;
Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#26469;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#20197;&#21450;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
YouTube&#38754;&#20020;&#30528;&#20840;&#29699;&#33539;&#22260;&#20869;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#30340;&#20256;&#25773;&#21361;&#26426;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;YouTube&#24050;&#23454;&#26045;&#20005;&#26684;&#35268;&#23450;&#65292;&#31105;&#27490;&#19978;&#20256;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#25110;&#23459;&#20256;&#20167;&#24680;&#35328;&#35770;&#30340;&#20869;&#23481;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#38477;&#20302;&#20882;&#29359;&#24615;&#33521;&#35821;&#20869;&#23481;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20711;&#20285;&#32599;&#35821;&#20869;&#23481;&#30340;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20943;&#23569;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#26292;&#21147;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#24320;&#21457;&#19968;&#20010;&#35780;&#32423;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#26631;&#39064;&#21644;&#25551;&#36848;&#19982;&#38899;&#39057;&#20869;&#23481;&#65292;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;Pytube&#24211;&#36827;&#34892;&#38899;&#39057;&#25552;&#21462;&#65292;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#36716;&#24405;&#65292;&#20351;&#29992;distilroberta-base&#27169;&#22411;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
YouTube faces a global crisis with the dissemination of false information and hate speech. To counter these issues, YouTube has implemented strict rules against uploading content that includes false information or promotes hate speech. While numerous studies have been conducted to reduce offensive English-language content, there's a significant lack of research on Sinhala content. This study aims to address the aforementioned gap by proposing a solution to minimize the spread of violence and misinformation in Sinhala YouTube videos. The approach involves developing a rating system that assesses whether a video contains false information by comparing the title and description with the audio content and evaluating whether the video includes hate speech. The methodology encompasses several steps, including audio extraction using the Pytube library, audio transcription via the fine-tuned Whisper model, hate speech detection employing the distilroberta-base model and a text classification L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.01749</link><description>&lt;p&gt;
&#36808;&#21521;&#22478;&#24066;&#26234;&#33021;&#65306;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29616;&#24050;&#25104;&#20026;&#26234;&#33021;&#22478;&#24066;&#26381;&#21153;&#36827;&#27493;&#30340;&#26680;&#24515;&#65292;&#23545;&#25552;&#39640;&#22478;&#24066;&#29615;&#22659;&#30340;&#25928;&#29575;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#23452;&#23621;&#24615;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;ChatGPT&#31561;&#22522;&#30784;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#12290;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#34920;&#26126;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25972;&#21512;&#21040;&#22478;&#24066;&#39046;&#22495;&#20013;&#21487;&#33021;&#23545;&#26234;&#33021;&#22478;&#24066;&#30340;&#21457;&#23637;&#20135;&#29983;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;&#23613;&#31649;&#23545;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#65288;UFMs&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#12289;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#21644;&#21487;&#26222;&#36941;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;UFM&#30340;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#26500;&#24314;&#23427;&#20204;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#24403;&#21069;&#19982;UFM&#30456;&#20851;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces challenges such as a lack of clear definitions, systematic reviews, and universalizable solutions. To this end, this paper first introduces the concept of UFM and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes current UFM-related works, base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.01748</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#30784;&#27169;&#22411;&#34987;&#23459;&#31216;&#20026;6G&#31995;&#32479;&#30340;&#25913;&#21464;&#32773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;LLMs&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20197;&#26080;&#32447;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#37326;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;(AI)&#21407;&#29983;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#22522;&#20110;NLP&#30340;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#20419;&#36827;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#30340;&#35774;&#35745;&#65306;1) &#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#65292;2) &#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;3) &#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20197;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
&lt;/p&gt;</description></item><item><title>3DG&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#19977;&#32500;&#24352;&#37327;&#24182;&#32467;&#21512;&#24352;&#37327;&#20998;&#35299;&#21644;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#26469;&#22788;&#29702;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23398;&#20064;&#32773;&#24615;&#33021;&#31232;&#30095;&#25968;&#25454;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01746</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22788;&#29702;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#23398;&#20064;&#32773;&#24615;&#33021;&#31232;&#30095;&#25968;&#25454;&#30340;3DG&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01746
&lt;/p&gt;
&lt;p&gt;
3DG&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#19977;&#32500;&#24352;&#37327;&#24182;&#32467;&#21512;&#24352;&#37327;&#20998;&#35299;&#21644;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#26469;&#22788;&#29702;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23398;&#20064;&#32773;&#24615;&#33021;&#31232;&#30095;&#25968;&#25454;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#34920;&#29616;&#25968;&#25454;&#65288;&#22914;&#27979;&#39564;&#20998;&#25968;&#21644;&#23581;&#35797;&#27425;&#25968;&#65289;&#23545;&#20110;&#29702;&#35299;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#21644;&#30693;&#35782;&#25484;&#25569;&#27700;&#24179;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#25910;&#38598;&#30340;&#23398;&#20064;&#34920;&#29616;&#25968;&#25454;&#24120;&#24120;&#31232;&#30095;&#65292;&#24433;&#21709;&#23398;&#20064;&#32773;&#24314;&#27169;&#21644;&#30693;&#35782;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;3DG&#26694;&#26550;&#65288;&#19977;&#32500;&#24352;&#37327;&#31264;&#23494;&#29983;&#25104;&#19982;&#29983;&#25104;&#24615;&#27169;&#22411;&#32467;&#21512;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24352;&#37327;&#20998;&#35299;&#21644;&#20808;&#36827;&#29983;&#25104;&#24615;&#27169;&#22411;&#65288;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25968;&#25454;&#22635;&#34917;&#21644;&#25193;&#20805;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#19977;&#32500;&#24352;&#37327;&#65292;&#25429;&#25417;&#23398;&#20064;&#32773;&#12289;&#38382;&#39064;&#21644;&#23581;&#35797;&#27425;&#25968;&#36825;&#19977;&#20010;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24352;&#37327;&#20998;&#35299;&#23545;&#25968;&#25454;&#36827;&#34892;&#31264;&#23494;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#38024;&#23545;&#20010;&#20307;&#23398;&#20064;&#27169;&#24335;&#30340;&#32858;&#31867;&#36827;&#34892;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#25193;&#20805;&#12290;&#24212;&#29992;&#20110;AutoTutor&#30340;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning performance data (e.g., quiz scores and attempts) is significant for understanding learner engagement and knowledge mastery level. However, the learning performance data collected from Intelligent Tutoring Systems (ITSs) often suffers from sparsity, impacting the accuracy of learner modeling and knowledge assessments. To address this, we introduce the 3DG framework (3-Dimensional tensor for Densification and Generation), a novel approach combining tensor factorization with advanced generative models, including Generative Adversarial Network (GAN) and Generative Pre-trained Transformer (GPT), for enhanced data imputation and augmentation. The framework operates by first representing the data as a three-dimensional tensor, capturing dimensions of learners, questions, and attempts. It then densifies the data through tensor factorization and augments it using Generative AI models, tailored to individual learning patterns identified via clustering. Applied to data from an AutoTutor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36755;&#20986;&#36136;&#37327;&#24182;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.01742</link><description>&lt;p&gt;
&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Optimizing the Costs of LLM Usage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36755;&#20986;&#36136;&#37327;&#24182;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;LLM&#22312;&#29616;&#20170;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25991;&#20214;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;LLM&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#12289;&#25104;&#26412;&#12289;&#26631;&#35760;&#21270;&#21644;&#24310;&#36831;&#12290;&#23454;&#38469;&#19978;&#65292;&#20225;&#19994;&#24050;&#32463;&#22312;&#20026;&#21508;&#33258;&#30340;&#29992;&#20363;&#36816;&#33829;&#25110;&#20351;&#29992;LLM&#32780;&#25215;&#25285;&#24040;&#22823;&#30340;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;LLM&#30340;&#36755;&#20986;&#36136;&#37327;&#65288;&#32780;&#26080;&#38656;&#23454;&#38469;&#35843;&#29992;LLM&#65289;&#65292;&#28982;&#21518;&#35299;&#20915;LLM&#36873;&#25321;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#20197;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;LLM&#22312;&#25688;&#35201;&#31561;&#25991;&#20214;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#36755;&#20986;&#36136;&#37327;&#65292;&#38543;&#21518;&#37319;&#29992;LP&#21462;&#25972;&#31639;&#27861;&#26469;&#20248;&#21270;LLM&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#36136;&#37327;&#21644;&#25104;&#26412;&#20043;&#38388;&#26435;&#34913;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente
&lt;/p&gt;</description></item><item><title>OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01739</link><description>&lt;p&gt;
OpenMoE&#65306;&#24320;&#28304;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26089;&#26399;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01739
&lt;/p&gt;
&lt;p&gt;
OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24110;&#21161;&#24320;&#28304;&#31038;&#21306;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;OpenMoE&#65292;&#19968;&#31995;&#21015;&#23436;&#20840;&#24320;&#25918;&#28304;&#30721;&#21644;&#21487;&#22797;&#29616;&#30340;&#20165;&#35299;&#30721;&#22120;MoE LLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;650M&#21040;34B&#65292;&#35757;&#32451;&#25968;&#25454;&#36229;&#36807;1T&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;MoE-based LLM&#21487;&#20197;&#25552;&#20379;&#27604;&#23494;&#38598;LLM&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;LLM&#24320;&#21457;&#30340;&#28508;&#22312;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#23545;&#25105;&#20204;&#30340;OpenMoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#26426;&#21046;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#19978;&#19979;&#25991;&#26080;&#20851;&#19987;&#19994;&#21270;&#12289;&#26089;&#26399;&#36335;&#30001;&#23398;&#20064;&#21644;&#26411;&#23614;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;ID&#65292;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#24456;&#23567;&#12290;&#26631;&#35760;&#21040;&#19987;&#23478;&#30340;&#20998;&#37197;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26089;&#26399;&#30830;&#23450;&#65292;&#24182;&#19988;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#19981;&#23436;&#20840;&#30340;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;...
&lt;/p&gt;
&lt;p&gt;
To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
&lt;/p&gt;</description></item><item><title>CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01734</link><description>&lt;p&gt;
CFTM: &#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CFTM: Continuous time fractional topic model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01734
&lt;/p&gt;
&lt;p&gt;
CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;&#65288;cFTM&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;fBm&#65289;&#26377;&#25928;&#22320;&#35782;&#21035;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#38543;&#26102;&#38388;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;cFTM&#21487;&#20197;&#25429;&#25417;&#21040;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#20013;&#30340;&#36825;&#20123;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#65292;&#21453;&#26144;&#20102;fBm&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;cFTM&#30340;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#19982;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;LDA&#30340;&#30456;&#24403;&#12290;&#20026;&#20102;&#35777;&#26126;cFTM&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#27982;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#20123;&#27979;&#35797;&#30340;&#32467;&#26524;&#25903;&#25345;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#38543;&#26102;&#38388;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
&lt;/p&gt;</description></item><item><title>CERM&#26159;&#19968;&#20010;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#25110;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#39135;&#26448;&#19982;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#26356;&#22909;&#22320;&#25903;&#25345;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.01724</link><description>&lt;p&gt;
CERM: &#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CERM: Context-aware Literature-based Discovery via Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01724
&lt;/p&gt;
&lt;p&gt;
CERM&#26159;&#19968;&#20010;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#25110;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#39135;&#26448;&#19982;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#26356;&#22909;&#22320;&#25903;&#25345;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29983;&#29289;&#21307;&#23398;&#20986;&#29256;&#29289;&#30340;&#20016;&#23500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#26469;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20043;&#21069;&#23558;&#20581;&#24247;&#32435;&#20837;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#30340;&#23581;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#19978;&#65292;&#25110;&#32773;&#21033;&#29992;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#22522;&#26412;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25429;&#25417;&#39135;&#26448;&#21644;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#22266;&#26377;&#20851;&#31995;&#30340;&#22686;&#24378;&#27169;&#22411;&#23545;&#20110;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#26356;&#26377;&#30410;&#22788;&#65292;&#37492;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#26114;&#36149;&#30340;&#25968;&#25454;&#26631;&#35760;&#36807;&#31243;&#65292;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#26377;&#25928;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#23454;&#20307;&#20851;&#31995;&#24773;&#24863;&#20998;&#26512;&#65288;ERSA&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22522;&#20110;&#23454;&#20307;&#23545;&#25429;&#25417;&#25991;&#26412;&#30340;&#24773;&#24863;&#12290;ERSA&#25193;&#23637;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;ERSA&#20219;&#21153;&#19978;&#65292;&#37325;&#28857;&#20851;&#27880;(entity-ent
&lt;/p&gt;
&lt;p&gt;
Driven by the abundance of biomedical publications, we introduce a sentiment analysis task to understand food-health relationship. Prior attempts to incorporate health into recipe recommendation and analysis systems have primarily focused on ingredient nutritional components or utilized basic computational models trained on curated labeled data. Enhanced models that capture the inherent relationship between food ingredients and biomedical concepts can be more beneficial for food-related research, given the wealth of information in biomedical texts. Considering the costly data labeling process, these models should effectively utilize both labeled and unlabeled data. This paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that captures the sentiment of a text based on an entity pair. ERSA extends the widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our study concentrates on the ERSA task applied to biomedical texts, focusing on (entity-ent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01720</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Amharic Chatbot for FAQs in Universities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#23398;&#29983;&#24120;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21521;&#31649;&#29702;&#21592;&#25110;&#25945;&#24072;&#23547;&#27714;&#24120;&#35265;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#36825;&#23545;&#21452;&#26041;&#26469;&#35828;&#37117;&#24456;&#32321;&#29712;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#38463;&#22982;&#21704;&#25289;&#35821;&#20013;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20316;&#20026;&#34394;&#25311;&#21161;&#25163;&#22788;&#29702;&#38382;&#39064;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31243;&#24207;&#20351;&#29992;&#26631;&#35760;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#21435;&#38500;&#20572;&#29992;&#35789;&#21644;&#35789;&#24178;&#25552;&#21462;&#23545;&#38463;&#22982;&#21704;&#25289;&#35821;&#36755;&#20837;&#21477;&#23376;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#26469;&#20998;&#31867;&#26631;&#35760;&#21644;&#26816;&#32034;&#21512;&#36866;&#30340;&#22238;&#31572;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#36890;&#36807;TensorFlow&#12289;Keras&#21644;NLTK&#23454;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.01719</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#36947;&#24503;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Moral Inconsistencies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35821;&#20041;&#31561;&#20215;&#30340;&#25552;&#31034;&#20135;&#29983;&#35821;&#20041;&#31561;&#20215;&#30340;&#21709;&#24212;&#65292;&#37027;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;LLMs&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#26041;&#38754;&#20063;&#23384;&#22312;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#20934;&#30830;&#24230;&#26469;&#34913;&#37327;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#27809;&#26377;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#30340;&#36947;&#24503;&#24773;&#26223;&#65288;&#20363;&#22914;&#65292;&#36947;&#36335;&#20132;&#36816;&#38382;&#39064;&#65289;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#26469;&#34913;&#37327;LLM&#22312;&#36947;&#24503;&#24773;&#26223;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#19982;&#20154;&#31867;&#21028;&#26029;&#22312;&#20116;&#20010;LLMs&#19978;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#22312;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24067;&#40065;&#22982;-&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#65288;BE-Sent&#65289;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25945;&#32946;&#35752;&#35770;&#35770;&#22363;&#20013;&#30340;&#24773;&#32490;&#21644;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#31867;&#12290;&#26041;&#27861;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#25991;&#26412;&#39044;&#22788;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35748;&#30693;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20102;&#35299;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.01716</link><description>&lt;p&gt;
Bloom-&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#23618;&#27425;&#20998;&#31867;&#22312;&#35838;&#31243;&#35752;&#35770;&#35770;&#22363;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24067;&#40065;&#22982;-&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#65288;BE-Sent&#65289;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25945;&#32946;&#35752;&#35770;&#35770;&#22363;&#20013;&#30340;&#24773;&#32490;&#21644;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#31867;&#12290;&#26041;&#27861;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#25991;&#26412;&#39044;&#22788;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35748;&#30693;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20102;&#35299;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35752;&#35770;&#35770;&#22363;&#24191;&#27867;&#34987;&#29992;&#20110;&#35762;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#31215;&#26497;&#25991;&#26412;&#20132;&#27969;&#65292;&#20197;&#21450;&#26816;&#26597;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#27604;&#36739;&#36866;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#25945;&#32946;&#35752;&#35770;&#35770;&#22363;&#20013;&#22522;&#20110;&#25991;&#26412;&#35780;&#35770;&#30340;&#24773;&#32490;&#21644;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#24067;&#40065;&#22982;&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65288;BE-Sent&#65289;&#12290;&#30740;&#31350;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#20174;&#20869;&#37096;&#35752;&#35770;&#35770;&#22363;&#21644;YouTube&#39057;&#36947;&#30340;&#35780;&#35770;&#20013;&#25910;&#38598;&#25968;&#25454;&#12290;&#19979;&#19968;&#27493;&#26159;&#23545;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#23545;&#25991;&#26412;&#36827;&#34892;&#26631;&#27880;&#24182;&#28165;&#38500;&#19981;&#37325;&#35201;&#30340;&#21333;&#35789;&#12290;&#27492;&#22806;&#65292;&#23545;&#24050;&#25104;&#21151;&#28165;&#29702;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#23558;&#22312;&#27599;&#20010;&#21477;&#23376;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#35748;&#30693;&#20998;&#31867;&#12290;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#31215;&#26497;&#30340;&#65292;&#28040;&#26497;&#30340;&#21644;&#20013;&#24615;&#30340;&#12290;&#24067;&#40065;&#22982;&#65288;Bloom&#65289;&#30340;&#35748;&#30693;&#20998;&#31867;&#26159;&#26681;&#25454;&#35748;&#30693;&#36807;&#31243;&#30340;&#20845;&#20010;&#23618;&#27425;&#36827;&#34892;&#30340;&#65292;&#20174;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#35760;&#24518;&#21040;&#39640;&#23618;&#27425;&#30340;&#35780;&#20215;&#21644;&#21019;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online discussion forums are widely used for active textual interaction between lecturers and students, and to see how the students have progressed in a learning process. The objective of this study is to compare appropriate machine-learning models to assess sentiments and Bloom\'s epistemic taxonomy based on textual comments in educational discussion forums. Our proposed method is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis (BE-Sent). The research methodology consists of three main steps. The first step is the data collection from the internal discussion forum and YouTube comments of a Web Programming channel. The next step is text preprocessing to annotate the text and clear unimportant words. Furthermore, with the text dataset that has been successfully cleaned, sentiment analysis and epistemic categorization will be done in each sentence of the text. Sentiment analysis is divided into three categories: positive, negative, and neutral. Bloom\'s epistem
&lt;/p&gt;</description></item><item><title>TrICy&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#24847;&#22270;&#21644;&#35302;&#21457;&#22120;&#24341;&#23548;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01714</link><description>&lt;p&gt;
TrICy: &#36890;&#36807;&#24847;&#22270;&#24863;&#30693;&#30340;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#24341;&#23548;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01714
&lt;/p&gt;
&lt;p&gt;
TrICy&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#24847;&#22270;&#21644;&#35302;&#21457;&#22120;&#24341;&#23548;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#65288;D2T&#65289;&#29983;&#25104;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#38754;&#21521;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#22312;&#21487;&#20197;&#30452;&#25509;&#19982;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#30340;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26550;&#26500;&#30001;&#20110;&#39640;&#20869;&#23384;&#21344;&#29992;&#32780;&#26080;&#27861;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrICy&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;D2T&#20219;&#21153;&#65292;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#24847;&#22270;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#21487;&#36827;&#19968;&#27493;&#30001;&#29992;&#25143;&#25552;&#20379;&#30340;&#35302;&#21457;&#22120;&#36827;&#34892;&#24341;&#23548;&#12290;&#25105;&#20204;&#21033;&#29992;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#65288;OOV&#65289;&#12290;&#23545;E2E NLG&#25968;&#25454;&#38598;&#65288;BLEU&#65306;66.43&#65285;&#65292;ROUGE-L&#65306;70.14&#65285;&#65289;&#65292;WebNLG&#25968;&#25454;&#38598;&#65288;BLEU&#65306;Seen 64.08&#65285;&#65292;Unseen 52.35&#65285;&#65289;&#21644;&#19982;&#25991;&#26412;&#28040;&#24687;&#24212;&#29992;&#30456;&#20851;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#20998;&#26512;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#21487;&#36873;&#30340;&#35302;&#21457;&#22120;&#36755;&#20837;&#65292;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01713</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#20854;&#19982;&#20256;&#32479;&#19978;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26032;&#30142;&#30149;&#29190;&#21457;&#26102;&#36805;&#36895;&#20915;&#31574;&#30340;&#32039;&#36843;&#38656;&#27714;&#30340;&#39537;&#20351;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;GPT-4&#30340;LLM&#23545;EHR&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#38024;&#23545;EHR&#25968;&#25454;&#30340;&#32437;&#21521;&#12289;&#31232;&#30095;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#30340;&#25552;&#31034;&#26041;&#27861;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;EHR&#29305;&#24449;&#65292;&#22914;&#21333;&#20301;&#21644;&#21442;&#32771;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;&#20102;&#19982;&#20020;&#24202;&#19978;&#19979;&#25991;&#30456;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LLM&#33021;&#22815;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;EHR&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#24863;&#30693;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01712</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#24863;&#30693;&#24335;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#24863;&#30693;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#25913;&#36827;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#31995;&#32479;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#26432;&#30456;&#20851;&#25968;&#25454;&#21608;&#22260;&#30340;&#25935;&#24863;&#24615;&#23548;&#33268;&#38590;&#20197;&#35775;&#38382;&#21040;&#22823;&#35268;&#27169;&#30340;&#12289;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#35757;&#32451;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;ChatGPT&#65292;Flan-T5&#21644;Llama&#31561;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20026;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#22522;&#20110;&#20174;&#24515;&#29702;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#31038;&#20250;&#22240;&#32032;&#65292;&#24182;&#26088;&#22312;&#30830;&#20445;&#28085;&#30422;&#19982;&#33258;&#26432;&#24847;&#24565;&#30456;&#20851;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19982;&#22522;&#20110;BERT&#31995;&#21015;&#32467;&#26500;&#30340;&#29616;&#26377;NLP&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#24403;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;UMD&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#36825;&#20123;&#20256;&#32479;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#36890;&#24120;&#22312;0.75&#21040;0.87&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#36873;&#25321;&#65292;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, i
&lt;/p&gt;</description></item><item><title>COVID-19&#30123;&#24773;&#23545;&#20052;&#27835;&#20122;&#24030;K-12&#25945;&#32946;&#31995;&#32479;&#36896;&#25104;&#20102;&#20005;&#37325;&#30340;&#19981;&#24179;&#31561;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#31181;&#26063;&#21644;&#27665;&#26063;&#25104;&#23601;&#24046;&#36317;&#26041;&#38754;&#12290;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#12289;&#22320;&#21306;&#21644;&#23398;&#31185;&#30340;&#23398;&#29983;&#25104;&#32489;&#65292;&#21457;&#29616;&#20102;&#33521;&#35821;&#21644;&#25968;&#23398;&#29087;&#32451;&#31243;&#24230;&#30340;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#22320;&#29702;&#22240;&#32032;&#23545;&#23398;&#19994;&#25104;&#23601;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01710</link><description>&lt;p&gt;
&#25506;&#32034;&#25945;&#32946;&#20844;&#24179;&#65306;&#25581;&#31034;&#20052;&#27835;&#20122;&#24030;&#25104;&#23601;&#24046;&#36317;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Educational Equity: A Machine Learning Approach to Unravel Achievement Disparities in Georgia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01710
&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#23545;&#20052;&#27835;&#20122;&#24030;K-12&#25945;&#32946;&#31995;&#32479;&#36896;&#25104;&#20102;&#20005;&#37325;&#30340;&#19981;&#24179;&#31561;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#31181;&#26063;&#21644;&#27665;&#26063;&#25104;&#23601;&#24046;&#36317;&#26041;&#38754;&#12290;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#12289;&#22320;&#21306;&#21644;&#23398;&#31185;&#30340;&#23398;&#29983;&#25104;&#32489;&#65292;&#21457;&#29616;&#20102;&#33521;&#35821;&#21644;&#25968;&#23398;&#29087;&#32451;&#31243;&#24230;&#30340;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#22320;&#29702;&#22240;&#32032;&#23545;&#23398;&#19994;&#25104;&#23601;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#26174;&#33879;&#21152;&#21095;&#20102;&#20052;&#27835;&#20122;&#24030;K-12&#25945;&#32946;&#31995;&#32479;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#31181;&#26063;&#21644;&#27665;&#26063;&#25104;&#23601;&#24046;&#36317;&#26041;&#38754;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#23545;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#12289;&#22320;&#21306;&#21644;&#23398;&#31185;&#30340;&#23398;&#29983;&#25104;&#32489;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30123;&#24773;&#26399;&#38388;&#33521;&#35821;&#21644;&#25968;&#23398;&#30340;&#29087;&#32451;&#31243;&#24230;&#26174;&#33879;&#19979;&#38477;&#65292;&#20998;&#25968;&#20998;&#24067;&#25910;&#32553;&#24182;&#23545;&#32463;&#27982;&#22256;&#22659;&#21644;&#40657;&#20154;&#23398;&#29983;&#20135;&#29983;&#26356;&#22823;&#24433;&#21709;&#12290;&#30452;&#25509;&#35777;&#26126;&#30340;&#30334;&#20998;&#27604;&#65288;Directly Certified Percentage&#65289;&#20195;&#34920;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#25104;&#20026;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#65292;&#27492;&#22806;&#36824;&#20174;&#25945;&#24072;&#36164;&#28304;&#65288;&#22914;&#25945;&#24072;&#24037;&#36164;&#65289;&#21644;&#25945;&#23398;&#24320;&#25903;&#20013;&#33719;&#24471;&#20102;&#20854;&#20182;&#27934;&#35265;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#22478;&#24066;&#21644;&#20892;&#26449;&#29615;&#22659;&#20043;&#38388;&#30340;&#25104;&#23601;&#24046;&#24322;&#65292;&#20197;&#21450;&#19981;&#21516;&#21439;&#20043;&#38388;&#30340;&#21464;&#21270;&#65292;&#20984;&#26174;&#20102;&#22320;&#29702;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has significantly exacerbated existing educational disparities in Georgia's K-12 system, particularly in terms of racial and ethnic achievement gaps. Utilizing machine learning methods, the study conducts a comprehensive analysis of student achievement rates across different demographics, regions, and subjects. The findings highlight a significant decline in proficiency in English and Math during the pandemic, with a noticeable contraction in score distribution and a greater impact on economically disadvantaged and Black students. Socio-economic status, as represented by the Directly Certified Percentage -- the percentage of students eligible for free lunch, emerges as the most crucial factor, with additional insights drawn from faculty resources such as teacher salaries and expenditure on instruction. The study also identifies disparities in achievement rates between urban and rural settings, as well as variations across counties, underscoring the influence of ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#22810;&#31181;&#35821;&#22659;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#25551;&#36848;&#21487;&#33021;&#19990;&#30028;&#65292;&#24182;&#21033;&#29992;&#32534;&#35793;&#22120;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#22659;&#19979;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25104;&#26412;&#36739;&#20302;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#30740;&#31350;LLM&#23545;&#40784;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01706</link><description>&lt;p&gt;
MULTIVERSE: &#22312;&#19981;&#21516;&#19990;&#30028;&#20013;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#22810;&#31181;&#35821;&#22659;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#25551;&#36848;&#21487;&#33021;&#19990;&#30028;&#65292;&#24182;&#21033;&#29992;&#32534;&#35793;&#22120;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#22659;&#19979;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25104;&#26412;&#36739;&#20302;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#30740;&#31350;LLM&#23545;&#40784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#26088;&#22312;&#30830;&#20445;LLM&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#30456;&#21305;&#37197;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#19968;&#31995;&#21015;&#36234;&#29425;&#25216;&#26415;&#23637;&#31034;&#20102;&#23545;&#40784;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#23545;&#35805;&#20013;&#35825;&#20351;LLMs&#20135;&#29983;&#24694;&#24847;&#20869;&#23481;&#12290;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#26234;&#33021;&#25110;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#25214;&#21040;&#30456;&#24212;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;LLMs&#22312;&#19981;&#21516;&#35821;&#22659;&#19979;&#23545;&#40784;&#27700;&#24179;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31995;&#32479;&#22320;&#26500;&#24314;&#35768;&#22810;&#34987;&#31216;&#20026;&#19990;&#30028;&#30340;&#35821;&#22659;&#12289;&#21033;&#29992;&#25551;&#36848;&#21487;&#33021;&#19990;&#30028;&#65288;&#22914;&#26102;&#38388;&#12289;&#22320;&#28857;&#12289;&#35282;&#33394;&#12289;&#34892;&#20026;&#21644;&#35821;&#35328;&#65289;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#21644;&#30456;&#24212;&#30340;&#32534;&#35793;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#36739;&#20302;&#25104;&#26412;&#25581;&#31034;&#28508;&#22312;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#37492;&#20110;&#25105;&#20204;&#26041;&#27861;&#30340;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#19981;&#21516;&#19990;&#30028;&#20013;LLM&#23545;&#40784;&#38382;&#39064;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#36234;&#29425;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.01705</link><description>&lt;p&gt;
&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#34920;&#24449;&#20260;&#23475;&#65306;&#24230;&#37327;&#21644;&#20943;&#36731;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20260;&#23475;&#36890;&#24120;&#34987;&#20998;&#20026;&#37197;&#32622;&#24615;&#25110;&#34920;&#24449;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#21518;&#32773;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#24403;&#21069;&#34920;&#24449;&#24615;&#20260;&#23475;&#23450;&#20041;&#30340;&#23457;&#26597;&#65292;&#20197;&#30830;&#23450;&#20854;&#20013;&#21253;&#21547;&#20160;&#20040;&#21644;&#19981;&#21253;&#21547;&#20160;&#20040;&#12290;&#36825;&#20010;&#20998;&#26512;&#20419;&#20351;&#25105;&#20204;&#25193;&#23637;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#21253;&#25324;&#23545;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#30340;&#20260;&#23475;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24230;&#37327;&#30340;&#39640;&#32423;&#35201;&#27714;&#65306;&#30830;&#23450;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#35828;&#26126;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#34920;&#24449;&#24615;&#20260;&#23475;&#26102;&#30340;&#29420;&#29305;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#20260;&#23475;&#26410;&#34987;&#24230;&#37327;&#21644;&#20943;&#36731;&#26102;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20943;&#36731;&#25514;&#26045;&#24182;&#30028;&#23450;&#20309;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#32467;&#26463;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26694;&#26550;&#65292;&#25193;&#22823;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#20844;&#24179;&#30740;&#31350;&#30340;&#35265;&#35299;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.01703</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#24220;&#23448;&#21592;&#19982;&#24066;&#27665;&#20043;&#38388;&#30340;&#20114;&#21160;&#24433;&#21709;&#20844;&#20849;&#31119;&#31049;&#21644;&#27665;&#20027;&#31038;&#20250;&#30340;&#27491;&#24403;&#24615;&#12290;&#35686;&#23519;&#26159;&#22269;&#23478;&#26368;&#26174;&#32780;&#26131;&#35265;&#12289;&#26368;&#25509;&#35302;&#24066;&#27665;&#30340;&#20195;&#29702;&#20154;&#65292;&#22312;&#20132;&#36890;&#31449;&#20572;&#26399;&#38388;&#65292;&#20182;&#20204;&#27599;&#24180;&#19982;&#20844;&#20247;&#20114;&#21160;&#36229;&#36807;2000&#19975;&#27425;&#12290;&#22914;&#20170;&#65292;&#36825;&#20123;&#20114;&#21160;&#32463;&#24120;&#34987;&#25140;&#22312;&#36523;&#19978;&#30340;&#25668;&#20687;&#26426;&#35760;&#24405;&#19979;&#26469;&#65292;&#36825;&#34987;&#35270;&#20026;&#25552;&#39640;&#35686;&#23519;&#38382;&#36131;&#21046;&#21644;&#25913;&#21892;&#35686;&#27665;&#20114;&#21160;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#20998;&#26512;&#36825;&#20123;&#22797;&#26434;&#32780;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#65292;&#36825;&#20123;&#35760;&#24405;&#30340;&#21450;&#26102;&#20998;&#26512;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35282;&#24230;&#12289;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#26469;&#33258;&#36825;&#20123;&#36523;&#19978;&#25668;&#20687;&#26426;&#35760;&#24405;&#30340;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#30830;&#23450;&#19982;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#26368;&#30456;&#20851;&#30340;&#27807;&#36890;&#26041;&#38754;&#65292;&#21253;&#25324;&#20849;&#21516;&#24863;&#30693;&#20114;&#21160;&#30340;&#26631;&#24535;&#26631;&#35760;&#20197;&#21450;&#20855;&#26377;&#36825;&#20123;&#26631;&#35760;&#30340;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
&lt;/p&gt;</description></item><item><title>HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;</title><link>https://arxiv.org/abs/2402.01696</link><description>&lt;p&gt;
HiGen: &#23618;&#27425;&#24863;&#30693;&#30340;&#23618;&#32423;&#25991;&#26412;&#20998;&#31867;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01696
&lt;/p&gt;
&lt;p&gt;
HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#65288;HTC&#65289;&#26159;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#23376;&#20219;&#21153;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#23618;&#32423;&#26631;&#31614;&#20998;&#31867;&#27861;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#21644;&#23618;&#32423;&#26631;&#31614;&#20449;&#24687;&#26469;&#23398;&#20064;&#38745;&#24577;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#22240;&#23618;&#32423;&#27700;&#24179;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#65292;&#38656;&#35201;&#21160;&#24577;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiGen&#65292;&#19968;&#20010;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#25991;&#26412;&#21644;&#26631;&#31614;&#21517;&#31216;&#35821;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#39046;&#22495;&#30693;&#35782;&#19978;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26679;&#26412;&#26377;&#38480;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21629;&#21517;&#20026;ENZYME&#30340;&#26032;&#39062;&#21644;&#26377;&#20215;&#20540;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#26469;&#33258;PubMed&#30340;&#25991;&#31456;&#32452;&#25104;&#65292;&#26088;&#22312;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>ARGS&#26159;&#19968;&#20010;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#35843;&#25972;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#19988;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#20855;&#26377;&#25345;&#32493;&#30340;&#22870;&#21169;&#22686;&#30410;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01694</link><description>&lt;p&gt;
ARGS: &#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
ARGS: Alignment as Reward-Guided Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01694
&lt;/p&gt;
&lt;p&gt;
ARGS&#26159;&#19968;&#20010;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#35843;&#25972;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#19988;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#20855;&#26377;&#25345;&#32493;&#30340;&#22870;&#21169;&#22686;&#30410;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30446;&#26631;&#23545;&#40784;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#28982;&#32780;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;RLHF&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#19981;&#31283;&#23450;&#21644;&#36164;&#28304;&#23494;&#38598;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;ARGS&#65292;&#21363;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#65292;&#23427;&#23558;&#23545;&#40784;&#34701;&#20837;&#21040;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;RL&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#20449;&#21495;&#35843;&#25972;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;ARGS&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#20026;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#19988;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#30340;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#30456;&#23545;&#20110;&#22522;&#32447;&#26174;&#31034;&#20986;&#25345;&#32493;&#30340;&#22870;&#21169;&#25913;&#36827;&#12290;&#20363;&#22914;&#65292;&#37319;&#29992;&#30456;&#21516;&#30340;&#36138;&#23146;&#35299;&#30721;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25552;&#39640;&#20102;19.56%&#30340;&#24179;&#22343;&#22870;&#21169;&#65292;&#24182;&#22312;GPT-4&#35780;&#20272;&#20013;&#33719;&#24471;&#20102;64.33%&#30340;&#20559;&#22909;&#25110;&#24182;&#21015;&#20998;&#25968;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#20102;&#35299;&#30721;&#30340;&#21019;&#26032;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#23569;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#23454;&#29616;&#35821;&#35328;&#33258;&#36866;&#24212;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22312;&#32454;&#35843;&#26399;&#38388;&#26367;&#25442;&#20266;&#26631;&#31614;&#22122;&#22768;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#24039;&#65292;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20165;&#26377;&#24456;&#23569;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26694;&#26550;&#20063;&#33021;&#21512;&#25104;&#21487;&#29702;&#35299;&#30340;&#26410;&#30693;&#35821;&#35328;&#35821;&#38899;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#25216;&#26415;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#39640;&#25928;&#35821;&#35328;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01692</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#28151;&#21512;&#21644;&#23884;&#20837;&#21021;&#22987;&#21270;&#23454;&#29616;&#36328;&#35821;&#35328;TTS&#33258;&#36866;&#24212;&#30340;&#26368;&#22823;&#25968;&#25454;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#23569;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#23454;&#29616;&#35821;&#35328;&#33258;&#36866;&#24212;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22312;&#32454;&#35843;&#26399;&#38388;&#26367;&#25442;&#20266;&#26631;&#31614;&#22122;&#22768;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#24039;&#65292;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20165;&#26377;&#24456;&#23569;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26694;&#26550;&#20063;&#33021;&#21512;&#25104;&#21487;&#29702;&#35299;&#30340;&#26410;&#30693;&#35821;&#35328;&#35821;&#38899;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#25216;&#26415;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#39640;&#25928;&#35821;&#35328;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#20013;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#23454;&#29616;&#35821;&#35328;&#33258;&#36866;&#24212;&#12290;&#34429;&#28982;&#35768;&#22810;&#24037;&#20316;&#20391;&#37325;&#20110;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#32771;&#34385;&#23613;&#37327;&#20943;&#23569;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#21033;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#65292;&#26367;&#25442;&#32454;&#35843;&#26399;&#38388;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#24039;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21033;&#29992;&#20102;&#26356;&#22810;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#20165;4&#20010;&#26631;&#35760;&#25968;&#25454;&#21644;15&#20998;&#38047;&#26410;&#26631;&#35760;&#25968;&#25454;&#21512;&#25104;&#21487;&#29702;&#35299;&#30340;&#26410;&#30693;&#35821;&#35328;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#26356;&#22810;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#36229;&#36807;&#20102;&#20256;&#32479;&#25216;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#39640;&#25928;&#35821;&#35328;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32769;&#24180;&#20154;&#20013;&#21306;&#20998;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#21644;&#27491;&#24120;&#35748;&#30693;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21477;&#23376;&#23884;&#20837;&#21644;&#21477;&#23376;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#35775;&#35848;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#25552;&#21462;&#26102;&#24207;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24314;&#31435;&#31283;&#20581;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.01690</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#25439;&#22833;&#30340;&#35821;&#35328;&#23398;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Linguistic-Based Mild Cognitive Impairment Detection Using Informative Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32769;&#24180;&#20154;&#20013;&#21306;&#20998;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#21644;&#27491;&#24120;&#35748;&#30693;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21477;&#23376;&#23884;&#20837;&#21644;&#21477;&#23376;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#35775;&#35848;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#25552;&#21462;&#26102;&#24207;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24314;&#31435;&#31283;&#20581;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#32769;&#24180;&#20154;&#20013;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#21644;&#27491;&#24120;&#35748;&#30693;&#65288;NC&#65289;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20998;&#26512;&#20102;&#22312;I-CONECT&#30740;&#31350;&#39033;&#30446;&#20013;&#25910;&#38598;&#30340;&#35270;&#39057;&#35775;&#35848;&#20013;&#29983;&#25104;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#35813;&#39033;&#30446;&#26159;&#19968;&#39033;&#26088;&#22312;&#36890;&#36807;&#35270;&#39057;&#32842;&#22825;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;NLP&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#21363;&#21477;&#23376;&#23884;&#20837;&#65288;SE&#65289;&#21644;&#21477;&#23376;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;SCA&#65289;&#12290;&#39318;&#20808;&#65292;SE&#27169;&#22359;&#25429;&#25417;&#27599;&#20010;&#21477;&#23376;&#20013;&#21333;&#35789;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;SCA&#27169;&#22359;&#25552;&#21462;&#21477;&#23376;&#24207;&#21015;&#30340;&#26102;&#24207;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#29305;&#24449;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#29992;&#20110;&#23558;&#34987;&#35797;&#20998;&#20026;MCI&#25110;NC&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#20449;&#24687;&#25439;&#22833;&#65288;InfoLoss&#65289;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#35266;&#23519;&#27599;&#20010;&#21477;&#23376;&#24207;&#21015;&#26469;&#32771;&#34385;&#29109;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning method using Natural Language Processing (NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and Normal Cognitive (NC) conditions in older adults. We propose a framework that analyzes transcripts generated from video interviews collected within the I-CONECT study project, a randomized controlled trial aimed at improving cognitive functions through video chats. Our proposed NLP framework consists of two Transformer-based modules, namely Sentence Embedding (SE) and Sentence Cross Attention (SCA). First, the SE module captures contextual relationships between words within each sentence. Subsequently, the SCA module extracts temporal features from a sequence of sentences. This feature is then used by a Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC. To build a robust model, we propose a novel loss function, called InfoLoss, that considers the reduction in entropy by observing each sequence of sentences
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#35265;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25351;&#23548;&#23398;&#29983;&#36873;&#25321;&#36866;&#21512;&#20182;&#20204;&#30340;LLM.</title><link>https://arxiv.org/abs/2402.01687</link><description>&lt;p&gt;
&#8220;&#25105;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;LLM&#65311;&#8221;&#65306;&#35780;&#20272;&#29992;&#20110;&#21360;&#24230;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#20219;&#21153;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
"Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students in India
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#35265;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25351;&#23548;&#23398;&#29983;&#36873;&#25321;&#36866;&#21512;&#20182;&#20204;&#30340;LLM.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#35265;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#25945;&#32946;&#30028;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#32570;&#20047;&#23545;&#19981;&#21516;LLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#21644;&#35780;&#20272;&#21738;&#20010;LLMs&#23545;&#19981;&#21516;&#20219;&#21153;&#26368;&#26377;&#25928;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19968;&#20123;&#20844;&#24320;&#21487;&#29992;&#30340;LLMs&#65292;&#20363;&#22914;Google Bard&#65292;ChatGPT&#65292;GitHub Copilot Chat&#21644;Microsoft Copilot&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#36935;&#21040;&#30340;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#65292;&#35299;&#37322;&#65292;&#39033;&#30446;&#26500;&#24605;&#65292;&#20869;&#23481;&#29983;&#25104;&#65292;&#35838;&#31243;&#20316;&#19994;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#39640;&#24180;&#32423;&#21644;&#20302;&#24180;&#32423;&#23398;&#29983;&#36827;&#34892;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25351;&#23548;&#23398;&#29983;&#36873;&#25321;&#36866;&#21512;&#20182;&#20204;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the effectiveness of various large language models (LLMs) in performing tasks common among undergraduate computer science students. Although a number of research studies in the computing education community have explored the possibility of using LLMs for a variety of tasks, there is a lack of comprehensive research comparing different LLMs and evaluating which LLMs are most effective for different tasks. Our research systematically assesses some of the publicly available LLMs such as Google Bard, ChatGPT, GitHub Copilot Chat, and Microsoft Copilot across diverse tasks commonly encountered by undergraduate computer science students. These tasks include code generation, explanation, project ideation, content generation, class assignments, and email composition. Evaluation for these tasks was carried out by junior and senior students in computer science, and provides insights into the models' strengths and limitations. This study aims to guide students in selecting su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01684</link><description>&lt;p&gt;
&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#28436;&#36827;&#65292;&#20154;&#20204;&#20026;&#20102;&#26377;&#25928;&#22320;&#24494;&#35843;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;LLMs&#20197;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#26377;&#20004;&#31181;&#20027;&#35201;&#30340;&#36866;&#24212;&#26041;&#24335;&#65306;&#65288;i&#65289;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65306;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#30456;&#24212;&#35757;&#32451;&#26679;&#26412;&#23545;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#29420;&#31435;&#30340;&#24494;&#35843;&#65307;&#65288;ii&#65289;&#38598;&#25104;&#27169;&#22411;&#65306;&#20351;&#29992;&#25152;&#26377;&#20219;&#21153;&#30340;&#26679;&#26412;&#26469;&#32852;&#21512;&#24494;&#35843;&#39044;&#35757;&#32451;LLMs &#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#21046;&#38376;&#25511;&#65288;CGC&#65289;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;&#20102;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;MTL&#65288;&#21363;CGC&#65289;&#21644;PEFT&#65288;&#21363;LoRA&#65289;&#26041;&#26696;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#20219;&#21153;&#38598;&#32676;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#23618;&#65292;&#20854;&#20013;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#25972;&#21512;&#30693;&#35782;&#32467;&#26500;&#21644;&#30693;&#35782;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32773;&#36712;&#36857;&#21457;&#29616;&#28508;&#22312;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20869;&#23481;&#25512;&#33616;&#21644;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01672</link><description>&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Prerequisite Structure Discovery in Intelligent Tutoring Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#25972;&#21512;&#30693;&#35782;&#32467;&#26500;&#21644;&#30693;&#35782;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32773;&#36712;&#36857;&#21457;&#29616;&#28508;&#22312;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20869;&#23481;&#25512;&#33616;&#21644;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#65292;&#30693;&#35782;&#32467;&#26500;&#65288;KS&#65289;&#21644;&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#23545;&#25552;&#39640;&#25945;&#32946;&#20869;&#23481;&#25512;&#33616;&#30340;&#37325;&#35201;&#24615;&#12290;KS&#34920;&#31034;&#19981;&#21516;&#30693;&#35782;&#32452;&#20214;&#65288;KCs&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;KT&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#36807;&#21435;&#21382;&#21490;&#26469;&#39044;&#27979;&#20854;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;KS&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#21512;&#24182;&#21040;KT&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#23398;&#20064;&#32773;&#36712;&#36857;&#20013;&#21457;&#29616;&#28508;&#22312;&#30340;KS&#12290;&#36890;&#36807;&#20351;&#29992;&#21457;&#29616;&#30340;KS&#36827;&#34892;&#20869;&#23481;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#27169;&#25311;&#23398;&#29983;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#65292;&#35780;&#20272;&#25581;&#31034;&#30340;KS&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the importance of Knowledge Structure (KS) and Knowledge Tracing (KT) in improving the recommendation of educational content in intelligent tutoring systems. The KS represents the relations between different Knowledge Components (KCs), while KT predicts a learner's success based on her past history. The contribution of this research includes proposing a KT model that incorporates the KS as a learnable parameter, enabling the discovery of the underlying KS from learner trajectories. The quality of the uncovered KS is assessed by using it to recommend content and evaluating the recommendation algorithm with simulated students.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01669</link><description>&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#30340;&#25913;&#36827;&#65306;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#20064;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#26657;&#20013;&#65292;&#22823;&#35268;&#27169;&#30340;&#35838;&#22530;&#35268;&#27169;&#32473;&#20010;&#24615;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#25945;&#32946;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#36827;&#23637;&#20551;&#35774;&#65288;LPH&#65289;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;ZPDES&#31639;&#27861;&#23545;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65288;LP&#65289;&#30340;&#32451;&#20064;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#31639;&#27861;&#22312;&#20043;&#21069;&#30340;&#23454;&#22320;&#30740;&#31350;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#23558;&#23398;&#20064;&#34920;&#29616;&#25552;&#21319;&#21040;&#26356;&#24191;&#27867;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#65292;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#35838;&#31243;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20854;&#21160;&#26426;&#24433;&#21709;&#23578;&#26410;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;ZPDES&#19981;&#20801;&#35768;&#23398;&#29983;&#21457;&#34920;&#36873;&#25321;&#24847;&#35265;&#12290;&#36825;&#31181;&#32570;&#20047;&#26426;&#26500;&#30340;&#38480;&#21046;&#19982;&#20851;&#27880;&#24314;&#27169;&#22909;&#22855;&#39537;&#21160;&#23398;&#20064;&#30340;LPH&#29702;&#35770;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30740;&#31350;&#20102;&#36825;&#31181;&#36873;&#25321;&#21487;&#33021;&#24615;&#30340;&#24341;&#20837;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#12290;&#32473;&#23450;&#30340;&#36873;&#25321;&#19982;&#32451;&#20064;&#38590;&#24230;&#27491;&#20132;&#30340;&#32500;&#24230;&#26377;&#20851;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large class sizes pose challenges to personalized learning in schools, which educational technologies, especially intelligent tutoring systems (ITS), aim to address. In this context, the ZPDES algorithm, based on the Learning Progress Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences exercises that maximize learning progress (LP). This algorithm was previously shown in field studies to boost learning performances for a wider diversity of students compared to a hand-designed curriculum. However, its motivational impact was not assessed. Also, ZPDES did not allow students to express choices. This limitation in agency is at odds with the LPH theory concerned with modeling curiosity-driven learning. We here study how the introduction of such choice possibilities impact both learning efficiency and motivation. The given choice concerns dimensions that are orthogonal to exercise difficulty, acting as a playful feature.   In an extensive field study (265 7-8 years
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;6G&#26080;&#32447;&#32593;&#32476;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01665</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;6G&#26080;&#32447;&#32593;&#32476;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven Deep Learning Paradigms for Wireless Network Optimization in 6G
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;6G&#26080;&#32447;&#32593;&#32476;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#32593;&#32476;&#20013;&#65292;&#38656;&#35201;&#36890;&#36807;&#22810;&#32500;&#24322;&#26500;&#36164;&#28304;&#28385;&#36275;&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#29992;&#25143;&#21644;&#21160;&#24577;&#32593;&#32476;&#29615;&#22659;&#30340;&#22810;&#26679;&#21270;&#26381;&#21153;&#12290;&#30001;&#27492;&#24341;&#21457;&#30340;&#22823;&#35268;&#27169;&#22797;&#26434;&#30340;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#36229;&#20986;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#29702;&#35770;&#26041;&#27861;&#30340;&#33021;&#21147;&#33539;&#22260;&#65292;&#22240;&#20026;&#35745;&#31639;&#22797;&#26434;&#24230;&#26497;&#39640;&#19988;&#22788;&#29702;&#26102;&#38388;&#24456;&#38271;&#12290;&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20855;&#26377;&#24555;&#36895;&#22312;&#32447;&#25512;&#29702;&#21644;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#20294;&#23427;&#20005;&#37325;&#20381;&#36182;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#19988;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;DL&#65292;&#26088;&#22312;&#23558;&#24050;&#35777;&#26126;&#30340;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#24314;&#20013;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#30693;&#35782;&#39537;&#21160;&#30340;DL&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#30693;&#35782;&#39537;&#21160;&#30340;DL&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#26694;&#26550;&#65292;
&lt;/p&gt;
&lt;p&gt;
In the sixth-generation (6G) networks, newly emerging diversified services of massive users in dynamic network environments are required to be satisfied by multi-dimensional heterogeneous resources. The resulting large-scale complicated network optimization problems are beyond the capability of model-based theoretical methods due to the overwhelming computational complexity and the long processing time. Although with fast online inference and universal approximation ability, data-driven deep learning (DL) heavily relies on abundant training data and lacks interpretability. To address these issues, a new paradigm called knowledge-driven DL has emerged, aiming to integrate proven domain knowledge into the construction of neural networks, thereby exploiting the strengths of both methods. This article provides a systematic review of knowledge-driven DL in wireless networks. Specifically, a holistic framework of knowledge-driven DL in wireless networks is proposed, where knowledge sources, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.01663</link><description>&lt;p&gt;
&#26432;&#25163;&#32423;&#24212;&#29992;&#65306;&#20302;&#36895;&#22823;&#35268;&#27169;AI&#27494;&#22120;
&lt;/p&gt;
&lt;p&gt;
Killer Apps: Low-Speed, Large-Scale AI Weapons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#30001;OpenAI&#12289;Meta&#21644;Anthropic&#31561;&#32452;&#32455;&#24320;&#21457;&#30340;&#23574;&#31471;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#32473;&#25112;&#20105;&#21644;&#23433;&#20840;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#30446;&#21069;&#20851;&#27880;&#30340;&#20027;&#35201;&#26159;AI&#22312;&#27494;&#22120;&#31995;&#32479;&#20013;&#30340;&#25972;&#21512;&#20197;&#21450;&#22312;&#21160;&#33021;&#20914;&#31361;&#20013;&#24555;&#36895;&#20915;&#31574;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21516;&#26679;&#37325;&#35201;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#22312;&#20449;&#24687;&#39046;&#22495;&#20013;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#20869;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#36896;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#25968;&#25454;&#21465;&#20107;&#23545;&#32452;&#32455;&#32489;&#25928;&#30340;&#27491;&#38754;&#24433;&#21709;&#65292;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#25152;&#20256;&#36798;&#30340;&#20915;&#31574;&#36136;&#37327;&#12290;&#36825;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25968;&#25454;&#21465;&#20107;&#30340;&#21069;&#22240;&#21644;&#21518;&#26524;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.01658</link><description>&lt;p&gt;
&#30740;&#31350;&#25968;&#25454;&#21465;&#20107;&#23545;&#20202;&#34920;&#30424;&#29992;&#25143;&#25968;&#25454;&#29702;&#35299;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Untersuchung der Wirkung von Data Storytelling auf das Datenverstaendnis von Dashboard-Nutzern
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#25968;&#25454;&#21465;&#20107;&#23545;&#32452;&#32455;&#32489;&#25928;&#30340;&#27491;&#38754;&#24433;&#21709;&#65292;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#25152;&#20256;&#36798;&#30340;&#20915;&#31574;&#36136;&#37327;&#12290;&#36825;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25968;&#25454;&#21465;&#20107;&#30340;&#21069;&#22240;&#21644;&#21518;&#26524;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#21830;&#19994;&#20998;&#26512;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#25968;&#25454;&#21465;&#20107;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#25163;&#27573;&#65292;&#29992;&#20110;&#21521;&#35266;&#20247;&#20256;&#36798;&#20998;&#26512;&#27934;&#23519;&#65292;&#20197;&#25903;&#25345;&#20915;&#31574;&#21644;&#25552;&#39640;&#19994;&#21153;&#32489;&#25928;&#65292;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#25968;&#25454;&#21465;&#20107;&#23545;&#25968;&#25454;&#29702;&#35299;&#30340;&#24433;&#21709;&#65292;&#30446;&#21069;&#32570;&#20047;&#23454;&#35777;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#25968;&#25454;&#21465;&#20107;&#20316;&#20026;&#19968;&#31181;&#26500;&#36896;&#22312;&#29992;&#25143;&#25968;&#25454;&#29702;&#35299;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25968;&#25454;&#21465;&#20107;&#33021;&#21147;&#19982;&#32452;&#32455;&#32489;&#25928;&#21576;&#27491;&#30456;&#20851;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#25152;&#20256;&#36798;&#30340;&#20915;&#31574;&#36136;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#25506;&#35752;&#25968;&#25454;&#21465;&#20107;&#30340;&#28508;&#22312;&#21069;&#22240;&#19982;&#21518;&#26524;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing use of big data and business analytics, data storytelling has gained popularity as an effective means of communicating analytical insights to audiences to support decision making and improve business performance. However, there is little empirical evidence on the impact of data storytelling on data understanding. This study validates the concept of data storytelling as a construct in terms of its impact on users' data understanding. Based on empirical data analysis, the results of this study show that data storytelling competence is positively associated with organizational performance, which is partly due to the quality of the decision is conveyed. These results provide a theoretical basis for further investigation of potential antecedents and consequences of data storytelling.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#39044;&#27979;&#22312;&#32447;&#35838;&#31243;&#23398;&#29983;&#34920;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01655</link><description>&lt;p&gt;
&#22312;&#20840;&#29699;&#35270;&#35282;&#19979;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#35838;&#31243;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach Towards Student Performance Prediction in Online Courses: Challenges Based on a Global Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#39044;&#27979;&#22312;&#32447;&#35838;&#31243;&#23398;&#29983;&#34920;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#35780;&#20272;&#23398;&#29983;&#22312;&#20219;&#20309;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#36827;&#23637;&#26159;&#32791;&#26102;&#19988;&#21387;&#21147;&#24040;&#22823;&#30340;&#12290;&#30001;&#20110;&#25945;&#32946;&#30028;&#23545;&#20110;&#25972;&#21512;&#20114;&#32852;&#32593;&#25216;&#26415;&#20197;&#21450;&#21521;&#30005;&#23376;&#23398;&#20064;&#12289;&#28151;&#21512;&#23398;&#20064;&#25110;&#22312;&#32447;&#23398;&#20064;&#27169;&#24335;&#30340;&#36716;&#21464;&#30340;&#20851;&#27880;&#65292;&#23398;&#29983;&#20154;&#25968;&#30340;&#22686;&#21152;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#31181;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#25104;&#20026;&#20102;&#36817;&#24180;&#26469;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;CNN&#21644;RNN-LSTM&#65289;&#26469;&#39044;&#27979;&#22312;&#32447;&#35838;&#31243;&#20132;&#20184;&#30340;&#20013;&#26399;&#38454;&#27573;&#23398;&#29983;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#19990;&#30028;&#19978;&#19977;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#20248;&#21270;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing and evaluating students' progress in any learning environment is stressful and time consuming if done using traditional analysis methods. This is further exasperated by the increasing number of students due to the shift of focus toward integrating the Internet technologies in education and the focus of academic institutions on moving toward e-Learning, blended, or online learning models. As a result, the topic of student performance prediction has become a vibrant research area in recent years. To address this, machine learning and data mining techniques have emerged as a viable solution. To that end, this work proposes the use of deep learning techniques (CNN and RNN-LSTM) to predict the students' performance at the midpoint stage of the online course delivery using three distinct datasets collected from three different regions of the world. Experimental results show that deep learning models have promising performance as they outperform other optimized traditional ML models
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#20998;&#26512;&#22312;&#31649;&#29702;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#33647;&#29289;&#27835;&#30103;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#21576;&#29616;&#20102;&#29992;&#25143;&#20013;&#24515;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#30340;&#30740;&#31350;&#38382;&#39064;&#21644;&#19979;&#19968;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.01652</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#30340;AI&#20998;&#26512;&#22312;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
User-Centric AI Analytics for Chronic Health Conditions Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#20998;&#26512;&#22312;&#31649;&#29702;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#33647;&#29289;&#27835;&#30103;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#21576;&#29616;&#20102;&#29992;&#25143;&#20013;&#24515;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#30340;&#30740;&#31350;&#38382;&#39064;&#21644;&#19979;&#19968;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#65292;AI&#20998;&#26512;&#30340;&#24212;&#29992;&#36817;&#24180;&#26469;&#36805;&#36895;&#22686;&#38271;&#12290;&#22312;&#26412;&#19987;&#39064;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AI&#20998;&#26512;&#22312;&#31649;&#29702;&#31958;&#23615;&#30149;&#12289;&#32933;&#32982;&#31561;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#22312;&#26080;&#33647;&#29289;&#27835;&#30103;&#26041;&#27861;&#20013;&#31649;&#29702;&#36825;&#20123;&#29366;&#20917;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#20010;&#20307;&#29366;&#20917;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#24046;&#24322;&#23548;&#33268;&#30740;&#31350;&#36827;&#20837;&#20102;&#29992;&#25143;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36817;&#26399;&#21644;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#30340;&#31034;&#20363;&#65292;&#24182;&#32467;&#35770;&#25105;&#20204;&#35748;&#20026;&#30340;&#19979;&#19968;&#27493;&#21644;&#19968;&#20123;&#23578;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of AI analytics in health informatics has seen a rapid growth in recent years. In this talk, we look at AI analytics use in managing chronic health conditions such as diabetes, obesity, etc. We focus on the challenges in managing these conditions especially with drug-free approaches due to the variations in individual circumstances. These variations directed the research into user-centric approach leading to variety of research questions. In this short paper, we give examples from recent and current research work and conclude with what, in our opinion, to be the next steps and some remaining open research questions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;LSTM&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#39044;&#27979;&#20102;&#36873;&#23450;&#30340;OECD&#25104;&#21592;&#22269;&#21644;&#20234;&#26391;&#22312;2021&#24180;&#21040;2025&#24180;&#30340;&#36827;&#21475;&#24773;&#20917;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;99%&#12290;</title><link>https://arxiv.org/abs/2402.01648</link><description>&lt;p&gt;
&#21033;&#29992;LSTM&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#39044;&#27979;OECD&#25104;&#21592;&#22269;&#21644;&#20234;&#26391;&#30340;&#36827;&#21475;
&lt;/p&gt;
&lt;p&gt;
Forecasting Imports in OECD Member Countries and Iran by Using Neural Network Algorithms of LSTM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;LSTM&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#39044;&#27979;&#20102;&#36873;&#23450;&#30340;OECD&#25104;&#21592;&#22269;&#21644;&#20234;&#26391;&#22312;2021&#24180;&#21040;2025&#24180;&#30340;&#36827;&#21475;&#24773;&#20917;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;99%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#39640;&#20215;&#20540;&#65292;&#24182;&#34987;&#29992;&#20316;&#19968;&#31181;&#36866;&#21512;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;ANN&#39044;&#27979;&#36873;&#23450;&#30340;OECD&#25104;&#21592;&#22269;&#21644;&#20234;&#26391;&#22312;2021&#24180;&#21040;2025&#24180;&#30340;20&#20010;&#23395;&#24230;&#30340;&#36827;&#21475;&#24773;&#20917;&#12290;&#20174;1970&#24180;&#33267;2019&#24180;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#38134;&#34892;&#12289;&#19990;&#30028;&#36152;&#26131;&#32452;&#32455;&#12289;&#22269;&#38469;&#36135;&#24065;&#22522;&#37329;&#32452;&#32455;&#31561;&#26377;&#25928;&#36164;&#28304;&#25910;&#38598;&#20102;&#36825;&#20123;&#22269;&#23478;&#36827;&#21475;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#24182;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#23395;&#24230;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#25910;&#38598;&#25968;&#25454;&#37327;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;LSTM&#22312;PyCharm&#20013;&#20998;&#26512;&#25968;&#25454;&#12290;&#20854;&#20013;75%&#30340;&#25968;&#25454;&#34987;&#35270;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;25%&#34987;&#35270;&#20026;&#27979;&#35797;&#25968;&#25454;&#65292;&#20998;&#26512;&#32467;&#26524;&#20197;99%&#30340;&#20934;&#30830;&#29575;&#36827;&#34892;&#39044;&#27979;&#65292;&#26174;&#31034;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#30001;&#20110;&#36827;&#21475;&#26159;&#19968;&#20010;&#28040;&#36153;&#20989;&#25968;&#65292;&#32780;&#28040;&#36153;&#21463;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks (ANN) which are a branch of artificial intelligence, have shown their high value in lots of applications and are used as a suitable forecasting method. Therefore, this study aims at forecasting imports in OECD member selected countries and Iran for 20 seasons from 2021 to 2025 by means of ANN. Data related to the imports of such countries collected over 50 years from 1970 to 2019 from valid resources including World Bank, WTO, IFM,the data turned into seasonal data to increase the number of collected data for better performance and high accuracy of the network by using Diz formula that there were totally 200 data related to imports. This study has used LSTM to analyse data in Pycharm. 75% of data considered as training data and 25% considered as test data and the results of the analysis were forecasted with 99% accuracy which revealed the validity and reliability of the output. Since the imports is consumption function and since the consumption is influenced 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#23398;&#21644;&#39640;&#20013;&#23398;&#29983;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#20249;&#20276;&#65292;&#24182;&#25552;&#20379;&#23454;&#36341;&#32463;&#39564;&#21644;&#20837;&#38376;&#30693;&#35782;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#26426;&#26800;&#24037;&#31243;&#31561;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22359;&#30528;&#37325;&#24378;&#35843;&#20197;&#20154;&#20026;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.01647</link><description>&lt;p&gt;
&#25171;&#36896;&#24744;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#26379;&#21451;&#65306;&#38754;&#21521;&#26222;&#21450;&#19988;&#24341;&#20154;&#20837;&#32988;&#30340;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#23398;&#21644;&#39640;&#20013;&#23398;&#29983;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#20249;&#20276;&#65292;&#24182;&#25552;&#20379;&#23454;&#36341;&#32463;&#39564;&#21644;&#20837;&#38376;&#30693;&#35782;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#26426;&#26800;&#24037;&#31243;&#31561;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22359;&#30528;&#37325;&#24378;&#35843;&#20197;&#20154;&#20026;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#21644;&#20840;&#29699;&#32463;&#27982;&#20013;&#30340;&#26085;&#30410;&#37325;&#35201;&#20316;&#29992;&#65292;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#21644;&#32032;&#20859;&#24050;&#25104;&#20026;&#22823;&#23398;&#21644;K-12&#25945;&#32946;&#20013;&#24517;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#22521;&#20859;&#23398;&#29983;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20250;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#23578;&#26410;&#36275;&#22815;&#26222;&#21450;&#21644;&#24341;&#20154;&#20837;&#32988;&#65292;&#26080;&#27861;&#28385;&#36275;&#26469;&#33258;&#19981;&#21516;&#25945;&#32946;&#30446;&#26631;&#21644;&#19981;&#21516;&#31038;&#20250;&#32463;&#27982;&#32972;&#26223;&#30340;&#23398;&#29983;&#21644;&#23398;&#26657;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#23398;&#21644;&#39640;&#20013;&#23398;&#29983;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#20249;&#20276;&#12290;&#36825;&#20010;&#24320;&#25918;&#24179;&#21488;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21508;&#20010;&#26041;&#38754;&#30340;&#23454;&#36341;&#32463;&#39564;&#21644;&#20837;&#38376;&#30693;&#35782;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#26426;&#26800;&#24037;&#31243;&#31561;&#12290;&#30001;&#20110;&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#20249;&#20276;&#30340;&#31038;&#20132;&#21644;&#20010;&#20154;&#24615;&#36136;&#65292;&#35813;&#27169;&#22359;&#36824;&#29305;&#21035;&#24378;&#35843;&#20197;&#20154;&#20026;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) is playing an increasingly important role in our society and global economy, AI education and literacy have become necessary components in college and K-12 education to prepare students for an AI-powered society. However, current AI curricula have not yet been made accessible and engaging enough for students and schools from all socio-economic backgrounds with different educational goals. In this work, we developed an open-source learning module for college and high school students, which allows students to build their own robot companion from the ground up. This open platform can be used to provide hands-on experience and introductory knowledge about various aspects of AI, including robotics, machine learning (ML), software engineering, and mechanical engineering. Because of the social and personal nature of a socially assistive robot companion, this module also puts a special emphasis on human-centered AI, enabling students to develop a better understa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.01643</link><description>&lt;p&gt;
L-TUNING&#65306;&#29992;&#20110;LLMs&#20013;&#30340;&#25552;&#31034;&#21644;&#21069;&#32512;&#30340;&#21516;&#27493;&#26631;&#31614;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20219;&#24847;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#24182;&#19988;&#36890;&#29992;&#26631;&#35760;&#22312;&#21508;&#31181;&#31867;&#21035;&#26631;&#31614;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;L-Tuning&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#35774;&#35745;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;L-Tuning&#19987;&#27880;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;LLM&#22788;&#29702;&#30340;&#26631;&#31614;&#26631;&#35760;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#21033;&#29992;&#20854;&#39044;&#20808;&#23384;&#22312;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36824;&#20419;&#36827;&#20102;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#19981;&#21516;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;L-Tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#23545;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25991;&#26412;&#22823;&#37327;&#23384;&#22312;&#20110;&#20844;&#20849;&#39046;&#22495;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#26469;&#24212;&#23545;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01642</link><description>&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Detection of Machine-Generated Text: Literature Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#23545;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25991;&#26412;&#22823;&#37327;&#23384;&#22312;&#20110;&#20844;&#20849;&#39046;&#22495;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#26469;&#24212;&#23545;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#20135;&#29983;&#34394;&#20551;&#25991;&#26412;&#65292;&#20844;&#20849;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#27492;&#31867;&#20869;&#23481;&#12290;&#22312;&#19981;&#26029;&#25552;&#21319;&#30340;&#22797;&#26434;&#24230;&#21644;&#20889;&#20316;&#39118;&#26684;&#19979;&#65292;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#20154;&#31867;&#25776;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#19982;&#20154;&#24037;&#20316;&#32773;&#30456;&#27604;&#65292;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20316;&#21697;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#23186;&#20307;&#20851;&#27880;&#24182;&#24341;&#36215;&#20102;&#20105;&#35758;&#12290;&#23545;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#30340;&#24433;&#21709;&#30340;&#25285;&#24551;&#20063;&#24212;&#36816;&#32780;&#29983;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#36807;&#31243;&#26377;&#26356;&#20805;&#20998;&#30340;&#20102;&#35299;&#12290;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65306;&#20854;&#33539;&#22260;&#19981;&#20165;&#28183;&#36879;&#21040;&#26032;&#38395;&#25253;&#36947;&#21644;&#23458;&#25143;&#26381;&#21153;&#65292;&#36824;&#28041;&#21450;&#21040;&#23398;&#26415;&#30028;&#12290;&#20026;&#20102;&#20943;&#36731;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24433;&#21709;&#65292;&#24517;&#39035;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#65292;&#20363;&#22914;&#20026;&#20154;&#31867;&#25805;&#20316;&#21592;&#25552;&#20379;&#21306;&#20998;&#34394;&#20551;&#25991;&#26412;&#21644;&#30495;&#23454;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00515</link><description>&lt;p&gt;
&#24320;&#21457;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#29992;&#20316;&#21453;&#24212;&#24615;&#26234;&#33021;&#20307;&#20197;&#22312;&#39640;&#24230;&#21160;&#33633;&#30340;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#24555;&#36895;&#23398;&#20064;&#24182;&#21709;&#24212;&#26032;&#30340;&#25237;&#36164;&#31574;&#30053;&#65292;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#37329;&#34701;&#34892;&#19994;&#20043;&#38388;&#23384;&#22312;&#38750;&#24120;&#22797;&#26434;&#30340;&#20851;&#32852;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#36235;&#21183;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#22522;&#20110;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#26368;&#22823;&#21270;&#26032;&#21046;&#23450;&#30340;&#25237;&#36164;&#32452;&#21512;&#30340;&#24635;&#22238;&#25253;&#65292;&#32780;&#24573;&#35270;&#20854;&#22312;&#20840;&#29699;&#25110;&#21306;&#22495;&#37096;&#38376;&#30340;&#21508;&#31181;&#24066;&#22330;&#26465;&#20214;&#21160;&#33633;&#19979;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASA&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21644;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#37319;&#29992;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20180;&#32454;&#21160;&#24577;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and p
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>EEG-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#21644;&#35299;&#35835;EEG&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22810;&#23610;&#24230;&#30005;&#29983;&#29702;&#29702;&#35299;&#21644;&#20998;&#31867;&#65292;&#19988;&#22312;few-shot&#23398;&#20064;&#33539;&#24335;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2401.18006</link><description>&lt;p&gt;
EEG-GPT: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;EEG&#20998;&#31867;&#21644;&#35299;&#35835;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18006
&lt;/p&gt;
&lt;p&gt;
EEG-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#21644;&#35299;&#35835;EEG&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22810;&#23610;&#24230;&#30005;&#29983;&#29702;&#29702;&#35299;&#21644;&#20998;&#31867;&#65292;&#19988;&#22312;few-shot&#23398;&#20064;&#33539;&#24335;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#20013;&#65292;&#24448;&#24448;&#26159;&#26377;&#38480;&#30340;&#32858;&#28966;&#65292;&#20165;&#20165;&#23396;&#31435;&#22320;&#20851;&#27880;&#36328;&#36234;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#65288;&#20174;&#27627;&#31186;&#30340;&#30636;&#26102;&#23574;&#23792;&#21040;&#25345;&#32493;&#20960;&#20998;&#38047;&#30340;&#30315;&#30187;&#21457;&#20316;&#65289;&#21644;&#31354;&#38388;&#23610;&#24230;&#65288;&#20174;&#23616;&#37096;&#39640;&#39057;&#25391;&#33633;&#21040;&#20840;&#23616;&#30561;&#30496;&#27963;&#21160;&#65289;&#30340;&#29305;&#23450;&#33041;&#27963;&#21160;&#12290;&#36825;&#31181;&#23396;&#31435;&#30340;&#26041;&#27861;&#38480;&#21046;&#20102;&#21457;&#23637;&#20986;&#20855;&#26377;&#22810;&#23610;&#24230;&#30005;&#29983;&#29702;&#29702;&#35299;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;EEG ML&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20856;&#22411;&#30340;ML EEG&#26041;&#27861;&#37319;&#29992;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EEG-GPT&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;EEG&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#12290;EEG-GPT&#22312;&#20165;&#21033;&#29992;2&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;few-shot&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#20248;&#24322;&#24615;&#33021;&#65292;&#33021;&#22815;&#23545;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;EEG&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
In conventional machine learning (ML) approaches applied to electroencephalography (EEG), this is often a limited focus, isolating specific brain activities occurring across disparate temporal scales (from transient spikes in milliseconds to seizures lasting minutes) and spatial scales (from localized high-frequency oscillations to global sleep activity). This siloed approach limits the development EEG ML models that exhibit multi-scale electrophysiological understanding and classification capabilities. Moreover, typical ML EEG approaches utilize black-box approaches, limiting their interpretability and trustworthiness in clinical contexts. Thus, we propose EEG-GPT, a unifying approach to EEG classification that leverages advances in large language models (LLM). EEG-GPT achieves excellent performance comparable to current state-of-the-art deep learning methods in classifying normal from abnormal EEG in a few-shot learning paradigm utilizing only 2% of training data. Furthermore, it off
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17870</link><description>&lt;p&gt;
&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#36827;&#34892;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27425;&#23395;&#33410;&#39044;&#25253;&#23545;&#20892;&#19994;&#12289;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#39044;&#35686;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22823;&#27668;&#30340;&#28151;&#27788;&#24615;&#65292;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#36890;&#36807;&#23454;&#29616;&#19982;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#38761;&#26032;&#20102;&#22825;&#27668;&#39044;&#25253;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#36825;&#23548;&#33268;&#30456;&#24403;&#22810;&#30340;&#30899;&#25490;&#25918;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#36890;&#36807;&#20135;&#29983;&#24179;&#28369;&#30340;&#32467;&#26524;&#26469;&#24858;&#24324;&#20687;&#32032;&#35823;&#24046;&#35780;&#20998;&#65292;&#36825;&#20123;&#32467;&#26524;&#32570;&#20047;&#29289;&#29702;&#19968;&#33268;&#24615;&#21644;&#27668;&#35937;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Pangu&#27169;&#22411;&#26469;&#33719;&#24471;&#33391;&#22909;&#30340;&#21021;&#22987;&#26435;&#37325;&#65292;&#24182;&#38598;&#25104;&#20102;&#19968;&#20010;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#22312;&#24310;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#35843;&#25972;Pangu&#27169;&#22411;&#30340;1.1%&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
&lt;/p&gt;</description></item><item><title>IGCN&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17612</link><description>&lt;p&gt;
IGCN&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
IGCN: Integrative Graph Convolutional Networks for Multi-modal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17612
&lt;/p&gt;
&lt;p&gt;
IGCN&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#29992;&#20110;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#21644;&#36793;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#29992;&#20110;&#32593;&#32476;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32508;&#21512;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#23545;&#20110;&#28041;&#21450;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#26576;&#20123;&#25968;&#25454;&#27169;&#24577;&#22312;&#39044;&#27979;&#19968;&#20010;&#31867;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#21487;&#33021;&#22312;&#39044;&#27979;&#19981;&#21516;&#31867;&#21035;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#32508;&#21512;&#24037;&#20855;&#32570;&#20047;&#23545;&#20854;&#29305;&#23450;&#39044;&#27979;&#32972;&#21518;&#21407;&#29702;&#30340;&#20840;&#38754;&#21644;&#36830;&#36143;&#29702;&#35299;&#65292;&#20351;&#20854;&#26080;&#27861;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#32593;&#32476;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21517;&#20026;&#32508;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;IGCN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Graph Neural Networks (GNN) have led to a considerable growth in graph data modeling for multi-modal data which contains various types of nodes and edges. Although some integrative prediction solutions have been developed recently for network-structured data, these methods have some restrictions. For a node classification task involving multi-modal data, certain data modalities may perform better when predicting one class, while others might excel in predicting a different class. Thus, to obtain a better learning representation, advanced computational methodologies are required for the integrative analysis of multi-modal data. Moreover, existing integrative tools lack a comprehensive and cohesive understanding of the rationale behind their specific predictions, making them unsuitable for enhancing model interpretability. Addressing these restrictions, we introduce a novel integrative neural network approach for multi-modal data networks, named Integrative Graph Convo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26465;&#20214;&#30456;&#20851;&#24615;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;HDformer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;Transformer&#21464;&#31181;&#65292;&#21033;&#29992;&#33976;&#39311;&#25216;&#26415;&#21644;&#24555;&#36895;&#32593;&#32476;&#36830;&#25509;&#23618;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.11929</link><description>&lt;p&gt;
&#8220;&#36234;&#22823;&#36234;&#22909;&#65311;&#8221;&#37325;&#26032;&#24605;&#32771;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#27169;&#22411;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26465;&#20214;&#30456;&#20851;&#24615;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;HDformer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;Transformer&#21464;&#31181;&#65292;&#21033;&#29992;&#33976;&#39311;&#25216;&#26415;&#21644;&#24555;&#36895;&#32593;&#32476;&#36830;&#25509;&#23618;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#21069;&#27839;&#65292;&#20854;&#29305;&#28857;&#26159;&#20851;&#27880;&#20110;&#22823;&#37327;&#36755;&#20837;&#24207;&#21015;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#26377;&#38480;&#38271;&#24230;&#30456;&#27604;&#26377;&#25152;&#19981;&#21516;&#12290;&#23613;&#31649;&#26356;&#38271;&#30340;&#24207;&#21015;&#26412;&#36136;&#19978;&#20256;&#36798;&#20102;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#20294;&#30446;&#21069;&#30340;&#25216;&#26415;&#24448;&#24448;&#36890;&#36807;&#25552;&#39640;&#27169;&#22411;&#22797;&#26434;&#24615;&#26469;&#24212;&#23545;&#12290;&#36825;&#20123;&#22797;&#26434;&#30340;&#27169;&#22411;&#21487;&#20197;&#33192;&#32960;&#20026;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#32534;&#30721;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#31561;&#21442;&#25968;&#23494;&#38598;&#22411;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#23548;&#33268;&#20102;&#31105;&#27490;&#24615;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35821;&#20041;&#31616;&#21333;&#24615;&#12290;&#20986;&#20110;&#36861;&#27714;&#31616;&#27905;&#24615;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#26465;&#20214;&#30456;&#20851;&#24615;&#21644;&#33258;&#30456;&#20851;&#24615;&#20316;&#20026;&#35843;&#26597;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#20887;&#20313;&#12290;&#20511;&#21161;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HDformer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;Transformer&#21464;&#20307;&#65292;&#32463;&#36807;&#22686;&#24378;&#65292;&#20351;&#29992;&#33976;&#39311;&#25216;&#26415;&#21644;&#24555;&#36895;&#32593;&#32476;&#36830;&#25509;&#23618;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BanglaNet&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#30340;&#26041;&#27861;&#23545;&#23391;&#21152;&#25289;&#25163;&#20889;&#23383;&#31526;&#36827;&#34892;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#26368;&#26032;&#30340;CNN&#30740;&#31350;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2401.08035</link><description>&lt;p&gt;
BanglaNet: &#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#30340;&#26041;&#27861;&#23545;&#23391;&#21152;&#25289;&#25163;&#20889;&#23383;&#31526;&#36827;&#34892;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
BanglaNet: Bangla Handwritten Character Recognition using Ensembling of Convolutional Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BanglaNet&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#30340;&#26041;&#27861;&#23545;&#23391;&#21152;&#25289;&#25163;&#20889;&#23383;&#31526;&#36827;&#34892;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#26368;&#26032;&#30340;CNN&#30740;&#31350;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#23391;&#21152;&#25289;&#25163;&#20889;&#23383;&#31526;&#30340;&#35782;&#21035;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23391;&#21152;&#25289;&#23383;&#31526;&#20855;&#26377;&#36830;&#20889;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#31181;&#20070;&#20889;&#26041;&#24335;&#30340;&#22797;&#21512;&#23383;&#31526;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#38598;&#21512;&#30340;&#20998;&#31867;&#27169;&#22411;&#8212;&#8212;BanglaNet&#65292;&#29992;&#20110;&#23545;&#23391;&#21152;&#25289;&#22522;&#26412;&#23383;&#31526;&#12289;&#22797;&#21512;&#23383;&#31526;&#12289;&#25968;&#23383;&#31526;&#21495;&#21644;&#20462;&#39280;&#31526;&#36827;&#34892;&#20998;&#31867;&#12290;&#22522;&#20110;Inception&#12289;ResNet&#21644;DenseNet&#31561;&#26368;&#26032;CNN&#27169;&#22411;&#30340;&#19977;&#31181;&#19981;&#21516;&#27169;&#22411;&#26681;&#25454;&#22686;&#24378;&#21644;&#38750;&#22686;&#24378;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#25110;&#38598;&#25104;&#65292;&#24471;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#38024;&#23545;&#19977;&#20010;&#22522;&#20934;&#23391;&#21152;&#25289;&#25163;&#20889;&#23383;&#31526;&#25968;&#25454;&#38598;&#65288;CMATERdb&#12289;BanglaLekha-Isolated&#21644;Ekush&#65289;&#65292;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#19982;&#19968;&#20123;&#26368;&#26032;&#30340;&#22522;&#20110;CNN&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handwritten character recognition is a crucial task because of its abundant applications. The recognition task of Bangla handwritten characters is especially challenging because of the cursive nature of Bangla characters and the presence of compound characters with more than one way of writing. In this paper, a classification model based on the ensembling of several Convolutional Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic characters, compound characters, numerals, and modifiers. Three different models based on the idea of state-of-the-art CNN models like Inception, ResNet, and DenseNet have been trained with both augmented and non-augmented inputs. Finally, all these models are averaged or ensembled to get the finishing model. Rigorous experimentation on three benchmark Bangla handwritten characters datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited significant recognition accuracies compared to some recent CNN-based research. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#31649;&#29702;&#20302;&#31354;&#39046;&#22495;&#25480;&#26435;&#32780;&#26500;&#24314;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#24037;&#31243;&#26041;&#27861;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39134;&#34892;&#29305;&#24449;&#21644;&#29615;&#22659;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#20294;&#36824;&#24212;&#32771;&#34385;&#39134;&#34892;&#21592;&#21644;&#26080;&#20154;&#26426;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21463;&#35775;&#32773;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#34920;&#31034;&#21453;&#23545;&#12290;</title><link>https://arxiv.org/abs/2401.07353</link><description>&lt;p&gt;
&#20026;&#31649;&#29702;&#20302;&#31354;&#39046;&#22495;&#25480;&#26435;&#32780;&#26500;&#24314;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24037;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#31649;&#29702;&#20302;&#31354;&#39046;&#22495;&#25480;&#26435;&#32780;&#26500;&#24314;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#24037;&#31243;&#26041;&#27861;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39134;&#34892;&#29305;&#24449;&#21644;&#29615;&#22659;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#20294;&#36824;&#24212;&#32771;&#34385;&#39134;&#34892;&#21592;&#21644;&#26080;&#20154;&#26426;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21463;&#35775;&#32773;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#34920;&#31034;&#21453;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#65288;sUAS&#65289;&#24050;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#24341;&#20837;&#20102;&#20849;&#20139;&#39046;&#22495;&#20869;&#30340;&#25805;&#20316;&#22797;&#26434;&#24615;&#21644;&#25253;&#21578;&#30340;&#20107;&#20214;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#23433;&#20840;&#25285;&#24551;&#12290;&#20026;&#27492;&#65292;&#32654;&#22269;&#32852;&#37030;&#33322;&#31354;&#31649;&#29702;&#23616;&#65288;FAA&#65289;&#27491;&#22312;&#24320;&#21457;&#26080;&#20154;&#26426;&#20132;&#36890;&#31649;&#29702;&#65288;UTM&#65289;&#31995;&#32479;&#65292;&#20197;&#22522;&#20110;sUAS&#39044;&#27979;&#33021;&#22815;&#23433;&#20840;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#26469;&#25511;&#21046;&#23545;&#31354;&#22495;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#24555;&#36895;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#65292;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#24517;&#39035;&#32771;&#34385;&#22810;&#26679;&#21270;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#23433;&#20840;&#24615;&#12289;&#36879;&#26126;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#24212;&#32771;&#34385;&#22312;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#22240;&#32032;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35266;&#28857;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39134;&#34892;&#29305;&#24449;&#21644;&#29615;&#22659;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#26368;&#37325;&#35201;&#30340;&#65292;&#20294;&#39134;&#34892;&#21592;&#21644;&#26080;&#20154;&#26426;&#30340;&#33021;&#21147;&#20063;&#24212;&#35813;&#34987;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#20960;&#20010;&#21463;&#35775;&#32773;&#34920;&#31034;&#20182;&#20204;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#30340;&#19981;&#28385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-LSTM&#28151;&#21512;&#27169;&#22411;&#30340;&#29289;&#32852;&#32593;&#24694;&#24847;&#36719;&#20214;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24050;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#21644;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#20934;&#30830;&#29575;&#21644;&#31361;&#20986;&#24615;&#33021;&#65292;&#20026;&#22686;&#24378;&#29289;&#32852;&#32593;&#23433;&#20840;&#24615;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.17683</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Malware Detection in IOT Systems Using Machine Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-LSTM&#28151;&#21512;&#27169;&#22411;&#30340;&#29289;&#32852;&#32593;&#24694;&#24847;&#36719;&#20214;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24050;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#21644;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#20934;&#30830;&#29575;&#21644;&#31361;&#20986;&#24615;&#33021;&#65292;&#20026;&#22686;&#24378;&#29289;&#32852;&#32593;&#23433;&#20840;&#24615;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#38656;&#35201;&#24378;&#22823;&#30340;&#26041;&#27861;&#23398;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;CNN-LSTM&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#29289;&#32852;&#32593;&#24694;&#24847;&#36719;&#20214;&#35782;&#21035;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#24050;&#26377;&#26041;&#27861;&#19978;&#30340;&#34920;&#29616;&#12290;&#20511;&#21161;K&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;95.5%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;CNN&#31639;&#27861;&#20419;&#36827;&#20102;&#26356;&#20248;&#31168;&#30340;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#65292;&#32780;LSTM&#20998;&#31867;&#22120;&#22312;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#27969;&#34892;&#25216;&#26415;&#36827;&#34892;&#30340;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#20854;&#22686;&#24378;&#29289;&#32852;&#32593;&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#25506;&#32034;SVM&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20998;&#24067;&#24335;&#26816;&#27979;&#31574;&#30053;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#39044;&#27979;&#20998;&#26512;&#23545;&#20110;&#26356;&#24378;&#22823;&#30340;&#29289;&#32852;&#32593;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#30740;&#31350;&#20026;&#22312;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#24320;&#21457;&#26356;&#20855;&#38887;&#24615;&#30340;&#23433;&#20840;&#25514;&#26045;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware detection in IoT environments necessitates robust methodologies. This study introduces a CNN-LSTM hybrid model for IoT malware identification and evaluates its performance against established methods. Leveraging K-fold cross-validation, the proposed approach achieved 95.5% accuracy, surpassing existing methods. The CNN algorithm enabled superior learning model construction, and the LSTM classifier exhibited heightened accuracy in classification. Comparative analysis against prevalent techniques demonstrated the efficacy of the proposed model, highlighting its potential for enhancing IoT security. The study advocates for future exploration of SVMs as alternatives, emphasizes the need for distributed detection strategies, and underscores the importance of predictive analyses for a more powerful IOT security. This research serves as a platform for developing more resilient security measures in IoT ecosystems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#36229;&#20302;&#21151;&#32791;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.17612</link><description>&lt;p&gt;
&#38754;&#21521;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#23450;&#21046;&#36817;&#20284;&#20056;&#31215;&#32047;&#21152;&#21644;&#28608;&#27963;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Bespoke Approximation of Multiplication-Accumulation and Activation Targeting Printed Multilayer Perceptrons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#36229;&#20302;&#21151;&#32791;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#23454;&#29616;&#30495;&#27491;&#26080;&#22788;&#19981;&#22312;&#35745;&#31639;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#36229;&#20302;&#21151;&#32791;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#36817;&#20284;&#35745;&#31639;&#21644;&#23450;&#21046;&#21270;&#35774;&#35745;&#30340;&#21407;&#21017;&#26469;&#20811;&#26381;&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Printed Electronics (PE) feature distinct and remarkable characteristics that make them a prominent technology for achieving true ubiquitous computing. This is particularly relevant in application domains that require conformal and ultra-low cost solutions, which have experienced limited penetration of computing until now. Unlike silicon-based technologies, PE offer unparalleled features such as non-recurring engineering costs, ultra-low manufacturing cost, and on-demand fabrication of conformal, flexible, non-toxic, and stretchable hardware. However, PE face certain limitations due to their large feature sizes, that impede the realization of complex circuits, such as machine learning classifiers. In this work, we address these limitations by leveraging the principles of Approximate Computing and Bespoke (fully-customized) design. We propose an automated framework for designing ultra-low power Multilayer Perceptron (MLP) classifiers which employs, for the first time, a holistic approac
&lt;/p&gt;</description></item><item><title>AdaNAS&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23545;&#38598;&#21512;&#38477;&#38632;&#39044;&#25253;&#36827;&#34892;&#22788;&#29702;&#65292;&#33021;&#22815;&#25552;&#39640;&#38477;&#38632;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#37319;&#29992;&#38754;&#21521;&#38477;&#38632;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#38477;&#38632;&#23618;&#32423;&#35268;&#21017;&#21270;&#20989;&#25968;&#65292;&#26377;&#25928;&#28040;&#38500;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.16046</link><description>&lt;p&gt;
AdaNAS&#65306;&#33258;&#25105;&#30417;&#30563;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#38598;&#21512;&#38477;&#38632;&#39044;&#25253;&#30340;&#33258;&#36866;&#24212;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16046
&lt;/p&gt;
&lt;p&gt;
AdaNAS&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23545;&#38598;&#21512;&#38477;&#38632;&#39044;&#25253;&#36827;&#34892;&#22788;&#29702;&#65292;&#33021;&#22815;&#25552;&#39640;&#38477;&#38632;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#37319;&#29992;&#38754;&#21521;&#38477;&#38632;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#38477;&#38632;&#23618;&#32423;&#35268;&#21017;&#21270;&#20989;&#25968;&#65292;&#26377;&#25928;&#28040;&#38500;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#20851;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;(NWP)&#38477;&#38632;&#39044;&#25253;&#30340;&#21518;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32479;&#35745;&#26041;&#38754;&#30340;&#20869;&#23481;&#65292;&#36739;&#23569;&#28041;&#21450;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#38754;&#12290;&#34429;&#28982;&#19968;&#20123;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#26159;&#23450;&#21046;&#30340;&#32593;&#32476;&#65292;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#21644;&#39564;&#35777;&#65292;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#20154;&#21147;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37325;&#35201;&#25163;&#21160;&#24037;&#20316;&#30340;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#26041;&#27861;&#65292;&#31216;&#20026;AdaNAS&#65292;&#29992;&#20110;&#36827;&#34892;&#38477;&#38632;&#39044;&#25253;&#21518;&#22788;&#29702;&#21644;&#39640;&#20934;&#30830;&#24615;&#30340;&#38477;&#38632;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#38477;&#38632;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#39640;&#38477;&#38632;&#21306;&#22495;&#30340;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38477;&#38632;&#23618;&#32423;&#35268;&#21017;&#21270;&#20989;&#25968;&#65292;&#20197;&#28040;&#38500;&#35757;&#32451;&#36807;&#31243;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22312;&#22823;&#35268;&#27169;&#39044;&#27979;&#23454;&#39564;&#20013;&#65292;&#26681;&#25454;\emph{&#26080;&#38632;}&#65292;\emph{&#23567;&#38632;}&#65292;\emph{&#20013;&#38632;}&#65292;\emph{&#22823;&#38632;}&#21644;\emph{&#26292;&#38632;}&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated. Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor. Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy. In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas. Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training. Validation experiments have been performed under the cases of \emph{None}, \emph{Light}, \emph{Moderate}, \emph{Heavy} and \emph{Violent} on a large-scale pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16043</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#19981;&#24179;&#34913;&#32447;&#24615;&#20998;&#31867;&#30340;&#25193;&#23637;&#38750;&#23545;&#31216;sigmoid&#21644;&#24863;&#30693;&#26426;(SIGTRON)
&lt;/p&gt;
&lt;p&gt;
An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;&#65292;&#31216;&#20026;SIGTRON&#65292;&#23427;&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#38750;&#23545;&#31216;sigmoid&#20989;&#25968;&#21644;&#24863;&#30693;&#26426;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#23427;&#30340;&#20276;&#38543;&#20984;&#27169;&#22411;SIGTRON-&#19981;&#24179;&#34913;&#20998;&#31867;(SIC)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#34394;&#25311;SIGTRON&#20135;&#29983;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;$\pi$-&#21152;&#26435;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;SIC&#27169;&#22411;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#27809;&#26377;&#22806;&#37096;&#30340;$\pi$-&#26435;&#37325;&#65292;&#32780;&#26159;&#22312;&#34394;&#25311;&#30340;SIGTRON&#20135;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#26377;&#20869;&#37096;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#24403;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#27604;&#22914;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#27604;&#20363;&#19981;&#24179;&#34913;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#36866;&#24212;&#26159;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#30340;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25311;&#29275;&#39039;&#20248;&#21270;(L-BFGS)&#26694;&#26550;&#30340;&#34394;&#25311;&#20984;&#25439;&#22833;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#20108;&#20998;&#32447;&#24615;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function. In contrast to the conventional $\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function. As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. This adaptation is achieved by creating a skewed hyperplane equation. Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line sear
&lt;/p&gt;</description></item><item><title>BiSwift&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#22810;&#27969;&#35270;&#39057;&#20998;&#26512;&#30340;&#24102;&#23485;&#32534;&#25490;&#22120;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#32534;&#35299;&#30721;&#22120;&#21644;&#20840;&#23616;&#24102;&#23485;&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#24182;&#21457;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#20197;&#21450;&#22810;&#20010;&#27969;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15740</link><description>&lt;p&gt;
BiSwift: &#36793;&#32536;&#22810;&#27969;&#35270;&#39057;&#20998;&#26512;&#30340;&#24102;&#23485;&#32534;&#25490;&#22120;
&lt;/p&gt;
&lt;p&gt;
BiSwift: Bandwidth Orchestrator for Multi-Stream Video Analytics on Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15740
&lt;/p&gt;
&lt;p&gt;
BiSwift&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#22810;&#27969;&#35270;&#39057;&#20998;&#26512;&#30340;&#24102;&#23485;&#32534;&#25490;&#22120;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#32534;&#35299;&#30721;&#22120;&#21644;&#20840;&#23616;&#24102;&#23485;&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#24182;&#21457;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#20197;&#21450;&#22810;&#20010;&#27969;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#30417;&#25511;&#21644;&#36947;&#36335;&#20132;&#36890;&#30340;&#39640;&#28165;&#65288;HD&#65289;&#25668;&#20687;&#22836;&#32463;&#21382;&#20102;&#24040;&#22823;&#30340;&#22686;&#38271;&#65292;&#23454;&#26102;&#20998;&#26512;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;&#23558;&#24103;&#20174;&#21069;&#31471;&#35774;&#22791;&#21368;&#36733;&#21040;&#21518;&#31471;&#36793;&#32536;&#26381;&#21153;&#22120;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#22810;&#27969;&#31454;&#20105;&#29615;&#22659;&#19979;&#65292;&#39640;&#25928;&#30340;&#24102;&#23485;&#31649;&#29702;&#21644;&#36866;&#24403;&#30340;&#35843;&#24230;&#23545;&#20110;&#20445;&#35777;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiSwift&#65292;&#19968;&#31181;&#21452;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#32423;&#27969;&#27700;&#32447;&#30340;&#21019;&#26032;&#33258;&#36866;&#24212;&#28151;&#21512;&#32534;&#35299;&#30721;&#22120;&#21644;&#20840;&#23616;&#24102;&#23485;&#25511;&#21046;&#22120;&#26469;&#25193;&#23637;&#24182;&#21457;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#65292;&#24182;&#20026;&#22810;&#20010;&#35270;&#39057;&#27969;&#25552;&#20379;&#20248;&#21270;&#30340;&#35843;&#24230;&#12290;&#20302;&#32423;&#21035;&#30340;&#21069;&#21518;&#31471;&#21327;&#21516;&#26426;&#21046;&#65288;&#31216;&#20026;&#33258;&#36866;&#24212;&#28151;&#21512;&#32534;&#35299;&#30721;&#22120;&#65289;&#22312;&#26412;&#22320;&#20248;&#21270;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#21333;&#20010;&#27969;&#30340;&#31471;&#21040;&#31471;&#35270;&#39057;&#20998;&#26512;&#12290;&#19978;&#23618;&#35843;&#24230;&#22120;&#36890;&#36807;&#20840;&#23616;&#24102;&#23485;&#25511;&#21046;&#22120;&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#20010;&#27969;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-definition (HD) cameras for surveillance and road traffic have experienced tremendous growth, demanding intensive computation resources for real-time analytics. Recently, offloading frames from the front-end device to the back-end edge server has shown great promise. In multi-stream competitive environments, efficient bandwidth management and proper scheduling are crucial to ensure both high inference accuracy and high throughput. To achieve this goal, we propose BiSwift, a bi-level framework that scales the concurrent real-time video analytics by a novel adaptive hybrid codec integrated with multi-level pipelines, and a global bandwidth controller for multiple video streams. The lower-level front-back-end collaborative mechanism (called adaptive hybrid codec) locally optimizes the accuracy and accelerates end-to-end video analytics for a single stream. The upper-level scheduler aims to accuracy fairness among multiple streams via the global bandwidth controller. The evaluation of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2312.15574</link><description>&lt;p&gt;
&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Rates for Switchback Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Switchback&#23454;&#39564;&#35774;&#35745;&#20013;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#21333;&#20803;&#65288;&#20363;&#22914;&#25972;&#20010;&#31995;&#32479;&#65289;&#22312;&#20132;&#26367;&#30340;&#26102;&#38388;&#22359;&#20013;&#26292;&#38706;&#20110;&#19968;&#20010;&#38543;&#26426;&#22788;&#29702;&#65292;&#22788;&#29702;&#24182;&#34892;&#22788;&#29702;&#20102;&#36328;&#21333;&#20803;&#21644;&#26102;&#38388;&#24178;&#25200;&#38382;&#39064;&#12290;Hu&#21644;Wager&#65288;2022&#65289;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#22359;&#36215;&#22987;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;Markov&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#29992;&#20110;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;GATE&#65289;&#30340;$T^{-1/3}$&#36895;&#29575;&#65292;&#20182;&#20204;&#22768;&#31216;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#24314;&#35758;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#19981;&#21516;&#65288;&#19988;&#20381;&#36182;&#35774;&#35745;&#65289;&#30340;&#20272;&#35745;&#37327;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#36895;&#29575;&#12290;&#23545;&#20110;&#30456;&#21516;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20272;&#35745;&#22120;&#65292;&#20351;&#29992;&#25972;&#20010;&#22359;&#65292;&#24182;&#24778;&#20154;&#22320;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#23454;&#38469;&#19978;&#36798;&#21040;&#20102;&#21407;&#22987;&#30340;&#35774;&#35745;&#29420;&#31435;GATE&#20272;&#35745;&#37327;&#30340;$\sqrt{\log T/T}$&#30340;&#20272;&#35745;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Switchback experimental design, wherein a single unit (e.g., a whole system) is exposed to a single random treatment for interspersed blocks of time, tackles both cross-unit and temporal interference. Hu and Wager (2022) recently proposed a treatment-effect estimator that truncates the beginnings of blocks and established a $T^{-1/3}$ rate for estimating the global average treatment effect (GATE) in a Markov setting with rapid mixing. They claim this rate is optimal and suggest focusing instead on a different (and design-dependent) estimand so as to enjoy a faster rate. For the same design we propose an alternative estimator that uses the whole block and surprisingly show that it in fact achieves an estimation rate of $\sqrt{\log T/T}$ for the original design-independent GATE estimand under the same assumptions.
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13933</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structured Probabilistic Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13933
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#65292;&#29992;&#20110;&#20174;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20013;&#23398;&#20064;&#32039;&#20945;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;SPC&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#32534;&#30721;&#25216;&#26415;&#65292;&#20855;&#26377;&#26469;&#33258;&#30446;&#26631;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27010;&#29575;&#32534;&#30721;&#22312;&#19968;&#20010;&#27169;&#22359;&#20013;&#21516;&#26102;&#36827;&#34892;&#20449;&#24687;&#32534;&#30721;&#21644;&#20219;&#21153;&#39044;&#27979;&#65292;&#20197;&#26356;&#20805;&#20998;&#22320;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#31354;&#38388;&#30340;&#21464;&#20998;&#25512;&#26029;&#26469;&#20943;&#23569;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25511;&#21046;&#27010;&#29575;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#65292;&#20197;&#20419;&#36827;&#31867;&#21035;&#20043;&#38388;&#30340;&#22343;&#21248;&#24615;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#65292;SPC&#21487;&#20197;&#20445;&#25345;&#28508;&#22312;&#32534;&#30721;&#30340;&#39640;&#26031;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#36974;&#32617;&#21644;&#23545;&#27604;&#22788;&#29702;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#30001;&#20110;&#27169;&#24577;&#32570;&#22833;&#24341;&#36215;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2312.13508</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#36807;&#21407;&#22411;&#36974;&#32617;&#21644;&#23545;&#27604;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#36974;&#32617;&#21644;&#23545;&#27604;&#22788;&#29702;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#30001;&#20110;&#27169;&#24577;&#32570;&#22833;&#24341;&#36215;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#32463;&#24120;&#38754;&#20020;&#22797;&#26434;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#26500;&#24314;&#32852;&#37030;&#26694;&#26550;&#21644;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#25512;&#29702;&#20934;&#30830;&#24615;&#20135;&#29983;&#38480;&#21046;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#28041;&#21450;&#22312;&#23458;&#25143;&#31471;&#24320;&#21457;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#26469;&#35299;&#20915;&#32570;&#22833;&#27169;&#24577;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#21333;&#27169;&#24577;&#23458;&#25143;&#31471;&#25110;&#23436;&#20840;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#30340;&#29305;&#23450;&#22330;&#26223;&#65292;&#22312;&#22797;&#26434;&#30340;&#27169;&#24577;&#32570;&#22833;&#22330;&#26223;&#20013;&#24456;&#38590;&#26377;&#25928;&#27867;&#21270;&#12290;&#26412;&#25991;&#23558;&#21407;&#22411;&#24211;&#24341;&#20837;&#22522;&#20110;FedAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#26694;&#26550;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#38388;&#32531;&#35299;&#30001;&#27169;&#24577;&#32570;&#22833;&#24341;&#36215;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#21407;&#22411;&#20316;&#20026;&#34920;&#31034;&#32570;&#22833;&#27169;&#24577;&#30340;&#36974;&#32617;&#65292;&#20197;&#21046;&#23450;&#20219;&#21153;&#26657;&#20934;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, multimodal federated learning often faces the practical challenge of intricate modality missing, which poses constraints on building federated frameworks and significantly degrades model inference accuracy. Existing solutions for addressing missing modalities generally involve developing modality-specific encoders on clients and training modality fusion modules on servers. However, these methods are primarily constrained to specific scenarios with either unimodal clients or complete multimodal clients, struggling to generalize effectively in the intricate modality missing scenarios. In this paper, we introduce a prototype library into the FedAvg-based Federated Learning framework, thereby empowering the framework with the capability to alleviate the global model performance degradation resulting from modality missing during both training and testing. The proposed method utilizes prototypes as masks representing missing modalities to formulate a task-calibrated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#26862;&#26519;&#20013;&#20351;&#29992;&#36739;&#23567;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#26469;&#20195;&#34920;&#19968;&#20010;&#20915;&#31574;&#26641;&#38598;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#22810;&#25968;&#20989;&#25968;&#21644;&#20998;&#31867;&#38169;&#35823;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;k-&#36873;-n&#20989;&#25968;&#30340;&#30456;&#20851;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.11540</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#26862;&#26519;&#20013;&#33410;&#28857;&#25968;&#21644;&#26641;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-off between the Number of Nodes and the Number of Trees in a Random Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#26862;&#26519;&#20013;&#20351;&#29992;&#36739;&#23567;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#26469;&#20195;&#34920;&#19968;&#20010;&#20915;&#31574;&#26641;&#38598;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#22810;&#25968;&#20989;&#25968;&#21644;&#20998;&#31867;&#38169;&#35823;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;k-&#36873;-n&#20989;&#25968;&#30340;&#30456;&#20851;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#38454;&#27573;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#36739;&#23567;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#20195;&#34920;&#19968;&#20010;&#20915;&#31574;&#26641;&#38598;&#21512;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20165;&#32771;&#34385;&#20108;&#36827;&#21046;&#20915;&#31574;&#38382;&#39064;&#21644;&#31616;&#21333;&#20915;&#31574;&#26641;&#65292;&#20869;&#37096;&#33410;&#28857;&#21482;&#26597;&#35810;&#21333;&#20010;&#21464;&#37327;&#30340;&#24067;&#23572;&#20540;&#12290;&#20316;&#20026;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;n-T&#26159;&#24120;&#25968;&#65292;&#21017;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#21253;&#21547;T&#65288;&lt;n&#65289;&#20010;&#20915;&#31574;&#26641;&#30340;&#38598;&#21512;&#65288;&#27599;&#20010;&#20915;&#31574;&#26641;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#65289;&#26469;&#34920;&#31034;n&#20010;&#21464;&#37327;&#30340;&#22810;&#25968;&#20989;&#25968;&#65292;&#20854;&#20013;n&#21644;T&#24517;&#39035;&#26159;&#22855;&#25968;&#65288;&#20197;&#36991;&#20813;&#20915;&#32988;&#23616;&#65289;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22914;&#26524;n-T&#26159;&#24120;&#25968;&#24182;&#19988;&#20801;&#35768;&#23567;&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#21017;&#21487;&#20197;&#29992;&#19968;&#20010;&#21253;&#21547;T&#20010;&#20915;&#31574;&#26641;&#30340;&#38598;&#21512;&#65288;&#27599;&#20010;&#20915;&#31574;&#26641;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#65289;&#26469;&#34920;&#31034;&#19968;&#20010;&#21253;&#21547;n&#20010;&#20915;&#31574;&#26641;&#30340;&#38598;&#21512;&#12290;&#36824;&#20171;&#32461;&#20102;k-&#36873;-n&#20989;&#25968;&#30340;&#30456;&#20851;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the prediction phase of a random forest and study the problem of representing a bag of decision trees using a smaller bag of decision trees, where we only consider binary decision problems on the binary domain and simple decision trees in which an internal node is limited to querying the Boolean value of a single variable. As a main result, we show that the majority function of $n$ variables can be represented by a bag of $T$ ($&lt; n$) decision trees each with polynomial size if $n-T$ is a constant, where $n$ and $T$ must be odd (in order to avoid the tie break). We also show that a bag of $n$ decision trees can be represented by a bag of $T$ decision trees each with polynomial size if $n-T$ is a constant and a small classification error is allowed. A related result on the $k$-out-of-$n$ functions is presented too.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#23454;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21644;&#22266;&#23450;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2312.06837</link><description>&lt;p&gt;
&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Spectral State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#23454;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21644;&#22266;&#23450;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#20219;&#21153;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#24418;&#24335;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26032;&#39062;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#12290;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24615;&#33021;&#26082;&#19981;&#20381;&#36182;&#20110;&#24213;&#23618;&#21160;&#21147;&#23398;&#30340;&#39057;&#35889;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#38382;&#39064;&#30340;&#32500;&#24230;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#36890;&#36807;&#22266;&#23450;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#26500;&#24314;&#30340;&#65292;&#19981;&#38656;&#35201;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#20173;&#28982;&#20248;&#20110;SSMs&#12290;&#22522;&#20110;&#20809;&#35889;&#36807;&#28388;&#31639;&#27861;&#30340;Spectral state space models&#22312;&#21512;&#25104;&#21160;&#24577;&#31995;&#32479;&#21644;&#21508;&#31181;&#27169;&#24577;&#30340;&#38271;&#31243;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20123;&#35780;&#20272;&#25903;&#25345;&#20102;&#20809;&#35889;&#28388;&#27874;&#22312;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model.   Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.   The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#39640;&#25928;&#24182;&#34892;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#26694;&#26550;&#22312;&#21516;&#27493;&#21644;&#36755;&#20837;/&#36755;&#20986;&#26041;&#38754;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.04704</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#39640;&#25928;&#24182;&#34892;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Efficient Parallel Reinforcement Learning Framework using the Reactor Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#39640;&#25928;&#24182;&#34892;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#26694;&#26550;&#22312;&#21516;&#27493;&#21644;&#36755;&#20837;/&#36755;&#20986;&#26041;&#38754;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning, RL)&#26694;&#26550;&#23545;&#20110;&#23558;RL&#24037;&#20316;&#36127;&#36733;&#26144;&#23556;&#21040;&#22810;&#20010;&#35745;&#31639;&#36164;&#28304;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#21152;&#24555;&#26679;&#26412;&#29983;&#25104;&#12289;&#20540;&#20272;&#35745;&#21644;&#31574;&#30053;&#25913;&#36827;&#30340;&#36895;&#24230;&#12290;&#36825;&#20123;&#35745;&#31639;&#27169;&#24335;&#35201;&#27714;&#26080;&#32541;&#38598;&#25104;&#35757;&#32451;&#12289;&#26381;&#21153;&#21644;&#27169;&#25311;&#24037;&#20316;&#36127;&#36733;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#65292;&#22914;Ray&#65292;&#22312;RL&#20219;&#21153;&#20013;&#23545;&#21333;&#20010;&#33410;&#28857;&#19978;&#30340;&#35282;&#33394;&#20043;&#38388;&#30340;&#36755;&#20837;/&#36755;&#20986;&#21644;&#21516;&#27493;&#35201;&#27714;&#19981;&#22815;&#39640;&#25928;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#27169;&#22411;&#24378;&#21046;&#19968;&#32452;&#35282;&#33394;&#20855;&#26377;&#22266;&#23450;&#30340;&#36890;&#20449;&#27169;&#24335;&#12290;&#36825;&#20351;&#24471;&#35843;&#24230;&#31243;&#24207;&#21487;&#20197;&#28040;&#38500;&#38656;&#35201;&#21516;&#27493;&#30340;&#24037;&#20316;&#65292;&#20363;&#22914;&#27599;&#20010;&#35282;&#33394;&#30340;&#38145;&#30340;&#33719;&#21462;&#21644;&#37322;&#25918;&#65292;&#25110;&#21457;&#36865;&#21644;&#22788;&#29702;&#21327;&#35843;&#30456;&#20851;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;Lingua Franca (LF) &#26159;&#19968;&#31181;&#22522;&#20110;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#21327;&#35843;&#35821;&#35328;&#65292;&#36824;&#22312;Python&#20013;&#25903;&#25345;&#30495;&#27491;&#30340;&#24182;&#34892;&#22788;&#29702;&#65292;&#24182;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallel Reinforcement Learning (RL) frameworks are essential for mapping RL workloads to multiple computational resources, allowing for faster generation of samples, estimation of values, and policy improvement. These computational paradigms require a seamless integration of training, serving, and simulation workloads. Existing frameworks, such as Ray, are not managing this orchestration efficiently, especially in RL tasks that demand intensive input/output and synchronization between actors on a single node. In this study, we have proposed a solution implementing the reactor model, which enforces a set of actors to have a fixed communication pattern. This allows the scheduler to eliminate work needed for synchronization, such as acquiring and releasing locks for each actor or sending and processing coordination-related messages. Our framework, Lingua Franca (LF), a coordination language based on the reactor model, also supports true parallelism in Python and provides a unified interf
&lt;/p&gt;</description></item><item><title>GraphMETRO&#26159;&#19968;&#31181;&#36890;&#36807;&#28151;&#21512;&#23545;&#40784;&#19987;&#23478;&#26469;&#20943;&#36731;&#22797;&#26434;&#22270;&#20998;&#24067;&#21464;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#38376;&#25511;&#27169;&#22411;&#21644;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#23545;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2312.04693</link><description>&lt;p&gt;
GraphMETRO&#65306;&#36890;&#36807;&#28151;&#21512;&#23545;&#40784;&#19987;&#23478;&#26469;&#20943;&#36731;&#22797;&#26434;&#30340;&#22270;&#20998;&#24067;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04693
&lt;/p&gt;
&lt;p&gt;
GraphMETRO&#26159;&#19968;&#31181;&#36890;&#36807;&#28151;&#21512;&#23545;&#40784;&#19987;&#23478;&#26469;&#20943;&#36731;&#22797;&#26434;&#22270;&#20998;&#24067;&#21464;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#38376;&#25511;&#27169;&#22411;&#21644;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#23545;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#22266;&#26377;&#22797;&#26434;&#19988;&#24322;&#26500;&#65292;&#23548;&#33268;&#33258;&#28982;&#22810;&#26679;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#24456;&#39640;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#26222;&#36941;&#36866;&#29992;&#20110;&#22797;&#26434;&#38750;&#21512;&#25104;&#20998;&#24067;&#21464;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GraphMETRO&#65292;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#21487;&#38752;&#22320;&#24314;&#27169;&#33258;&#28982;&#22810;&#26679;&#24615;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;GraphMETRO&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#25511;&#27169;&#22411;&#21644;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#38024;&#23545;&#29305;&#23450;&#30340;&#20998;&#24067;&#21464;&#21270;&#20135;&#29983;&#19981;&#21464;&#34920;&#31034;&#65292;&#38376;&#25511;&#27169;&#22411;&#21017;&#35782;&#21035;&#21464;&#21270;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#19981;&#21516;&#19987;&#23478;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#20197;&#30830;&#20445;&#24179;&#28369;&#20248;&#21270;&#12290;GraphMETRO&#22312;&#30001;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#21464;&#21270;&#32452;&#25104;&#30340;GOOD&#22522;&#20934;&#27979;&#35797;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#25552;&#39640;&#20102;67&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data are inherently complex and heterogeneous, leading to a high natural diversity of distributional shifts. However, it remains unclear how to build machine learning architectures that generalize to complex non-synthetic distributional shifts naturally occurring in the real world. Here we develop GraphMETRO, a Graph Neural Network architecture, that reliably models natural diversity and captures complex distributional shifts. GraphMETRO employs a Mixture-of-Experts (MoE) architecture with a gating model and multiple expert models, where each expert model targets a specific distributional shift to produce a shift-invariant representation, and the gating model identifies shift components. Additionally, we design a novel objective that aligns the representations from different expert models to ensure smooth optimization. GraphMETRO achieves state-of-the-art results on four datasets from GOOD benchmark comprised of complex and natural real-world distribution shifts, improving by 67%
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35745;&#31639;&#39640;&#38454;&#23548;&#25968;&#65292;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.03885</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#23548;&#25968;&#24635;&#32467;&#65292;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35745;&#31639;&#39640;&#38454;&#23548;&#25968;&#65292;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#21521;&#37327;&#21464;&#37327;$\boldsymbol{\theta}$&#19978;&#30340;&#20989;&#25968;$\mathcal{L}$&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;$\boldsymbol{\theta}$&#34987;&#34920;&#31034;&#20026;&#20803;&#32452;$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$&#30340;&#24352;&#37327;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#35768;&#22810;&#24120;&#35265;&#30340;&#29992;&#20363;&#65292;&#20363;&#22914;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#21644;&#35745;&#31639;&#25216;&#24039;&#65292;&#25552;&#20379;&#20851;&#20110;$\mathcal{L}$&#21450;&#20854;&#24352;&#37327;$\mathbf{T}_s$&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#38454;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#24314;&#31435;&#19968;&#20010;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#21508;&#31181;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#20108;&#38454;&#26041;&#27861;&#21033;&#29992;&#20102;$\boldsymbol{\theta}$&#34987;&#20998;&#21106;&#20026;&#24352;&#37327;$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$&#30340;&#20998;&#21306;&#32467;&#26500;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#35745;&#31639;$\mathcal{L}$&#30340;Hessian&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a gradient-based optimization method applied to a function $\mathcal{L}$ of a vector of variables $\boldsymbol{\theta}$, in the case where $\boldsymbol{\theta}$ is represented as a tuple of tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on $\mathcal{L}$, especially about the interactions between the tensors $\mathbf{T}_s$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of $\boldsymbol{\theta}$ into tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$, in such a way that it requires neither the computation of the Hessian of $\mathcal{
&lt;/p&gt;</description></item><item><title>Elijah&#26159;&#19968;&#31181;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21518;&#38376;&#24182;&#23558;&#20854;&#25928;&#26524;&#20943;&#23569;&#33267;&#25509;&#36817;&#38646;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#29306;&#29298;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.00050</link><description>&lt;p&gt;
Elijah: &#36890;&#36807;&#20998;&#24067;&#21464;&#21270;&#28040;&#38500;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00050
&lt;/p&gt;
&lt;p&gt;
Elijah&#26159;&#19968;&#31181;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21518;&#38376;&#24182;&#23558;&#20854;&#25928;&#26524;&#20943;&#23569;&#33267;&#25509;&#36817;&#38646;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#29306;&#29298;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DM)&#22240;&#20854;&#33021;&#22815;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#24403;&#19968;&#20010;&#25968;&#25454;&#36755;&#20837;&#65288;&#20363;&#22914;&#19968;&#20123;&#39640;&#26031;&#22122;&#22768;&#65289;&#34987;&#27880;&#20837;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#19968;&#20010;&#30333;&#33394;&#26001;&#28857;&#65289;&#26102;&#65292;&#24102;&#26377;&#21518;&#38376;&#30340;&#27169;&#22411;&#24635;&#26159;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65288;&#20363;&#22914;&#19981;&#24688;&#24403;&#30340;&#29031;&#29255;&#65289;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#20943;&#36731;DM&#20013;&#30340;&#21518;&#38376;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;DM&#30340;&#21518;&#38376;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;13&#31181;&#37319;&#26679;&#22120;&#23545;&#21253;&#25324;DDPM&#12289;NCSN&#21644;LDM&#22312;&#20869;&#30340;&#25968;&#30334;&#20010;DM&#36827;&#34892;&#35780;&#20272;&#65292;&#38024;&#23545;3&#31181;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25509;&#36817;100%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#23558;&#21518;&#38376;&#25928;&#26524;&#20943;&#23569;&#33267;&#25509;&#36817;&#38646;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#29306;&#29298;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DM) have become state-of-the-art generative models because of their capability to generate high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies. When a data input (e.g., some Gaussian noise) is stamped with a trigger (e.g., a white patch), the backdoored model always generates the target image (e.g., an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18751</link><description>&lt;p&gt;
&#22312;Web&#19978;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#39034;&#24207;&#20219;&#21153;&#32452;&#21512;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;(LMA)&#20316;&#20026;&#19968;&#31181;&#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21644;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#22312;&#36890;&#24120;&#28041;&#21450;&#20219;&#21153;&#32452;&#21512;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21483;&#20570;CompWoB-&#21453;&#26144;&#26356;&#29616;&#23454;&#20551;&#35774;&#30340;50&#20010;&#32452;&#21512;&#24615;&#32593;&#31449;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#25552;&#31034;&#22411;LMA&#65288;gpt-3.5-turbo&#25110;gpt-4&#65289;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;94.0&#65285;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#38477;&#33267;24.9&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21482;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#36716;&#31227;&#24615;LMA&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#27867;&#21270;&#24615;&#24046;&#36317;&#65292;&#20174;85.4&#65285;&#19979;&#38477;&#21040;54.8&#65285;&#12290;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;HTML-T5++&#65292;&#22312;MiniWoB&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65288;95.2&#65285;&#65289;&#65292;&#24182;&#22312;CompWoB&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#38646;-shot&#24615;&#33021;&#65288;61.5%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.18703</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#29575;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#39044;&#27979;&#30340;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#27809;&#26377;&#21160;&#26426;&#23637;&#31034;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#36890;&#24120;&#36890;&#36807;&#31574;&#30053;&#29109;&#27491;&#21017;&#21270;&#25512;&#21160;&#26234;&#33021;&#20307;&#22312;&#25506;&#32034;&#19978;&#38543;&#26426;&#21270;&#20854;&#34892;&#20026;&#12290;&#20174;&#20154;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24456;&#38590;&#35299;&#37322;&#21644;&#39044;&#27979;&#65307;&#20174;&#23433;&#20840;&#35282;&#24230;&#26469;&#30475;&#65292;&#26356;&#38590;&#20197;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39044;&#27979;&#24615;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;PA-RL&#65289;&#65292;&#29992;&#20110;&#24341;&#23548;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#20854;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#29109;&#29575;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29109;&#29575;&#21046;&#23450;&#20026;&#24179;&#22343;&#22870;&#21169;&#30446;&#26631;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#29109;&#22870;&#21169;&#20989;&#25968;&#20381;&#36182;&#20110;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#20316;&#30456;&#20851;&#30340;&#26367;&#20195;&#29109;&#65292;&#20197;&#21033;&#29992;PG&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#23384;&#22312;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;&#23454;&#38469;&#29109;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#19982;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20173;&#28982;&#21463;&#30410;&#12290;</title><link>https://arxiv.org/abs/2311.17539</link><description>&lt;p&gt;
&#22312;&#36807;&#21442;&#25968;&#21270;&#19979;&#20998;&#26512;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Analyzing Sharpness-aware Minimization under Overparameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20173;&#28982;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#23613;&#31649;&#35757;&#32451;&#25439;&#22833;&#30456;&#21516;&#65292;&#20294;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#19981;&#21516;&#27867;&#21270;&#33021;&#21147;&#30340;&#26497;&#23567;&#20540;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#26497;&#23567;&#20540;&#30340;&#38160;&#24230;&#19982;&#20854;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#24050;&#32463;&#20570;&#20986;&#20102;&#26356;&#22810;&#21162;&#21147;&#24320;&#21457;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#26174;&#24335;&#22320;&#25214;&#21040;&#25153;&#24179;&#26497;&#23567;&#20540;&#20316;&#20026;&#26356;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31574;&#30053;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#36824;&#19981;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#36807;&#21442;&#25968;&#21270;&#19979;&#30340;SAM&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;&#36807;&#21442;&#25968;&#21270;&#23545;SAM&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#24182;&#34920;&#26126;&#23384;&#22312;&#19968;&#31181;&#19968;&#33268;&#30340;&#36235;&#21183;&#65292;&#21363;SAM&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21463;&#30410;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20123;&#20196;&#20154;&#20449;&#26381;&#30340;&#26696;&#20363;&#65292;&#35828;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an overparameterized neural network can yield minimizers of different generalization capabilities despite the same level of training loss. With evidence that suggests a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. However, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to whether and how it is affected by overparameterization.   In this work, we analyze SAM under overparameterization of varying degrees and present both empirical and theoretical results that indicate a critical influence of overparameterization on SAM. Specifically, we conduct extensive numerical experiments across various domains, and show that there exists a consistent trend that SAM continues to benefit from increasing overparameterization. We also discover compelling cases where the effect of overparameterization is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChatTraffic&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#24471;&#21040;&#19982;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#19968;&#33268;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.16203</link><description>&lt;p&gt;
ChatTraffic&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatTraffic: Text-to-Traffic Generation via Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChatTraffic&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#24471;&#21040;&#19982;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#19968;&#33268;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#22522;&#30784;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#21482;&#20381;&#36182;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26469;&#39044;&#27979;&#20132;&#36890;&#36235;&#21183;&#65292;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#23545;&#24322;&#24120;&#20107;&#20214;&#19981;&#25935;&#24863;&#65307;2&#65289;&#22312;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#24615;&#33021;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#20132;&#36890;&#29983;&#25104;&#65292;&#23558;&#27492;&#20219;&#21153;&#21629;&#21517;&#20026;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#65288;TTG&#65289;&#12290;TTG&#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#23558;&#25991;&#26412;&#19982;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#20132;&#36890;&#25968;&#25454;&#30456;&#20851;&#32852;&#65292;&#29992;&#20110;&#29983;&#25104;&#20132;&#36890;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatTraffic&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#35777;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#20132;&#36890;&#25968;&#25454;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23618;&#31216;&#20026;&#21333;&#21464;&#37327;&#24452;&#21521;&#22522;&#20989;&#25968;&#23618;&#65292;&#27169;&#25311;&#20102;&#22823;&#33041;&#20013;&#24863;&#35273;&#31070;&#32463;&#20803;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20837;&#32500;&#24230;&#36827;&#34892;&#19987;&#38376;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#32500;&#36755;&#20837;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#20989;&#25968;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16148</link><description>&lt;p&gt;
&#21333;&#21464;&#37327;&#24452;&#21521;&#22522;&#20989;&#25968;&#23618;&#65306;&#38754;&#21521;&#20302;&#32500;&#36755;&#20837;&#30340;&#33041;&#21551;&#21457;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
Univariate Radial Basis Function Layers: Brain-inspired Deep Neural Layers for Low-Dimensional Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23618;&#31216;&#20026;&#21333;&#21464;&#37327;&#24452;&#21521;&#22522;&#20989;&#25968;&#23618;&#65292;&#27169;&#25311;&#20102;&#22823;&#33041;&#20013;&#24863;&#35273;&#31070;&#32463;&#20803;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20837;&#32500;&#24230;&#36827;&#34892;&#19987;&#38376;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#32500;&#36755;&#20837;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#20989;&#25968;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#20989;&#25968;&#36924;&#36817;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#22823;&#22810;&#25968;&#20171;&#32461;&#30340;&#26550;&#26500;&#37117;&#26159;&#38024;&#23545;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#24320;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#20302;&#32500;&#36755;&#20837;&#65292;&#26631;&#20934;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#26159;&#40664;&#35748;&#36873;&#25321;&#65292;&#32780;&#23545;&#20110;&#19987;&#29992;&#26550;&#26500;&#30340;&#35843;&#26597;&#21017;&#32570;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#23618;&#65292;&#31216;&#20026;&#21333;&#21464;&#37327;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;U-RBF&#65289;&#23618;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#22823;&#33041;&#20013;&#30340;&#24863;&#35273;&#31070;&#32463;&#20803;&#65292;U-RBF&#23618;&#20351;&#29992;&#19968;&#32452;&#31070;&#32463;&#20803;&#22788;&#29702;&#27599;&#20010;&#21333;&#29420;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#20854;&#28608;&#27963;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#39318;&#36873;&#36755;&#20837;&#20540;&#12290;&#25105;&#20204;&#22312;&#20302;&#32500;&#20989;&#25968;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#19982;MLP&#30456;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#21464;&#24471;&#22797;&#26434;&#19988;&#38590;&#20197;&#36924;&#36817;&#26102;&#65292;U-RBF&#23588;&#20026;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) became the standard tool for function approximation with most of the introduced architectures being developed for high-dimensional input data. However, many real-world problems have low-dimensional inputs for which standard Multi-Layer Perceptrons (MLPs) are the default choice. An investigation into specialized architectures is missing. We propose a novel DNN layer called Univariate Radial Basis Function (U-RBF) layer as an alternative. Similar to sensory neurons in the brain, the U-RBF layer processes each individual input dimension with a population of neurons whose activations depend on different preferred input values. We verify its effectiveness compared to MLPs in low-dimensional function regressions and reinforcement learning tasks. The results show that the U-RBF is especially advantageous when the target function becomes complex and difficult to approximate.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.16054</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
Metric Space Magnitude for Evaluating the Diversity of Latent Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16054
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#30340;&#22823;&#23567;&#26159;&#19968;&#31181;&#36817;&#26399;&#24314;&#31435;&#30340;&#19981;&#21464;&#24615;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#25552;&#20379;&#31354;&#38388;&#30340;&#8220;&#26377;&#25928;&#22823;&#23567;&#8221;&#30340;&#34913;&#37327;&#65292;&#24182;&#25429;&#25417;&#21040;&#35768;&#22810;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#20869;&#22312;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24418;&#24335;&#21270;&#20102;&#26377;&#38480;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#20989;&#25968;&#20043;&#38388;&#30340;&#26032;&#39062;&#19981;&#30456;&#20284;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#25968;&#25454;&#25200;&#21160;&#19979;&#20445;&#35777;&#31283;&#23450;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#20005;&#26684;&#30340;&#22810;&#23610;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#23454;&#39564;&#22871;&#20214;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#21331;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#12289;&#27169;&#24335;&#23849;&#28291;&#26816;&#27979;&#20197;&#21450;&#29992;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The magnitude of a metric space is a recently-established invariant, providing a measure of the 'effective size' of a space across multiple scales while also capturing numerous geometrical properties. We develop a family of magnitude-based measures of the intrinsic diversity of latent representations, formalising a novel notion of dissimilarity between magnitude functions of finite metric spaces. Our measures are provably stable under perturbations of the data, can be efficiently calculated, and enable a rigorous multi-scale comparison of latent representations. We show the utility and superior performance of our measures in an experimental suite that comprises different domains and tasks, including the evaluation of diversity, the detection of mode collapse, and the evaluation of generative models for text, image, and graph data.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.14736</link><description>&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Data Diversity Matters for Robust Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14736
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31934;&#36873;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#22256;&#38590;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#31934;&#36873;&#25110;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#12290;&#33258;&#21160;&#25968;&#25454;&#31934;&#36873;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#20173;&#19981;&#28165;&#26970;&#22914;&#20309;&#20026;&#25351;&#20196;&#35843;&#25972;&#23450;&#20041;&#22810;&#26679;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#65292;&#20197;&#21450;&#22914;&#20309;&#20248;&#21270;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36136;&#37327;-&#22810;&#26679;&#24615;&#25351;&#20196;&#35843;&#25972;(QDIT)&#12290;QDIT&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#28145;&#20837;&#30740;&#31350;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#65306;(1)&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#33258;&#28982;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;(2)&#22686;&#21152;&#25968;&#25454;&#22810;&#26679;&#24615;&#26174;&#33879;&#25552;&#39640;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25351;&#20196;&#36319;&#38543;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36229;&#38271;&#19978;&#19979;&#25991;&#19979;&#20869;&#23384;&#25928;&#29575;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#36229;&#38271;Token&#27880;&#24847;&#21147;&#36817;&#20284;&#30340;&#21333;&#27425;&#27969;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30697;&#38453;$U_1, U_2$&#21152;&#36895;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#35745;&#31639;&#36164;&#28304;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.14652</link><description>&lt;p&gt;
&#19968;&#31181;&#36229;&#38271;Token&#27880;&#24847;&#21147;&#36817;&#20284;&#30340;&#21333;&#27425;&#27969;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36229;&#38271;&#19978;&#19979;&#25991;&#19979;&#20869;&#23384;&#25928;&#29575;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#36229;&#38271;Token&#27880;&#24847;&#21147;&#36817;&#20284;&#30340;&#21333;&#27425;&#27969;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30697;&#38453;$U_1, U_2$&#21152;&#36895;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#35745;&#31639;&#36164;&#28304;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#35745;&#31639;&#21516;&#26102;&#20855;&#26377;$O(n^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;$O(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#36825;&#20351;&#24471;&#22312;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#30340;&#27969;&#24212;&#29992;&#20013;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models&#65292;LLMs)&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26368;&#36817;&#30340;OpenAI DevDay&#65288;2023&#24180;11&#26376;6&#26085;&#65289;&#65292;OpenAI&#21457;&#24067;&#20102;&#19968;&#31181;&#33021;&#22815;&#25903;&#25345;128K&#38271;&#25991;&#26723;&#30340;&#26032;&#27169;&#22411;&#65292;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;$n$&#36828;&#22823;&#20110;128K ($n \gg 2^d$)&#26102;&#30340;&#20869;&#23384;&#26377;&#25928;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#20855;&#26377; Query&#12289;Key &#21644; Value &#30697;&#38453;$Q, K, V \in \mathbb{R}^{n \times d}$&#30340;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;&#65292;&#22810;&#39033;&#24335;&#26041;&#27861;&#36817;&#20284;&#20102;&#27880;&#24847;&#21147;&#36755;&#20986;$T \in \mathbb{R}^{n \times d}$&#12290;&#23427;&#36890;&#36807;&#26500;&#24314;$U_1, U_2 \in \mathbb{R}^{n \times t}$&#22312;$n^{1+o(1)}$&#27425;&#26102;&#38388;&#25191;&#34892;&#20869;&#21152;&#36895;&#27880;&#24847;&#21147;&#35745;&#31639;${\sf Attn}(Q, K, V)$&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35745;&#31639;&#36817;&#20284;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;$U_1U_2^\top \in \mathbb{R}^{n \times n}$&#20173;&#38656;&#35201;$O(n^2)$&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \in \mathbb{R}^{n \times d}$, the polynomial method approximates the attention output $T \in \mathbb{R}^{n \times d}$. It accomplishes this by constructing $U_1, U_2 \in \mathbb{R}^{n \times t}$ to expedite attention ${\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, computing the approximated attention matrix $U_1U_2^\top \in \mathbb{R}^{n \times n}$ still necessitates $O(n^2
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#19988;&#26377;&#21147;&#22320;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2311.14220</link><description>&lt;p&gt;
&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Assumption-lean and Data-adaptive Post-Prediction Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#19988;&#26377;&#21147;&#22320;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#26082;&#32791;&#36153;&#26102;&#38388;&#21448;&#36153;&#21147;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31185;&#23398;&#23478;&#20204;&#20381;&#36182;&#20110;ML&#31639;&#27861;&#20351;&#29992;&#26131;&#24471;&#30340;&#21327;&#21464;&#37327;&#26469;&#39044;&#27979;&#36825;&#20123;&#40644;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#24120;&#24120;&#30452;&#25509;&#29992;&#20110;&#21518;&#32493;&#30340;&#32479;&#35745;&#20998;&#26512;&#20013;&#65292;&#24573;&#30053;&#20102;&#39044;&#27979;&#36807;&#31243;&#24341;&#20837;&#30340;&#19981;&#31934;&#30830;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#34394;&#20551;&#30340;&#27491;&#38754;&#32467;&#26524;&#21644;&#26080;&#25928;&#30340;&#31185;&#23398;&#32467;&#35770;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#23427;&#20801;&#35768;&#22522;&#20110;ML&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#21644;&#26377;&#21147;&#30340;&#25512;&#26029;&#12290;&#23427;&#30340;&#8220;&#20551;&#35774;&#31616;&#21270;&#8221;&#23646;&#24615;&#20445;&#35777;&#22312;&#24191;&#27867;&#30340;&#32479;&#35745;&#37327;&#19978;&#19981;&#22522;&#20110;ML&#39044;&#27979;&#20570;&#20986;&#21487;&#38752;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#23427;&#30340;&#8220;&#25968;&#25454;&#33258;&#36866;&#24212;&#8221;&#29305;&#24615;&#20445;&#35777;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary challenge facing modern scientific research is the limited availability of gold-standard data which can be both costly and labor-intensive to obtain. With the rapid development of machine learning (ML), scientists have relied on ML algorithms to predict these gold-standard outcomes with easily obtained covariates. However, these predicted outcomes are often used directly in subsequent statistical analyses, ignoring imprecision and heterogeneity introduced by the prediction procedure. This will likely result in false positive findings and invalid scientific conclusions. In this work, we introduce an assumption-lean and data-adaptive Post-Prediction Inference (POP-Inf) procedure that allows valid and powerful inference based on ML-predicted outcomes. Its "assumption-lean" property guarantees reliable statistical inference without assumptions on the ML-prediction, for a wide range of statistical quantities. Its "data-adaptive'" feature guarantees an efficiency gain over existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#37319;&#26679;&#20026;&#23548;&#21521;&#30340;Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;PC&#25512;&#29702;&#36807;&#31243;&#27880;&#20837;&#39640;&#26031;&#22122;&#22768;&#23454;&#29616;&#36807;&#38459;&#23612;&#30340;Langevin&#37319;&#26679;&#65292;&#24182;&#25913;&#36827;&#20102;&#32467;&#26524;&#32534;&#30721;&#22120;&#33258;&#30001;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;&#32593;&#32476;&#25552;&#20379;&#25674;&#38144;&#30340;&#28909;&#21551;&#21160;&#12290;&#27492;&#22806;&#65292;&#36824;&#39564;&#35777;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#39044;&#22788;&#29702;&#24418;&#24335;&#65292;&#20351;&#24471;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13664</link><description>&lt;p&gt;
&#20197;&#37319;&#26679;&#20026;&#23548;&#21521;: Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sample as You Infer: Predictive Coding With Langevin Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#37319;&#26679;&#20026;&#23548;&#21521;&#30340;Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;PC&#25512;&#29702;&#36807;&#31243;&#27880;&#20837;&#39640;&#26031;&#22122;&#22768;&#23454;&#29616;&#36807;&#38459;&#23612;&#30340;Langevin&#37319;&#26679;&#65292;&#24182;&#25913;&#36827;&#20102;&#32467;&#26524;&#32534;&#30721;&#22120;&#33258;&#30001;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;&#32593;&#32476;&#25552;&#20379;&#25674;&#38144;&#30340;&#28909;&#21551;&#21160;&#12290;&#27492;&#22806;&#65292;&#36824;&#39564;&#35777;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#39044;&#22788;&#29702;&#24418;&#24335;&#65292;&#20351;&#24471;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36890;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#21442;&#25968;&#65292;&#35813;&#31639;&#27861;&#24314;&#31435;&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;(PC)&#26694;&#26550;&#20043;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20462;&#25913;&#20102;&#26631;&#20934;&#30340;PC&#31639;&#27861;&#65292;&#20351;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#35757;&#32451;&#30340;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#12290;&#36890;&#36807;&#23558;&#39640;&#26031;&#22122;&#22768;&#27880;&#20837;PC&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23558;&#20854;&#26500;&#24819;&#20026;&#36807;&#38459;&#23612;&#30340;Langevin&#37319;&#26679;&#65292;&#20174;&#32780;&#26041;&#20415;&#23545;&#32039;&#20945;&#35777;&#25454;&#19979;&#30028;(ELBO)&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#32467;&#26524;&#32534;&#30721;&#22120;&#33258;&#30001;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32534;&#30721;&#22120;&#32593;&#32476;&#32435;&#20837;&#20854;&#20013;&#65292;&#20026;&#25105;&#20204;&#30340;Langevin&#37319;&#26679;&#25552;&#20379;&#20102;&#19968;&#31181;&#25674;&#38144;&#30340;&#28909;&#21551;&#21160;&#65292;&#24182;&#27979;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#22686;&#21152;&#23545;&#37319;&#26679;&#27493;&#38271;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#23569;&#23545;&#26354;&#29575;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#39044;&#22788;&#29702;&#24418;&#24335;&#65292;&#21463;&#21040;Riemann Manifold Langevin&#21644;SGD&#25991;&#29486;&#20013;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19982;...
&lt;/p&gt;
&lt;p&gt;
We present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (PC) framework of computational neuroscience. Our approach modifies the standard PC algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (ELBO). We improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our Langevin sampling and test three different objectives for doing so. Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers from the SGD literature. We compare agains
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL/D&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36830;&#25509;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;MORL/D&#20998;&#31867;&#27861;&#65292;&#20026;&#29616;&#26377;&#21644;&#28508;&#22312;&#30340;MORL&#24037;&#20316;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2311.12495</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65306;&#20998;&#31867;&#21644;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Reinforcement Learning Based on Decomposition: A Taxonomy and Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12495
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL/D&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36830;&#25509;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;MORL/D&#20998;&#31867;&#27861;&#65292;&#20026;&#29616;&#26377;&#21644;&#28508;&#22312;&#30340;MORL&#24037;&#20316;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#36890;&#36807;&#23547;&#27714;&#26435;&#34913;&#20914;&#31361;&#30446;&#26631;&#30340;&#31574;&#30053;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#23545;MORL&#30340;&#19981;&#26029;&#20852;&#36259;&#24341;&#21457;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#24120;&#20511;&#37492;&#20102;&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29616;&#26377;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#32570;&#20047;&#22522;&#20110;RL&#21644;MOO/D&#30340;&#28165;&#26224;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;MORL&#30740;&#31350;&#20154;&#21592;&#22312;&#23581;&#35797;&#23558;&#36129;&#29486;&#24402;&#31867;&#21040;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#20013;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#20998;&#31867;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL/D&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;RL&#21644;MOO&#25991;&#29486;&#32852;&#31995;&#36215;&#26469;&#30340;&#26032;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;MORL/D&#20998;&#31867;&#27861;&#65292;&#20026;&#20998;&#31867;&#29616;&#26377;&#21644;&#28508;&#22312;&#30340;MORL&#24037;&#20316;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#30340;&#22522;&#30784;&#12290;&#28982;&#21518;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#23545;MORL&#30740;&#31350;&#36827;&#34892;&#32454;&#33268;&#23457;&#26597;&#65292;&#22686;&#24378;&#20102;&#28165;&#26224;&#24230;&#21644;&#31616;&#27905;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective reinforcement learning (MORL) extends traditional RL by seeking policies making different compromises among conflicting objectives. The recent surge of interest in MORL has led to diverse studies and solving methods, often drawing from existing knowledge in multi-objective optimization based on decomposition (MOO/D). Yet, a clear categorization based on both RL and MOO/D is lacking in the existing literature. Consequently, MORL researchers face difficulties when trying to classify contributions within a broader context due to the absence of a standardized taxonomy. To tackle such an issue, this paper introduces multi-objective reinforcement learning based on decomposition (MORL/D), a novel methodology bridging the literature of RL and MOO. A comprehensive taxonomy for MORL/D is presented, providing a structured foundation for categorizing existing and potential MORL works. The introduced taxonomy is then used to scrutinize MORL research, enhancing clarity and concisenes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19968;&#33324;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#31181;&#29615;&#22659;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;&#20102;&#21463;&#21040;&#22260;&#32469;&#33410;&#28857;&#27495;&#20041;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#20986;&#22320;&#38754;&#30495;&#23454;&#27169;&#22411;</title><link>https://arxiv.org/abs/2311.12267</link><description>&lt;p&gt;
&#20174;&#19968;&#33324;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65306;&#21487;&#36776;&#35782;&#24615;&#21644;&#20869;&#22312;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19968;&#33324;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#31181;&#29615;&#22659;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;&#20102;&#21463;&#21040;&#22260;&#32469;&#33410;&#28857;&#27495;&#20041;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#20986;&#22320;&#38754;&#30495;&#23454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#21363;&#20174;&#20302;&#32423;&#35266;&#27979;&#25968;&#25454;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#20013;&#24674;&#22797;&#39640;&#32423;&#28508;&#22312;&#21464;&#37327;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20174;&#22810;&#20010;&#29615;&#22659;&#29983;&#25104;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#20043;&#21069;&#20851;&#20110;&#22240;&#26524;&#34920;&#31034;&#21487;&#36776;&#35782;&#24615;&#30340;&#32467;&#26524;&#36890;&#24120;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21333;&#33410;&#28857;&#24178;&#39044;&#65292;&#20294;&#23454;&#38469;&#19978;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#28508;&#22312;&#21464;&#37327;&#26412;&#36523;&#23601;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#26469;&#33258;&#19968;&#33324;&#29615;&#22659;&#30340;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#32447;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#34429;&#28982;&#21487;&#20197;&#23436;&#20840;&#24674;&#22797;&#22240;&#26524;&#22270;&#65292;&#20294;&#28508;&#22312;&#21464;&#37327;&#21482;&#33021;&#34987;&#35782;&#21035;&#21040;&#21463;&#21040;&#22260;&#32469;&#33410;&#28857;&#27495;&#20041;&#65288;SNA&#65289;&#30340;&#31243;&#24230;&#19978;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#20445;&#35777;&#30340;&#23545;&#24212;&#23545;&#65292;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;SNA&#22522;&#26412;&#19978;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;LiNGCReL&#65292;&#21487;&#20197;&#34987;&#35777;&#26126;&#21487;&#20197;&#24674;&#22797;&#20986;&#22320;&#38754;&#30495;&#23454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \texttt{LiNGCReL} which provably recovers the ground-truth model up to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#27973;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;"&#26080;&#27880;&#24847;&#21147;&#30340;Transformers"&#21487;&#20197;&#19982;&#21407;&#22987;&#26550;&#26500;&#30340;&#24615;&#33021;&#23218;&#32654;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#31616;&#21270;&#22797;&#26434;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.10642</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27880;&#24847;&#21147;&#65306;&#25506;&#32034;&#23558;&#27973;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;Transformers&#20013;&#27880;&#24847;&#21147;&#23618;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#27973;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;"&#26080;&#27880;&#24847;&#21147;&#30340;Transformers"&#21487;&#20197;&#19982;&#21407;&#22987;&#26550;&#26500;&#30340;&#24615;&#33021;&#23218;&#32654;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#31616;&#21270;&#22797;&#26434;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#26631;&#20934;&#30340;&#27973;&#23618;&#21069;&#39304;&#32593;&#32476;&#26469;&#27169;&#20223;Transformer&#27169;&#22411;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23558;Transformer&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#26367;&#25442;&#20026;&#31616;&#21333;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;&#32452;&#20214;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;IWSLT2017&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#8220;&#26080;&#27880;&#24847;&#21147;&#30340;Transformers&#8221;&#21487;&#20197;&#19982;&#21407;&#22987;&#26550;&#26500;&#30340;&#24615;&#33021;&#23218;&#32654;&#12290;&#36890;&#36807;&#20005;&#35880;&#30340;&#23454;&#39564;&#21644;&#19981;&#21516;&#26367;&#20195;&#32593;&#32476;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#21487;&#34892;&#24615;&#30340;&#35265;&#35299;&#12290;&#36825;&#19981;&#20165;&#25581;&#31034;&#20102;&#27973;&#23618;&#21069;&#39304;&#32593;&#32476;&#22312;&#27169;&#20223;&#27880;&#24847;&#21147;&#26426;&#21046;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#65292;&#32780;&#19988;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#31616;&#21270;&#24207;&#21015;&#20219;&#21153;&#30340;&#22797;&#26434;&#26550;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these "attentionless Transformers" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24778;&#21916;&#24615;&#39537;&#21160;&#30340;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;k-NN&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#20256;&#32479;&#31639;&#27861;&#36827;&#34892;&#26032;&#30340;&#38416;&#37322;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#23494;&#24230;&#20272;&#35745;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.10246</link><description>&lt;p&gt;
&#22522;&#20110;&#24778;&#21916;&#24615;&#39537;&#21160;&#30340;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;k-NN&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24778;&#21916;&#24615;&#39537;&#21160;&#30340;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;k-NN&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#20256;&#32479;&#31639;&#27861;&#36827;&#34892;&#26032;&#30340;&#38416;&#37322;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#23494;&#24230;&#20272;&#35745;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20851;&#31995;&#65292;&#32780;&#19981;&#23545;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#12290;&#22312;&#36825;&#19968;&#33539;&#24335;&#19979;&#65292;&#26368;&#20026;&#33879;&#21517;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#20256;&#32479;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#38416;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#23494;&#24230;&#20272;&#35745;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#22686;&#21152;&#29305;&#24449;&#26102;&#30340;&#26465;&#20214;&#29109;&#26469;&#30830;&#23450;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#21644;&#29305;&#24449;&#30340;&#36129;&#29486;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#25968;&#25454;&#28857;&#24433;&#21709;&#26435;&#37325;&#26469;&#35745;&#31639;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. We can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. This allows us to compute feature contributions by providing detailed data point influence weights with perfect attributi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09308</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#33041;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Divergences between Language Models and Human Brains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21644;&#20154;&#31867;&#26159;&#21542;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#35328;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#26263;&#31034;&#32943;&#23450;&#65292;&#21457;&#29616;&#22823;&#33041;&#20449;&#21495;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#34920;&#31034;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#32467;&#26524;&#34987;&#35748;&#20026;&#21453;&#26144;&#20102;LMs&#21644;&#20154;&#31867;&#22823;&#33041;&#20043;&#38388;&#30340;&#20849;&#20139;&#35745;&#31639;&#21407;&#29702;&#65292;&#20294;LMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#34920;&#31034;&#21644;&#20351;&#29992;&#19978;&#20063;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;LM&#34920;&#31034;&#21644;&#20154;&#31867;&#22823;&#33041;&#23545;&#35821;&#35328;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;&#21463;&#35797;&#32773;&#38405;&#35835;&#21644;&#21548;&#21465;&#36848;&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#39046;&#22495;&#65292;&#21363;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#65292;&#36825;&#20123;&#39046;&#22495;&#22312;LMs&#20013;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06233</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;: &#19968;&#31181;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27745;&#26579;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#24182;&#20272;&#35745;&#20854;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#35270;&#20026;&#19968;&#31995;&#21015;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#27979;&#39564;&#24418;&#24335;&#65292;&#20854;&#20013;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#19977;&#20010;&#25200;&#21160;&#29256;&#26412;&#12290;&#36825;&#20123;&#21464;&#21270;&#20165;&#21253;&#25324;&#35789;&#32423;&#25200;&#21160;&#12290;&#29983;&#25104;&#30340;&#25200;&#21160;&#29256;&#26412;&#19982;&#21407;&#22987;&#23454;&#20363;&#19968;&#36215;&#24418;&#25104;DCQ&#20013;&#30340;&#36873;&#39033;&#65292;&#39069;&#22806;&#30340;&#36873;&#39033;&#36866;&#24212;&#20102;&#25552;&#20379;&#30340;&#36873;&#25321;&#37117;&#19981;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#37492;&#20110;&#22312;&#36873;&#25321;&#20043;&#38388;&#21807;&#19968;&#30340;&#21306;&#21035;&#20449;&#21495;&#26159;&#19982;&#21407;&#22987;&#23454;&#20363;&#30340;&#30830;&#20999;&#25514;&#36766;&#30456;&#20851;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24050;&#32463;&#25509;&#35302;&#21040;&#21407;&#22987;&#23454;&#20363;&#65292;&#35821;&#35328;&#27169;&#22411;&#24403;&#34987;&#35201;&#27714;&#20174;&#36873;&#39033;&#20013;&#35782;&#21035;&#21407;&#22987;&#23454;&#20363;&#26102;&#65292;&#20542;&#21521;&#20110;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;--&#36825;&#26159;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GPT-4/3.5&#36827;&#34892;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23436;&#20840;&#32570;&#23569;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
&lt;/p&gt;</description></item><item><title>PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.03415</link><description>&lt;p&gt;
PowerFlowNet: &#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03415
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#21151;&#29575;&#27969;&#20998;&#26512;&#23545;&#20110;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#30340;&#36816;&#34892;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#20026;&#23567;&#22411;&#21644;&#22823;&#22411;&#30005;&#21147;&#32593;&#32476;&#25552;&#20379;&#20934;&#30830;&#21644;&#24555;&#36895;&#35299;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#12290;&#30001;&#20110;&#30005;&#21147;&#32593;&#32476;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#20010;&#22270;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#25104;&#20026;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#25913;&#21892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PowerFlowNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#21151;&#29575;&#27969;&#36817;&#20284;&#65292;&#22312;&#31616;&#21333;&#30340;IEEE 14&#24635;&#32447;&#31995;&#32479;&#20013;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;(6470rte)&#30340;&#30495;&#23454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;4&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#19982;&#20854;&#20182;&#20256;&#32479;&#30340;&#36817;&#20284;&#26041;&#27861;(&#22914;&#30452;&#27969;&#26494;&#24347;&#27861;)&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#23427;&#20204;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#31034;&#20363;&#19982;&#19968;&#38454;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#25972;&#21512;&#21040;&#26680;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#36716;&#21270;&#20026;&#36830;&#32493;&#23454;&#29616;&#30340;&#24418;&#24335;&#65292;&#26377;&#25928;&#22788;&#29702;&#22522;&#20110;&#26680;&#30340;&#35859;&#35789;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2311.03340</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#26680;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask Kernel-based Learning with First-Order Logic Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#31034;&#20363;&#19982;&#19968;&#38454;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#25972;&#21512;&#21040;&#26680;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#36716;&#21270;&#20026;&#36830;&#32493;&#23454;&#29616;&#30340;&#24418;&#24335;&#65292;&#26377;&#25928;&#22788;&#29702;&#22522;&#20110;&#26680;&#30340;&#35859;&#35789;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#30001;&#19968;&#31995;&#21015;&#19968;&#38454;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30340;&#32972;&#26223;&#30693;&#35782;&#19982;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#31034;&#20363;&#25972;&#21512;&#21040;&#26680;&#26426;&#22120;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#65292;&#22312;&#35813;&#26041;&#26696;&#20013;&#65292;&#23450;&#20041;&#22312;&#19968;&#32452;&#23545;&#35937;&#19978;&#30340;&#22810;&#20010;&#35859;&#35789;&#38656;&#35201;&#20174;&#31034;&#20363;&#20013;&#20849;&#21516;&#23398;&#20064;&#65292;&#24182;&#23545;&#20854;&#20540;&#30340;&#21512;&#27861;&#37197;&#32622;&#26045;&#21152;&#19968;&#31995;&#21015;&#30340;FOL&#32422;&#26463;&#12290;&#36825;&#20123;&#35859;&#35789;&#26159;&#23450;&#20041;&#22312;&#36755;&#20837;&#23545;&#35937;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26159;&#24050;&#30693;&#30340;&#20107;&#20808;&#23450;&#20041;&#22909;&#30340;&#65292;&#20063;&#21487;&#20197;&#30001;&#36866;&#24403;&#30340;&#22522;&#20110;&#26680;&#30340;&#23398;&#20064;&#22120;&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#23558;FOL&#23376;&#21477;&#36716;&#21270;&#20026;&#19968;&#20010;&#36830;&#32493;&#23454;&#29616;&#65292;&#33021;&#22815;&#22788;&#29702;&#30001;&#22522;&#20110;&#26680;&#30340;&#35859;&#35789;&#35745;&#31639;&#20986;&#30340;&#36755;&#20986;&#12290;&#35813;&#23398;&#20064;&#38382;&#39064;&#34987;&#35270;&#20026;&#21322;&#30417;&#30563;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#21407;&#23646;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21253;&#25324;&#23545;&#26377;&#30417;&#30563;&#31034;&#20363;&#30340;&#25311;&#21512;&#25439;&#22833;&#24230;&#37327;&#12289;&#27491;&#21017;&#21270;&#39033;&#21644;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2311.03099</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23601;&#20687;&#36229;&#32423;&#39532;&#37324;&#22885;&#65306;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;DARE&#26469;&#23558;&#22823;&#22810;&#25968;delta&#21442;&#25968;&#65288;&#21363;&#24494;&#35843;&#21644;&#39044;&#35757;&#32451;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65289;&#35774;&#32622;&#20026;&#38646;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#30417;&#30563;&#24494;&#35843;(SFT) LMs&#30340;&#33021;&#21147;&#65292;DARE&#36890;&#36807;&#38543;&#26426;&#21024;&#38500;&#27604;&#29575;&#20026;p&#30340;delta&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;1/(1 - p)&#37325;&#26032;&#32553;&#25918;&#21097;&#20313;&#21442;&#25968;&#26469;&#36817;&#20284;&#21407;&#22987;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;DARE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#22810;&#20010;SFT&#21516;&#28304;&#27169;&#22411;&#30340;delta&#21442;&#25968;&#65292;&#20197;&#20943;&#36731;&#21442;&#25968;&#24178;&#25200;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#34701;&#21512;&#23558;&#23427;&#20204;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20026;&#22522;&#30784;&#30340;LM&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;SFT delta&#21442;&#25968;&#20540;&#33539;&#22260;&#36890;&#24120;&#24456;&#23567;&#65288;&#22312;0.005&#20197;&#20869;&#65289;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#20887;&#20313;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;90%&#29978;&#33267;99%&#30340;&#21442;&#25968;&#12290;&#65288;2&#65289;DARE&#21487;&#20197;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;LM&#21512;&#24182;&#20026;&#19968;&#20010;LM&#65292;&#24182;&#26377;&#39550;&#39542;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#32593;&#32476;&#24178;&#25200;&#19979;&#20010;&#20307;&#21270;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20551;&#35774;&#21322;&#21442;&#25968;&#32467;&#26500;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#21644;&#23398;&#20064;&#26368;&#20248;&#30340;&#20010;&#20307;&#21270;&#22788;&#29702;&#35268;&#21017;&#12290;</title><link>https://arxiv.org/abs/2311.02467</link><description>&lt;p&gt;
&#38598;&#32676;&#32593;&#32476;&#24178;&#25200;&#19979;&#30340;&#20010;&#20307;&#21270;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Individualized Policy Evaluation and Learning under Clustered Network Interference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#32593;&#32476;&#24178;&#25200;&#19979;&#20010;&#20307;&#21270;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20551;&#35774;&#21322;&#21442;&#25968;&#32467;&#26500;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#21644;&#23398;&#20064;&#26368;&#20248;&#30340;&#20010;&#20307;&#21270;&#22788;&#29702;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#22312;&#26377;&#24456;&#22810;&#20851;&#20110;&#25919;&#31574;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#25991;&#29486;&#65292;&#20294;&#22823;&#37096;&#20998;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#19968;&#20010;&#20010;&#20307;&#30340;&#22788;&#29702;&#20998;&#37197;&#19981;&#20250;&#24433;&#21709;&#21478;&#19968;&#20010;&#20010;&#20307;&#30340;&#32467;&#26524;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24573;&#35270;&#24178;&#25200;&#21487;&#33021;&#23548;&#33268;&#35780;&#20272;&#20559;&#35823;&#21644;&#26080;&#25928;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#22788;&#29702;&#26377;&#24456;&#22810;&#26379;&#21451;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#20010;&#20307;&#21487;&#33021;&#20135;&#29983;&#27491;&#21521;&#28322;&#20986;&#25928;&#24212;&#65292;&#20174;&#32780;&#25913;&#21892;&#20010;&#20307;&#21270;&#22788;&#29702;&#35268;&#21017;&#65288;ITR&#65289;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#38598;&#32676;&#32593;&#32476;&#24178;&#25200;&#65288;&#20063;&#31216;&#20026;&#37096;&#20998;&#24178;&#25200;&#65289;&#19979;&#35780;&#20272;&#21644;&#23398;&#20064;&#26368;&#20248;ITR&#30340;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#21333;&#20301;&#32858;&#31867;&#20174;&#19968;&#20010;&#24635;&#20307;&#20013;&#25277;&#26679;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#21333;&#20301;&#20043;&#38388;&#21487;&#33021;&#20114;&#30456;&#24433;&#21709;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#24378;&#21046;&#38480;&#21046;&#28322;&#20986;&#25928;&#24212;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21482;&#20551;&#35774;&#21322;&#21442;&#25968;&#32467;&#26500;&#27169;&#22411;&#65292;&#27599;&#20010;&#21333;&#20301;&#30340;&#32467;&#26524;&#26159;&#32858;&#31867;&#20013;&#30340;&#20010;&#20307;&#22788;&#29702;&#30340;&#21152;&#27861;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there now exists a large literature on policy evaluation and learning, much of prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. Unfortunately, ignoring interference may lead to biased policy evaluation and ineffective learned policies. For example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (ITR). We consider the problem of evaluating and learning an optimal ITR under clustered network interference (also known as partial interference) where clusters of units are sampled from a population and units may influence one another within each cluster. Unlike previous methods that impose strong restrictions on spillover effects, the proposed methodology only assumes a semiparametric structural model where each unit's outcome is an additive function of individual treatments within the cluster. Under this 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#22810;&#20010;&#23398;&#20064;&#33539;&#24335;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23545;&#25239;&#26679;&#26412;&#30340;&#29702;&#35770;&#65292;&#21457;&#29616;&#38750;&#40065;&#26834;&#29305;&#24449;&#22312;&#22810;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#20013;&#30340;&#25928;&#29992;&#36739;&#24046;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#24182;&#19981;&#20687;&#40065;&#26834;&#29305;&#24449;&#25110;&#33258;&#28982;&#29305;&#24449;&#37027;&#26679;&#30495;&#27491;&#26377;&#29992;&#12290;</title><link>https://arxiv.org/abs/2310.18936</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#19981;&#26159;&#30495;&#27491;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples Are Not Real Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22810;&#20010;&#23398;&#20064;&#33539;&#24335;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23545;&#25239;&#26679;&#26412;&#30340;&#29702;&#35770;&#65292;&#21457;&#29616;&#38750;&#40065;&#26834;&#29305;&#24449;&#22312;&#22810;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#20013;&#30340;&#25928;&#29992;&#36739;&#24046;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#24182;&#19981;&#20687;&#40065;&#26834;&#29305;&#24449;&#25110;&#33258;&#28982;&#29305;&#24449;&#37027;&#26679;&#30495;&#27491;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#30340;&#23384;&#22312;&#22810;&#24180;&#26469;&#19968;&#30452;&#26159;&#19968;&#20010;&#35868;&#22242;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#19968;&#31181;&#30001;Ilyas&#31561;&#20154;&#25552;&#20986;&#30340;&#33879;&#21517;&#29702;&#35770;&#20174;&#25968;&#25454;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#65292;&#21363;&#36890;&#36807;&#23637;&#31034;&#21487;&#20197;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#25552;&#21462;&#38750;&#40065;&#26834;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21333;&#29420;&#29992;&#20110;&#20998;&#31867;&#26159;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#37322;&#20173;&#28982;&#30456;&#24403;&#21453;&#30452;&#35273;&#65292;&#22240;&#20026;&#38750;&#40065;&#26834;&#29305;&#24449;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#22823;&#37096;&#20998;&#26159;&#22122;&#22768;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26356;&#22823;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#32467;&#21512;&#20102;&#22810;&#20010;&#23398;&#20064;&#33539;&#24335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#33391;&#22909;&#25928;&#29992;&#30456;&#21453;&#65292;&#24403;&#23558;&#38750;&#40065;&#26834;&#29305;&#24449;&#36716;&#31227;&#21040;&#20854;&#20182;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#26102;&#65288;&#22914;&#23545;&#27604;&#23398;&#20064;&#12289;&#36974;&#25377;&#22270;&#20687;&#24314;&#27169;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#23427;&#20204;&#30340;&#25928;&#29992;&#21464;&#24046;&#12290;&#36825;&#25581;&#31034;&#20102;&#38750;&#40065;&#26834;&#29305;&#24449;&#24182;&#19981;&#20687;&#22312;&#36825;&#20123;&#33539;&#24335;&#20043;&#38388;&#20139;&#26377;&#33391;&#22909;&#21487;&#36801;&#31227;&#24615;&#30340;&#40065;&#26834;&#29305;&#24449;&#25110;&#33258;&#28982;&#29305;&#24449;&#37027;&#26679;&#30495;&#27491;&#26377;&#29992;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#40065;&#26834;&#29305;&#24449;&#32780;&#35328;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;&#19982;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by \citet{ilyas2019adversarial} explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#21644;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#22270;&#25193;&#20805;&#25216;&#26415;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#39033;&#20943;&#36731;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#35299;&#20915;GNN&#20013;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2310.18765</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#21644;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#22270;&#25193;&#20805;&#25216;&#26415;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#39033;&#20943;&#36731;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#35299;&#20915;GNN&#20013;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#21644;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23558;&#25968;&#25454;&#19981;&#24179;&#34913;&#19982;&#27169;&#22411;&#26041;&#24046;&#23494;&#20999;&#30456;&#20851;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#22270;&#25193;&#20805;&#25216;&#26415;&#26469;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#27491;&#21017;&#39033;&#26469;&#20943;&#36731;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#33258;&#28982;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21010;&#20998;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#35813;&#24037;&#20316;&#20026;&#35299;&#20915;GNN&#20013;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new approach to address the issue of class imbalance in graph neural networks (GNNs) for learning on graph-structured data. Our approach integrates imbalanced node classification and Bias-Variance Decomposition, establishing a theoretical framework that closely relates data imbalance to model variance. We also leverage graph augmentation technique to estimate the variance, and design a regularization term to alleviate the impact of imbalance. Exhaustive tests are conducted on multiple benchmarks, including naturally imbalanced datasets and public-split class-imbalanced datasets, demonstrating that our approach outperforms state-of-the-art methods in various imbalanced scenarios. This work provides a novel theoretical perspective for addressing the problem of imbalanced node classification in GNNs.
&lt;/p&gt;</description></item><item><title>MicroNAS&#26159;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31995;&#32479;&#65292;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#25191;&#34892;&#24310;&#36831;&#21644;&#20869;&#23384;&#28040;&#32791;&#38480;&#21046;&#30340;&#20248;&#21270;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2310.18384</link><description>&lt;p&gt;
MicroNAS: &#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35760;&#24518;&#21644;&#24310;&#36831;&#32422;&#26463;&#30340;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
MicroNAS: Memory and Latency Constrained Hardware-Aware Neural Architecture Search for Time Series Classification on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18384
&lt;/p&gt;
&lt;p&gt;
MicroNAS&#26159;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31995;&#32479;&#65292;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#25191;&#34892;&#24310;&#36831;&#21644;&#20869;&#23384;&#28040;&#32791;&#38480;&#21046;&#30340;&#20248;&#21270;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29305;&#23450;&#39046;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#32791;&#26102;&#12289;&#23481;&#26131;&#20986;&#38169;&#19988;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#30340;&#23384;&#22312;&#26159;&#20026;&#20102;&#31616;&#21270;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#20294;&#25991;&#29486;&#20013;&#32570;&#20047;&#20851;&#20110;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#21487;&#24494;&#20998;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(DNAS)&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#30340;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;MicroNAS&#65292;&#36825;&#26159;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;DNAS&#12289;&#24310;&#36831;&#26597;&#25214;&#34920;&#12289;&#21160;&#24577;&#21367;&#31215;&#21644;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#26032;&#22411;&#25628;&#32034;&#31354;&#38388;&#12290;&#25152;&#24471;&#21040;&#30340;&#31995;&#32479;&#20855;&#26377;&#30828;&#20214;&#24863;&#30693;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#25191;&#34892;&#24310;&#36831;&#21644;&#23792;&#20540;&#20869;&#23384;&#28040;&#32791;&#38480;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#24494;&#25511;&#21046;&#22120;&#21644;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;MicroNAS&#21487;&#20197;&#25214;&#21040;&#36866;&#29992;&#20110;&#24494;&#25511;&#21046;&#22120;&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#24615;&#33021;(F1-score)&#30340;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing domain specific neural networks is a time-consuming, error-prone, and expensive task. Neural Architecture Search (NAS) exists to simplify domain-specific model development but there is a gap in the literature for time series classification on microcontrollers. Therefore, we adapt the concept of differentiable neural architecture search (DNAS) to solve the time-series classification problem on resource-constrained microcontrollers (MCUs). We introduce MicroNAS, a domain-specific HW-NAS system integration of DNAS, Latency Lookup Tables, dynamic convolutions and a novel search space specifically designed for time-series classification on MCUs. The resulting system is hardware-aware and can generate neural network architectures that satisfy user-defined limits on the execution latency and peak memory consumption. Our extensive studies on different MCUs and standard benchmark datasets demonstrate that MicroNAS finds MCU-tailored architectures that achieve performance (F1-score) ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HelmFluid&#65292;&#19968;&#20010;&#31934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#27969;&#20307;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#65292;&#23558;&#27969;&#20307;&#21160;&#21147;&#23398;&#20998;&#35299;&#20026;&#26356;&#21487;&#35299;&#30340;&#26080;&#26059;&#21644;&#26080;&#25955;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#22810;&#23610;&#24230;&#22810;&#22836;&#31215;&#20998;&#26550;&#26500;&#36827;&#34892;&#38598;&#25104;&#65292;HelmFluid&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#27969;&#20307;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2310.10565</link><description>&lt;p&gt;
HelmFluid&#65306;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#27969;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HelmFluid&#65292;&#19968;&#20010;&#31934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#27969;&#20307;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#65292;&#23558;&#27969;&#20307;&#21160;&#21147;&#23398;&#20998;&#35299;&#20026;&#26356;&#21487;&#35299;&#30340;&#26080;&#26059;&#21644;&#26080;&#25955;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#22810;&#23610;&#24230;&#22810;&#22836;&#31215;&#20998;&#26550;&#26500;&#36827;&#34892;&#38598;&#25104;&#65292;HelmFluid&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#27969;&#20307;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#20307;&#39044;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#28145;&#24230;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#33021;&#21147;&#30452;&#25509;&#20272;&#35745;&#26410;&#26469;&#39044;&#27979;&#30340;&#36895;&#24230;&#22330;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23398;&#20064;&#34920;&#38754;&#36895;&#24230;&#22330;&#32780;&#36339;&#36807;&#22266;&#26377;&#30340;&#29289;&#29702;&#29305;&#24615;&#23558;&#23548;&#33268;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#31934;&#30830;&#25110;&#20855;&#26377;&#29289;&#29702;&#21487;&#38752;&#24615;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HelmFluid&#65292;&#38024;&#23545;&#27969;&#20307;&#30340;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#12290;&#21463;Helmholtz&#23450;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;HelmDynamics&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#65292;&#23558;&#27969;&#20307;&#21160;&#21147;&#23398;&#20998;&#35299;&#20026;&#26356;&#21487;&#35299;&#30340;&#26080;&#26059;&#21644;&#26080;&#25955;&#37096;&#20998;&#65292;&#29289;&#29702;&#19978;&#23545;&#24212;&#20110;&#27969;&#20307;&#30340;&#21183;&#20989;&#25968;&#21644;&#27969;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;HelmDynamics&#27169;&#22359;&#23884;&#20837;&#21040;&#22810;&#23610;&#24230;&#22810;&#22836;&#31215;&#20998;&#26550;&#26500;&#20013;&#65292;HelmFluid&#21487;&#20197;&#22312;&#22810;&#20010;&#31354;&#38388;&#23610;&#24230;&#19978;&#27839;&#26102;&#38388;&#32500;&#24230;&#38598;&#25104;&#23398;&#21040;&#30340;Helmholtz&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#20135;&#29983;&#26410;&#26469;&#30340;&#27969;&#20307;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HelmFluid&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27969;&#20307;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#27969;&#20307;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fluid prediction is a long-standing challenge due to the intrinsic high-dimensional non-linear dynamics. Previous methods usually utilize the non-linear modeling capability of deep models to directly estimate velocity fields for future prediction. However, skipping over inherent physical properties but directly learning superficial velocity fields will overwhelm the model from generating precise or physics-reliable results. In this paper, we propose the HelmFluid toward an accurate and interpretable predictor for fluid. Inspired by the Helmholtz theorem, we design a HelmDynamics block to learn Helmholtz dynamics, which decomposes fluid dynamics into more solvable curl-free and divergence-free parts, physically corresponding to potential and stream functions of fluid. By embedding the HelmDynamics block into a Multiscale Multihead Integral Architecture, HelmFluid can integrate learned Helmholtz dynamics along temporal dimension in multiple spatial scales to yield future fluid. Compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;3D&#24418;&#29366;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#28151;&#21512;&#31995;&#32479;&#21644;&#36830;&#32493;&#21487;&#24494;&#30340;&#35299;&#30721;&#22120;&#65292;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65292;&#36824;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20165;&#20351;&#29992;&#38646;&#27700;&#24179;&#38598;&#30340;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#38024;&#23545;&#26354;&#38754;&#27861;&#32447;&#19981;&#23384;&#22312;&#24773;&#20917;&#30340;&#25439;&#22833;&#20989;&#25968;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2310.06644</link><description>&lt;p&gt;
&#31070;&#32463;&#36317;&#31163;&#22330;&#30340;&#38646;&#27700;&#24179;&#38598;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Zero-Level-Set Encoder for Neural Distance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;3D&#24418;&#29366;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#28151;&#21512;&#31995;&#32479;&#21644;&#36830;&#32493;&#21487;&#24494;&#30340;&#35299;&#30721;&#22120;&#65292;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65292;&#36824;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20165;&#20351;&#29992;&#38646;&#27700;&#24179;&#38598;&#30340;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#38024;&#23545;&#26354;&#38754;&#27861;&#32447;&#19981;&#23384;&#22312;&#24773;&#20917;&#30340;&#25439;&#22833;&#20989;&#25968;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#29366;&#34920;&#31034;&#36890;&#24120;&#25351;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;3D&#20960;&#20309;&#65292;&#20363;&#22914;&#65292;&#22312;&#29305;&#23450;&#31354;&#38388;&#20301;&#32622;&#35745;&#31639;&#26377;&#31526;&#21495;&#36317;&#31163;&#25110;&#21344;&#25454;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23884;&#20837;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22522;&#20110;&#22810;&#23610;&#24230;&#28151;&#21512;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#22270;&#24418;&#21644;&#22522;&#20110;&#20307;&#32032;&#30340;&#32452;&#20214;&#65292;&#20197;&#21450;&#36830;&#32493;&#21487;&#24494;&#30340;&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#20197;&#35299;&#20915;Eikonal&#26041;&#31243;&#65292;&#20165;&#38656;&#35201;&#38646;&#27700;&#24179;&#38598;&#30340;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#19982;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#36755;&#20986;&#26377;&#25928;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#38750;&#38646;&#36317;&#31163;&#20540;&#25110;&#24418;&#29366;&#21344;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20462;&#25913;&#65292;&#20197;&#35299;&#20915;&#26354;&#38754;&#27861;&#32447;&#19981;&#23384;&#22312;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#65292;&#38750;&#23553;&#38381;&#26354;&#38754;&#21644;&#38750;&#27969;&#24418;&#20960;&#20309;&#30340;&#19978;&#19979;&#25991;&#12290;&#24635;&#20307;&#19978;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#24517;&#35201;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural shape representation generally refers to representing 3D geometry using neural networks, e.g., to compute a signed distance or occupancy value at a specific spatial position. In this paper, we present a novel encoder-decoder neural network for embedding 3D shapes in a single forward pass. Our architecture is based on a multi-scale hybrid system incorporating graph-based and voxel-based components, as well as a continuously differentiable decoder. Furthermore, the network is trained to solve the Eikonal equation and only requires knowledge of the zero-level set for training and inference. This means that in contrast to most previous work, our network is able to output valid signed distance fields without explicit prior knowledge of non-zero distance values or shape occupancy. We further propose a modification of the loss function in case that surface normals are not well defined, e.g., in the context of non-watertight surfaces and non-manifold geometry. Overall, this can help red
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;</title><link>https://arxiv.org/abs/2310.05707</link><description>&lt;p&gt;
&#29992;&#35268;&#21010;&#26631;&#35760;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Model Math Reasoning with Planning Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#22914;&#24605;&#32500;&#38142;&#25512;&#29702;&#65289;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24573;&#35270;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#32467;&#26500;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;LLMs&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#20010;&#21035;&#25512;&#29702;&#27493;&#39588;&#65292;&#20294;&#22312;&#25972;&#20010;&#25512;&#29702;&#38142;&#19978;&#20445;&#25345;&#19968;&#33268;&#24615;&#26041;&#38754;&#21364;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#24320;&#22987;&#22788;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#65292;&#20316;&#20026;&#27169;&#22411;&#30340;&#24341;&#23548;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#28155;&#21152;&#21040;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22686;&#21152;&#38750;&#24120;&#23567;&#65288;&#20165;&#20026;0.001%&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#23436;&#20840;&#24494;&#35843;&#25110;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#26041;&#26696;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;LLMs&#65292;&#22312;&#19977;&#20010;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27169;&#22411;&#21253;&#25324;&#20840;&#23616;&#36235;&#21183;&#20174;&#21152;&#27861;&#21040;&#20056;&#27861;&#30340;&#24179;&#28369;&#21464;&#21270;&#65292;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20056;&#27861;&#23395;&#33410;&#24615;&#21644;&#24322;&#26041;&#24046;&#30340;&#21152;&#27861;&#35823;&#24046;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#22312;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.13950</link><description>&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#21152;&#27861;&#21644;&#20056;&#27861;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#29992;&#20110;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24320;&#21457;&#26159;&#22522;&#20110;&#24555;&#36895;&#22686;&#38271;&#12289;&#27874;&#21160;&#24615;&#36739;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20855;&#26377;&#20174;&#21152;&#27861;&#21040;&#20056;&#27861;&#24179;&#28369;&#21464;&#21270;&#30340;&#20840;&#23616;&#36235;&#21183;&#65292;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#23395;&#33410;&#24615;(&#22914;&#26524;&#26377;&#20351;&#29992;)&#26159;&#20056;&#27861;&#30340;&#65292;&#35823;&#24046;&#22987;&#32456;&#26159;&#21152;&#27861;&#30340;&#65292;&#20294;&#20855;&#26377;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#21487;&#36890;&#36807;&#21442;&#25968;sigma&#22686;&#38271;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#20934;&#30830;&#25311;&#21512;&#36825;&#20123;&#27604;&#26631;&#20934;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#26356;&#22797;&#26434;&#12289;&#26356;&#28789;&#27963;&#30340;&#27169;&#22411;&#12290;&#24403;&#24212;&#29992;&#20110;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#31454;&#36187;&#20013;&#30340;&#26368;&#20339;&#31639;&#27861;&#21644;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#21462;&#24471;&#20102;&#27599;&#20010;&#24207;&#21015;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local and Global Trend Bayesian Exponential Smoothing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27169;&#22411;&#21253;&#25324;&#20840;&#23616;&#36235;&#21183;&#20174;&#21152;&#27861;&#21040;&#20056;&#27861;&#30340;&#24179;&#28369;&#21464;&#21270;&#65292;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20056;&#27861;&#23395;&#33410;&#24615;&#21644;&#24322;&#26041;&#24046;&#30340;&#21152;&#27861;&#35823;&#24046;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#22312;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#21152;&#27861;&#21644;&#20056;&#27861;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#29992;&#26469;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#20840;&#23616;&#36235;&#21183;&#65292;&#21487;&#20197;&#20174;&#21152;&#27861;&#24179;&#28369;&#24179;&#28369;&#22320;&#36716;&#21464;&#20026;&#20056;&#27861;&#24179;&#28369;&#65292;&#24182;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#30340;&#23395;&#33410;&#24615;&#26159;&#20056;&#27861;&#30340;&#65292;&#35823;&#24046;&#22987;&#32456;&#26159;&#21152;&#27861;&#30340;&#65292;&#20294;&#20855;&#26377;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;sigma&#22686;&#38271;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#20934;&#30830;&#25311;&#21512;&#36825;&#20123;&#27604;&#26631;&#20934;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#26356;&#22797;&#26434;&#12289;&#26356;&#28789;&#27963;&#30340;&#27169;&#22411;&#12290;&#22312;&#24212;&#29992;&#20110;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#31454;&#36187;&#20013;&#30340;&#26368;&#20339;&#31639;&#27861;&#21644;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#21462;&#24471;&#20102;&#27599;&#20010;&#24207;&#21015;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a family of seasonal and non-seasonal time series models that can be viewed as generalisations of additive and multiplicative exponential smoothing models, to model series that grow faster than linear but slower than exponential. Their development is motivated by fast-growing, volatile time series. In particular, our models have a global trend that can smoothly change from additive to multiplicative, and is combined with a linear local trend. Seasonality when used is multiplicative in our models, and the error is always additive but is heteroscedastic and can grow through a parameter sigma. We leverage state-of-the-art Bayesian fitting techniques to accurately fit these models that are more complex and flexible than standard exponential smoothing models. When applied to the M3 competition data set, our models outperform the best algorithms in the competition as well as other benchmarks, thus achieving to the best of our knowledge the best results of per-series univ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;DiffusionWorldViewer&#65292;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#25299;&#23485;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#12290;&#36890;&#36807;&#22312;&#36755;&#20986;&#30340;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20043;&#38388;&#25581;&#31034;&#19990;&#30028;&#35266;&#65292;&#24182;&#25552;&#20379;&#32534;&#36753;&#24037;&#20855;&#65292;&#24110;&#21161;&#29992;&#25143;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20195;&#34920;&#20182;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#28857;&#65292;&#24182;&#25361;&#25112;&#24403;&#21069;&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#26377;&#38480;&#19990;&#30028;&#35266;&#12290;</title><link>https://arxiv.org/abs/2309.09944</link><description>&lt;p&gt;
DiffusionWorldViewer&#65306;&#25581;&#31034;&#21644;&#25299;&#23485;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21453;&#26144;&#30340;&#19990;&#30028;&#35266;
&lt;/p&gt;
&lt;p&gt;
DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by Generative Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;DiffusionWorldViewer&#65292;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#25299;&#23485;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#12290;&#36890;&#36807;&#22312;&#36755;&#20986;&#30340;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20043;&#38388;&#25581;&#31034;&#19990;&#30028;&#35266;&#65292;&#24182;&#25552;&#20379;&#32534;&#36753;&#24037;&#20855;&#65292;&#24110;&#21161;&#29992;&#25143;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20195;&#34920;&#20182;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#28857;&#65292;&#24182;&#25361;&#25112;&#24403;&#21069;&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#26377;&#38480;&#19990;&#30028;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#22312;&#23398;&#26415;&#21644;&#21019;&#24847;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;TTI&#27169;&#22411;&#26377;&#19968;&#20010;&#19990;&#30028;&#35266;&#65292;&#21363;&#20174;&#35757;&#32451;&#25968;&#25454;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#65292;&#36825;&#20250;&#24433;&#21709;&#23427;&#20204;&#20026;&#32473;&#23450;&#25552;&#31034;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;TTI&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#36890;&#24120;&#23545;&#29992;&#25143;&#38544;&#34255;&#65292;&#36825;&#20351;&#29992;&#25143;&#38590;&#20197;&#24314;&#31435;&#23545;TTI&#36755;&#20986;&#30340;&#30452;&#35273;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#24120;&#19982;&#29992;&#25143;&#30340;&#19990;&#30028;&#35266;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#36755;&#20986;&#30340;&#22270;&#20687;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#26399;&#26395;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffusionWorldViewer&#65292;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21487;&#22312;&#36755;&#20986;&#30340;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20043;&#38388;&#25581;&#31034;TTI&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#65292;&#24182;&#25552;&#20379;&#32534;&#36753;&#24037;&#20855;&#20197;&#20351;&#36755;&#20986;&#22270;&#20687;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#19968;&#33268;&#12290;&#22312;&#23545;18&#20301;&#22810;&#26679;&#21270;TTI&#29992;&#25143;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;DiffusionWorldViewer&#24110;&#21161;&#29992;&#25143;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20195;&#34920;&#20182;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#28857;&#65292;&#24182;&#25361;&#25112;&#24403;&#21069;TTI&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#26377;&#38480;&#19990;&#30028;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative text-to-image (TTI) models produce high-quality images from short textual descriptions and are widely used in academic and creative domains. Like humans, TTI models have a worldview, a conception of the world learned from their training data and task that influences the images they generate for a given prompt. However, the worldviews of TTI models are often hidden from users, making it challenging for users to build intuition about TTI outputs, and they are often misaligned with users' worldviews, resulting in output images that do not match user expectations. In response, we introduce DiffusionWorldViewer, an interactive interface that exposes a TTI model's worldview across output demographics and provides editing tools for aligning output images with user perspectives. In a user study with 18 diverse TTI users, we find that DiffusionWorldViewer helps users represent their varied viewpoints in generated images and challenge the limited worldview reflected in current TTI mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36817;&#20284;O(T^3/4)&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2309.01922</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36817;&#20284;O(T^3/4)&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#30456;&#20851;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36890;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35299;&#25918;&#20102;&#23427;&#22312;&#20551;&#35774;&#32447;&#24615;MDP&#32467;&#26500;&#30340;&#38480;&#21046;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20840;&#23616;&#25910;&#25947;&#24615;&#36136;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#36817;&#20284;O(T^3/4)&#30340;&#36951;&#25022;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#24179;&#22343;&#22870;&#21169;&#22330;&#26223;&#19979;&#65292;&#23545;&#20110;&#36890;&#29992;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has $\tilde{\mathcal{O}}({T}^{3/4})$ regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret-bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#21518;&#39564;&#25277;&#26679;&#25110;BayesUCB&#36827;&#34892;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10699</link><description>&lt;p&gt;
&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#65306;&#19968;&#31181;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#21518;&#39564;&#25277;&#26679;&#25110;BayesUCB&#36827;&#34892;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20915;&#31574;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20915;&#31574;&#26159;&#22522;&#20110;&#23545;&#20256;&#20837;&#25968;&#25454;&#28857;&#36827;&#34892;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#25191;&#34892;&#25152;&#26377;&#27979;&#35797;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#65292;&#24182;&#32771;&#34385;&#20102;&#25191;&#34892;&#27979;&#35797;&#30340;&#65288;&#21487;&#33021;&#26159;&#38543;&#26426;&#30340;&#65289;&#25104;&#26412;&#12290;&#22522;&#20110;&#36825;&#31181;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#21518;&#39564;&#25277;&#26679;&#25110;BayesUCB&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#23545;&#29992;&#20110;&#25104;&#26412;&#26377;&#25928;&#22312;&#32447;&#20915;&#31574;&#30340;Thompson&#25277;&#26679;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online decision making plays a crucial role in numerous real-world applications. In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points. However, performing all tests can be expensive and is not always possible. In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the (possibly stochastic) cost of performing tests into account. Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration. We provide a theoretical analysis of Thompson Sampling for cost-efficient online decision making, and present various experimental results that demonstrate the applicability of our framework to real-world problems.
&lt;/p&gt;</description></item><item><title>UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2307.16375</link><description>&lt;p&gt;
UniAP: &#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#33258;&#21160;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16375
&lt;/p&gt;
&lt;p&gt;
UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#24120;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#25163;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#24182;&#19988;&#28789;&#27963;&#24615;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#24182;&#34892;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#23384;&#22312;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20250;&#21516;&#26102;&#20248;&#21270;&#36328;&#23618;&#24182;&#34892;&#21270;&#21644;&#20869;&#23618;&#24182;&#34892;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniAP&#30340;&#26032;&#22411;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;UniAP&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#20197;&#27714;&#24471;&#26368;&#20248;&#35299;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26368;&#22810;1.71&#20493;&#65292;&#24182;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 1.71$\times$ in throughput and reduces strategy optimizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#29109;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2307.01171</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#29109;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Quantum Neural Estimation of Entropies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.01171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#29109;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#24230;&#37327;&#37327;&#23376;&#31995;&#32479;&#20013;&#20449;&#24687;&#21644;&#30456;&#20851;&#24615;&#30340;&#37327;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24403;&#37327;&#23376;&#24577;&#26410;&#30693;&#19988;&#20165;&#26377;&#20854;&#21103;&#26412;&#21487;&#29992;&#26102;&#65292;&#24517;&#39035;&#36827;&#34892;&#29109;&#24230;&#37327;&#30340;&#20272;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#20911;&#35834;&#20381;&#26364;&#29109;&#12289;R\'enyi&#29109;&#12289;&#27979;&#37327;&#30456;&#23545;&#29109;&#21644;&#27979;&#37327;R\'enyi&#30456;&#23545;&#29109;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#37327;&#23376;&#30005;&#36335;&#21644;&#19968;&#20010;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#23545;&#24863;&#20852;&#36259;&#30340;&#24230;&#37327;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#28982;&#21518;&#22312;&#21442;&#25968;&#31354;&#38388;&#19978;&#20248;&#21270;&#24471;&#21040;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#26080;&#22122;&#22768;&#37327;&#23376;&#27169;&#25311;&#22120;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;&#35813;&#31639;&#27861;&#23545;&#25152;&#27979;&#35797;&#30340;&#21508;&#31181;&#29109;&#24230;&#37327;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#34920;&#26126;&#23427;&#26159;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropy measures quantify the amount of information and correlation present in a quantum system. In practice, when the quantum state is unknown and only copies thereof are available, one must resort to the estimation of such entropy measures. Here we propose a variational quantum algorithm for estimating the von Neumann and R\'enyi entropies, as well as the measured relative entropy and measured R\'enyi relative entropy. Our approach first parameterizes a variational formula for the measure of interest by a quantum circuit and a classical neural network, and then optimizes the resulting objective over parameter space. Numerical simulations of our quantum algorithm are provided, using a noiseless quantum simulator. The algorithm provides accurate estimates of the various entropy measures for the examples tested, which renders it as a promising approach for usage in downstream tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;STAR&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#39640;&#24230;&#19981;&#23436;&#25972;&#30340;&#20449;&#36947;&#27979;&#37327;&#20013;&#37325;&#24314;&#20154;&#20307;&#36816;&#21160;&#30340;&#24494;&#22810;&#26222;&#21202;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2306.14233</link><description>&lt;p&gt;
&#31232;&#30095;&#24207;&#21015;&#24494;&#22810;&#26222;&#21202;&#37325;&#24314;&#30340;&#27880;&#24847;&#21147;&#20248;&#21270;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Attention-Refined Unrolling for Sparse Sequential micro-Doppler Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14233
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;STAR&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#39640;&#24230;&#19981;&#23436;&#25972;&#30340;&#20449;&#36947;&#27979;&#37327;&#20013;&#37325;&#24314;&#20154;&#20307;&#36816;&#21160;&#30340;&#24494;&#22810;&#26222;&#21202;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20154;&#20307;&#36816;&#21160;&#30340;&#24494;&#22810;&#26222;&#21202;&#29305;&#24449;&#30340;&#37325;&#24314;&#26159;&#32454;&#31890;&#24230;&#27963;&#21160;&#35782;&#21035;&#26080;&#32447;&#24863;&#30693;&#30340;&#20851;&#38190;&#12290;&#22312;&#32852;&#21512;&#36890;&#20449;&#21644;&#24863;&#30693;&#31995;&#32479;&#20013;&#65292;&#19982;&#19987;&#29992;&#38647;&#36798;&#24863;&#30693;&#31995;&#32479;&#19981;&#21516;&#65292;&#24517;&#39035;&#22312;&#24863;&#30693;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#24320;&#38144;&#20043;&#38388;&#21462;&#24471;&#36866;&#24403;&#30340;&#25240;&#34935;&#12290;&#22240;&#27492;&#65292;&#24494;&#22810;&#26222;&#21202;&#24517;&#39035;&#20174;&#20174;&#36890;&#20449;&#25968;&#25454;&#21253;&#20013;&#33719;&#21462;&#30340;&#19981;&#23436;&#25972;&#30340;&#20449;&#36947;&#20272;&#35745;&#31383;&#21475;&#20013;&#37325;&#24314;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#65292;&#20294;&#22312;&#21482;&#26377;&#23569;&#37327;&#20449;&#36947;&#27979;&#37327;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65288;&#36825;&#22312;&#23454;&#38469;&#36890;&#20449;&#27169;&#24335;&#20013;&#32463;&#24120;&#21457;&#29983;&#65289;&#26102;&#20135;&#29983;&#38750;&#24120;&#24046;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#26102;&#31995;&#32479;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;STAR&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#19981;&#23436;&#25972;&#30340;&#20449;&#36947;&#27979;&#37327;&#26465;&#20214;&#19979;&#65292;&#20063;&#33021;&#37325;&#24314;&#20154;&#20307;&#36816;&#21160;&#30340;&#24494;&#22810;&#26222;&#21202;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reconstruction of micro-Doppler signatures of human movements is a key enabler for fine-grained activity recognition wireless sensing. In Joint Communication and Sensing (JCS) systems, unlike in dedicated radar sensing systems, a suitable trade-off between sensing accuracy and communication overhead has to be attained. It follows that the micro-Doppler has to be reconstructed from incomplete windows of channel estimates obtained from communication packets. Existing approaches exploit compressed sensing, but produce very poor reconstructions when only a few channel measurements are available, which is often the case with real communication patterns. In addition, the large number of iterations they need to converge hinders their use in real-time systems. In this work, we propose and validate STAR, a neural network that reconstructs micro-Doppler sequences of human movement even from highly incomplete channel measurements. STAR is based upon a new architectural design that combines a 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25991;&#31456;&#23545;&#19978;&#19979;&#25991;&#20248;&#21270;&#39046;&#22495;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23558;&#39044;&#27979;&#31639;&#27861;&#21644;&#20248;&#21270;&#25216;&#26415;&#32467;&#21512;&#35299;&#20915;&#19981;&#30830;&#23450;&#20915;&#31574;&#38382;&#39064;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#25991;&#31456;&#36890;&#36807;&#30740;&#31350;&#21333;&#38454;&#27573;&#21644;&#20004;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#38382;&#39064;&#65292;&#30830;&#23450;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2306.10374</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#20248;&#21270;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Contextual Optimization Methods for Decision Making under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25991;&#31456;&#23545;&#19978;&#19979;&#25991;&#20248;&#21270;&#39046;&#22495;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23558;&#39044;&#27979;&#31639;&#27861;&#21644;&#20248;&#21270;&#25216;&#26415;&#32467;&#21512;&#35299;&#20915;&#19981;&#30830;&#23450;&#20915;&#31574;&#38382;&#39064;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#25991;&#31456;&#36890;&#36807;&#30740;&#31350;&#21333;&#38454;&#27573;&#21644;&#20004;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#38382;&#39064;&#65292;&#30830;&#23450;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#23545;&#20110;&#23558;&#39044;&#27979;&#31639;&#27861;&#21644;&#20248;&#21270;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#20197;&#35299;&#20915;&#38754;&#20020;&#19981;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#38382;&#39064;&#20135;&#29983;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#36825;&#20652;&#29983;&#20102;&#19978;&#19979;&#25991;&#20248;&#21270;&#39046;&#22495;&#65292;&#20854;&#20013;&#24320;&#21457;&#20986;&#25968;&#25454;&#39537;&#21160;&#30340;&#31243;&#24207;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26368;&#26032;&#26356;&#26032;&#20449;&#24687;&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;&#22312;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12289;&#35268;&#23450;&#24615;&#20248;&#21270;&#12289;&#39044;&#27979;&#24615;&#38543;&#26426;&#35268;&#21010;&#12289;&#31574;&#30053;&#20248;&#21270;&#12289;(&#26234;&#33021;)&#39044;&#27979;/&#20272;&#35745;-&#20248;&#21270;&#12289;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#23398;&#20064;&#12289;(&#22522;&#20110;&#20219;&#21153;&#30340;)&#31471;&#21040;&#31471;&#23398;&#20064;/&#39044;&#27979;/&#20248;&#21270;&#31561;&#31561;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#20851;&#27880;&#21333;&#38454;&#27573;&#21644;&#20004;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#38382;&#39064;&#65292;&#30830;&#23450;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. Focusing on single and two-stage stochastic programming problems, this review article identifies three main frameworks for learning policies from data and discusses their strengths and li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;&#65288;FID&#65289;&#26469;&#35843;&#26597;&#25968;&#25454;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.01704</link><description>&lt;p&gt;
&#25968;&#25454;&#20559;&#24046;&#35843;&#26597;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Feature Importance Disparities for Data Bias Investigations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;&#65288;FID&#65289;&#26469;&#35843;&#26597;&#25968;&#25454;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#35748;&#20026;&#65292;&#20998;&#31867;&#22120;&#20013;&#30340;&#19979;&#28216;&#20559;&#24046;&#30340;&#19968;&#31181;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#12290;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#28041;&#21450;&#21040;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20363;&#22914;&#22312;&#23376;&#38598;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#22312;&#25910;&#38598;&#36807;&#31243;&#20013;&#21024;&#38500;&#20855;&#26377;&#20559;&#24046;&#30340;&#29305;&#24449;&#65292;&#29978;&#33267;&#36827;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20197;&#30830;&#23450;&#20559;&#24046;&#28304;&#12290;&#23613;&#31649;&#38656;&#35201;&#36827;&#34892;&#36825;&#26679;&#30340;&#25968;&#25454;&#20559;&#24046;&#35843;&#26597;&#65292;&#20294;&#30446;&#21069;&#24456;&#23569;&#26377;&#33258;&#21160;&#21270;&#26041;&#27861;&#21487;&#20197;&#36741;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#36825;&#20123;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32473;&#23450;&#25968;&#25454;&#38598;$X$&#65292;&#21253;&#25324;&#20445;&#25252;&#21644;&#19981;&#20445;&#25252;&#30340;&#29305;&#24449;&#65292;&#32467;&#26524;$y$&#65292;&#20197;&#21450;&#19968;&#20010;&#39044;&#27979;&#32473;&#23450;$X$&#30340;&#22238;&#24402;&#22120;$h$&#30340;&#20803;&#32452;$(f_j, g)$, &#20854;&#20013;$g$&#23545;&#24212;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;$(X, y)$&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#31532;$j$&#20010;&#29305;&#24449;$f_j$&#22312;&#23376;&#32452;$g$&#20013;&#30340;&#24433;&#21709;&#35201;&#27604;&#25972;&#20307;&#25968;&#25454;&#38598;&#20013;&#22823;&#24471;&#22810;&#65288;&#25110;&#32773;&#23567;&#24471;&#22810;&#65289;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;&#65288;FID&#65289;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#21644;4&#20010;&#24120;&#35265;&#29305;&#24449;&#37325;&#35201;&#24615;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) influence in the subgroup $g$, than on the dataset overall, which we call feature importance disparity (FID). We show across $4$ datasets and $4$ common feature import
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2302.05793</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Distributional GFlowNets with Quantile Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#37319;&#26679;&#22120;&#31995;&#21015;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19968;&#31995;&#21015;&#20915;&#31574;&#27493;&#39588;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#30340;&#38543;&#26426;&#31574;&#30053;&#12290;&#23613;&#31649;&#21463;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#65292;&#24403;&#21069;&#30340;GFlowNet&#26694;&#26550;&#22312;&#36866;&#29992;&#24615;&#19978;&#30456;&#23545;&#26377;&#38480;&#65292;&#26080;&#27861;&#22788;&#29702;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#33539;&#24335;&#26469;&#22788;&#29702;GFlowNets&#65292;&#23558;&#27599;&#20010;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#23545;&#27599;&#20010;&#36793;&#27969;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#37327;&#21270;&#21305;&#37197;&#8221; GFlowNet&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#36825;&#26159;&#22788;&#29702;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#30001;&#20110;&#25105;&#20204;&#22686;&#24378;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#23454;&#29616;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12289;&#35757;&#32451;&#25216;&#26415;&#21644;&#29983;&#25104;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36739;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2302.05737</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Reparameterized Discrete Diffusion Model for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12289;&#35757;&#32451;&#25216;&#26415;&#21644;&#29983;&#25104;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36739;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20174;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#20013;&#37319;&#26679;&#30340;&#21478;&#19968;&#31181;&#31561;&#20215;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#24320;&#21457;&#20102;&#19968;&#26063;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#20010;&#27966;&#29983;&#30340;&#36890;&#29992;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#20026;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#22791;&#26356;&#26377;&#25928;&#30340;&#35757;&#32451;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#21644;&#20248;&#21270;&#25552;&#20986;&#20102;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#36890;&#29992;GMM&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#32467;&#21512;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#23616;&#37096;&#26368;&#23567;&#20540;&#25968;&#37327;&#36739;&#22810;&#20197;&#21450;&#35299;&#20915;&#26041;&#26696;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2302.02450</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#21644;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regularization and Optimization in Model-Based Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#21644;&#20248;&#21270;&#25552;&#20986;&#20102;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#36890;&#29992;GMM&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#32467;&#21512;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#23616;&#37096;&#26368;&#23567;&#20540;&#25968;&#37327;&#36739;&#22810;&#20197;&#21450;&#35299;&#20915;&#26041;&#26696;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23427;&#20204;&#30340;&#27010;&#24565;&#31616;&#21333;&#24615;&#65292;k-means&#31639;&#27861;&#30340;&#21464;&#20307;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#32858;&#31867;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22522;&#26412;&#19978;&#23558;&#30456;&#21516;&#30340;&#29699;&#29366;&#39640;&#26031;&#28151;&#21512;&#36866;&#29992;&#20110;&#19982;&#36825;&#31181;&#20998;&#24067;&#22823;&#30456;&#24452;&#24237;&#30340;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#29992;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21487;&#20197;&#36866;&#24212;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#20272;&#35745;&#27599;&#20010;&#31751;&#34920;&#31034;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20108;&#27425;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;&#36825;&#24102;&#26469;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#65288;i&#65289;&#30001;&#20110;&#23616;&#37096;&#26368;&#23567;&#20540;&#25968;&#37327;&#36739;&#22810;&#65292;&#24213;&#23618;&#30340;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#65288;ii&#65289;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26082;&#33021;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#25628;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26356;&#26377;&#25928;&#30340;&#36890;&#29992;GMM&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#31639;&#27861;&#19982;&#36991;&#20813;&#36807;&#25311;&#21512;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21333;&#29420;&#36827;&#34892;&#20248;&#21270;&#25110;&#27491;&#21017;&#21270;&#19981;&#20250;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their conceptual simplicity, k-means algorithm variants have been extensively used for unsupervised cluster analysis. However, one main shortcoming of these algorithms is that they essentially fit a mixture of identical spherical Gaussians to data that vastly deviates from such a distribution. In comparison, general Gaussian Mixture Models (GMMs) can fit richer structures but require estimating a quadratic number of parameters per cluster to represent the covariance matrices. This poses two main issues: (i) the underlying optimization problems are challenging due to their larger number of local minima, and (ii) their solutions can overfit the data. In this work, we design search strategies that circumvent both issues. We develop more effective optimization algorithms for general GMMs, and we combine these algorithms with regularization strategies that avoid overfitting. Through extensive computational analyses, we observe that optimization or regularization in isolation does not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32473;&#23450;&#38544;&#31169;&#32423;&#21035;&#19979;&#22522;&#20110;&#20844;&#27491;&#23450;&#20041;&#30340;&#20844;&#24179;&#34917;&#20607;&#29992;&#25143;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#38544;&#31169;&#32422;&#26463;&#65292;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#30340;&#25968;&#25454;&#20844;&#24179;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#12289;&#25968;&#25454;&#37327;&#21644;&#24322;&#36136;&#31243;&#24230;&#30340;&#20844;&#24179;&#20998;&#37197;&#19979;&#29992;&#25143;&#33719;&#24471;&#30340;&#34917;&#20607;&#37329;&#39069;&#65292;&#24182;&#19988;&#35752;&#35770;&#20102;&#24179;&#21488;&#34987;&#36843;&#35774;&#35745;&#20844;&#24179;&#28608;&#21169;&#25514;&#26045;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2301.13336</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#30340;&#20844;&#20801;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32473;&#23450;&#38544;&#31169;&#32423;&#21035;&#19979;&#22522;&#20110;&#20844;&#27491;&#23450;&#20041;&#30340;&#20844;&#24179;&#34917;&#20607;&#29992;&#25143;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#38544;&#31169;&#32422;&#26463;&#65292;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#30340;&#25968;&#25454;&#20844;&#24179;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#12289;&#25968;&#25454;&#37327;&#21644;&#24322;&#36136;&#31243;&#24230;&#30340;&#20844;&#24179;&#20998;&#37197;&#19979;&#29992;&#25143;&#33719;&#24471;&#30340;&#34917;&#20607;&#37329;&#39069;&#65292;&#24182;&#19988;&#35752;&#35770;&#20102;&#24179;&#21488;&#34987;&#36843;&#35774;&#35745;&#20844;&#24179;&#28608;&#21169;&#25514;&#26045;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#32858;&#21512;&#36890;&#24120;&#28041;&#21450;&#24179;&#21488;&#20174;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#38544;&#31169;&#36873;&#39033;&#30340;&#29992;&#25143;&#25910;&#38598;&#25968;&#25454;&#12290;&#24179;&#21488;&#24517;&#39035;&#35299;&#20915;&#22914;&#20309;&#21521;&#29992;&#25143;&#20998;&#37197;&#28608;&#21169;&#30340;&#38382;&#39064;&#65292;&#20197;&#35828;&#26381;&#20182;&#20204;&#20849;&#20139;&#20182;&#20204;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#26681;&#25454;&#19968;&#20010;&#20844;&#24179;&#30340;&#37329;&#39069;&#26469;&#34917;&#20607;&#29992;&#25143;&#30340;&#25968;&#25454;&#65292;&#22522;&#20110;&#19968;&#20010;&#20844;&#24179;&#23450;&#20041;&#30340;&#20844;&#24179;&#27010;&#24565;&#65292;&#31867;&#20284;&#20110;&#33879;&#21517;&#30340;Shapley&#20540;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#30340;&#25968;&#25454;&#20844;&#24179;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#20026;&#24179;&#21488;&#21046;&#23450;&#20102;&#19968;&#20010;&#20855;&#26377;&#29992;&#25143;&#38544;&#31169;&#32423;&#21035;&#36873;&#39033;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#12289;&#25968;&#25454;&#37327;&#21644;&#24322;&#36136;&#31243;&#24230;&#19979;&#20844;&#24179;&#20998;&#37197;&#19979;&#29992;&#25143;&#33719;&#24471;&#30340;&#34917;&#20607;&#37329;&#39069;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#24179;&#21488;&#34987;&#36843;&#35774;&#35745;&#20844;&#24179;&#28608;&#21169;&#25514;&#26045;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#38544;&#31169;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Modern data aggregation often involves a platform collecting data from a network of users with various privacy options. Platforms must solve the problem of how to allocate incentives to users to convince them to share their data. This paper puts forth an idea for a \textit{fair} amount to compensate users for their data at a given privacy level based on an axiomatic definition of fairness, along the lines of the celebrated Shapley value. To the best of our knowledge, these are the first fairness concepts for data that explicitly consider privacy constraints. We also formulate a heterogeneous federated learning problem for the platform with privacy level options for users. By studying this problem, we investigate the amount of compensation users receive under fair allocations with different privacy levels, amounts of data, and degrees of heterogeneity. We also discuss what happens when the platform is forced to design fair incentives. Under certain conditions we find that when privacy s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21457;&#29616;&#26410;&#32463;&#35757;&#32451;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#23454;&#29616;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;GNNs&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;GNNs&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20026;&#23454;&#29616;&#26356;&#28145;&#23618;GNNs&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2211.15335</link><description>&lt;p&gt;
&#19981;&#35757;&#32451;&#26435;&#37325;&#23601;&#33021;&#25317;&#26377;&#26356;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#21457;&#29616;&#26410;&#35757;&#32451;&#30340;GNN&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.15335
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#26410;&#32463;&#35757;&#32451;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#23454;&#29616;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;GNNs&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;GNNs&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20026;&#23454;&#29616;&#26356;&#28145;&#23618;GNNs&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#22320;&#35777;&#26126;&#65292;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#23384;&#22312;&#19968;&#20010;&#23376;&#32593;&#32476;&#65292;&#23427;&#22312;&#21021;&#22987;&#21270;&#26102;&#26080;&#38656;&#23545;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20248;&#21270;&#65288;&#21363;&#26410;&#35757;&#32451;&#32593;&#32476;&#65289;&#23601;&#33021;&#36798;&#21040;&#23436;&#20840;&#35757;&#32451;&#30340;&#31264;&#23494;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#30340;&#26410;&#35757;&#32451;&#23376;&#32593;&#32476;&#20173;&#28982;&#26159;&#20010;&#35868;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#21457;&#29616;&#21305;&#37197;&#30340;&#26410;&#35757;&#32451;GNNs&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#20316;&#20026;&#26680;&#24515;&#24037;&#20855;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#25214;&#21040;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;&#31264;&#23494;GNNs&#24615;&#33021;&#21305;&#37197;&#30340;&#26410;&#35757;&#32451;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#38500;&#20102;&#36825;&#20010;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21457;&#29616;&#30340;&#26410;&#35757;&#32451;&#23376;&#32593;&#32476;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;GNNs&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25104;&#20026;&#23454;&#29616;&#26356;&#28145;&#23618;GNNs&#32780;&#26080;&#38656;&#32321;&#29712;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#36825;&#26679;&#30340;&#31232;&#30095;&#26410;&#35757;&#32451;&#23376;&#32593;&#32476;&#20855;&#26377;&#36739;&#39640;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have impressively demonstrated that there exists a subnetwork in randomly initialized convolutional neural networks (CNNs) that can match the performance of the fully trained dense networks at initialization, without any optimization of the weights of the network (i.e., untrained networks). However, the presence of such untrained subnetworks in graph neural networks (GNNs) still remains mysterious. In this paper we carry out the first-of-its-kind exploration of discovering matching untrained GNNs. With sparsity as the core tool, we can find \textit{untrained sparse subnetworks} at the initialization, that can match the performance of \textit{fully trained dense} GNNs. Besides this already encouraging finding of comparable performance, we show that the found untrained subnetworks can substantially mitigate the GNN over-smoothing problem, hence becoming a powerful tool to enable deeper GNNs without bells and whistles. We also observe that such sparse untrained subnetworks ha
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#35745;&#25968;&#20013;&#20943;&#23567;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20854;&#22343;&#26041;&#35823;&#24046;&#27604;&#20108;&#36827;&#21046;&#26426;&#21046;&#23567;&#19968;&#20010;&#22240;&#23376;10&#12290;&#30740;&#31350;&#36824;&#32473;&#20986;&#20102;&#20960;&#20046;&#20005;&#26684;&#30340;&#24120;&#25968;&#30028;&#38480;&#65292;&#20197;&#21450;&#23545;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#36229;&#20986;&#39118;&#38505;&#30340;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2211.05006</link><description>&lt;p&gt;
&#20851;&#20110;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#35745;&#25968;&#30340;&#20960;&#20046;&#20005;&#26684;&#30340;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Almost Tight Error Bounds on Differentially Private Continual Counting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.05006
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#35745;&#25968;&#20013;&#20943;&#23567;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20854;&#22343;&#26041;&#35823;&#24046;&#27604;&#20108;&#36827;&#21046;&#26426;&#21046;&#23567;&#19968;&#20010;&#22240;&#23376;10&#12290;&#30740;&#31350;&#36824;&#32473;&#20986;&#20102;&#20960;&#20046;&#20005;&#26684;&#30340;&#24120;&#25968;&#30028;&#38480;&#65292;&#20197;&#21450;&#23545;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#36229;&#20986;&#39118;&#38505;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#30340;&#39318;&#20010;&#22823;&#35268;&#27169;&#37096;&#32626;&#22312;&#25345;&#32493;&#21457;&#24067;&#27169;&#22411;&#20013;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35745;&#25968;&#20316;&#20026;&#23376;&#31243;&#24207; (&#26631;&#39064;&#20026; "&#24102;&#26377;&#27491;&#24335;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#32852;&#37030;&#23398;&#20064;")&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#35823;&#24046;&#30340;&#20855;&#20307;&#30028;&#38480;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#20943;&#23567;&#38544;&#31169;&#21442;&#25968;&#12290;&#25345;&#32493;&#35745;&#25968;&#30340;&#26631;&#20934;&#26426;&#21046;&#26159;&#20108;&#36827;&#21046;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#65292;&#24182;&#19988;&#35777;&#26126;&#20854;&#22343;&#26041;&#35823;&#24046;&#26082;&#26159;&#28176;&#36817;&#20248;&#21270;&#30340;&#65292;&#21448;&#27604;&#20108;&#36827;&#21046;&#26426;&#21046;&#30340;&#35823;&#24046;&#23567;&#19968;&#20010;&#22240;&#23376;10&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#32473;&#20986;&#38750;&#28176;&#36817;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#20998;&#26512;&#20013;&#30340;&#24120;&#25968;&#20960;&#20046;&#26159;&#20005;&#26684;&#30340;&#65292;&#21482;&#22312;&#20302;&#38454;&#39033;&#30340;&#24120;&#25968;&#20013;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#35745;&#25968;&#30697;&#38453;&#30340;&#30697;&#38453;&#26426;&#21046;&#65292;&#24182;&#19988;&#27599;&#27425;&#21457;&#24067;&#37117;&#38656;&#35201;&#24120;&#25968;&#26102;&#38388;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#25105;&#20204;&#23545;&#35745;&#25968;&#30697;&#38453;&#30340;&#26126;&#30830;&#22240;&#24335;&#20998;&#35299;&#65292;&#32473;&#20986;&#20102;Denisov&#31561;&#20154;&#30340;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#20986;&#39118;&#38505;&#30340;&#19978;&#30028; (NeurIPS 2022)&#12290;
&lt;/p&gt;
&lt;p&gt;
The first large-scale deployment of private federated learning uses differentially private counting in the continual release model as a subroutine (Google AI blog titled "Federated Learning with Formal Differential Privacy Guarantees"). In this case, a concrete bound on the error is very relevant to reduce the privacy parameter. The standard mechanism for continual counting is the binary mechanism. We present a novel mechanism and show that its mean squared error is both asymptotically optimal and a factor 10 smaller than the error of the binary mechanism. We also show that the constants in our analysis are almost tight by giving non-asymptotic lower and upper bounds that differ only in the constants of lower-order terms. Our algorithm is a matrix mechanism for the counting matrix and takes constant time per release. We also use our explicit factorization of the counting matrix to give an upper bound on the excess risk of the private learning algorithm of Denisov et al. (NeurIPS 2022).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#24182;&#25233;&#21046;&#22122;&#22768;&#65292;&#20174;&#32780;&#36171;&#20104;&#35821;&#38899;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2209.03275</link><description>&lt;p&gt;
&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Multimodal Speech Enhancement Using Burst Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.03275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#24182;&#25233;&#21046;&#22122;&#22768;&#65292;&#20174;&#32780;&#36171;&#20104;&#35821;&#38899;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MBURST&#30340;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#32771;&#34385;&#20102;&#26377;&#20851;&#21069;&#39069;&#21494;&#30382;&#23618;&#21644;&#20854;&#20182;&#33041;&#21306;&#37329;&#23383;&#22612;&#32454;&#32990;&#30340;&#26368;&#26032;&#31070;&#32463;&#23398;&#21457;&#29616;&#12290;&#25152;&#35859;&#30340;&#29190;&#21457;&#20256;&#25773;&#36890;&#36807;&#21453;&#39304;&#26041;&#24335;&#23454;&#29616;&#20102;&#20960;&#20010;&#20934;&#21017;&#65292;&#20197;&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#30340;&#26041;&#24335;&#35299;&#20915;&#20449;&#20219;&#20998;&#37197;&#38382;&#39064;&#65306;&#36890;&#36807;&#21453;&#39304;&#25511;&#21046;&#22609;&#24615;&#30340;&#31526;&#21495;&#21644;&#24133;&#24230;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26435;&#37325;&#36830;&#25509;&#22312;&#21508;&#23618;&#20043;&#38388;&#22810;&#36335;&#22797;&#29992;&#21453;&#39304;&#21644;&#21069;&#39304;&#20449;&#24687;&#65292;&#36817;&#20284;&#21453;&#39304;&#21644;&#21069;&#39304;&#36830;&#25509;&#65292;&#24182;&#32447;&#24615;&#21270;&#21453;&#39304;&#20449;&#21495;&#12290;MBURST&#21033;&#29992;&#36825;&#20123;&#21151;&#33021;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#36890;&#36807;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#21644;&#25233;&#21046;&#22122;&#22768;&#36171;&#20104;&#35821;&#38899;&#20197;&#21547;&#20041;&#12290;&#22312;Grid Corpus&#21644;&#22522;&#20110;CHiME3&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MBURST&#33021;&#22815;&#22797;&#29616;&#31867;&#20284;&#30340;&#25513;&#27169;&#37325;&#24314;&#65292;&#19982;&#22810;&#27169;&#24577;&#21453;&#21521;&#20256;&#25773;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the MBURST, a novel multimodal solution for audio-visual speech enhancements that consider the most recent neurological discoveries regarding pyramidal cells of the prefrontal cortex and other brain regions. The so-called burst propagation implements several criteria to address the credit assignment problem in a more biologically plausible manner: steering the sign and magnitude of plasticity through feedback, multiplexing the feedback and feedforward information across layers through different weight connections, approximating feedback and feedforward connections, and linearizing the feedback signals. MBURST benefits from such capabilities to learn correlations between the noisy signal and the visual stimuli, thus attributing meaning to the speech by amplifying relevant information and suppressing noise. Experiments conducted over a Grid Corpus and CHiME3-based dataset show that MBURST can reproduce similar mask reconstructions to the multimodal backpropagation-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#30340;&#28608;&#27963;&#20989;&#25968;&#26041;&#27861;ANAct&#26469;&#20445;&#25345;&#19968;&#33268;&#30340;&#26799;&#24230;&#24046;&#24322;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2208.13315</link><description>&lt;p&gt;
ANAct: &#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#30340;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
ANAct: Adaptive Normalization for Activation Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#30340;&#28608;&#27963;&#20989;&#25968;&#26041;&#27861;ANAct&#26469;&#20445;&#25345;&#19968;&#33268;&#30340;&#26799;&#24230;&#24046;&#24322;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#25269;&#28040;&#36825;&#31181;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#24046;&#24322;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#25193;&#23637;&#20102;&#27492;&#39046;&#22495;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23567;&#25209;&#37327;&#32479;&#35745;&#26469;&#21160;&#24577;&#26356;&#26032;&#24402;&#19968;&#21270;&#22240;&#23376;&#65292;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#24402;&#19968;&#21270;&#23646;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#32771;&#34385;&#26435;&#37325;&#21021;&#22987;&#21270;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#29366;&#24577;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ANAct&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20197;&#22312;&#21508;&#23618;&#20043;&#38388;&#20445;&#25345;&#19968;&#33268;&#30340;&#26799;&#24230;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25910;&#25947;&#36895;&#24230;&#19982;&#24402;&#19968;&#21270;&#23646;&#24615;&#22823;&#33268;&#30456;&#20851;&#12290;&#25105;&#20204;&#23558;ANAct&#19982;&#20960;&#31181;&#24120;&#35265;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27531;&#24046;&#32593;&#32476;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#26174;&#31034;ANAct&#21487;&#20197;&#25345;&#32493;&#25913;&#21892;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the negative effect of activation functions on forward and backward propagation and how to counteract this effect. First, We examine how activation functions affect the forward and backward propagation of neural networks and derive a general form for gradient variance that extends the previous work in this area. We try to use mini-batch statistics to dynamically update the normalization factor to ensure the normalization property throughout the training process, rather than only accounting for the state of the neural network after weight initialization. Second, we propose ANAct, a method that normalizes activation functions to maintain consistent gradient variance across layers and demonstrate its effectiveness through experiments. We observe that the convergence rate is roughly related to the normalization property. We compare ANAct with several common activation functions on CNNs and residual networks and show that ANAct consistently improves their perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.05248</link><description>&lt;p&gt;
&#21152;&#36895;&#31639;&#27861;&#29992;&#20110;&#32422;&#26463;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#65292;&#19968;&#31867;&#32467;&#26500;&#21270;&#30340;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20197;&#21450;&#23427;&#20204;&#23545;&#20849;&#21333;&#35843;&#21253;&#21547;&#30340;&#25512;&#24191;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#21021;&#30001;Yoon&#21644;Ryu&#65288;2021&#65289;&#25552;&#20986;&#30340;&#26080;&#32422;&#26463;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#30340;Extra Anchored Gradient&#65288;EAG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#25152;&#26377;&#19968;&#38454;&#26041;&#27861;&#20013;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;$O\left(\frac{1}{T}\right)$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36845;&#20195;&#25910;&#25947;&#21040;&#35299;&#38598;&#20013;&#30340;&#19968;&#20010;&#28857;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#30001;Lee&#21644;Kim&#65288;2021&#65289;&#24320;&#21457;&#30340;&#24555;&#36895;&#39069;&#22806;&#26799;&#24230;&#65288;FEG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;$O\left(\frac{1}{T}\right)$&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#20010;&#36895;&#29575;&#36866;&#29992;&#20110;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26368;&#24191;&#27867;&#30340;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;s&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\left(\frac{1}{T}\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the Fast Extra Gradient (FEG) algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\left(\frac{1}{T}\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2204.04510</link><description>&lt;p&gt;
&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#35753;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#19978;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.04510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#22823;&#22411;&#20840;&#23616;&#22270;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20294;&#25361;&#25112;&#23376;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#22270;&#21040;&#33410;&#28857;&#65288;S2N&#65289;&#36716;&#25442;&#30340;&#26032;&#39062;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20840;&#23616;&#22270;&#20013;&#30340;&#19968;&#32452;&#23376;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;&#31895;&#30053;&#22320;&#23558;&#23376;&#22270;&#36716;&#25442;&#25104;&#33410;&#28857;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#22270;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;S2N&#19981;&#20165;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#36890;&#36807;&#25429;&#25417;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#20063;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31895;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#27169;&#22411;&#21518;&#25928;&#26524;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2202.03482</link><description>&lt;p&gt;
&#39046;&#33322;&#31070;&#32463;&#31354;&#38388;&#65306;&#37325;&#26032;&#23457;&#35270;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#20197;&#20811;&#26381;&#26041;&#21521;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31574;&#30053;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#36890;&#24120;&#65292;CAVs&#26159;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#35745;&#31639;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#20248;&#21270;&#20855;&#26377;&#32473;&#23450;&#27010;&#24565;&#21644;&#26080;&#32473;&#23450;&#27010;&#24565;&#30340;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20197;&#21487;&#20998;&#31163;&#24615;&#20026;&#23548;&#21521;&#30340;&#35745;&#31639;&#26041;&#27861;&#20250;&#23548;&#33268;&#19982;&#31934;&#30830;&#24314;&#27169;&#27010;&#24565;&#26041;&#21521;&#30340;&#23454;&#38469;&#30446;&#26631;&#21457;&#25955;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#20998;&#25955;&#26041;&#21521;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21363;&#19982;&#27010;&#24565;&#26080;&#20851;&#30340;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#34987;&#32447;&#24615;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#65288;&#21363;&#26435;&#37325;&#65289;&#25429;&#33719;&#20197;&#20248;&#21270;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#65292;&#20165;&#20851;&#27880;&#27010;&#24565;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;CAV&#26041;&#27861;&#19982;&#30495;&#23454;&#27010;&#24565;&#26041;&#21521;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept dire
&lt;/p&gt;</description></item><item><title>&#21487;&#23454;&#29616;&#23398;&#20064;&#19982;&#26080;&#20559;&#23398;&#20064;&#30340;&#31561;&#20215;&#24615;&#26159;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#22522;&#26412;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#20010;&#31561;&#20215;&#24615;&#65292;&#23427;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#35774;&#32622;&#65292;&#24182;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#21508;&#31181;&#23398;&#20064;&#24773;&#20917;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2111.04746</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#23398;&#20064;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Realizable Learning is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.04746
&lt;/p&gt;
&lt;p&gt;
&#21487;&#23454;&#29616;&#23398;&#20064;&#19982;&#26080;&#20559;&#23398;&#20064;&#30340;&#31561;&#20215;&#24615;&#26159;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#22522;&#26412;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#20010;&#31561;&#20215;&#24615;&#65292;&#23427;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#35774;&#32622;&#65292;&#24182;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#21508;&#31181;&#23398;&#20064;&#24773;&#20917;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#23454;&#29616;&#23398;&#20064;&#19982;&#26080;&#20559;&#23398;&#20064;&#30340;&#31561;&#20215;&#24615;&#26159;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#22522;&#26412;&#29616;&#35937;&#12290;&#20174;&#32463;&#20856;&#30340;PAC&#23398;&#20064;&#21644;&#22238;&#24402;&#21040;&#26368;&#36817;&#30340;&#36235;&#21183;&#22914;&#23545;&#25239;&#40065;&#26834;&#23398;&#20064;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65307;&#20256;&#32479;&#30340;&#31561;&#20215;&#24615;&#35777;&#26126;&#24448;&#24448;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#24378;&#30340;&#27169;&#22411;&#29305;&#23450;&#20551;&#35774;&#65292;&#22914;&#22343;&#21248;&#25910;&#25947;&#21644;&#26679;&#26412;&#21387;&#32553;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#21487;&#23454;&#29616;&#23398;&#20064;&#19982;&#26080;&#20559;&#23398;&#20064;&#30340;&#31561;&#20215;&#24615;&#65306;&#19968;&#20010;&#19977;&#34892;&#20195;&#30721;&#30340;&#40657;&#30418;&#31616;&#21270;&#65292;&#32479;&#19968;&#21644;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#21508;&#31181;&#35774;&#32622;&#30340;&#29702;&#35299;&#12290;&#36825;&#21253;&#25324;&#20102;&#27809;&#26377;&#24050;&#30693;&#21487;&#23398;&#20064;&#24615;&#25551;&#36848;&#30340;&#27169;&#22411;&#65292;&#22914;&#20855;&#26377;&#20219;&#24847;&#20998;&#24067;&#20551;&#35774;&#21644;&#26356;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#20854;&#20182;&#27969;&#34892;&#30340;&#35774;&#32622;&#65292;&#22914;&#40065;&#26834;&#23398;&#20064;&#12289;&#37096;&#20998;&#23398;&#20064;&#12289;&#20844;&#24179;&#23398;&#20064;&#21644;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The equivalence of realizable and agnostic learnability is a fundamental phenomenon in learning theory. With variants ranging from classical settings like PAC learning and regression to recent trends such as adversarially robust learning, it's surprising that we still lack a unified theory; traditional proofs of the equivalence tend to be disparate, and rely on strong model-specific assumptions like uniform convergence and sample compression.   In this work, we give the first model-independent framework explaining the equivalence of realizable and agnostic learnability: a three-line blackbox reduction that simplifies, unifies, and extends our understanding across a wide variety of settings. This includes models with no known characterization of learnability such as learning with arbitrary distributional assumptions and more general loss functions, as well as a host of other popular settings such as robust learning, partial learning, fair learning, and the statistical query model.   Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#27835;&#30103;&#29615;&#22659;&#30340;&#22810;&#37325;&#31283;&#20581;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#37319;&#29992;&#20102;&#26680;&#24179;&#28369;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#37325;&#31283;&#20581;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;</title><link>https://arxiv.org/abs/2105.09254</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#27835;&#30103;&#19979;&#30340;&#22810;&#37325;&#31283;&#20581;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multiply Robust Causal Mediation Analysis with Continuous Treatments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.09254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#27835;&#30103;&#29615;&#22659;&#30340;&#22810;&#37325;&#31283;&#20581;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#37319;&#29992;&#20102;&#26680;&#24179;&#28369;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#37325;&#31283;&#20581;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#27835;&#30103;&#25110;&#26292;&#38706;&#23545;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20013;&#20171;&#20998;&#26512;&#20026;&#37492;&#23450;&#21644;&#20272;&#35745;&#36825;&#20123;&#22240;&#26524;&#25928;&#24212;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#26694;&#26550;&#12290;&#23545;&#20110;&#20108;&#20803;&#27835;&#30103;&#65292;Tchetgen Tchetgen&#21644;Shpitser (2012)&#25552;&#20986;&#20102;&#30452;&#25509;&#21644;&#38388;&#25509;&#25928;&#24212;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#65292;&#22522;&#20110;&#21442;&#25968;&#30340;&#24433;&#21709;&#20989;&#25968;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;&#65292;&#22914;&#22810;&#37325;&#31283;&#20581;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#23545;&#24178;&#25200;&#21442;&#25968;&#36827;&#34892;&#20302;&#20110;&#26681;&#21495;n&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#36830;&#32493;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#20272;&#35745;&#22120;&#27809;&#26377;&#20934;&#22791;&#22909;&#24212;&#29992;&#65292;&#38500;&#38750;&#36827;&#34892;&#24378;&#21442;&#25968;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;&#24179;&#28369;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#27835;&#30103;&#29615;&#22659;&#30340;&#20272;&#35745;&#22120;&#65292;&#21463;&#21040;Tchetgen Tchetgen&#30340;&#24433;&#21709;&#20989;&#25968;&#20272;&#35745;&#22120;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, researchers are interested in the direct and indirect causal effects of a treatment or exposure on an outcome of interest. Mediation analysis offers a rigorous framework for identifying and estimating these causal effects. For binary treatments, efficient estimators for the direct and indirect effects are presented in Tchetgen Tchetgen and Shpitser (2012) based on the influence function of the parameter of interest. These estimators possess desirable properties, such as multiple-robustness and asymptotic normality, while allowing for slower than root-n rates of convergence for the nuisance parameters. However, in settings involving continuous treatments, these influence function-based estimators are not readily applicable without making strong parametric assumptions. In this work, utilizing a kernel-smoothing approach, we propose an estimator suitable for settings with continuous treatments inspired by the influence function-based estimator of Tchetgen Tchetgen an
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#26088;&#22312;&#21521;&#35835;&#32773;&#25552;&#20379;&#23545;&#32447;&#24615;&#27169;&#22411;&#21450;&#20854;&#29702;&#35770;&#30340;&#20005;&#26684;&#20171;&#32461;&#65292;&#24182;&#24635;&#32467;&#20102;&#32447;&#24615;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2105.04240</link><description>&lt;p&gt;
&#19968;&#20010;&#23545;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#20171;&#32461;&#30340;&#20070;&#31821;
&lt;/p&gt;
&lt;p&gt;
A rigorous introduction to linear models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.04240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#21521;&#35835;&#32773;&#25552;&#20379;&#23545;&#32447;&#24615;&#27169;&#22411;&#21450;&#20854;&#29702;&#35770;&#30340;&#20005;&#26684;&#20171;&#32461;&#65292;&#24182;&#24635;&#32467;&#20102;&#32447;&#24615;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#21521;&#35835;&#32773;&#20171;&#32461;&#32447;&#24615;&#27169;&#22411;&#21450;&#20854;&#32972;&#21518;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#35835;&#32773;&#25552;&#20379;&#19968;&#20010;&#20005;&#35880;&#30340;&#20171;&#32461;&#65292;&#21069;&#25552;&#26159;&#35835;&#32773;&#20855;&#26377;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#20808;&#21069;&#32463;&#39564;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#36755;&#20986;&#36890;&#24120;&#26159;&#36755;&#20837;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#28145;&#24230;&#23398;&#20064;&#29978;&#33267;&#26088;&#22312;&#25214;&#21040;&#20855;&#26377;&#35768;&#22810;&#23618;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#37117;&#26159;&#22522;&#20110;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#26500;&#24314;&#30340;&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25551;&#36848;&#32447;&#24615;&#27169;&#22411;&#65292;&#25214;&#21040;&#27169;&#22411;&#32972;&#21518;&#30340;&#24615;&#36136;&#21644;&#29702;&#35770;&#12290;&#32447;&#24615;&#27169;&#22411;&#26159;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#20027;&#35201;&#25216;&#26415;&#65292;&#26368;&#20027;&#35201;&#30340;&#24037;&#20855;&#26159;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;&#24179;&#26041;&#35823;&#24046;&#30340;&#21644;&#12290;&#24403;&#25105;&#20204;&#26377;&#20852;&#36259;&#25214;&#21040;&#26368;&#23567;&#21270;&#30456;&#24212;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#30340;&#22238;&#24402;&#20989;&#25968;&#26102;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#28982;&#30340;&#36873;&#25321;&#12290;&#26412;&#20070;&#20027;&#35201;&#24635;&#32467;&#20102;&#32447;&#24615;&#27169;&#22411;&#32972;&#21518;&#30340;&#30446;&#30340;&#21644;&#37325;&#35201;&#29702;&#35770;&#30340;&#24847;&#20041;&#65292;&#20363;&#22914;&#27010;&#29575;&#20998;&#24067;&#12289;&#25512;&#23548;&#21644;&#20272;&#35745;&#26041;&#27861;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
This book is meant to provide an introduction to linear models and the theories behind them. Our goal is to give a rigorous introduction to the readers with prior exposure to ordinary least squares. In machine learning, the output is usually a nonlinear function of the input. Deep learning even aims to find a nonlinear dependence with many layers, which require a large amount of computation. However, most of these algorithms build upon simple linear models. We then describe linear models from different perspectives and find the properties and theories behind the models. The linear model is the main technique in regression problems, and the primary tool for it is the least squares approximation, which minimizes a sum of squared errors. This is a natural choice when we're interested in finding the regression function which minimizes the corresponding expected squared error. This book is primarily a summary of purpose, significance of important theories behind linear models, e.g., distrib
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20892;&#20316;&#29289;&#31867;&#22411;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#33258;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#31361;&#20986;&#35821;&#20041;&#36793;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#20102;&#35813;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23494;&#38598;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2104.04310</link><description>&lt;p&gt;
&#20892;&#20316;&#29289;&#31867;&#22411;&#35821;&#20041;&#20998;&#21106;&#30340;&#19978;&#19979;&#25991;&#33258;&#23545;&#27604;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Context-self contrastive pretraining for crop type semantic segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.04310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20892;&#20316;&#29289;&#31867;&#22411;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#33258;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#31361;&#20986;&#35821;&#20041;&#36793;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#20102;&#35813;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23494;&#38598;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23494;&#38598;&#20998;&#31867;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;Context-Self&#23545;&#27604;&#25439;&#22833;&#65288;CSCL&#65289;&#36890;&#36807;&#22312;&#35757;&#32451;&#26679;&#26412;&#30340;&#27599;&#20010;&#20301;&#32622;&#21644;&#20854;&#23616;&#37096;&#19978;&#19979;&#25991;&#20043;&#38388;&#20351;&#29992;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#35821;&#20041;&#36793;&#30028;&#31361;&#20986;&#12290;&#38024;&#23545;&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#65288;SITS&#65289;&#20013;&#30340;&#20892;&#20316;&#29289;&#31867;&#22411;&#35821;&#20041;&#20998;&#21106;&#65292;&#25105;&#20204;&#21457;&#29616;&#22320;&#22359;&#36793;&#30028;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#35299;&#37322;&#20102;CSCL&#22914;&#20309;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#25913;&#36827;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;Sentinel-2&#65288;S2&#65289;&#21355;&#26143;&#20219;&#21153;&#30340;&#22270;&#20687;&#32534;&#21046;&#20102;&#30446;&#21069;&#25105;&#20204;&#25152;&#30693;&#30340;&#26368;&#22823;&#30340;&#23494;&#38598;&#27880;&#37322;&#30340;&#20892;&#20316;&#29289;&#31867;&#22411;&#21644;&#22320;&#22359;&#36523;&#20221;&#30340;SITS&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#36830;&#21516;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20844;&#24320;&#12290;&#20351;&#29992;&#35813;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#23567;&#30340;&#39044;&#35757;&#32451;&#65292;CSCL&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#30456;&#24212;&#22522;&#20934;&#32447;&#65292;&#24182;&#23637;&#31034;&#19968;&#20010;&#20808;&#36827;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a fully supervised pre-training scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in a training sample and its local context. For crop type semantic segmentation from Satellite Image Time Series (SITS) we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, SITS dataset densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pre-training, to improve all respective baselines and present a pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20108;&#36827;&#21046;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#32858;&#31867;&#24674;&#22797;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/1910.06002</link><description>&lt;p&gt;
&#26469;&#33258;&#22122;&#22768;&#20108;&#36827;&#21046;&#21453;&#39304;&#30340;&#26368;&#20248;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Optimal Clustering from Noisy Binary Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.06002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20108;&#36827;&#21046;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#32858;&#31867;&#24674;&#22797;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#20108;&#36827;&#21046;&#29992;&#25143;&#21453;&#39304;&#26469;&#36827;&#34892;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#38382;&#39064;&#22312;&#22823;&#35268;&#27169;&#26631;&#35760;&#20219;&#21153;&#20013;&#20197;&#26368;&#23567;&#30340;&#29992;&#25143;&#24037;&#20316;&#37327;&#35299;&#20915;&#30340;&#20247;&#21253;&#24179;&#21488;&#19978;&#20986;&#29616;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20123;&#26368;&#36817;&#30340;reCAPTCHA&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#30340;&#28857;&#20987;&#65288;&#20108;&#36827;&#21046;&#31572;&#26696;&#65289;&#21487;&#20197;&#29992;&#26469;&#26377;&#25928;&#22320;&#26631;&#35760;&#22270;&#20687;&#12290;&#22312;&#25105;&#20204;&#30340;&#25512;&#29702;&#38382;&#39064;&#20013;&#65292;&#39033;&#30446;&#34987;&#20998;&#25104;&#26368;&#21021;&#26410;&#30693;&#30340;&#19981;&#37325;&#21472;&#30340;&#32858;&#31867;&#12290;&#20026;&#20102;&#24674;&#22797;&#36825;&#20123;&#32858;&#31867;&#65292;&#23398;&#20064;&#32773;&#25353;&#39034;&#24207;&#21521;&#29992;&#25143;&#21576;&#29616;&#19968;&#31995;&#21015;&#39033;&#30446;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#38468;&#26377;&#19968;&#20010;&#20174;&#22266;&#23450;&#26377;&#38480;&#38598;&#21512;&#20013;&#36873;&#25321;&#30340;&#20855;&#26377;&#20108;&#36827;&#21046;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#36825;&#20123;&#39033;&#30446;&#20013;&#30340;&#27599;&#19968;&#20010;&#65292;&#29992;&#25143;&#25552;&#20379;&#30340;&#26159;&#19968;&#20010;&#30001;&#39033;&#30446;&#32858;&#31867;&#12289;&#38382;&#39064;&#21644;&#19968;&#20010;&#25551;&#36848;&#23545;&#39033;&#30446;&#36827;&#34892;&#20998;&#31867;&#30340;&#38590;&#24230;&#30340;&#39033;&#30446;&#29305;&#23450;&#21442;&#25968;&#20915;&#23450;&#26399;&#26395;&#30340;&#22122;&#22768;&#31572;&#26696;&#12290;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#32858;&#31867;&#24674;&#22797;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#20219;&#20309;&#31639;&#27861;&#28385;&#36275;&#30340;&#38382;&#39064;&#29305;&#23450;&#30340;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#65292;&#29992;&#20110;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of clustering a set of items from binary user feedback. Such a problem arises in crowdsourcing platforms solving large-scale labeling tasks with minimal effort put on the users. For example, in some of the recent reCAPTCHA systems, users clicks (binary answers) can be used to efficiently label images. In our inference problem, items are grouped into initially unknown non-overlapping clusters. To recover these clusters, the learner sequentially presents to users a finite list of items together with a question with a binary answer selected from a fixed finite set. For each of these items, the user provides a noisy answer whose expectation is determined by the item cluster and the question and by an item-specific parameter characterizing the {\it hardness} of classifying the item. The objective is to devise an algorithm with a minimal cluster recovery error rate. We derive problem-specific information-theoretical lower bounds on the error rate satisfied by any algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32477;&#23545;&#20540;&#25439;&#22833;&#20989;&#25968;&#20026; $\ell_p$ &#30340;&#19981;&#30830;&#23450;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36924;&#36817;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#32500;&#24230;&#22823;&#23567;&#30340;&#21387;&#32553;&#65292;&#23545;&#20110; $\ell_1$ &#21644; $\ell_\infty$ &#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#32500;&#24230;&#22823;&#23567;&#30340;&#26377;&#25928;&#23436;&#20840;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65307;&#21516;&#26102;&#65292;&#35777;&#26126;&#20102;&#20854;&#20182; $\ell_p$ &#25439;&#22833;&#20989;&#25968;&#19981;&#23384;&#22312;&#26377;&#38480;&#23610;&#23544;&#30340;&#23436;&#20840;&#19981;&#21487;&#30693;&#21387;&#32553;&#26041;&#26696;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/1810.01864</link><description>&lt;p&gt;
&#26080;&#30693;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#19981;&#21487;&#30693;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Agnostic Sample Compression Schemes for Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1810.01864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32477;&#23545;&#20540;&#25439;&#22833;&#20989;&#25968;&#20026; $\ell_p$ &#30340;&#19981;&#30830;&#23450;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36924;&#36817;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#32500;&#24230;&#22823;&#23567;&#30340;&#21387;&#32553;&#65292;&#23545;&#20110; $\ell_1$ &#21644; $\ell_\infty$ &#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#32500;&#24230;&#22823;&#23567;&#30340;&#26377;&#25928;&#23436;&#20840;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65307;&#21516;&#26102;&#65292;&#35777;&#26126;&#20102;&#20854;&#20182; $\ell_p$ &#25439;&#22833;&#20989;&#25968;&#19981;&#23384;&#22312;&#26377;&#38480;&#23610;&#23544;&#30340;&#23436;&#20840;&#19981;&#21487;&#30693;&#21387;&#32553;&#26041;&#26696;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32477;&#23545;&#20540;&#25439;&#22833;&#20989;&#25968;&#20026; $\ell_p$ &#30340;&#19981;&#30830;&#23450;&#22238;&#24402;&#35774;&#32622;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#21387;&#32553;&#30340;&#31215;&#26497;&#32467;&#26524;&#65292;&#20854;&#20013; $p \in [1, \infty]$&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36924;&#36817;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#23637;&#31034;&#20102;&#25351;&#25968;&#32423;&#22823;&#23567;&#30340;fat-shattering&#32500;&#24230;&#20294;&#19982;&#26679;&#26412;&#25968;&#37327;&#26080;&#20851;&#30340;&#23454;&#20540;&#20989;&#25968;&#31867;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#32447;&#24615;&#32500;&#24230;&#22823;&#23567;&#30340;&#36924;&#36817;&#21387;&#32553;&#12290;&#27492;&#22806;&#65292;&#22312;$\ell_1$&#21644;$\ell_\infty$&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#23637;&#31034;&#20986;&#19968;&#20010;&#32447;&#24615;&#32500;&#24230;&#22823;&#23567;&#30340;&#26377;&#25928;&#23436;&#20840;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#23545;&#20110;&#20854;&#20182;&#27599;&#19968;&#20010; $\ell_p$ &#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013; $p \in (1,\infty)$&#65292;&#19981;&#23384;&#22312;&#26377;&#38480;&#23610;&#23544;&#30340;&#23436;&#20840;&#19981;&#21487;&#30693;&#21387;&#32553;&#26041;&#26696;&#12290;&#36825;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25512;&#24191;&#20102;David&#12289;Moran&#21644;Yehudayoff&#23545;&#20110;$\ell_2$&#25439;&#22833;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;&#25105;&#20204;&#26368;&#21518;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#24320;&#25918;&#38382;&#39064;&#65306;&#23545;&#20110; $\ell_1$ &#25439;&#22833;&#30340;&#19981;&#21487;&#30693;&#22238;&#24402;&#38382;&#39064;&#65292;&#26159;&#21542;&#27599;&#20010;&#20989;&#25968;&#31867;&#37117;&#23384;&#22312;&#23610;&#23544;&#20026;...&#30340;&#23436;&#20840;&#21387;&#32553;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
We obtain the first positive results for bounded sample compression in the agnostic regression setting with the $\ell_p$ loss, where $p\in [1,\infty]$. We construct a generic approximate sample compression scheme for real-valued function classes exhibiting exponential size in the fat-shattering dimension but independent of the sample size. Notably, for linear regression, an approximate compression of size linear in the dimension is constructed. Moreover, for $\ell_1$ and $\ell_\infty$ losses, we can even exhibit an efficient exact sample compression scheme of size linear in the dimension. We further show that for every other $\ell_p$ loss, $p\in (1,\infty)$, there does not exist an exact agnostic compression scheme of bounded size. This refines and generalizes a negative result of David, Moran, and Yehudayoff for the $\ell_2$ loss. We close by posing general open questions: for agnostic regression with $\ell_1$ loss, does every function class admits an exact compression scheme of size 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.16808</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#26469;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26102;&#38388;&#32500;&#24230;&#19982;&#35768;&#22810;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#39640;&#22122;&#22768;&#20449;&#21495;&#27604;&#12289;&#38750;&#27491;&#24577;&#24615;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#25968;&#25454;&#32570;&#20047;&#20173;&#28982;&#26159;&#25361;&#25112;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#22686;&#24378;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22686;&#24378;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20316;&#20026;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#36827;&#34892;&#32534;&#30721;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#32479;&#35745;&#31354;&#38388;&#22686;&#24378;&#34920;&#31034;&#65288;SSAR&#65289;&#12290;&#22522;&#20110;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21551;&#21457;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#19979;&#28216;&#26102;&#38388;&#23398;&#20064;&#31639;&#27861;&#30340;&#32463;&#39564;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20987;&#36133;&#20102;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#24615;&#36136;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
&lt;/p&gt;</description></item><item><title>Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16736</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16736
&lt;/p&gt;
&lt;p&gt;
Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#21019;&#26032;&#25216;&#26415;&#30340;&#24320;&#21457;&#21644;&#21457;&#24067;&#12290;Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#29420;&#29305;&#30340;&#37197;&#32622;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#37197;&#32622;&#65292;&#23427;&#21487;&#20197;&#25552;&#21462;&#29305;&#24449;&#24182;&#23398;&#20064;&#22797;&#26434;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#12290;Atinuke&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#12290;softmax&#12289;&#23884;&#20837;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#31561;&#39640;&#32423;&#30697;&#38453;&#25805;&#20316;&#20351;&#24471;&#23545;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#32454;&#33268;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#23558;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#36719;&#20214;&#35774;&#35745;&#21407;&#21017;&#21644;&#25968;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
&lt;/p&gt;</description></item><item><title>Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16452</link><description>&lt;p&gt;
Context-Former&#65306;&#22522;&#20110;&#28508;&#22312;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#25340;&#25509;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16452
&lt;/p&gt;
&lt;p&gt;
Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#20351;RL&#33021;&#22815;&#23398;&#20064;&#20248;&#20110;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Decision Transformer&#65288;DT&#65289;&#23558;&#20915;&#31574;&#24314;&#27169;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#22312;&#31163;&#32447;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;DT&#32570;&#20047;&#25340;&#25509;&#33021;&#21147;&#65292;&#22240;&#27492;&#25552;&#39640;DT&#24615;&#33021;&#38656;&#35201;&#21033;&#29992;&#25340;&#25509;&#33021;&#21147;&#12290;&#20026;&#20102;&#36171;&#20104;DT&#25340;&#25509;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36712;&#36857;&#25340;&#25509;&#25277;&#35937;&#20026;&#19987;&#23478;&#21305;&#37197;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;ContextFormer&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#34920;&#31034;&#26469;&#38598;&#25104;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#27969;&#37327;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21644;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15285</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#32531;&#35299;&#21202;&#32034;&#36719;&#20214;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Ransomware threat mitigation through network traffic analysis and machine learning techniques. (arXiv:2401.15285v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#27969;&#37327;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21644;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#21202;&#32034;&#36719;&#20214;&#36827;&#34892;&#32593;&#32476;&#25915;&#20987;&#30340;&#24773;&#20917;&#26126;&#26174;&#22686;&#21152;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#31181;&#24694;&#24847;&#36719;&#20214;&#20405;&#20837;&#32593;&#32476;&#24182;&#25439;&#23475;&#35745;&#31639;&#26426;&#31995;&#32479;&#12290;&#36825;&#32473;&#21508;&#31181;&#32452;&#32455;&#24102;&#26469;&#20102;&#37325;&#22823;&#21644;&#38271;&#26399;&#30340;&#25439;&#23475;&#65292;&#21253;&#25324;&#25919;&#24220;&#12289;&#31169;&#33829;&#20844;&#21496;&#21644;&#26222;&#36890;&#29992;&#25143;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#23548;&#33268;&#25935;&#24863;&#20449;&#24687;&#30340;&#20002;&#22833;&#25110;&#27844;&#38706;&#65292;&#27491;&#24120;&#36816;&#33829;&#30340;&#20013;&#26029;&#20197;&#21450;&#25345;&#20037;&#30340;&#28431;&#27934;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#19968;&#31181;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#35782;&#21035;&#21644;&#36776;&#21035;&#21202;&#32034;&#36719;&#20214;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20998;&#26512;&#32593;&#32476;&#27969;&#37327;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#30740;&#31350;&#36825;&#20123;&#27969;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#12290;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22522;&#20110;&#32593;&#32476;&#27969;&#37327;&#20934;&#30830;&#22320;&#23450;&#20301;&#21202;&#32034;&#36719;&#20214;&#65292;&#23454;&#29616;&#39640;&#27700;&#24179;&#30340;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a noticeable increase in cyberattacks using ransomware. Attackers use this malicious software to break into networks and harm computer systems. This has caused significant and lasting damage to various organizations, including government, private companies, and regular users. These attacks often lead to the loss or exposure of sensitive information, disruptions in normal operations, and persistent vulnerabilities. This paper focuses on a method for recognizing and identifying ransomware in computer networks. The approach relies on using machine learning algorithms and analyzing the patterns of network traffic. By collecting and studying this traffic, and then applying machine learning models, we can accurately identify and detect ransomware. The results of implementing this method show that machine learning algorithms can effectively pinpoint ransomware based on network traffic, achieving high levels of precision and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15268</link><description>&lt;p&gt;
&#26397;&#30528;&#31283;&#23450;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#19968;&#33268;&#21270;&#26426;&#22120;&#23398;&#20064;&#20559;&#22909;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32958;&#33039;&#20998;&#37197;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#21363;&#38656;&#27714;&#22686;&#38271;&#19982;&#21033;&#30410;&#30456;&#20851;&#26041;&#20215;&#20540;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#31181;&#23398;&#20064;&#20010;&#20154;&#21644;&#22242;&#20307;&#20851;&#20110;&#32958;&#33039;&#20998;&#37197;&#30340;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#8220;&#25104;&#23545;&#32958;&#33039;&#24739;&#32773;&#22312;&#32447;&#35843;&#26597;&#8221;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20010;&#20154;&#12289;&#22242;&#20307;&#21644;&#31283;&#23450;&#24615;&#19977;&#20010;&#23618;&#38754;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24182;&#36890;&#36807;&#20960;&#31181;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#20010;&#20154;&#23618;&#38754;&#27169;&#22411;&#39044;&#27979;&#20010;&#20307;&#21442;&#19982;&#32773;&#30340;&#20559;&#22909;&#65292;&#22242;&#20307;&#23618;&#38754;&#27169;&#22411;&#27719;&#24635;&#21442;&#19982;&#32773;&#20559;&#22909;&#65292;&#31283;&#23450;&#24615;&#23618;&#38754;&#27169;&#22411;&#26159;&#22242;&#20307;&#21319;&#32423;&#30340;&#25193;&#23637;&#65292;&#35780;&#20272;&#36825;&#20123;&#20559;&#22909;&#38543;&#26102;&#38388;&#30340;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23558;&#21033;&#30410;&#30456;&#20851;&#26041;&#30340;&#20559;&#22909;&#32435;&#20837;&#32958;&#33039;&#20998;&#37197;&#36807;&#31243;&#65292;&#25105;&#20204;&#24076;&#26395;&#25512;&#21160;&#20262;&#29702;&#32500;&#24230;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15222</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#20197;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30149;&#20363;&#26816;&#27979;&#20026;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;&#30340;&#35821;&#20041;&#21487;&#33021;&#20250;&#21463;&#21040;&#20462;&#39280;&#35821;&#30340;&#26174;&#33879;&#25913;&#21464;&#65292;&#21253;&#25324;&#23454;&#20307;&#30340;&#21542;&#23450;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#26465;&#20214;&#24615;&#12289;&#20005;&#37325;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#29616;&#26377;&#30340;&#30830;&#23450;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#27169;&#22411;&#28041;&#21450;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#25110;&#29305;&#24449;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#20462;&#39280;&#35821;&#30340;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#21464;&#25442;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;SemEval 2015&#20219;&#21153;14&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#26032;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#23398;&#20064;&#21644;&#39044;&#27979;&#20462;&#39280;&#35821;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;SemEval&#20849;&#20139;&#30340;&#20462;&#39280;&#35821;&#20197;&#21450;OUD&#29305;&#23450;&#30340;&#26032;&#20462;&#39280;&#35821;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#19982;&#20197;&#21069;&#21457;&#34920;&#30340;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#20165;&#20849;&#20139;&#37096;&#20998;&#20020;&#24202;&#20462;&#39280;&#35821;&#26102;&#30340;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;SemEval 2015&#30340;ShARe&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
&lt;/p&gt;</description></item><item><title>EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15077</link><description>&lt;p&gt;
EAGLE: &#25512;&#27979;&#37319;&#26679;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29305;&#24449;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15077
&lt;/p&gt;
&lt;p&gt;
EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#21464;&#24471;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;EAGLE&#65288;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#30340;&#22806;&#25512;&#31639;&#27861;&#65289;&#65292;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#27979;&#37319;&#26679;&#26041;&#27861;&#19981;&#21516;&#65292;EAGLE&#22312;&#26356;&#35268;&#24459;&#30340;&#65288;&#27425;&#39030;&#23618;&#65289;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#36827;&#34892;&#32534;&#20889;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#25552;&#21069;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#26631;&#35760;&#26469;&#35299;&#20915;&#19979;&#19968;&#20010;&#29305;&#24449;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#12290;EAGLE&#25152;&#25552;&#20379;&#30340;&#21152;&#36895;&#26159;&#26080;&#25439;&#30340;&#65306;&#23427;&#19981;&#38656;&#35201;&#24494;&#35843;&#30446;&#26631;LLM&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#25130;&#33267;&#26412;&#25991;&#25552;&#20132;&#26102;&#65292;EAGLE&#26159;&#24050;&#30693;&#25512;&#27979;&#37319;&#26679;&#23478;&#26063;&#20013;&#36895;&#24230;&#26368;&#24555;&#30340;&#26694;&#26550;&#12290;&#22312;MT-bench&#19978;&#65292;EAGLE&#27604;&#21407;&#22987;&#35299;&#30721;&#24555;3&#20493;&#65292;&#27604;Lookahead&#24555;2&#20493;&#65292;&#27604;Medusa&#24555;1.6&#20493;&#12290;&#20351;&#29992;gpt-fast&#65292;EAGLE&#24179;&#22343;&#27599;&#31186;&#36798;&#21040;160&#20010;&#26631;&#35760;&#19982;LLaMA2-Chat&#25645;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15022
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33014;&#36136;&#30244;&#30340;&#35786;&#26029;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#33014;&#36136;&#30244;&#32452;&#32455;&#36827;&#34892;&#32452;&#32455;&#23398;&#35780;&#20272;&#65292;&#20026;&#35786;&#26029;&#21644;&#39044;&#27979;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#29616;&#29366;&#36827;&#34892;&#27010;&#36848;&#65292;&#26412;&#32508;&#36848;&#23545;70&#20010;&#20844;&#24320;&#21487;&#24471;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#35770;&#25991;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26579;&#33394;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#65288;16/70&#65289;&#65292;&#20998;&#32423;&#65288;23/70&#65289;&#65292;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#65288;13/70&#65289;&#21644;&#29983;&#23384;&#39044;&#27979;&#65288;27/70&#65289;&#31561;&#35786;&#26029;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#22312;&#26041;&#27861;&#23398;&#26041;&#38754;&#21450;&#20854;&#20020;&#24202;&#36866;&#29992;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#36827;&#34892;&#35780;&#20272;&#12290;&#22810;&#25968;&#30740;&#31350;&#65288;49/70&#65289;&#22522;&#20110;&#20844;&#24320;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#21644;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#25968;&#25454;&#38598;&#65292;&#20165;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14591</link><description>&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#21464;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14591
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#20013;&#27969;&#24418;&#28508;&#31354;&#38388;&#26681;&#25454;Ricci&#27969;&#21457;&#23637;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29289;&#29702;&#20449;&#24687;&#35774;&#32622;&#20013;&#27169;&#25311;Ricci&#27969;&#26469;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#21305;&#37197;&#27969;&#24418;&#37327;&#65292;&#20197;&#20415;&#23454;&#29616;Ricci&#27969;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27969;&#24418;&#26159;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#35782;&#21035;&#20986;&#29702;&#24819;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21516;&#26102;&#28436;&#21464;&#20063;&#33021;&#22312;&#38745;&#24577;&#26041;&#27861;&#19978;&#24341;&#36215;&#26356;&#23485;&#23481;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#31561;&#29702;&#24819;&#29305;&#24449;&#30340;PDE&#65292;&#24182;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#36827;&#34892;&#35823;&#24046;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14580</link><description>&lt;p&gt;
&#35774;&#35745;&#20320;&#33258;&#24049;&#30340;&#23431;&#23449;&#65306;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks. (arXiv:2401.14580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#32531;&#35299;&#24120;&#35265;&#30340;GNN&#25361;&#25112;&#65288;&#22914;&#36807;&#24230;&#24179;&#28369;&#21270;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#24322;&#36136;&#36866;&#24212;&#65289;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20173;&#28982;&#22312;&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33539;&#24335;&#26469;&#36866;&#24403;&#22320;&#25972;&#21512;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;GNN&#30340;&#20256;&#25773;&#19982;&#29289;&#29702;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#31867;&#27604;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22686;&#24378;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#21463;&#33410;&#28857;&#26631;&#35760;&#20449;&#24687;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#30340;GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#23545;&#36807;&#24230;&#21387;&#32553;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#37325;&#36830;&#22270;&#36827;&#34892;&#20102;&#35889;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;GNN&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13421</link><description>&lt;p&gt;
&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated learning with distributed fixed design quantum chips and quantum channels. (arXiv:2401.13421v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#23458;&#25143;&#31471;&#30340;&#31934;&#24515;&#35774;&#35745;&#26597;&#35810;&#65292;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#21487;&#20197;&#34987;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#27979;&#37327;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#37327;&#23376;&#36890;&#20449;&#20449;&#36947;&#34987;&#35748;&#20026;&#26356;&#21152;&#23433;&#20840;&#65292;&#22240;&#20026;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#20449;&#24687;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#37327;&#23376;&#29256;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37327;&#23376;&#20449;&#36947;&#21457;&#36865;N&#32500;&#25968;&#25454;&#21521;&#37327;&#38656;&#35201;&#21457;&#36865;log N&#20010;&#32416;&#32544;&#24577;&#37327;&#23376;&#27604;&#29305;&#65292;&#22914;&#26524;&#25968;&#25454;&#21521;&#37327;&#20316;&#20026;&#37327;&#23376;&#24577;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#25552;&#20379;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#22522;&#20110;&#30001;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#21457;&#36865;&#30340;&#37327;&#23376;&#24577;&#65292;&#25805;&#20316;&#22266;&#23450;&#35774;&#35745;&#30340;&#37327;&#23376;&#33455;&#29255;&#12290;&#22522;&#20110;&#25509;&#25910;&#21040;&#30340;&#21472;&#21152;&#24577;&#65292;&#23458;&#25143;&#31471;&#35745;&#31639;&#24182;&#23558;&#20854;&#26412;&#22320;&#26799;&#24230;&#20316;&#20026;&#37327;&#23376;&#24577;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#36825;&#20123;&#26799;&#24230;&#32858;&#21512;&#20197;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#20110;&#26381;&#21153;&#22120;&#19981;&#21457;&#36865;&#27169;&#22411;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients. However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.  In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#20102;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11940</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent. (arXiv:2401.11940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#20102;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#23569;&#37327;&#34987;&#30772;&#22351;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;&#20855;&#26377;&#20302;&#32990;&#29366;&#31209;&#32467;&#26500;&#30340;&#24352;&#37327;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#30340;&#36807;&#31243;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#24352;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#20284;&#20110;Burer-Monteiro&#65288;BM&#65289;&#26041;&#27861;&#30340;&#20998;&#35299;&#36807;&#31243;&#30340;&#39640;&#25928;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#26412;&#26041;&#27861;&#28041;&#21450;&#23558;&#19968;&#20010;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#65288;FGD&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#28040;&#38500;&#20102;t-SVD&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20445;&#35777;FGD&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it 
&lt;/p&gt;</description></item><item><title>LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2401.09486</link><description>&lt;p&gt;
LoMA: &#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09486
&lt;/p&gt;
&lt;p&gt;
LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#38271;&#25991;&#26412;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#38543;&#30528;&#25991;&#26412;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#36164;&#28304;&#28040;&#32791;&#20063;&#24613;&#21095;&#22686;&#21152;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#21387;&#32553;KV&#32531;&#23384;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#32570;&#28857;&#65306;&#21387;&#32553;&#26159;&#26377;&#25439;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20002;&#22833;&#12290;&#22914;&#26524;&#21387;&#32553;&#29575;&#24456;&#39640;&#65292;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#30340;&#27010;&#29575;&#20250;&#22823;&#22823;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#65288;LoMA&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19968;&#32452;&#21387;&#32553;&#27604;&#29575;&#23558;&#20449;&#24687;&#26080;&#25439;&#21387;&#32553;&#25104;&#29305;&#27530;&#30340;&#20869;&#23384;&#20196;&#29260;KV&#23545;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LoMA&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#39640;&#25928;&#35757;&#32451;&#19988;&#20855;&#26377;&#38750;&#24120;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.
&lt;/p&gt;</description></item><item><title>DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.08875</link><description>&lt;p&gt;
DCRMTA: &#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#30340;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08875
&lt;/p&gt;
&lt;p&gt;
DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35302;&#28857;&#24402;&#22240;&#65288;MTA&#65289;&#22312;&#23454;&#29616;&#23545;&#27599;&#20010;&#24191;&#21578;&#35302;&#28857;&#23545;&#20110;&#36716;&#21270;&#34892;&#20026;&#30340;&#36129;&#29486;&#30340;&#20844;&#27491;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28145;&#21051;&#24433;&#21709;&#39044;&#31639;&#20998;&#37197;&#21644;&#24191;&#21578;&#25512;&#33616;&#12290;&#20256;&#32479;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23398;&#20064;&#35302;&#28857;&#24207;&#21015;&#21644;&#29992;&#25143;&#36141;&#20080;&#34892;&#20026;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20174;&#21407;&#22987;&#24207;&#21015;&#23376;&#38598;&#20013;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#36716;&#21270;&#65292;&#20174;&#32780;&#35745;&#31639;&#24191;&#21578;&#36129;&#29486;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#30340;&#26080;&#20559;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#20559;&#22909;&#21644;&#20114;&#32852;&#32593;&#25512;&#33616;&#26426;&#21046;&#65288;&#22914;&#36807;&#21435;&#30340;&#36141;&#29289;&#35760;&#24405;&#23548;&#33268;&#30340;&#24191;&#21578;&#25512;&#33616;&#21516;&#36136;&#21270;&#65289;&#24341;&#36215;&#30340;&#28151;&#26434;&#21464;&#37327;&#22240;&#32032;&#65292;&#36716;&#21270;&#20013;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#39044;&#27979;&#20316;&#20026;&#22240;&#21464;&#37327;&#30340;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#38416;&#36848;&#20102;&#19977;&#20010;&#21487;&#33021;&#30340;&#38169;&#35823;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.08702</link><description>&lt;p&gt;
&#25105;&#20204;&#30495;&#30340;&#38656;&#35201;&#25968;&#25454;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do We Really Even Need Data?. (arXiv:2401.08702v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#39044;&#27979;&#20316;&#20026;&#22240;&#21464;&#37327;&#30340;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#38416;&#36848;&#20102;&#19977;&#20010;&#21487;&#33021;&#30340;&#38169;&#35823;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#26222;&#21450;&#65292;&#31185;&#23398;&#23478;&#22312;&#25968;&#25454;&#25910;&#38598;&#26041;&#38754;&#38754;&#20020;&#30528;&#26032;&#30340;&#38556;&#30861;&#65288;&#20363;&#22914;&#25104;&#26412;&#19978;&#21319;&#12289;&#35843;&#26597;&#21709;&#24212;&#29575;&#19979;&#38477;&#65289;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#39044;&#27979;&#20316;&#20026;&#22240;&#21464;&#37327;&#12290;&#34429;&#28982;&#20174;&#36130;&#21153;&#21644;&#21518;&#21220;&#30340;&#35282;&#24230;&#26469;&#30475;&#36825;&#26679;&#20570;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#20351;&#29992;&#26631;&#20934;&#30340;&#25512;&#35770;&#24037;&#20855;&#21487;&#33021;&#20250;&#22312;&#26367;&#25442;&#30495;&#23454;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#20540;&#26102;&#35823;&#20195;&#34920;&#33258;&#21464;&#37327;&#19982;&#25152;&#20851;&#24515;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#25152;&#35859;&#8220;&#21518;&#39044;&#27979;&#25512;&#26029;&#8221;&#38382;&#39064;&#30340;&#32479;&#35745;&#25361;&#25112;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#26126;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;&#19977;&#20010;&#38169;&#35823;&#26469;&#28304;&#65306;&#65288;i&#65289;&#39044;&#27979;&#32467;&#26524;&#19982;&#20854;&#30495;&#23454;&#26410;&#35266;&#23519;&#21040;&#30340;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#37325;&#26032;&#37319;&#26679;&#25110;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#36866;&#24403;&#22320;&#23558;&#39044;&#27979;&#32467;&#26524;&#30340;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;&#26368;&#32456;&#30340;&#25512;&#26029;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g. rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as outcome variables. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to this so-called ``post-prediction inference'' problem and elucidate three potential sources of error: (i) the relationship between predicted outcomes and their true, unobserved counterparts, (ii) robustness of the machine learning model to resampling or uncertainty about the training data, and (iii) appropriately propagating not just bias but also uncertainty from predictions into the ultimate inference
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#65292;&#24179;&#34913;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#19978;&#19979;&#30028;&#20197;&#21450;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#27010;&#24565;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.08224</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#19979;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;CATE&#30340;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Estimation of CATE in Adaptive Experiment. (arXiv:2401.08224v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#65292;&#24179;&#34913;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#19978;&#19979;&#30028;&#20197;&#21450;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#27010;&#24565;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#24191;&#27867;&#24212;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#21644;&#20854;&#20182;&#22330;&#26223;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#12290;&#34429;&#28982;&#23454;&#39564;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20272;&#35745;&#31934;&#24230;&#65292;&#20294;&#30001;&#20110;&#31038;&#20250;&#31119;&#21033;&#30340;&#35201;&#27714;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#20855;&#26377;&#20248;&#36234;&#32467;&#26524;&#30340;&#27835;&#30103;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29615;&#22659;&#25209;&#27425;&#26694;&#26550;&#20013;&#30340;&#36951;&#25022;&#26469;&#34913;&#37327;&#12290;&#36825;&#20004;&#20010;&#30446;&#26631;&#32463;&#24120;&#23548;&#33268;&#23545;&#27604;&#20248;&#21270;&#20998;&#37197;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#22312;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#65288;&#22914;&#24739;&#32773;&#20581;&#24247;&#35760;&#24405;&#65289;&#30340;&#20020;&#24202;&#22330;&#26223;&#20013;&#20986;&#29616;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#27835;&#30103;&#20998;&#37197;&#26426;&#21046;&#24517;&#39035;&#32435;&#20837;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29615;&#22659;&#25209;&#27425;&#23454;&#39564;&#20013;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#37319;&#29992;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#30340;&#27010;&#24565;&#26469;&#25968;&#23398;&#22320;&#21051;&#30011;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23545;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#21644;&#27604;&#29305;&#24065;&#25903;&#37197;&#24230;&#12289;&#20197;&#22826;&#22346;&#25903;&#37197;&#24230;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05441</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#26041;&#27861;&#23545;&#21152;&#23494;&#36135;&#24065;&#20215;&#20540;&#36827;&#34892;&#20808;&#36827;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An adaptive network-based approach for advanced forecasting of cryptocurrency values. (arXiv:2401.05441v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23545;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#21644;&#27604;&#29305;&#24065;&#25903;&#37197;&#24230;&#12289;&#20197;&#22826;&#22346;&#25903;&#37197;&#24230;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#22522;&#20110;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479; (ANFIS) &#39044;&#27979;&#26410;&#26469;&#19971;&#22825;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#26550;&#26500;&#12290;&#22312;&#27599;&#26085;&#26102;&#38388;&#26694;&#26550;&#19979;&#32771;&#34385;&#20102;&#27604;&#29305;&#24065; (BTC)&#12289;&#20197;&#22826;&#22346; (ETH)&#12289;&#27604;&#29305;&#24065;&#25903;&#37197;&#24230; (BTC.D) &#21644;&#20197;&#22826;&#22346;&#25903;&#37197;&#24230; (ETH.D)&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#28151;&#21512;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#21450;&#32593;&#26684;&#21010;&#20998;&#12289;&#20943;&#27861;&#32858;&#31867;&#21644;&#27169;&#31946;C&#22343;&#20540;&#32858;&#31867; (FCM) &#31639;&#27861;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;&#12290;&#36890;&#36807;&#32479;&#35745;&#35780;&#20272;&#26631;&#20934;&#27604;&#36739;&#20102;&#26412;&#25991;&#35774;&#35745;&#30340;&#26550;&#26500;&#24615;&#33021;&#19982;&#19981;&#21516;&#36755;&#20837;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#26368;&#32456;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#39044;&#27979;&#25968;&#23383;&#36135;&#24065;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.
&lt;/p&gt;</description></item><item><title>CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.05043</link><description>&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05043
&lt;/p&gt;
&lt;p&gt;
CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Credal-Set Interval Neural Networks&#65288;CreINNs&#65289;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;CreINNs&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#21306;&#38388;&#25429;&#25417;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#22312;&#19968;&#20010;&#36229;&#20986;&#20998;&#21457;&#26816;&#27979;&#22522;&#20934;&#65288;CIFAR10 vs SVHN&#65289;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20013;&#65292;CreINNs&#30456;&#27604;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#65288;DEs&#65289;&#65292;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#19982;&#21464;&#20998;BNNs&#30456;&#27604;&#65292;CreINNs&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#27604;DEs&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20854;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#25511;&#21046;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#19982;&#20854;&#20182;&#25511;&#21046;&#22120;&#30456;&#23218;&#32654;&#12290;&#21516;&#26102;&#65292;&#35813;&#25511;&#21046;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.05332</link><description>&lt;p&gt;
&#28040;&#38500;&#24046;&#36317;&#65306;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#21487;&#39564;&#35777;&#27169;&#22411;&#26080;&#20851;&#20108;&#27425;&#35268;&#21010;&#25511;&#21046;&#22120;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20854;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#25511;&#21046;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#19982;&#20854;&#20182;&#25511;&#21046;&#22120;&#30456;&#23218;&#32654;&#12290;&#21516;&#26102;&#65292;&#35813;&#25511;&#21046;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21463;&#21040;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30340;&#21551;&#21457;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#32447;&#24615;MPC&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#65288;QP&#65289;&#27714;&#35299;&#22120;&#65292;&#20294;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#26159;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20174;&#31995;&#32479;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#20351;&#29992;MLP&#25110;&#20854;&#20182;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;DRL&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#20855;&#26377;&#19982;MPC&#31867;&#20284;&#30340;&#25345;&#32493;&#21487;&#34892;&#24615;&#21644;&#28176;&#36817;&#31283;&#23450;&#24615;&#31561;&#21487;&#39564;&#35777;&#23646;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22312;&#25511;&#21046;&#24615;&#33021;&#19978;&#19982;MPC&#21644;MLP&#25511;&#21046;&#22120;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#23545;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#20855;&#26377;&#26356;&#20248;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26126;&#26174;&#20248;&#20110;MPC&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). The controller resembles a Quadratic Programming (QP) solver of a linear MPC problem, with the parameters of the controller being trained via Deep Reinforcement Learning (DRL) rather than derived from system models. This approach addresses the limitations of common controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture used in DRL, in terms of verifiability and performance guarantees, and the learned controllers possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC. On the other hand, numerical examples illustrate that the proposed controller empirically matches MPC and MLP controllers in terms of control performance and has superior robustness against modeling uncertainty and noises. Furthermore, the proposed controller is significantly more computationally efficient compared to MPC a
&lt;/p&gt;</description></item><item><title>SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.01187</link><description>&lt;p&gt;
SASSL:&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#22686;&#24378;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01187
&lt;/p&gt;
&lt;p&gt;
SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26469;&#20174;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22686;&#24378;&#27969;&#27700;&#32447;&#21253;&#25324;&#20102;&#21508;&#31181;&#21407;&#22987;&#30340;&#36716;&#25442;&#65292;&#20294;&#36890;&#24120;&#24573;&#30053;&#20102;&#33258;&#28982;&#22270;&#20687;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#26679;&#26412;&#21487;&#33021;&#26174;&#31034;&#20986;&#36864;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#20302;&#39118;&#26684;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#33258;&#30417;&#30563;&#34920;&#24449;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASSL&#30340;&#26032;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#23427;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#12290;&#35813;&#26041;&#27861;&#23558;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#35299;&#32806;&#65292;&#24182;&#20165;&#23545;&#39118;&#26684;&#24212;&#29992;&#36716;&#25442;&#65292;&#20445;&#25345;&#20869;&#23481;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#23427;&#20204;&#30340;&#35821;&#20041;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24191;&#20026;&#25509;&#21463;&#30340;MoCo v2&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;ImageNet&#19978;&#30340;top-1&#20998;&#31867;&#24615;&#33021;&#25552;&#21319;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13544</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#23646;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#21363;&#25152;&#35859;&#30340;&#28201;&#39034;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#20986;&#29616;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65306;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#25110;&#23567;&#20998;&#23376;&#30340;&#27874;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28201;&#39034;&#20989;&#25968;&#22312;&#20219;&#20309;&#23436;&#20840;&#32500;&#24230;&#30340;&#31435;&#26041;&#20307;&#19978;&#21487;&#29992;&#20998;&#27573;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#28201;&#39034;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;A-CMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ACRL&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.01568</link><description>&lt;p&gt;
Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG]) &#65288;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#65289;
&lt;/p&gt;
&lt;p&gt;
Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;A-CMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ACRL&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;A-CMDP&#65289;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21463;&#38480;&#21046;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#38543;&#26426;&#21160;&#24577;&#20013;&#20248;&#21270;&#26399;&#26395;&#22238;&#25253;&#21516;&#26102;&#32422;&#26463;&#26399;&#26395;&#25104;&#26412;&#65292;&#20294;&#26159;&#29305;&#23450;&#24773;&#33410;&#20013;&#30340;&#25104;&#26412;&#20173;&#28982;&#21487;&#33021;&#38750;&#24120;&#39640;&#12290;&#30456;&#21453;&#65292;A-CMDP&#30340;&#30446;&#26631;&#26159;&#22312;&#20219;&#24847;&#19968;&#36718;&#30340;&#20219;&#20309;&#24773;&#33410;&#20013;&#65292;&#20248;&#21270;&#26399;&#26395;&#22238;&#25253;&#21516;&#26102;&#20445;&#35777;&#26377;&#30028;&#30340;&#25104;&#26412;&#65292;&#24182;&#38024;&#23545;&#31574;&#30053;&#20808;&#39564;&#36827;&#34892;&#20102;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Anytime-Competitive Reinforcement Learning&#65288;ACRL&#65289;&#65292;&#23427;&#21487;&#20197;&#35777;&#26126;&#22320;&#20445;&#35777;&#20102;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#12290;&#36951;&#25022;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#22312;&#20219;&#24847;&#30340;&#31454;&#20105;&#24615;&#32422;&#26463;&#19979;&#28176;&#36817;&#22320;&#19982;&#26368;&#20248;&#22238;&#25253;&#30456;&#21305;&#37197;&#12290;&#22312;&#30899;&#26234;&#33021;&#35745;&#31639;&#24212;&#29992;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;ACRL&#30340;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2311.01441</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#32467;&#21512;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#33976;&#39311;&#65292;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#22686;&#30410;&#65292;&#20197;&#27492;&#21453;&#39539;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#20250;&#25104;&#20026;&#26356;&#22909;&#30340;&#25945;&#24072;&#30340;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#65288;Discrete Adversarial Distillation&#65292;DAD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;VQGAN&#23558;&#20854;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#21019;&#36896;&#20986;&#27604;&#26631;&#20934;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#30340;&#40065;&#26834;&#24615;&#21644;&#24178;&#20928;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#31867;&#20284;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#21152;&#20102;&#23569;&#37327;&#30340;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.01378</link><description>&lt;p&gt;
Vision-Language Foundation Models&#20316;&#20026;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#23427;&#20204;&#29702;&#35299;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#35270;&#35273;&#35821;&#35328;&#25805;&#20316;&#26694;&#26550;&#65292;&#21517;&#20026;RoboFlamingo&#65292;&#23427;&#24314;&#31435;&#22312;&#24320;&#28304;&#30340;VLMs&#65292;OpenFlamingo&#20043;&#19978;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;RoboFlamingo&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VLMs&#36827;&#34892;&#21333;&#27493;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#20351;&#29992;&#26174;&#24335;&#31574;&#30053;&#22836;&#27169;&#25311;&#39034;&#24207;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#21482;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#20998;&#35299;&#20026;RoboFlamingo&#25552;&#20379;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#36827;&#34892;&#24320;&#29615;&#25511;&#21046;&#21644;&#37096;&#32626;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#22312;&#27979;&#35797;&#22522;&#20934;&#19978;&#22823;&#24133;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RoboFlamingo&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01256</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#28304;&#30340;&#27861;&#24459;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#24120;&#35265;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35780;&#20272;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36861;&#27714;&#26368;&#20339;&#24615;&#33021;&#30340;&#31454;&#20105;&#20013;&#65292;&#32463;&#24120;&#24573;&#35270;&#35768;&#22810;&#37325;&#35201;&#22240;&#32032;&#65292;&#32780;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#22240;&#32032;&#24212;&#35813;&#34987;&#20180;&#32454;&#32771;&#34385;&#12290;&#23454;&#38469;&#19978;&#65292;&#26377;&#26102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#32780;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#22240;&#32032;&#24517;&#39035;&#32771;&#34385;&#22312;&#20869;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;NLP&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LexGLUE&#22522;&#20934;&#19978;&#23545;LLM&#21644;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;SVM&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#21516;&#26102;&#32771;&#34385;&#24615;&#33021;&#65288;&#26631;&#20934;&#25351;&#26631;&#65289;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#22914;&#26102;&#38388;&#12289;&#32791;&#33021;&#21644;&#25104;&#26412;&#65292;&#24635;&#20043;&#23601;&#26159;&#30899;&#36275;&#36857;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#32771;&#34385;&#20102;&#21407;&#22411;&#35774;&#35745;&#38454;&#27573;&#65288;&#36890;&#36807;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#36845;&#20195;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65289;&#21644;&#29983;&#20135;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#30001;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#20013;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#23567;&#26041;&#24046;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#36215;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.17496</link><description>&lt;p&gt;
&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#65306;&#19968;&#31181;&#21152;&#26435;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach. (arXiv:2310.17496v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#30001;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#20013;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#23567;&#26041;&#24046;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#36215;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26631;&#20934;&#27969;&#31243;&#28041;&#21450;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#29992;&#25143;&#34892;&#20026;&#24182;&#25345;&#32493;&#25913;&#36827;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#21487;&#33021;&#22312;A/B&#27979;&#35797;&#20013;&#24341;&#20837;&#24178;&#25200;&#65292;&#20854;&#20013;&#25511;&#21046;&#32452;&#21644;&#23454;&#39564;&#32452;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21152;&#26435;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#20986;&#29616;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#25968;&#25454;&#20013;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#20272;&#35745;&#37327;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#26041;&#24046;&#65292;&#19988;&#19981;&#20250;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators without causing shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#21270;&#21644;&#29983;&#25104;&#22810;&#20010;&#22270;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20165;&#20351;&#29992;&#21333;&#20010;&#35266;&#23519;&#22270;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#22238;&#24402;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#32593;&#32476;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#22270;&#19978;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#32467;&#21512;PAC-Bayesian&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21462;&#24471;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.16401</link><description>&lt;p&gt;
&#20855;&#26377;&#21442;&#25968;&#21270;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with a Distribution of Parametrized Graphs. (arXiv:2310.16401v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#21270;&#21644;&#29983;&#25104;&#22810;&#20010;&#22270;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20165;&#20351;&#29992;&#21333;&#20010;&#35266;&#23519;&#22270;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#22238;&#24402;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#32593;&#32476;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#22270;&#19978;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#32467;&#21512;PAC-Bayesian&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21462;&#24471;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#20351;&#29992;&#21333;&#20010;&#35266;&#23519;&#21040;&#30340;&#22270;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#30340;&#22270;&#20165;&#20195;&#34920;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#23454;&#29616;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22270;&#21487;&#33021;&#20250;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#23384;&#22312;&#38169;&#35823;&#25110;&#32570;&#22833;&#30340;&#36793;&#65292;&#20197;&#21450;&#25552;&#20379;&#24456;&#23569;&#20449;&#24687;&#20215;&#20540;&#30340;&#36793;&#26435;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#25429;&#25417;&#20808;&#21069;&#22312;&#35266;&#23519;&#21040;&#30340;&#22270;&#20013;&#32570;&#22833;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#21442;&#25968;&#21270;&#21644;&#29983;&#25104;&#22810;&#20010;&#22270;&#12290;&#25105;&#20204;&#22522;&#20110;&#22810;&#20010;&#22270;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#32593;&#32476;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#36845;&#20195;&#22320;&#30830;&#23450;&#22270;&#30340;&#20998;&#24067;&#65292;&#32467;&#21512;PAC-Bayesian&#29702;&#35770;&#30340;&#21407;&#21017;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#24322;&#36136;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#21270;&#23398;&#25968;&#25454;&#38598;&#30340;&#22270;&#22238;&#24402;&#19978;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, graph neural networks have been trained using a single observed graph. However, the observed graph represents only one possible realization. In many applications, the graph may encounter uncertainties, such as having erroneous or missing edges, as well as edge weights that provide little informative value. To address these challenges and capture additional information previously absent in the observed graph, we introduce latent variables to parameterize and generate multiple graphs. We obtain the maximum likelihood estimate of the network parameters in an Expectation-Maximization (EM) framework based on the multiple graphs. Specifically, we iteratively determine the distribution of the graphs using a Markov Chain Monte Carlo (MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical experiments demonstrate improvements in performance against baseline models on node classification for heterogeneous graphs and graph regression on chemistry datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#38706;&#30340;&#27934;&#23519;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#20013;&#38544;&#31169;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.15524</link><description>&lt;p&gt;
&#20851;&#20110;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20869;&#22312;&#38544;&#31169;&#23646;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#38706;&#30340;&#27934;&#23519;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#20013;&#38544;&#31169;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#38382;&#39064;&#23548;&#33268;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#28608;&#22686;&#65292;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#20294;&#22312;&#25552;&#20379;&#25968;&#23398;&#29305;&#24449;&#21270;&#20854;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#20869;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24320;&#21019;&#24615;&#29702;&#35770;&#30740;&#31350;&#65292;&#29992;&#20110;&#31163;&#25955;&#25968;&#25454;&#38598;&#29983;&#25104;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#27599;&#20010;&#23454;&#20363;&#24046;&#24322;&#38544;&#31169;&#65288;pDP&#65289;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#38416;&#26126;&#20102;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#28508;&#22312;&#38544;&#31169;&#27844;&#38706;&#65292;&#20174;&#32780;&#20026;&#36890;&#36807;DDMs&#38477;&#20302;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#38544;&#31169;&#39118;&#38505;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;$s$&#20010;&#22823;&#23567;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#20174;$(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP&#21040;$(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP&#30340;&#28608;&#22686;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy concerns have led to a surge in the creation of synthetic datasets, with diffusion models emerging as a promising avenue. Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pu
&lt;/p&gt;</description></item><item><title>DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15393</link><description>&lt;p&gt;
DoGE: &#20351;&#29992;&#27867;&#21270;&#20272;&#35745;&#36827;&#34892;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15393
&lt;/p&gt;
&lt;p&gt;
DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#35821;&#26009;&#24211;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#32452;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30001;&#21508;&#31181;&#26469;&#28304;&#39046;&#22495;&#65288;&#22914;CommonCrawl&#12289;Wikipedia&#12289;Github&#31561;&#65289;&#25353;&#29031;&#29305;&#23450;&#30340;&#37319;&#26679;&#27010;&#29575;&#65288;&#39046;&#22495;&#26435;&#37325;&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#26435;&#37325;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DOmain reweighting with Generalization Estimation&#65288;DoGE&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#26032;&#35843;&#25972;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#65292;&#26681;&#25454;&#23427;&#23545;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27867;&#21270;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#35757;&#32451;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#33719;&#21462;&#37325;&#26032;&#21152;&#26435;&#30340;&#39046;&#22495;&#26435;&#37325;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#38236;&#20687;&#19979;&#38477;&#27861;&#26356;&#26032;&#39046;&#22495;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#30340;&#27867;&#21270;&#22686;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#33719;&#24471;&#30340;&#39046;&#22495;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#23436;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#65292;&#22312;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#36827;&#34892;&#20102;&#20855;&#20307;&#30340;&#20272;&#35745;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#25269;&#25239;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#31561;&#20851;&#38190;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15330</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#24322;&#26500;&#28151;&#21512;&#27169;&#22411;&#30340;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Federated Learning: A Federated Gradient EM Algorithm for Heterogeneous Mixture Models with Robustness against Adversarial Attacks. (arXiv:2310.15330v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#65292;&#22312;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#36827;&#34892;&#20102;&#20855;&#20307;&#30340;&#20272;&#35745;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#25269;&#25239;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#31561;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#65292;&#28982;&#21518;&#23558;&#36825;&#19968;&#36890;&#29992;&#29702;&#35770;&#24212;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#20197;&#25551;&#36848;&#27169;&#22411;&#21442;&#25968;&#21644;&#28151;&#21512;&#27604;&#20363;&#30340;&#26174;&#24335;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#24377;&#24615;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. In this paper, we introduce a novel federated gradient EM algorithm designed for the unsupervised learning of mixture models with heterogeneous mixture proportions across tasks. We begin with a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on Gaussian Mixture Models (GMMs) and Mixture of Regressions (MoRs) to characterize the explicit estimation error of model parameters and mixture proportions. Our proposed federated gradient EM algorithm demonstrates several key advantages: adaptability to unknown task similarity, resilience against adversarial attacks on a small fraction of data sources, protection of local data privacy, and computational and communication efficiency.
&lt;/p&gt;</description></item><item><title>RealFM &#26159;&#19968;&#20010;&#30495;&#23454;&#30340;&#32852;&#37030;&#26426;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#20013;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39547;&#27850;&#32773;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#25311;&#35774;&#22791;&#25928;&#29992;&#12289;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#30340;&#25928;&#29992;&#21644;&#25968;&#25454;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13681</link><description>&lt;p&gt;
RealFM: &#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation. (arXiv:2310.13681v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13681
&lt;/p&gt;
&lt;p&gt;
RealFM &#26159;&#19968;&#20010;&#30495;&#23454;&#30340;&#32852;&#37030;&#26426;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#20013;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39547;&#27850;&#32773;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#25311;&#35774;&#22791;&#25928;&#29992;&#12289;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#30340;&#25928;&#29992;&#21644;&#25968;&#25454;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#24120;&#22312;&#35774;&#22791;-&#26381;&#21153;&#22120;&#36890;&#20449;&#30340;&#35270;&#35282;&#19979;&#36827;&#34892;&#30740;&#31350;&#65288;&#20363;&#22914;&#35774;&#22791;&#25481;&#32447;&#65289;&#65292;&#24182;&#20551;&#35774;&#36793;&#32536;&#35774;&#22791;&#26377;&#25345;&#32493;&#21442;&#19982;FL&#30340;&#24895;&#26395;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23454;&#26045;&#24403;&#21069;&#30340;FL&#26694;&#26550;&#23384;&#22312;&#32570;&#38519;&#65292;&#35768;&#22810;&#26694;&#26550;&#36935;&#21040;&#20102;&#39547;&#27850;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#23558;FL&#25512;&#21521;&#26356;&#30495;&#23454;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RealFM&#65306;&#31532;&#19968;&#20010;&#30495;&#27491;&#30340;&#32852;&#37030;&#26426;&#21046;&#65292;&#23427;&#65288;1&#65289;&#23454;&#38469;&#22320;&#27169;&#25311;&#35774;&#22791;&#25928;&#29992;&#65292;&#65288;2&#65289;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;&#65292;&#65288;3&#65289;&#21487;&#35777;&#26126;&#22320;&#28040;&#38500;&#20102;&#39547;&#27850;&#32773;&#29616;&#35937;&#12290;RealFM&#19981;&#38656;&#35201;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#23384;&#22312;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#19981;&#21442;&#19982;&#21644;&#20854;&#20182;FL&#26426;&#21046;&#30340;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#21644;&#21442;&#19982;&#35774;&#22791;&#30340;&#25928;&#29992;&#21644;&#25968;&#25454;&#36129;&#29486;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#65292;RealFM&#25552;&#39640;&#20102;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#30340;&#25928;&#29992;&#20197;&#21450;&#25968;&#25454;&#36129;&#29486;&#65292;&#26368;&#22810;&#21487;&#36798;...
&lt;/p&gt;
&lt;p&gt;
Edge device participation in federating learning (FL) has been typically studied under the lens of device-server communication (e.g., device dropout) and assumes an undying desire from edge devices to participate in FL. As a result, current FL frameworks are flawed when implemented in real-world settings, with many encountering the free-rider problem. In a step to push FL towards realistic settings, we propose RealFM: the first truly federated mechanism which (1) realistically models device utility, (2) incentivizes data contribution and device participation, and (3) provably removes the free-rider phenomena. RealFM does not require data sharing and allows for a non-linear relationship between model accuracy and utility, which improves the utility gained by the server and participating devices compared to non-participating devices as well as devices participating in other FL mechanisms. On real-world data, RealFM improves device and server utility, as well as data contribution, by up t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13397</link><description>&lt;p&gt;
&#31561;&#21464;&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Equivariant Deep Weight Space Alignment. (arXiv:2310.13397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#20351;&#24471;&#31616;&#21333;&#25805;&#20316;&#22914;&#27169;&#22411;&#24179;&#22343;&#21644;&#30456;&#20284;&#24230;&#20272;&#35745;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23545;&#40784;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#21363;&#25214;&#21040;&#23427;&#20204;&#20043;&#38388;&#26368;&#20248;&#25490;&#21015;&#65292;&#26159;&#24517;&#35201;&#30340;&#12290;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26435;&#37325;&#23545;&#40784;&#23545;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20174;&#27169;&#22411;&#21512;&#24182;&#65292;&#36890;&#36807;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#31354;&#38388;&#65292;&#21040;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#26377;&#24847;&#20041;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26435;&#37325;&#23545;&#40784;&#26159;&#19968;&#20010;NP-hard&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#23548;&#33268;&#26041;&#27861;&#32791;&#26102;&#25110;&#32773;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26435;&#37325;&#23545;&#40784;&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#23545;&#31216;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11677</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35774;&#35745;&#26679;&#26412;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;ANPG&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#26469;&#33719;&#21462;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#12290;ANPG&#31639;&#27861;&#22312;&#19968;&#33324;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;O(&#949;^{-2})&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;O(&#949;^{-1})&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#949;&#23450;&#20041;&#20102;&#26368;&#20248;&#24615;&#35823;&#24046;&#12290;&#36825;&#23558;&#26679;&#26412;&#22797;&#26434;&#24230;&#25552;&#39640;&#20102;&#19968;&#20010;log(1/&#949;)&#30340;&#22240;&#23376;&#12290;ANPG&#26159;&#19968;&#20010;&#19968;&#38454;&#31639;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29616;&#26377;&#25991;&#29486;&#20013;&#21487;&#33021;&#26080;&#27861;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;(IS)&#26435;&#37325;&#26041;&#24046;&#19978;&#30028;&#30340;&#20551;&#35774;&#12290;&#22312;&#26080;Hessian&#21644;&#26080;IS&#31639;&#27861;&#31867;&#20013;&#65292;ANPG&#36229;&#36807;&#20102;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;O(&#949;^{-\frac{1}{2}})&#30340;&#22240;&#23376;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their state-of-the-art it
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08164</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#21644;&#32463;&#36807;RLHF&#35843;&#25972;&#30340;&#29256;&#26412;&#30340;&#28608;&#27963;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#21453;&#26144;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24773;&#26223;&#65292;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20196;&#29260;-&#22870;&#21169;&#26144;&#23556;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36825;&#26159;&#39318;&#27425;&#24212;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#37322;&#23398;&#20064;&#22870;&#21169;&#21644;&#24191;&#27867;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#65292;&#36825;&#20026;&#30830;&#20445;&#25351;&#23450;&#30446;&#26631;&#21644;&#27169;&#22411;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2310.07891</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#30340;&#22522;&#26412;&#21407;&#22240;&#20043;&#19968;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24050;&#32463;&#20005;&#26684;&#35777;&#26126;&#65292;&#22312;&#20004;&#23618;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31532;&#19968;&#23618;&#36827;&#34892;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#23618;&#36827;&#34892;&#23725;&#22238;&#24402;&#21487;&#20197;&#23548;&#33268;&#29305;&#24449;&#23398;&#20064;&#65307;&#29305;&#24449;&#30697;&#38453;&#30340;&#35889;&#20013;&#20250;&#20986;&#29616;&#20998;&#31163;&#30340;&#19968;&#32500;&#32452;&#20214;&#65292;&#31216;&#20026;&#8220;spike&#8221;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22266;&#23450;&#26799;&#24230;&#19979;&#38477;&#27493;&#38271;&#26102;&#65292;&#36825;&#20010;&#8220;spike&#8221;&#20165;&#25552;&#20379;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#20214;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#23398;&#20064;&#38750;&#32447;&#24615;&#32452;&#20214;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23398;&#20064;&#29575;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#38271;&#26102;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#23454;&#38469;&#19978;&#24341;&#20837;&#20102;&#22810;&#20010;&#19968;&#32500;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#19968;&#20010;&#29305;&#23450;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#22823;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#35757;&#32451;&#21644;&#27979;&#35797;&#35823;&#24046;&#23436;&#20840;&#30001;&#36825;&#20123;&#8220;spike&#8221;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#20998;&#26512;&#20102;&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adv-NTK&#30340;AT&#31639;&#27861;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06112</link><description>&lt;p&gt;
&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#20998;&#26512;&#65306;&#19968;&#31181;NTK&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach. (arXiv:2310.06112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#20998;&#26512;&#20102;&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adv-NTK&#30340;AT&#31639;&#27861;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(AT)&#26159;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#40065;&#26834;&#24615;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23454;&#39564;&#35777;&#26126;&#23427;&#23384;&#22312;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#21363;&#38271;&#26102;&#38388;&#30340;AT&#21487;&#33021;&#23545;DNNs&#30340;&#40065;&#26834;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;DNNs&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#29702;&#35770;&#38750;&#24179;&#20961;&#22320;&#25193;&#23637;&#21040;AT&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;AT&#35757;&#32451;&#30340;&#23485;DNN&#21487;&#20197;&#24456;&#22909;&#22320;&#36817;&#20284;&#20026;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;DNN&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24179;&#26041;&#25439;&#22833;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#32447;&#24615;&#21270;DNN&#30340;&#38381;&#24335;AT&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;AT&#36864;&#21270;&#29616;&#35937;&#65306;&#38271;&#26399;&#30340;AT&#23558;&#23548;&#33268;&#23485;DNN&#36864;&#21270;&#20026;&#27809;&#26377;AT&#30340;DNN&#65292;&#20174;&#32780;&#24341;&#36215;&#40065;&#26834;&#36807;&#25311;&#21512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Adv-NTK&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#38024;&#23545;&#26080;&#38480;&#23485;&#30340;DNNs&#30340;AT&#31639;&#27861;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Adv-NTK&#21487;&#20197;&#24110;&#21161;&#26080;&#38480;&#23485;&#30340;DNNs&#25552;&#21319;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05401</link><description>&lt;p&gt;
Entropy-MCMC: &#36731;&#26494;&#20174;&#24179;&#22374;&#30406;&#22320;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#23545;&#21518;&#39564;&#20998;&#24067;&#30340;&#36136;&#37327;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#20998;&#24067;&#22312;&#24615;&#36136;&#19978;&#26159;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#65292;&#23616;&#37096;&#27169;&#24335;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#65292;&#20174;&#21407;&#22987;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#19968;&#20123;&#26679;&#26412;&#21487;&#33021;&#20250;&#38519;&#20837;&#8220;&#22351;&#8221;&#27169;&#24335;&#24182;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#20302;&#27867;&#21270;&#35823;&#24046;&#30340;&#8220;&#22909;&#8221;&#27169;&#24335;&#36890;&#24120;&#23384;&#22312;&#20110;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#22374;&#30406;&#22320;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20559;&#32622;&#37319;&#26679;&#26397;&#21521;&#36825;&#20123;&#24179;&#22374;&#21306;&#22495;&#30340;&#21518;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#24341;&#23548;&#21464;&#37327;&#65292;&#20854;&#31283;&#24577;&#20998;&#24067;&#31867;&#20284;&#20110;&#24179;&#28369;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#27809;&#26377;&#23574;&#38160;&#30340;&#27169;&#24577;&#65292;&#20197;&#24341;&#23548;MCMC&#37319;&#26679;&#22120;&#22312;&#24179;&#22374;&#30340;&#30406;&#22320;&#20013;&#37319;&#26679;&#12290;&#36890;&#36807;&#23558;&#27492;&#24341;&#23548;&#21464;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#19979;&#23454;&#29616;&#39640;&#25928;&#37319;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20803;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#35299;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35821;&#20041;&#29702;&#35299;&#20013;&#30340;&#38544;&#21547;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.04861</link><description>&lt;p&gt;
&#36890;&#36807;&#21306;&#20998;&#20301;&#32622;&#21644;&#19978;&#19979;&#25991;&#26469;&#25581;&#31034;Transformers&#20013;&#30340;&#38544;&#34255;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Uncovering hidden geometry in Transformers via disentangling position and context. (arXiv:2310.04861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#35299;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35821;&#20041;&#29702;&#35299;&#20013;&#30340;&#38544;&#21547;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24191;&#27867;&#29992;&#20110;&#20174;&#36755;&#20837;&#20196;&#29260;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#35821;&#20041;&#24847;&#20041;&#65292;&#28982;&#32780;&#23427;&#20204;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#36816;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#20449;&#24687;&#20016;&#23500;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22909;&#30340;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65288;&#25110;&#23884;&#20837;&#65289;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#32452;&#20214;&#12290;&#23545;&#20110;&#20219;&#20309;&#23618;&#65292;&#36755;&#20837;&#24207;&#21015;&#26679;&#26412;&#30340;&#23884;&#20837;&#21521;&#37327;&#30001;&#19968;&#20010;&#24352;&#37327;&#34920;&#31034; $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$&#12290;&#32473;&#23450;&#22312;&#24207;&#21015;&#65288;&#25110;&#19978;&#19979;&#25991;&#65289; $c \le C$ &#30340;&#20301;&#32622; $t \le T$ &#22788;&#30340;&#23884;&#20837;&#21521;&#37327; $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$&#65292;&#25552;&#21462;&#22343;&#20540;&#25928;&#26524;&#24471;&#21040;&#20998;&#35299;&#24418;&#24335; \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] &#20854;&#20013; $\boldsymbol{\mu}$ &#26159;&#20840;&#23616;&#22343;&#20540;&#21521;&#37327;&#65292;$\mathbf{pos}_t$ &#21644; $\mathbf{ctx}_c$ &#20998;&#21035;&#26159;&#36328;&#19978;&#19979;&#25991;&#21644;&#36328;&#20301;&#32622;&#30340;&#22343;&#20540;&#21521;&#37327;&#65292;$\mathbf{resid}_{c,t}$ &#26159;&#27531;&#20313;&#21521;&#37327;&#12290;&#38024;&#23545;&#27969;&#34892;&#30340;transformer&#26550;&#26500;&#21644;&#22810;&#26679;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Transformers are widely used to extract complex semantic meanings from input tokens, yet they usually operate as black-box models. In this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. For any layer, embedding vectors of input sequence samples are represented by a tensor $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$. Given embedding vector $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a sequence (or context) $c \le C$, extracting the mean effects yields the decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the residual vector. For popular transformer architectures and diverse text datasets, empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;IMU&#21644;&#20998;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30417;&#27979;&#24180;&#38271;&#32773;&#22885;&#22612;&#21733;&#38203;&#28860;&#30340;&#20934;&#30830;&#31995;&#32479;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21028;&#26029;&#24739;&#32773;&#26159;&#22312;&#36827;&#34892;OEP&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65292;&#21487;&#20197;&#30417;&#27979;OEP&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.03512</link><description>&lt;p&gt;
&#29992;&#21333;&#19968;IMU&#21644;&#20998;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30417;&#27979;&#24180;&#38271;&#32773;&#30340;&#22885;&#22612;&#21733;&#38203;&#28860;
&lt;/p&gt;
&lt;p&gt;
Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models. (arXiv:2310.03512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;IMU&#21644;&#20998;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30417;&#27979;&#24180;&#38271;&#32773;&#22885;&#22612;&#21733;&#38203;&#28860;&#30340;&#20934;&#30830;&#31995;&#32479;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21028;&#26029;&#24739;&#32773;&#26159;&#22312;&#36827;&#34892;OEP&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65292;&#21487;&#20197;&#30417;&#27979;OEP&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22885;&#22612;&#21733;&#38203;&#28860;&#35745;&#21010;(OEP)&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#24180;&#38271;&#32773;&#33030;&#24369;&#12289;&#32908;&#23569;&#30151;&#21644;&#24179;&#34913;&#30340;&#24247;&#22797;&#35745;&#21010;&#12290;&#20934;&#30830;&#22320;&#30417;&#27979;&#24739;&#32773;&#21442;&#19982;OEP&#30340;&#24773;&#20917;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#33258;&#25105;&#25253;&#21578;(&#26085;&#35760;)&#36890;&#24120;&#19981;&#21487;&#38752;&#12290;&#38543;&#30528;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#21457;&#23637;&#65292;&#21033;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;(HAR)&#30340;&#31995;&#32479;&#24050;&#32463;&#25913;&#21464;&#20102;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;OEP&#30340;&#20351;&#29992;&#20173;&#28982;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#19981;&#26174;&#30524;&#19988;&#20934;&#30830;&#30340;&#31995;&#32479;&#65292;&#20197;&#30417;&#27979;&#24180;&#38271;&#32773;&#21442;&#19982;OEP&#30340;&#24773;&#20917;&#12290;&#25968;&#25454;&#26159;&#20174;&#20329;&#25140;&#21333;&#20010;&#33136;&#37096;IMU&#30340;&#24180;&#38271;&#32773;&#36523;&#19978;&#25910;&#38598;&#30340;&#12290;&#25910;&#38598;&#20102;&#20004;&#32452;&#25968;&#25454;&#65292;&#19968;&#32452;&#26159;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#19968;&#32452;&#26159;&#22312;&#24739;&#32773;&#23478;&#20013;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#31995;&#32479;&#65292;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;1)&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;10&#20998;&#38047;&#30340;&#28369;&#21160;&#31383;&#21475;&#35782;&#21035;&#24739;&#32773;&#26159;&#22312;&#36827;&#34892;OEP&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;(ADLs)&#65307;2)&#22522;&#20110;&#31532;&#19968;&#38454;&#27573;&#65292;&#20351;&#29992;6&#31186;&#30340;&#28369;&#21160;&#31383;&#21475;&#37325;&#26032;&#35782;&#21035;OEP&#19982;ADLs&#12290;
&lt;/p&gt;
&lt;p&gt;
Otago Exercise Program (OEP) is a rehabilitation program for older adults to improve frailty, sarcopenia, and balance. Accurate monitoring of patient involvement in OEP is challenging, as self-reports (diaries) are often unreliable. With the development of wearable sensors, Human Activity Recognition (HAR) systems using wearable sensors have revolutionized healthcare. However, their usage for OEP still shows limited performance. The objective of this study is to build an unobtrusive and accurate system to monitor OEP for older adults. Data was collected from older adults wearing a single waist-mounted Inertial Measurement Unit (IMU). Two datasets were collected, one in a laboratory setting, and one at the homes of the patients. A hierarchical system is proposed with two stages: 1) using a deep learning model to recognize whether the patients are performing OEP or activities of daily life (ADLs) using a 10-minute sliding window; 2) based on stage 1, using a 6-second sliding window to re
&lt;/p&gt;</description></item><item><title>Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03052</link><description>&lt;p&gt;
Memoria: &#29992;&#20110;&#31867;&#20154;&#39034;&#24207;&#22788;&#29702;&#30340;&#28023;&#27604;&#23433;&#35760;&#24518;&#20307;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03052
&lt;/p&gt;
&lt;p&gt;
Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#23481;&#37327;&#65292;Transformer &#24456;&#38590;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#12290;&#34429;&#28982;&#22686;&#21152;&#36755;&#20837;&#38271;&#24230;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26080;&#27490;&#22659;&#22320;&#22686;&#21152;&#38271;&#24230;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#27492;&#22806;&#65292;&#19982; Transformer &#19981;&#21516;&#65292;&#20154;&#31867;&#26377;&#36873;&#25321;&#24615;&#22320;&#35760;&#20303;&#21644;&#20351;&#29992;&#20165;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#21040;&#23614;&#22788;&#29702;&#25152;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Memoria&#65292;&#19968;&#20010;&#24212;&#29992;&#28023;&#27604;&#23433;&#35760;&#24518;&#24418;&#25104;&#29702;&#35770;&#30340;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;Memoria &#22312;&#24037;&#20316;&#35760;&#24518;&#12289;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22810;&#20010;&#35760;&#24518;&#23618;&#32423;&#19978;&#23384;&#20648;&#21644;&#26816;&#32034;&#31216;&#20026; engram &#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#12290;&#36890;&#36807;&#19982;&#35832;&#22914; BERT &#21644; GPT &#31561;&#27969;&#34892;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986; Memoria &#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSL-GFN&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02823</link><description>&lt;p&gt;
&#23398;&#20064;&#28201;&#24230;&#26465;&#20214;&#19979;&#23610;&#24230;&#26631;&#37327;&#21270;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Learning to Scale Logits for Temperature-Conditional GFlowNets. (arXiv:2310.02823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02823
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSL-GFN&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26159;&#19968;&#31181;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#31574;&#30053;&#26469;&#39034;&#24207;&#29983;&#25104;&#32452;&#21512;&#32467;&#26500;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#12290;&#23427;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#25353;&#27604;&#20363;&#37319;&#26679;&#20855;&#26377;&#30456;&#24212;&#28201;&#24230;&#35843;&#33410;&#30340;&#23545;&#35937;&#30340;&#22870;&#21169;&#12290;&#22312;GFlowNets&#20013;&#65292;&#28201;&#24230;&#26465;&#20214;&#19979;&#30340;GFlowNets&#20195;&#34920;&#20102;&#19968;&#31995;&#21015;&#30001;&#28201;&#24230;&#32034;&#24341;&#30340;&#31574;&#30053;&#65292;&#27599;&#20010;&#31574;&#30053;&#19982;&#30456;&#24212;&#30340;&#28201;&#24230;&#35843;&#33410;&#22870;&#21169;&#20989;&#25968;&#30456;&#20851;&#32852;&#12290;&#28201;&#24230;&#26465;&#20214;&#19979;&#30340;GFlowNets&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#26469;&#25511;&#21046;&#23545;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#28201;&#24230;&#26465;&#20214;&#19979;&#23610;&#24230;&#26631;&#37327;&#21270;&#30340;GFlowNets&#65288;LSL-GFN&#65289;&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23427;&#26497;&#22823;&#22320;&#21152;&#36895;&#20102;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#20043;&#21069;&#25552;&#20986;&#30340;&#28201;&#24230;&#26465;&#20214;&#26041;&#27861;&#22312;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#24341;&#20837;&#20102;&#25968;&#20540;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#28201;&#24230;&#21487;&#33021;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
GFlowNets are probabilistic models that learn a stochastic policy that sequentially generates compositional structures, such as molecular graphs. They are trained with the objective of sampling such objects with probability proportional to the object's reward. Among GFlowNets, the temperature-conditional GFlowNets represent a family of policies indexed by temperature, and each is associated with the correspondingly tempered reward function. The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02702</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#28151;&#21512;&#24322;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization. (arXiv:2310.02702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#25968;&#25454;&#26679;&#26412;&#34987;&#20998;&#25955;&#21644;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#34920;&#29616;&#20986;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#26159;&#29420;&#31435;&#21644;&#30456;&#21516;&#30340;&#12290;&#27492;&#22806;&#65292;&#31995;&#32479;&#24322;&#36136;&#24615;&#65292;&#21363;&#23458;&#25143;&#31471;&#35745;&#31639;&#33021;&#21147;&#30340;&#21464;&#21270;&#65292;&#20250;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20559;&#24046;&#12290;&#32479;&#35745;&#21644;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#32508;&#21512;&#25928;&#24212;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#32852;&#37030;&#20248;&#21270;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#35880;&#30340;&#35752;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26381;&#21153;&#22120;&#31471;&#20248;&#21270;&#65292;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26381;&#21153;&#22120;&#26356;&#26032;&#26041;&#21521;&#19978;&#33258;&#36866;&#24212;&#22320;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26381;&#21153;&#22120;&#31471;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning refers to a distributed machine learning paradigm in which data samples are decentralized and distributed among multiple clients. These samples may exhibit statistical heterogeneity, which refers to data distributions are not independent and identical across clients. Additionally, system heterogeneity, or variations in the computational power of the clients, introduces biases into federated learning. The combined effects of statistical and system heterogeneity can significantly reduce the efficiency of federated optimization. However, the impact of hybrid heterogeneity is not rigorously discussed. This paper explores how hybrid heterogeneity affects federated optimization by investigating server-side optimization. The theoretical results indicate that adaptively maximizing gradient diversity in server update direction can help mitigate the potential negative consequences of hybrid heterogeneity. To this end, we introduce a novel server-side gradient-based optimizer \
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#65292;&#25506;&#32034;&#20102;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02698</link><description>&lt;p&gt;
&#25506;&#32034;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#30340;&#32852;&#37030;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling. (arXiv:2310.02698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#65292;&#25506;&#32034;&#20102;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#23545;&#19968;&#37096;&#20998;&#23458;&#25143;&#36827;&#34892;&#37319;&#26679;&#26469;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;&#26469;&#33258;&#37319;&#26679;&#23458;&#25143;&#30340;&#20449;&#24687;&#24314;&#31435;&#20840;&#23616;&#27169;&#22411;&#30340;&#20840;&#23616;&#20272;&#35745;&#26041;&#24046;&#19982;&#32852;&#37030;&#20248;&#21270;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#8220;&#20813;&#36153;&#8221;&#30340;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#26500;&#24314;&#20102;&#26377;&#21069;&#36884;&#30340;&#37319;&#26679;&#27010;&#29575;&#21644;&#21487;&#38752;&#30340;&#20840;&#23616;&#20272;&#35745;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26412;&#22320;&#36890;&#20449;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#25429;&#25417;&#20102;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#23567;&#21464;&#20307;&#65292;&#24182;&#30456;&#24212;&#25913;&#36827;&#20102;&#20840;&#23616;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#20248;&#21270;&#20013;&#36981;&#24490;&#23458;&#25143;&#37319;&#26679;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#23427;&#22312;&#36890;&#20449;&#39044;&#31639;K&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32447;&#24615;&#36895;&#29575;&#19978;&#21319;&#65292;&#20855;&#26377;&#36951;&#25022;&#36793;&#30028;$\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{}3}\big)$&#12290;&#32467;&#26524;&#26159;&#65292;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) systems usually sample a fraction of clients to conduct a training process. Notably, the variance of global estimates for updating the global model built on information from sampled clients is highly related to federated optimization quality. This paper explores a line of "free" adaptive client sampling techniques in federated optimization, where the server builds promising sampling probability and reliable global estimates without requiring additional local communication and computation. We capture a minor variant in the sampling procedure and improve the global estimation accordingly. Based on that, we propose a novel sampler called K-Vib, which solves an online convex optimization respecting client sampling in federated optimization. It achieves improved a linear speed up on regret bound $\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{3}}\big)$ with communication budget $K$. As a result, it significantly improves the performance of federat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02025</link><description>&lt;p&gt;
DeepZero: &#23558;&#38646;&#38454;&#20248;&#21270;&#24212;&#29992;&#20110;&#28145;&#24230;&#27169;&#22411;&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02025
&lt;/p&gt;
&lt;p&gt;
DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#27861;&#33719;&#21462;&#19968;&#38454;&#20449;&#24687;&#26102;&#65292;&#38646;&#38454;&#20248;&#21270;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20854;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#22312;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;DeepZero&#65292;&#19968;&#20010;&#22522;&#20110;&#38646;&#38454;&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#23558;&#38646;&#38454;&#20248;&#21270;&#25193;&#23637;&#21040;&#20174;&#38646;&#24320;&#22987;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.01886</link><description>&lt;p&gt;
&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#32447;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#31181;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20063;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#32791;&#26102;&#19988;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20250;&#32473;&#23384;&#20648;&#21644;&#26381;&#21153;&#24102;&#26469;&#24040;&#22823;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#26080;&#38656;&#35757;&#32451;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#23558;&#22810;&#20010;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#37325;&#22797;&#20351;&#29992;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#12290;&#38024;&#23545;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PERU-FFT&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#20219;&#21153;&#21521;&#37327;&#27880;&#20837;&#21040;&#19968;&#20010;mer&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
&lt;/p&gt;</description></item><item><title>Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00229</link><description>&lt;p&gt;
&#22312;&#35268;&#21010;&#20013;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00229
&lt;/p&gt;
&lt;p&gt;
Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#26377;&#24847;&#35782;&#35268;&#21010;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skipper&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#25512;&#24191;&#22312;&#26032;&#24773;&#22659;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23427;&#33258;&#21160;&#23558;&#32473;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#36825;&#20381;&#36182;&#20110;&#20174;&#22238;&#28335;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#20026;&#26377;&#21521;&#22270;&#30340;&#25277;&#35937;&#20195;&#29702;&#38382;&#39064;&#30340;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#26377;&#26395;&#25552;&#20379;&#24110;&#21161;&#12290;&#38024;&#23545;&#27867;&#21270;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17196</link><description>&lt;p&gt;
ResBit: &#22522;&#20110;&#27531;&#24046;&#20301;&#21521;&#37327;&#30340;&#31163;&#25955;&#20540;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#29420;&#28909;&#32534;&#30721;&#21521;&#37327;&#19968;&#30452;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#32500;&#24230;&#38543;&#30528;&#35201;&#34920;&#31034;&#30340;&#31163;&#25955;&#25968;&#25454;&#32447;&#24615;&#22686;&#21152;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#35270;&#20026;&#31354;&#38388;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20301;&#24207;&#21015;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21363;Analog Bits&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#35201;&#34920;&#31034;&#30340;&#31867;&#21035;&#31867;&#22411;&#25968;&#37327;&#19981;&#19968;&#23450;&#26159;2&#30340;&#24130;&#27425;&#65292;&#23548;&#33268;Analog Bits&#33021;&#22815;&#34920;&#31034;&#30340;&#33539;&#22260;&#19982;&#31867;&#21035;&#25968;&#25454;&#30340;&#33539;&#22260;&#23384;&#22312;&#24046;&#24322;&#12290;&#22914;&#26524;&#29983;&#25104;&#20102;&#36825;&#26679;&#30340;&#20540;&#65292;&#38382;&#39064;&#23601;&#26159;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#30340;&#31867;&#21035;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27531;&#24046;&#20301;&#21521;&#37327;&#65288;ResBit&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618;&#30340;&#20301;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16965</link><description>&lt;p&gt;
&#25511;&#21046;&#32452;&#21512;&#20248;&#21270;&#30340;&#36830;&#32493;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25214;&#21040;&#36817;&#20284;&#35299;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;CO&#38382;&#39064;&#19978;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30456;&#23545;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#24615;&#33021;&#24694;&#21270;&#65292;&#20294;&#23545;&#20110;PI-GNN&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#21364;&#27809;&#26377;&#22826;&#22810;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PI-GNN&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#25918;&#26494;&#31574;&#30053;&#65292;&#23398;&#20064;&#21518;&#38656;&#35201;&#20174;&#36830;&#32493;&#31354;&#38388;&#20154;&#24037;&#36716;&#25442;&#22238;&#21407;&#22987;&#31163;&#25955;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#30340;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#35299;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#25152;&#26377;&#21464;&#37327;&#37117;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16519</link><description>&lt;p&gt;
AtomSurf&#65306;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#30340;&#23398;&#20064;&#30340;&#34920;&#38754;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AtomSurf : Surface Representation for Learning on Protein Structures. (arXiv:2309.16519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;Cryo-EM&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#32467;&#26500;&#21487;&#33719;&#24471;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21151;&#33021;&#27880;&#37322;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20851;&#27880;&#21019;&#24314;&#36866;&#29992;&#20110;&#20960;&#20309;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20174;&#34507;&#30333;&#36136;&#32467;&#26500;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#23558;&#36825;&#20123;&#32467;&#26500;&#34920;&#31034;&#20026;&#20960;&#20309;&#23545;&#35937;&#65288;&#22914;&#32593;&#26684;&#12289;&#22270;&#25110;&#34920;&#38754;&#65289;&#24182;&#24212;&#29992;&#36866;&#21512;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#26041;&#27861;&#30340;&#24615;&#33021;&#23558;&#21462;&#20915;&#20110;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;&#34507;&#30333;&#36136;&#34920;&#31034;&#20026;$\textit{3D mesh surfaces}$&#24182;&#23558;&#20854;&#32435;&#20837;&#24050;&#24314;&#31435;&#30340;&#34920;&#31034;&#22522;&#20934;&#20013;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21457;&#29616;&#26159;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#20165;&#21333;&#29420;&#34920;&#38754;&#34920;&#31034;&#20284;&#20046;&#26080;&#27861;&#19982;3D&#32593;&#26684;&#31454;&#20105;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#26041;&#27861;&#65292;&#23558;&#34920;&#38754;&#34920;&#31034;&#19982;&#22270;&#34920;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.  In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.15395</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;CMDPs&#20013;&#65292;&#26080;&#27169;&#22411;&#12289;&#36951;&#25022;&#26368;&#20248;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65288;BPI&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#20302;&#36951;&#25022;&#24182;&#19988;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#26368;&#20248;&#31574;&#30053;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;CMDPs&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#22312;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#36829;&#32422;&#26102;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#21482;&#22312;&#20174;&#20197;&#21069;&#20351;&#29992;&#30340;&#31574;&#30053;&#20013;&#38543;&#26426;&#22343;&#21248;&#25277;&#26679;&#26102;&#25552;&#20379;&#24179;&#22343;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRUNING-REFINEMENT-IDENTIFICATION&#65288;PRI&#65289;&#65292;&#22522;&#20110;&#25105;&#20204;&#21457;&#29616;&#30340;CMDPs&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26500;&#24615;&#36136;&#65292;&#31216;&#20026;&#26377;&#38480;&#38543;&#26426;&#24615;&#12290;&#35813;&#23646;&#24615;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;N&#32422;&#26463;&#30340;CMDP&#65292;&#23384;&#22312;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#33267;&#22810;&#26377;N&#20010;&#38543;&#26426;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#35782;&#21035;&#20986;&#22312;&#21738;&#20010;&#27493;&#39588;&#21644;&#21738;&#20010;&#29366;&#24577;&#38656;&#35201;&#36827;&#34892;&#38543;&#26426;&#20915;&#31574;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#20915;&#31574;&#30340;&#20998;&#24067;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07794</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#24773;&#24863;&#20998;&#26512;&#12289;&#35773;&#21050;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21305;&#37197;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23384;&#22312;&#38544;&#34255;&#25110;&#20114;&#34917;&#20449;&#24687;&#30340;&#29420;&#29305;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26102;&#32852;&#21512;&#20351;&#29992;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#26469;&#30452;&#25509;&#24314;&#27169;&#36825;&#19968;&#38382;&#39064;&#12290;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#65288;ITC&#65289;&#23558;&#19968;&#31687;&#24086;&#23376;&#30340;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#38752;&#36817;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#24086;&#23376;&#20998;&#31163;&#24320;&#26469;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;ITM&#65289;&#36890;&#36807;&#24809;&#32602;&#19981;&#30456;&#20851;&#30340;&#23545;&#26469;&#20419;&#36827;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30446;&#26631;&#19982;&#20116;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22235;&#20010;&#28909;&#38376;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#33268;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection and hate speech classification. However, combining text and image information is challenging because of the idiosyncratic cross-modal semantics with hidden or complementary information present in matching image-text pairs. In this work, we aim to directly model this by proposing the use of two auxiliary losses jointly with the main task when fine-tuning any pre-trained multimodal model. Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates the understanding of semantic correspondence between images and text by penalizing unrelated pairs. We combine these objectives with five multimodal models, demonstrating consistent improvements across four popular social media datasets. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06991</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06991
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22522;&#20110;&#25490;&#24207;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26159;&#22788;&#29702;&#19978;&#19979;&#25991;&#25490;&#21517;&#20219;&#21153;&#30340;&#24378;&#22823;&#35299;&#20915;&#32773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#37197;&#23545;&#12289;&#28857;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#25490;&#24207;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#20180;&#32454;&#26657;&#20934;&#21644;&#38480;&#21046;&#35299;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#22312;&#20135;&#29983;&#30340;&#25490;&#24207;&#20013;&#20063;&#19981;&#24635;&#26159;&#33258;&#27965;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#19968;&#31181;&#21463;&#26080;&#30417;&#30563;&#25506;&#27979;&#26041;&#27861;Contrast-Consistent Search&#65288;CCS&#65289;&#21551;&#21457;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65306;&#27169;&#22411;&#23545;&#19968;&#20010;&#35821;&#21477;&#21450;&#20854;&#21542;&#23450;&#30340;&#34920;&#31034;&#24517;&#39035;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#12290;&#25105;&#20204;&#20551;&#35774;&#31867;&#20284;&#30340;&#32422;&#26463;&#36866;&#29992;&#20110;&#25152;&#26377;&#39033;&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#30456;&#20851;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pair
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#27714;&#35299;&#22120;&#22312;&#36866;&#24212;&#37325;&#24314;&#20219;&#21153;&#22256;&#38590;&#31243;&#24230;&#12289;&#25512;&#29702;&#26102;&#38388;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.06642</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25193;&#25955;&#65306;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models. (arXiv:2309.06642v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#27714;&#35299;&#22120;&#22312;&#36866;&#24212;&#37325;&#24314;&#20219;&#21153;&#22256;&#38590;&#31243;&#24230;&#12289;&#25512;&#29702;&#26102;&#38388;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#39064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#22024;&#26434;&#21644;&#21487;&#33021;&#26159;&#65288;&#38750;&#65289;&#32447;&#24615;&#30340;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#19968;&#20010;&#24178;&#20928;&#30340;&#20449;&#21495;&#12290;&#37325;&#24314;&#38382;&#39064;&#30340;&#22256;&#38590;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#21407;&#22987;&#20449;&#21495;&#30340;&#32467;&#26500;&#65292;&#36864;&#21270;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#37325;&#24314;&#27169;&#22411;&#30340;&#38544;&#24335;&#20559;&#24046;&#20197;&#21450;&#19978;&#36848;&#22240;&#32032;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#12290;&#36825;&#23548;&#33268;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#22312;&#26679;&#26412;&#38388;&#23384;&#22312;&#33258;&#28982;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;&#29616;&#20195;&#25216;&#26415;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#38382;&#39064;&#27714;&#35299;&#22120;&#22312;&#21508;&#31181;&#37325;&#24314;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#26159;&#35745;&#31639;&#22797;&#26434;&#65292;&#38590;&#20197;&#23454;&#26045;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#22823;&#22810;&#25968;&#29616;&#26377;&#27714;&#35299;&#22120;&#32570;&#20047;&#26681;&#25454;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#33258;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#38271;&#65292;&#24615;&#33021;&#19981;&#20339;&#19988;&#36164;&#28304;&#20998;&#37197;&#28010;&#36153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24494;&#35843;&#36807;&#31243;&#20013;&#36861;&#27714;&#19987;&#19994;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.06256</link><description>&lt;p&gt;
&#19987;&#19994;&#24615;&#19982;&#24191;&#27867;&#24615;&#65306;&#20851;&#20110;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24494;&#35843;&#36807;&#31243;&#20013;&#36861;&#27714;&#19987;&#19994;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20855;&#26377;&#22788;&#29702;&#22810;&#26679;&#20998;&#24067;&#21644;&#20219;&#21153;&#30340;&#24191;&#27867;&#24615;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26159;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#25110;&#35843;&#25972;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#20351;&#20854;&#33719;&#24471;&#19987;&#19994;&#24615;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#24494;&#35843;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#35206;&#30422;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#20998;&#24067;&#21644;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36861;&#27714;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#19987;&#19994;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#65292;&#36825;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;(Catastrophic Forgetting, CF)&#30456;&#20851;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;VLMs&#21644;LLMs&#20013;&#30340;&#23384;&#22312;&#12290;&#20363;&#22914;&#65292;&#23545;&#20687;CLIP&#36825;&#26679;&#30340;VLM&#36827;&#34892;&#22312;ImageNet&#19978;&#30340;&#24494;&#35843;&#20250;&#23548;&#33268;&#22788;&#29702;&#22810;&#26679;&#20998;&#24067;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;Galactica&#36827;&#34892;&#24494;&#35843;&#21017;&#20250;&#23548;&#33268;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#21516;&#26102;&#21387;&#32553;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20197;&#20445;&#30041;&#20449;&#24687;&#30340;&#32500;&#24230;&#32422;&#31616;&#25216;&#26415;&#65292;&#30456;&#36739;&#20110;&#36880;&#20010;&#21387;&#32553;&#21464;&#37327;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#36798;&#21040;&#30456;&#21516;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.05649</link><description>&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#12289;&#32500;&#24230;&#32422;&#31616;&#21644;&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck. (arXiv:2309.05649v1 [cs.IT] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05649
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#21516;&#26102;&#21387;&#32553;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20197;&#20445;&#30041;&#20449;&#24687;&#30340;&#32500;&#24230;&#32422;&#31616;&#25216;&#26415;&#65292;&#30456;&#36739;&#20110;&#36880;&#20010;&#21387;&#32553;&#21464;&#37327;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#36798;&#21040;&#30456;&#21516;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#65288;SIB&#65289;&#26159;&#19968;&#31181;&#32500;&#24230;&#32422;&#31616;&#25216;&#26415;&#65292;&#23427;&#26159;&#26356;&#24120;&#35265;&#30340;&#20449;&#24687;&#29942;&#39048;&#30340;&#25193;&#23637;&#65292;&#21516;&#26102;&#21387;&#32553;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20197;&#20445;&#30041;&#23427;&#20204;&#30340;&#21387;&#32553;&#29256;&#26412;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#65288;GSIB&#65289;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#21151;&#33021;&#24418;&#24335;&#30340;&#21516;&#26102;&#32422;&#31616;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21516;&#26102;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#28041;&#21450;&#25439;&#22833;&#20989;&#25968;&#30340;&#32479;&#35745;&#27874;&#21160;&#30340;&#30028;&#38480;&#21644;&#22343;&#26041;&#26681;&#20272;&#35745;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20856;&#22411;&#24773;&#20917;&#19979;&#65292;&#19982;&#36880;&#20010;&#21387;&#32553;&#21464;&#37327;&#30456;&#27604;&#65292;&#21516;&#26102;&#30340;GSIB&#21387;&#32553;&#22312;&#36798;&#21040;&#30456;&#21516;&#35823;&#24046;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#21407;&#21017;&#30340;&#20363;&#23376;&#65292;&#21363;&#21516;&#26102;&#21387;&#32553;&#27604;&#29420;&#31435;&#21387;&#32553;&#36755;&#20837;&#21464;&#37327;&#26356;&#20855;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#25581;&#31034;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#20013;&#35774;&#35745;&#20915;&#31574;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.16681</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20999;&#65292;&#26080;&#22788;&#19981;&#22312;&#65292;&#20840;&#26041;&#20301;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness. (arXiv:2308.16681v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16681
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#25581;&#31034;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#20013;&#35774;&#35745;&#20915;&#31574;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#35768;&#22810;&#31995;&#32479;&#37117;&#21033;&#29992;&#31639;&#27861;&#20915;&#31574;&#26469;&#65288;&#37096;&#20998;&#65289;&#33258;&#21160;&#21270;&#20197;&#21069;&#30001;&#20154;&#31867;&#36827;&#34892;&#30340;&#20915;&#31574;&#12290;&#24403;&#35774;&#35745;&#33391;&#22909;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#25215;&#35834;&#26356;&#23458;&#35266;&#30340;&#20915;&#31574;&#65292;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#36164;&#28304;&#65292;&#33410;&#32422;&#20154;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#35774;&#35745;&#19981;&#33391;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#31038;&#20250;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#30340;&#19981;&#20844;&#24179;&#20915;&#31574;&#12290;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#30340;&#19979;&#28216;&#25928;&#24212;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#26045;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#65292;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32531;&#35299;&#25110;&#21152;&#24378;&#12290;&#35768;&#22810;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#26159;&#38544;&#21547;&#36827;&#34892;&#30340;&#65292;&#19981;&#30693;&#36947;&#23427;&#20204;&#30830;&#20999;&#22320;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#26126;&#30830;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#24182;&#20102;&#35299;&#36825;&#20123;&#20915;&#31574;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#24515;&#29702;&#23398;&#39046;&#22495;&#30340;&#35265;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. When designed well, these systems promise more objective decisions while saving large amounts of resources and freeing up human time. However, when ADM systems are not designed well, they can lead to unfair decisions which discriminate against societal groups. The downstream effects of ADMs critically depend on the decisions made during the systems' design and implementation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these design decisions are made implicitly, without knowing exactly how they will influence the final system. It is therefore important to make explicit the decisions made during the design of ADM systems and understand how these decisions affect the fairness of the resulting system.  To study this issue, we draw on insights from the field of psychology and introduce the metho
&lt;/p&gt;</description></item><item><title>Matbench Discovery&#26159;&#19968;&#20010;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26230;&#20307;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#22312;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#39044;&#27979;&#26041;&#38754;&#30340;&#27979;&#35797;&#20013;&#65292;CHGNet&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.14920</link><description>&lt;p&gt;
Matbench Discovery - &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26230;&#20307;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Matbench Discovery -- An evaluation framework for machine learning crystal stability prediction. (arXiv:2308.14920v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14920
&lt;/p&gt;
&lt;p&gt;
Matbench Discovery&#26159;&#19968;&#20010;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26230;&#20307;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#22312;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#39044;&#27979;&#26041;&#38754;&#30340;&#27979;&#35797;&#20013;&#65292;CHGNet&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Matbench Discovery&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#33021;&#28304;&#27169;&#22411;&#22312;&#39640;&#36890;&#37327;&#25628;&#32034;&#31283;&#23450;&#26080;&#26426;&#26230;&#20307;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#21644;&#24418;&#25104;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#22495;&#20869;&#19982;&#22495;&#22806;&#24615;&#33021;&#20043;&#38388;&#30340;&#33073;&#33410;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;Python&#21253;&#65292;&#20197;&#20415;&#20110;&#26410;&#26469;&#27169;&#22411;&#30340;&#25552;&#20132;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32447;&#25490;&#34892;&#27036;&#65292;&#36827;&#19968;&#27493;&#27934;&#23519;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#23545;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#27979;&#35797;&#38598;F1&#24471;&#20998;&#36827;&#34892;&#25490;&#21517;&#65292;&#25105;&#20204;&#21457;&#29616;CHGNet &gt; M3GNet &gt; MACE &gt; ALIGNN &gt; MEGNet &gt; CGCNN &gt; CGCNN+P &gt; Wrenformer &gt; BOWSR &gt; Voronoi tessellation fingerprints with random forest&#12290;
&lt;/p&gt;
&lt;p&gt;
Matbench Discovery simulates the deployment of machine learning (ML) energy models in a high-throughput search for stable inorganic crystals. We address the disconnect between (i) thermodynamic stability and formation energy and (ii) in-domain vs out-of-distribution performance. Alongside this paper, we publish a Python package to aid with future model submissions and a growing online leaderboard with further insights into trade-offs between various performance metrics. To answer the question which ML methodology performs best at materials discovery, our initial release explores a variety of models including random forests, graph neural networks (GNN), one-shot predictors, iterative Bayesian optimizers and universal interatomic potentials (UIP). Ranked best-to-worst by their test set F1 score on thermodynamic stability prediction, we find CHGNet &gt; M3GNet &gt; MACE &gt; ALIGNN &gt; MEGNet &gt; CGCNN &gt; CGCNN+P &gt; Wrenformer &gt; BOWSR &gt; Voronoi tessellation fingerprints with random forest. The top 3 mod
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2308.13320</link><description>&lt;p&gt;
&#24494;&#35843;&#21487;&#33021;&#21066;&#24369;&#22522;&#30784;&#27169;&#22411;&#65307;&#20445;&#30041;&#29305;&#24449;&#21487;&#33021;&#26159;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13320
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20027;&#35201;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#23481;&#37327;&#21644;&#23545;&#20174;&#20114;&#32852;&#32593;&#19978;&#29228;&#21462;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26292;&#38706;&#65292;&#20139;&#26377;&#23384;&#20648;&#20851;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#30340;&#20248;&#21183;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20135;&#29983;&#20986;&#33394;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#24494;&#35843;&#27169;&#22411;&#22312;&#19982;&#19979;&#28216;&#20219;&#21153;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#35782;&#21035;&#27010;&#24565;&#30340;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26174;&#28982;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#22312;&#39318;&#27425;&#23398;&#20064;&#36825;&#20123;&#27010;&#24565;&#26102;&#65292;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19981;&#21487;&#21462;&#30340;&#29616;&#35937;&#31216;&#20026;&#8220;&#27010;&#24565;&#36951;&#24536;&#8221;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#24494;&#35843;&#26041;&#27861;&#37117;&#20005;&#37325;&#21463;&#21040;&#36825;&#31181;&#21103;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#24403;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon "concept forgetting" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12420</link><description>&lt;p&gt;
ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65306;&#23545;&#25991;&#29486;&#36827;&#34892;NLP&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DLT)&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;DLT&#30340;&#29615;&#22659;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#27835;&#29702;(ESG)&#32452;&#25104;&#37096;&#20998;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36824;&#19981;&#36275;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;107&#31687;&#31181;&#23376;&#25991;&#29486;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;63,083&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#24341;&#29992;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#31934;&#28860;&#20026;24,539&#31687;&#25991;&#29486;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#20998;&#31867;&#27861;&#20174;46&#31687;&#35770;&#25991;&#20013;&#26631;&#35760;&#20102;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#25214;&#20986;DLT&#30340;ESG&#35201;&#32032;&#26469;&#23436;&#21892;&#36825;&#20010;&#20998;&#31867;&#27861;&#12290;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#35843;&#25972;&#65292;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#25105;&#20204;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#35843;&#25972;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#31934;&#31616;&#65292;&#24471;&#21040;&#20102;505&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#21644;&#26102;&#38388;&#22270;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12252</link><description>&lt;p&gt;
&#25105;&#30475;&#21040;&#30340;&#19996;&#35199;&#26377;&#22810;&#23433;&#20840;&#65311;&#22522;&#20110;&#22270;&#20687;&#25511;&#21046;&#30340;&#33258;&#27835;&#23433;&#20840;&#24615;&#39044;&#27979;&#30340;&#26657;&#20934;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#24320;&#21457;&#33258;&#27835;&#31995;&#32479;&#30340;&#20027;&#35201;&#33539; paradigm&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#38543;&#30528;&#20854;&#24615;&#33021;&#21644;&#20415;&#21033;&#24615;&#65292;&#23433;&#20840;&#20445;&#35777;&#38754;&#20020;&#30528;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#25361;&#25112;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#32570;&#20047;&#20302;&#32500;&#21487;&#35299;&#37322;&#21160;&#24577;&#29366;&#24577;&#30340;&#27010;&#24565;&#65292;&#20256;&#32479;&#30340;&#20445;&#35777;&#26041;&#27861;&#37117;&#22260;&#32469;&#36825;&#19968;&#27010;&#24565;&#23637;&#24320;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#21487;&#37197;&#32622;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#19981;&#38656;&#35201;&#20302;&#32500;&#29366;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#28508;&#22312;&#34920;&#31034;&#21644;&#39044;&#27979;&#24341;&#36215;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#23545;&#20854;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#25552;&#20379;&#20102;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#22312;&#20004;&#20010;&#22270;&#20687;&#25511;&#21046;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65306;&#36187;&#36710;&#21644;&#27773;&#36710;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a car
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26085;&#24120;&#25235;&#21462;&#30340;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#26512;&#32593;&#32476;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#35782;&#21035;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#32593;&#31449;&#12290;&#35813;&#31995;&#32479;&#21487;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.02068</link><description>&lt;p&gt;
&#34394;&#20551;&#32593;&#31449;&#65306;&#22312;&#35268;&#27169;&#19978;&#36861;&#36394;&#21644;&#24433;&#21709;&#34394;&#20551;&#26032;&#38395;&#25925;&#20107;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale. (arXiv:2308.02068v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26085;&#24120;&#25235;&#21462;&#30340;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#26512;&#32593;&#32476;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#35782;&#21035;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#32593;&#31449;&#12290;&#35813;&#31995;&#32479;&#21487;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#12289;&#23459;&#20256;&#21644;&#24443;&#22836;&#24443;&#23614;&#30340;&#35854;&#35328;&#22312;&#32593;&#32476;&#19978;&#22823;&#37327;&#20256;&#25773;&#65292;&#20854;&#20013;&#19968;&#20123;&#21465;&#36848;&#23545;&#20844;&#20849;&#20581;&#24247;&#12289;&#36873;&#20030;&#21644;&#20010;&#20154;&#23433;&#20840;&#20135;&#29983;&#21361;&#38505;&#30340;&#29616;&#23454;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#30028;&#22312;&#36861;&#36394;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#21465;&#36848;&#26041;&#38754;&#20027;&#35201;&#32570;&#20047;&#33258;&#21160;&#21270;&#21644;&#31243;&#24207;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;&#23545;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26085;&#24120;&#25235;&#21462;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MPNet&#21644;DP-Means&#32858;&#31867;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20998;&#31163;&#21644;&#20998;&#26512;&#22312;&#32447;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;1,404&#20010;&#32593;&#31449;&#19978;&#35782;&#21035;&#20102;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#32593;&#31449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25105;&#20204;&#30340;&#31995;&#32479;&#26469;&#26816;&#27979;&#28304;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;Politifact&#12289;&#36335;&#36879;&#31038;&#21644;&#32654;&#32852;&#31038;&#31561;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15299</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#36127;&#33655;&#39044;&#27979;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#22312;&#20247;&#22810;&#39046;&#22495;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20934;&#30830;&#25429;&#25417;&#21160;&#21147;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#20173;&#28982;&#26159;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;ARIMA&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;ANN&#65292;LSTM&#65292;GRU&#31561;&#65289;&#32463;&#24120;&#34987;&#20351;&#29992;&#65292;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;Transformer-based&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;Transformer&#27169;&#22411;&#26377;&#26395;&#25913;&#36827;&#36127;&#33655;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20854;Attention&#26426;&#21046;&#23398;&#20064;&#21040;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#20960;&#31181;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#24046;&#20998;&#36827;&#21270;&#65292;&#20197;&#23547;&#25214;Transformer-based&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#24046;&#20998;&#36827;&#21270;&#20026;&#38750;&#21487;&#24494;&#20998;&#12289;&#22810;&#30446;&#26631;&#25110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#12289;&#24378;&#20581;&#21644;&#20840;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#22122;&#22768;&#35266;&#27979;&#21644;&#30456;&#20851;&#35266;&#27979;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.13147</link><description>&lt;p&gt;
&#23558;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#25193;&#23637;&#21040;&#26377;&#22122;&#22768;&#30340;&#35266;&#27979;&#21644;&#30456;&#20851;&#35266;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework. (arXiv:2307.13147v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#22122;&#22768;&#35266;&#27979;&#21644;&#30456;&#20851;&#35266;&#27979;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE (PD-NJ-ODE) &#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20855;&#26377;&#19981;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#35266;&#27979;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#32473;&#23450;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#19981;&#23436;&#25972;&#36807;&#21435;&#35266;&#27979;&#30340;&#26368;&#20248;&#39044;&#27979;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20551;&#35774;&#36807;&#31243;&#26412;&#36523;&#21644;&#22352;&#26631;&#20998;&#21035;&#35266;&#27979;&#26102;&#38388;&#26159;&#29420;&#31435;&#30340;&#65292;&#24182;&#19988;&#20551;&#35774;&#35266;&#27979;&#26159;&#26080;&#22122;&#22768;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#25193;&#23637;&#26469;&#35299;&#38500;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#20197;&#21450;&#23427;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.12971</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#30340;&#39044;&#27979;&#65306;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#35782;&#21035;&#21644;&#27604;&#36739;&#20998;&#26512;&#26368;&#20808;&#36827;&#30340;&#20379;&#24212;&#38142;&#39044;&#27979;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#65292;&#21253;&#25324;&#38382;&#39064;&#35782;&#21035;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12289;&#36229;&#21442;&#25968;&#35843;&#20248;&#12289;&#24615;&#33021;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#20197;&#21450;&#39044;&#27979;&#23545;&#20154;&#21147;&#12289;&#24211;&#23384;&#21644;&#25972;&#20010;&#20379;&#24212;&#38142;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#26681;&#25454;&#20379;&#24212;&#38142;&#31574;&#30053;&#25910;&#38598;&#25968;&#25454;&#30340;&#38656;&#27714;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#26681;&#25454;&#21608;&#26399;&#25110;&#20379;&#24212;&#38142;&#30446;&#26631;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#30340;&#39044;&#27979;&#12290;&#25512;&#33616;&#20351;&#29992;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#21644;&#35823;&#24046;&#27979;&#37327;&#31995;&#32479;&#26469;&#20248;&#21270;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;&#36824;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#31649;&#29702;&#20915;&#31574;&#20381;&#36182;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#21442;&#25968;&#21644;&#25913;&#36827;&#36816;&#33829;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;&#21367;&#31215;LSTM&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20840;&#29699;&#38477;&#27700;&#30340;&#21363;&#26102;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#26497;&#31471;&#38477;&#27700;&#26041;&#38754;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.10843</link><description>&lt;p&gt;
&#20840;&#29699;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#65306;&#22522;&#20110;GPM&#30340;&#38598;&#25104;&#22810;&#21355;&#26143;&#26816;&#32034;&#30340;U-Net&#21367;&#31215;LSTM&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture. (arXiv:2307.10843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;&#21367;&#31215;LSTM&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20840;&#29699;&#38477;&#27700;&#30340;&#21363;&#26102;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#26497;&#31471;&#38477;&#27700;&#26041;&#38754;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#27599;30&#20998;&#38047;&#20840;&#29699;&#36817;4&#23567;&#26102;&#30340;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#12290;&#35813;&#26550;&#26500;&#34701;&#21512;&#20102;U-Net&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#38598;&#25104;&#22810;&#21355;&#26143;&#26816;&#32034;(GPM)&#21644;&#20840;&#29699;&#39044;&#27979;&#31995;&#32479;(GFS)&#30340;&#20851;&#38190;&#38477;&#27700;&#39537;&#21160;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65288;&#21253;&#25324;&#22343;&#26041;&#24046;&#22238;&#24402;&#21644;&#32858;&#28966;&#25439;&#22833;&#20998;&#31867;&#65289;&#23545;&#38477;&#27700;&#29616;&#22312;&#39044;&#27979;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22238;&#24402;&#32593;&#32476;&#22312;&#25429;&#25417;&#36731;&#24230;&#38477;&#27700;&#65288;&#23567;&#20110;1.6 mm/hr&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20998;&#31867;&#32593;&#32476;&#22312;&#26497;&#31471;&#38477;&#27700;&#65288;&#22823;&#20110;8 mm/hr&#65289;&#30340;&#39044;&#27979;&#26041;&#38754;&#65292;&#20197;&#20851;&#38190;&#25104;&#21151;&#25351;&#25968;(CSI)&#34913;&#37327;&#65292;&#21487;&#20197;&#32988;&#36807;&#22238;&#24402;&#32593;&#32476;&#12290;&#20351;&#29992;Wasserstein&#36317;&#31163;&#34920;&#26126;&#65292;&#20998;&#31867;&#32593;&#32476;&#39044;&#27979;&#30340;&#38477;&#27700;&#20855;&#26377;&#26356;&#25509;&#36817;&#30340;&#31867;&#21035;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (&gt;8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probabili
&lt;/p&gt;</description></item><item><title>HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.09653</link><description>&lt;p&gt;
HAT-CL: &#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;
&lt;/p&gt;
&lt;p&gt;
HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09653
&lt;/p&gt;
&lt;p&gt;
HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20007;&#22833;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30828;&#27880;&#24847;&#21147;&#20219;&#21153;(HAT)&#26426;&#21046;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20854;&#23454;&#38469;&#23454;&#29616;&#21463;&#21040;&#20102;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HAT-CL&#65292;&#36825;&#26159;HAT&#26426;&#21046;&#30340;&#29992;&#25143;&#21451;&#22909;&#12289;&#19982;PyTorch&#20860;&#23481;&#30340;&#37325;&#26032;&#35774;&#35745;&#12290;HAT-CL&#19981;&#20165;&#33258;&#21160;&#21270;&#20102;&#26799;&#24230;&#25805;&#20316;&#65292;&#36824;&#31616;&#21270;&#20102;PyTorch&#27169;&#22359;&#36716;&#21270;&#20026;HAT&#27169;&#22359;&#30340;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#25552;&#20379;&#19968;&#22871;&#20840;&#38754;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#26550;&#26500;&#20013;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#25552;&#20379;&#20102;&#19982;TIMM&#24211;&#24179;&#28369;&#38598;&#25104;&#30340;&#21487;&#29992;&#30340;HAT&#32593;&#32476;&#12290;&#38500;&#20102;&#23545;HAT&#30340;&#37325;&#26032;&#35774;&#35745;&#21644;&#37325;&#26032;&#23454;&#29616;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29992;&#20110;HAT&#30340;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39640;&#22833;&#30495;&#24773;&#20917;&#19979;&#65292;&#23558;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#21387;&#32553;&#21040;&#26368;&#23569;&#27604;&#29305;&#25968;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07941</link><description>&lt;p&gt;
&#39640;&#22833;&#30495;&#26465;&#20214;&#19979;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#30340;&#26368;&#20248;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Optimal Compression of Unit Norm Vectors in the High Distortion Regime. (arXiv:2307.07941v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39640;&#22833;&#30495;&#24773;&#20917;&#19979;&#65292;&#23558;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#21387;&#32553;&#21040;&#26368;&#23569;&#27604;&#29305;&#25968;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#36890;&#20449;&#39640;&#25928;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38656;&#27714;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#21387;&#32553;&#21040;&#26368;&#23569;&#27604;&#29305;&#25968;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#19968;&#23450;&#31243;&#24230;&#30340;&#22833;&#30495;&#24674;&#22797;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#36895;&#29575;-&#22833;&#30495;/&#35206;&#30422;&#32534;&#30721;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#30740;&#31350;&#36807;&#65292;&#20294;&#25105;&#20204;&#30340;&#37325;&#28857;&#20165;&#38480;&#20110;&#8220;&#39640;&#22833;&#30495;&#8221;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#32771;&#34385;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#21521;&#37327;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#20294;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#21387;&#32553;&#26144;&#23556;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#20559;&#21644;&#26080;&#20559;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#21387;&#32553;&#27604;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#34429;&#28982;&#32467;&#26524;&#26159;&#26032;&#26087;&#38382;&#39064;&#30340;&#28151;&#21512;&#65292;&#20294;&#20026;&#20102;&#23436;&#25972;&#36215;&#35265;&#65292;&#23427;&#20204;&#22312;&#26412;&#25991;&#20013;&#20104;&#20197;&#25972;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the need for communication-efficient distributed learning, we investigate the method for compressing a unit norm vector into the minimum number of bits, while still allowing for some acceptable level of distortion in recovery. This problem has been explored in the rate-distortion/covering code literature, but our focus is exclusively on the "high-distortion" regime. We approach this problem in a worst-case scenario, without any prior information on the vector, but allowing for the use of randomized compression maps. Our study considers both biased and unbiased compression methods and determines the optimal compression rates. It turns out that simple compression schemes are nearly optimal in this scenario. While the results are a mix of new and known, they are compiled in this paper for completeness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.</title><link>http://arxiv.org/abs/2307.07604</link><description>&lt;p&gt;
&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#32441;&#32534;&#30721;&#26041;&#27861;&#26159;&#26368;&#24191;&#27867;&#29992;&#20110;&#30830;&#23450;&#32422;&#26463;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25110;&#38169;&#35823;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#65292;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#36866;&#24403;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#25105;&#20204;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#19979;&#30028;&#20063;&#19981;&#24179;&#28369;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35823;&#24046;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#21464;&#24471;&#26080;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22635;&#20805;&#21644;&#32622;&#25442;&#36716;&#25442;&#24212;&#29992;&#20110;&#25351;&#32441;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22256;&#38590;&#23454;&#20363;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25552;&#20379;&#26032;&#30340;&#19979;&#30028;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;1. &#20302;&#20934;&#30830;&#24230;&#24773;&#26223;&#19979;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#30340;&#32039;&#23494;&#19979;&#30028;&#65292;&#36825;&#23588;&#20854;&#24847;&#21619;&#30528;&#26032;&#30340;&#31169;&#26377;1&#31751;&#38382;&#39064;&#30340;&#19979;&#30028; 2. &#36817;&#20284;k
&lt;/p&gt;
&lt;p&gt;
Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.06324</link><description>&lt;p&gt;
&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20801;&#35768;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#27425;&#36845;&#20195;&#30340;&#25972;&#20307;&#25928;&#26524;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#19968;&#27425;&#36845;&#20195;&#24402;&#32435;&#20351;&#29992;&#30340;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#30772;&#22351;&#19979;&#38477;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38271;&#36317;&#31163;&#27493;&#39588;&#65292;&#21487;&#33021;&#22312;&#30701;&#26399;&#20869;&#22686;&#21152;&#30446;&#26631;&#20540;&#65292;&#20294;&#22312;&#38271;&#26399;&#20869;&#24102;&#26469;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#29468;&#24819;&#65292;&#24182;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#24555;&#36895;&#30340;&#32463;&#39564;&#22330;&#26223;&#25552;&#21462;&#31639;&#27861;&#65292;&#19968;&#31181;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#19968;&#33268;&#65292;&#36825;&#20123;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#36866;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.03927</link><description>&lt;p&gt;
&#24555;&#36895;&#32463;&#39564;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Fast Empirical Scenarios. (arXiv:2307.03927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#24555;&#36895;&#30340;&#32463;&#39564;&#22330;&#26223;&#25552;&#21462;&#31639;&#27861;&#65292;&#19968;&#31181;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#19968;&#33268;&#65292;&#36825;&#20123;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#36866;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24076;&#26395;&#20174;&#22823;&#22411;&#21644;&#39640;&#32500;&#38754;&#26495;&#25968;&#25454;&#20013;&#25552;&#21462;&#19968;&#23567;&#37096;&#20998;&#19982;&#26679;&#26412;&#30697;&#19968;&#33268;&#30340;&#20195;&#34920;&#24615;&#22330;&#26223;&#12290;&#22312;&#20004;&#31181;&#26032;&#31639;&#27861;&#20013;&#65292;&#31532;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#20449;&#24687;&#19968;&#33268;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#21487;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#21644;&#22312;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#25903;&#25345;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#26893;&#20837;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#65292;&#26469;&#26816;&#27979;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28155;&#21152;&#20102;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#27880;&#20837;&#20869;&#23481;&#30340;&#35760;&#24518;&#26469;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03108</link><description>&lt;p&gt;
&#22914;&#20309;&#26816;&#27979;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#26893;&#20837;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#65292;&#26469;&#26816;&#27979;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28155;&#21152;&#20102;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#27880;&#20837;&#20869;&#23481;&#30340;&#35760;&#24518;&#26469;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#19968;&#20010;&#20363;&#23376;&#26159;&#24403;&#27169;&#22411;&#35757;&#32451;&#32773;&#25910;&#38598;&#20102;&#19968;&#20010;&#29305;&#23450;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#19968;&#31995;&#21015;&#22270;&#20687;&#65292;&#24182;&#35797;&#22270;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#33719;&#24471;&#33402;&#26415;&#23478;&#30340;&#35768;&#21487;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#26893;&#20837;&#20445;&#25252;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#26469;&#26816;&#27979;&#27492;&#31867;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#19978;&#28155;&#21152;&#29420;&#29305;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;&#23545;&#20154;&#31867;&#35270;&#35273;&#19981;&#21487;&#23519;&#35273;&#20294;&#33021;&#22815;&#34987;&#25193;&#25955;&#27169;&#22411;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#38544;&#31192;&#22270;&#20687;&#21253;&#35013;&#20989;&#25968;&#65292;&#26469;&#20462;&#25913;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#26159;&#21542;&#23545;&#27880;&#20837;&#30340;&#20869;&#23481;&#36827;&#34892;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#36825;&#19968;&#35760;&#24518;&#65288;&#21363;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the gener
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#26041;&#27861;&#65288;GGS&#65289;&#20248;&#21270;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#65292;&#28040;&#38500;&#20102;&#31361;&#21464;&#36317;&#31163;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#21457;&#29616;&#39640;&#36866;&#24212;&#24615;&#34507;&#30333;&#36136;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.00494</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#20248;&#21270;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing. (arXiv:2307.00494v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00494
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#26041;&#27861;&#65288;GGS&#65289;&#20248;&#21270;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#65292;&#28040;&#38500;&#20102;&#31361;&#21464;&#36317;&#31163;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#21457;&#29616;&#39640;&#36866;&#24212;&#24615;&#34507;&#30333;&#36136;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35774;&#35745;&#20986;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#39640;&#36866;&#24212;&#24615;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#23545;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#26469;&#35828;&#37117;&#26159;&#38761;&#21629;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#28023;&#37327;&#24207;&#21015;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#23558;&#25628;&#32034;&#38480;&#21046;&#22312;&#20174;&#21442;&#32771;&#24207;&#21015;&#30340;&#23567;&#31361;&#21464;&#21322;&#24452;&#33539;&#22260;&#20869;&#65292;&#20294;&#36825;&#26679;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#35774;&#35745;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#28040;&#38500;&#31361;&#21464;&#36317;&#31163;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#65288;GGS&#65289;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#24212;&#29992;&#24102;&#26377;&#26799;&#24230;&#30340;Gibbs&#26469;&#25552;&#20986;&#26377;&#21033;&#30340;&#31361;&#21464;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;&#26041;&#27861;&#21435;&#38500;&#23548;&#33268;&#20551;&#38451;&#24615;&#30340;&#22122;&#22768;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#21457;&#29616;&#20102;&#39640;&#36866;&#24212;&#24615;&#34507;&#30333;&#36136;&#65292;&#26368;&#22810;&#20855;&#26377;8&#20010;&#31361;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;GFP&#21644;AAV&#35774;&#35745;&#38382;&#39064;&#12289;&#28040;&#34701;&#35797;&#39564;&#21644;&#22522;&#20934;&#27169;&#22411;&#26469;&#38416;&#26126;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14275</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#22686;&#24378;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;&#36973;&#21463;&#20102;&#20196;&#20154;&#30031;&#32553;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23548;&#33268;&#40065;&#26834;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22914;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#12289;&#23545;&#25239;&#26435;&#37325;&#25200;&#21160;&#21644;&#26356;&#22810;&#25968;&#25454;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27867;&#21270;&#30340;&#25913;&#36827;&#20173;&#28982;&#36828;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;--&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#30340;&#31934;&#32454;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#20248;&#21270;&#36712;&#36857;&#22312;&#26102;&#38388;&#19978;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;WOT&#22312;&#21508;&#31181;&#26368;&#26032;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WOT&#19982;&#29616;&#26377;&#26041;&#27861;&#23436;&#32654;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.09912</link><description>&lt;p&gt;
&#36208;&#21521;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Quantum Federated Learning. (arXiv:2306.09912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09912
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#65292;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#35813;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#32454;&#33268;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#12289;&#25216;&#26415;&#20197;&#21450;&#26032;&#20852;&#24212;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#25353;&#20854;&#29305;&#24449;&#21644;&#25152;&#37319;&#29992;&#30340;&#37327;&#23376;&#25216;&#26415;&#20998;&#31867;&#12290;&#38543;&#30528;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#35745;&#23558;&#22312;&#21508;&#20010;&#34892;&#19994;&#23454;&#29616;&#26356;&#22810;&#30340;&#31361;&#30772;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21463;&#38544;&#31169;&#32422;&#26463;&#19988;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#20174;&#20844;&#24320;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#19968;&#33324;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#26377;&#21033;&#30340;&#29702;&#35770;&#23398;&#20064;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.08838</link><description>&lt;p&gt;
&#20855;&#26377;&#29702;&#35770;&#20445;&#38556;&#30340;&#24046;&#20998;&#38544;&#31169;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Domain Adaptation with Theoretical Guarantees. (arXiv:2306.08838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21463;&#38544;&#31169;&#32422;&#26463;&#19988;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#20174;&#20844;&#24320;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#19968;&#33324;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#26377;&#21033;&#30340;&#29702;&#35770;&#23398;&#20064;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#21487;&#29992;&#30340;&#26631;&#35760;&#25968;&#25454;&#21463;&#21040;&#38544;&#31169;&#32422;&#26463;&#24182;&#30456;&#23545;&#26377;&#38480;&#12290;&#20026;&#20102;&#20026;&#30446;&#26631;&#39046;&#22495;&#23548;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#22120;&#65292;&#36890;&#24120;&#26377;&#21033;&#20110;&#21033;&#29992;&#26469;&#33258;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#36817;&#30340;&#21478;&#19968;&#39046;&#22495;&#30340;&#20844;&#24320;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#26159;&#20174;&#20844;&#20849;&#28304;&#39046;&#22495;&#21040;&#31169;&#26377;&#30446;&#26631;&#39046;&#22495;&#30340;&#29616;&#20195;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181; $(\epsilon, \delta)$-&#24046;&#20998;&#38544;&#31169;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#30417;&#30563;&#24615;&#33258;&#36866;&#24212;&#12290;&#23545;&#20110;&#20854;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#33324;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26368;&#36817;&#34987;&#35777;&#26126;&#20855;&#26377;&#26377;&#21033;&#30340;&#29702;&#35770;&#23398;&#20064;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#26159;&#20026;&#20855;&#26377;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#22238;&#24402;&#35774;&#35745;&#30340;&#65292;&#24182;&#26174;&#31034;&#20026;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#31639;&#27861;&#26159;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21487;&#33021;&#26159;&#38750;&#20984;&#20294;Lipschitz&#21644;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#20294;&#25105;&#20204;&#20063;&#25253;&#21578;&#20102;&#20960;&#20010;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, the labeled data at the learner's disposal is subject to privacy constraints and is relatively limited. To derive a more accurate predictor for the target domain, it is often beneficial to leverage publicly available labeled data from an alternative domain, somewhat close to the target domain. This is the modern problem of supervised domain adaptation from a public source to a private target domain. We present two $(\epsilon, \delta)$-differentially private adaptation algorithms for supervised adaptation, for which we make use of a general optimization problem, recently shown to benefit from favorable theoretical learning guarantees. Our first algorithm is designed for regression with linear predictors and shown to solve a convex optimization problem. Our second algorithm is a more general solution for loss functions that may be non-convex but Lipschitz and smooth. While our main objective is a theoretical analysis, we also report the results of several experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#38543;&#26426;Kronecker&#22270;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#36817;&#20284;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#8220;&#21435;&#22122;&#22768;&#21644;&#27714;&#35299;&#8221;&#20803;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#25512;&#26029;&#22270;&#21442;&#25968;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.08489</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#23494;&#38598;&#38543;&#26426;Kronecker&#22270;&#30340;&#20998;&#26512;&#21644;&#36817;&#20284;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Analysis and Approximate Inference of Large and Dense Random Kronecker Graphs. (arXiv:2306.08489v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#38543;&#26426;Kronecker&#22270;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#36817;&#20284;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#8220;&#21435;&#22122;&#22768;&#21644;&#27714;&#35299;&#8221;&#20803;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#25512;&#26029;&#22270;&#21442;&#25968;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22270;&#27169;&#22411;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#20132;&#21644;&#20132;&#36890;&#32593;&#32476;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#20998;&#23376;&#36951;&#20256;&#23398;&#12290;&#26412;&#25991;&#23545;\cite{leskovec2010kronecker}&#20013;&#25552;&#20986;&#30340;&#38543;&#26426;Kronecker&#22270;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#24403;&#22270;&#39030;&#28857;&#25968;&#37327;$N$&#24456;&#22823;&#26102;&#12290;&#22522;&#20110;&#26368;&#36817;&#22312;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23494;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;Kronecker&#22270;&#37051;&#25509;&#30697;&#38453;&#36817;&#20284;&#36981;&#24490;&#19968;&#20010;&#20449;&#21495;&#21152;&#22122;&#22768;&#27169;&#22411;&#65292;&#20854;&#20013;&#20449;&#21495;&#30697;&#38453;&#30340;&#31209;&#24456;&#23567;&#65288;&#26368;&#22810;&#20026;$\log N$&#38454;&#65289;&#65292;&#22312;&#22270;&#21442;&#25968;&#20013;&#26159;&#32447;&#24615;&#30340;&#65292;&#32780;&#38543;&#26426;&#30340;&#22122;&#22768;&#30697;&#38453;&#20855;&#26377;&#22235;&#20998;&#20043;&#19968;&#22278;&#24418;&#22855;&#24322;&#20540;&#20998;&#24067;&#12290;&#36825;&#20010;&#35266;&#23519;&#20801;&#35768;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#21435;&#22122;&#22768;&#21644;&#27714;&#35299;&#8221;&#20803;&#31639;&#27861;&#26469;&#36817;&#20284;&#25512;&#26029;&#22270;&#21442;&#25968;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#65288;&#28176;&#36817;&#30340;&#65289;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22270;i&#30340;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random graph models are playing an increasingly important role in science and industry, and finds their applications in a variety of fields ranging from social and traffic networks, to recommendation systems and molecular genetics. In this paper, we perform an in-depth analysis of the random Kronecker graph model proposed in \cite{leskovec2010kronecker}, when the number of graph vertices $N$ is large. Built upon recent advances in random matrix theory, we show, in the dense regime, that the random Kronecker graph adjacency matrix follows approximately a signal-plus-noise model, with a small-rank (of order at most $\log N$) signal matrix that is linear in the graph parameters and a random noise matrix having a quarter-circle-form singular value distribution. This observation allows us to propose a ``denoise-and-solve'' meta algorithm to approximately infer the graph parameters, with reduced computational complexity and (asymptotic) performance guarantee. Numerical experiments of graph i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07629</link><description>&lt;p&gt;
SqueezeLLM&#65306;&#23494;&#38598;&#31232;&#30095;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#22312;&#24191;&#27867;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#26524;&#12290;&#20294;&#26159;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;&#37096;&#32626;&#26694;&#26550;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#25512;&#29702;&#31649;&#36947;&#65292;&#36825;&#36890;&#24120;&#26159;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#20351;&#29992;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#20302;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#20110;LLMs&#29983;&#25104;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#20869;&#23384;&#24102;&#23485;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#21333;&#20010;&#25209;&#27425;&#25512;&#29702;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20943;&#23569;&#31934;&#24230;&#26469;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;SqueezeLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2306.07392</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#23398;&#20064;&#20219;&#24847;&#35270;&#35282;&#30340;6DoF&#26426;&#22120;&#20154;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07392
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#22312;&#26234;&#33021;&#36741;&#21161;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26434;&#20081;&#30340;&#29615;&#22659;&#20013;&#20174;&#20219;&#20309;&#35270;&#35282;&#26377;&#25928;&#22320;&#25235;&#21462;&#23545;&#35937;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22330;&#26223;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NeuGraspNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;6DoF&#25235;&#21462;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#20307;&#31215;&#34920;&#31034;&#21644;&#34920;&#38754;&#28210;&#26579;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#20840;&#23616;&#65288;&#22330;&#26223;&#32423;&#21035;&#65289;&#21644;&#23616;&#37096;&#65288;&#25235;&#21462;&#32423;&#21035;&#65289;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#22330;&#26223;&#30340;&#26410;&#35265;&#37096;&#20998;&#65292;&#20063;&#33021;&#26377;&#25928;&#22320;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25235;&#21462;&#37325;&#26032;&#35299;&#37322;&#20026;&#19968;&#20010;&#23616;&#37096;&#30340;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#38382;&#39064;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21644;&#23545;&#35937;&#34920;&#38754;&#20960;&#20309;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;NeuGraspNet&#22312;&#21333;&#20010;&#35270;&#35282;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#38544;&#24335;&#21644;&#21322;&#38544;&#24335;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;Periodformer&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Transformer-based&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05035</link><description>&lt;p&gt;
&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#39069;&#22806;&#30340;&#38271;&#36755;&#20837;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;Periodformer&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Transformer-based&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#22312;&#36817;&#24180;&#26469;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#25152;&#22266;&#26377;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38656;&#35201;&#38271;&#24207;&#21015;&#65292;&#23427;&#22312;LTSF&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65306;1&#65289;&#36825;&#20123;&#26041;&#27861;&#35774;&#35745;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#26159;&#21542;&#23454;&#38469;&#19978;&#32553;&#30701;&#20102;&#30495;&#23454;&#35774;&#22791;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#65307;2&#65289;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#23427;&#20204;&#30340;&#24615;&#33021;&#65311;&#26412;&#35770;&#25991;&#30340;&#31572;&#26696;&#26159;&#21542;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65288;Periodformer&#65289;&#65292;&#36890;&#36807;&#26174;&#24335;&#21608;&#26399;&#24615;&#21644;&#20869;&#32622;&#30340;&#25509;&#36817;&#24615;&#26469;&#37325;&#26032;&#35774;&#35745;&#38271;&#26399;&#23376;&#24207;&#21015;&#21644;&#30701;&#26399;&#23376;&#24207;&#21015;&#30340;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23884;&#20837;&#20102;&#19968;&#20010;&#38376;&#25511;&#26426;&#21046;&#21040;Periodformer&#20013;&#20197;&#35843;&#25972;&#27880;&#24847;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Transformer-based models have achieved impressive performance on various time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received extensive attention in recent years. However, due to the inherent computational complexity and long sequences demanding of Transformer-based methods, its application on LTSF tasks still has two major issues that need to be further investigated: 1) Whether the sparse attention mechanism designed by these methods actually reduce the running time on real devices; 2) Whether these models need extra long input sequences to guarantee their performance? The answers given in this paper are negative. Therefore, to better copy with these two issues, we design a lightweight Period-Attention mechanism (Periodformer), which renovates the aggregation of long-term subseries via explicit periodicity and short-term subseries via built-in proximity. Meanwhile, a gating mechanism is embedded into Periodformer to regulate the influence of the attention
&lt;/p&gt;</description></item><item><title>&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.04940</link><description>&lt;p&gt;
&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04940
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#26426;&#21046;&#65292;&#26088;&#22312;&#24314;&#31435;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#21151;&#33021;&#65288;LayerAct&#65289;&#12290;&#36825;&#20123;&#21151;&#33021;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36755;&#20837;&#20559;&#31227;&#25152;&#23548;&#33268;&#30340;&#28608;&#27963;&#36755;&#20986;&#30340;&#20998;&#23618;&#32423;&#27874;&#21160;&#26469;&#38477;&#20302;&#20256;&#32479;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#30340;&#22122;&#38899;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LayerAct&#21151;&#33021;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#38646;&#30340;&#24179;&#22343;&#28608;&#27963;&#36755;&#20986;&#65292;&#32780;&#19981;&#38480;&#21046;&#28608;&#27963;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;LayerAct&#21151;&#33021;&#22312;&#22122;&#22768;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#65292;&#24182;&#19988;&#32463;&#39564;&#35777;&#26126;&#36825;&#20123;&#21151;&#33021;&#30340;&#24179;&#22343;&#28608;&#27963;&#32467;&#26524;&#31867;&#20284;&#20110;&#38646;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22024;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;LayerAct&#21151;&#33021;&#27604;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#20063;&#26159;&#20248;&#36234;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04775</link><description>&lt;p&gt;
&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#25552;&#39640;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Observation Bias to Improve Matrix Completion. (arXiv:2306.04775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#21464;&#24418;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#20197;&#20559;&#24046;&#30340;&#26041;&#24335;&#21576;&#29616;&#65292;&#31867;&#20284;&#20110;Ma&#21644;Chen&#25152;&#24341;&#20837;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20559;&#24046;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#26469;&#25913;&#36827;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65306;&#65288;i&#65289;&#23558;&#35266;&#27979;&#27169;&#24335;&#35299;&#37322;&#20026;&#23436;&#20840;&#35266;&#27979;&#30340;&#22122;&#22768;&#30697;&#38453;&#65292;&#25105;&#20204;&#23545;&#35266;&#27979;&#27169;&#24335;&#24212;&#29992;&#20256;&#32479;&#30340;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#26469;&#20272;&#35745;&#28508;&#22312;&#22240;&#32032;&#20043;&#38388;&#30340;&#36317;&#31163;&#65307; (ii)&#25105;&#20204;&#23545;&#24674;&#22797;&#30340;&#29305;&#24449;&#24212;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#22635;&#34917;&#32570;&#22833;&#35266;&#23519;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#29575;&#65292;&#36825;&#20123;&#35823;&#24046;&#29575;&#19982;&#30456;&#24212;&#30340;&#30417;&#30563;&#23398;&#20064;&#21442;&#25968;&#29575;&#30456;&#31454;&#20105;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#24615;&#33021;&#19982;&#20351;&#29992;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30456;&#24403;&#12290;&#23454;&#35777;&#35780;&#20272;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21453;&#26144;&#20102;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#20844;&#24179;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25293;&#21334;&#31639;&#27861;&#23398;&#20064;&#26679;&#26412;&#26368;&#20248;&#21305;&#37197;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#38454;&#27573;&#21644;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#32479;&#35745;&#30340;&#36951;&#25022;&#20998;&#26512;&#23454;&#29616;&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#32467;&#26524;&#36951;&#25022;&#38454;&#25968;&#20174;$O(\log T \log\log T)$&#21040;&#20102;$O\left(N^3 \log N \log T \right)$&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04498</link><description>&lt;p&gt;
&#20844;&#24179;&#22810;&#26234;&#33021;&#20307;&#36172;&#21338;&#26426;&#30340;&#26368;&#20248;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Fair Multi-Agent Bandits. (arXiv:2306.04498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#20844;&#24179;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25293;&#21334;&#31639;&#27861;&#23398;&#20064;&#26679;&#26412;&#26368;&#20248;&#21305;&#37197;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#38454;&#27573;&#21644;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#32479;&#35745;&#30340;&#36951;&#25022;&#20998;&#26512;&#23454;&#29616;&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#32467;&#26524;&#36951;&#25022;&#38454;&#25968;&#20174;$O(\log T \log\log T)$&#21040;&#20102;$O\left(N^3 \log N \log T \right)$&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#19981;&#30456;&#20114;&#36890;&#20449;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21482;&#26377;&#22312;&#21516;&#26102;&#35775;&#38382;&#21516;&#19968;&#20010;&#33218;&#26102;&#25165;&#25552;&#20379;&#30896;&#25758;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#20026;$O\left(N^3 \log N \log T \right)$&#65288;&#20551;&#35774;&#22870;&#21169;&#26377;&#30028;&#65292;&#20294;&#26410;&#30693;&#19978;&#30028;&#65289;&#12290;&#36825;&#22823;&#22823;&#25913;&#36827;&#20102;&#20043;&#21069;&#32467;&#26524;&#65292;&#20854;&#36951;&#25022;&#38454;&#25968;&#20026;$O(\log T \log\log T)$&#65292;&#24182;&#19988;&#23545;&#26234;&#33021;&#20307;&#25968;&#37327;&#20855;&#26377;&#25351;&#25968;&#20381;&#36182;&#24615;&#12290;&#32467;&#26524;&#26159;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#25293;&#21334;&#31639;&#27861;&#26469;&#23398;&#20064;&#26679;&#26412;&#26368;&#20248;&#21305;&#37197;&#65292;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#38454;&#27573;&#65292;&#20854;&#38271;&#24230;&#26469;&#33258;&#20110;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#32479;&#35745;&#30340;&#36951;&#25022;&#20998;&#26512;&#23454;&#29616;&#30340;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#20102;&#36951;&#25022;&#23545;$\log T$&#30340;&#20381;&#23384;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of fair multi-agent multi-arm bandit learning when agents do not communicate with each other, except collision information, provided to agents accessing the same arm simultaneously. We provide an algorithm with regret $O\left(N^3 \log N \log T \right)$ (assuming bounded rewards, with unknown bound). This significantly improves previous results which had regret of order $O(\log T \log\log T)$ and exponential dependence on the number of agents. The result is attained by using a distributed auction algorithm to learn the sample-optimal matching, a new type of exploitation phase whose length is derived from the observed samples, and a novel order-statistics-based regret analysis. Simulation results present the dependence of the regret on $\log T$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03163</link><description>&lt;p&gt;
&#22914;&#20309;&#36328;&#36234;&#20113;&#21644;&#22823;&#38470;&#22521;&#35757;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65311;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#31471;&#25110;&#19987;&#29992;&#30828;&#20214;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#26114;&#36149;&#30340;&#12290;&#19968;&#31181;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#26159;&#25552;&#20379;&#28857;&#23454;&#20363;&#30340;&#39640;&#36229;&#35268;&#27169;&#20113;&#65292;&#36825;&#26159;&#19968;&#20010;&#20415;&#23452;&#20294;&#30701;&#26242;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#26367;&#20195;&#25353;&#38656;&#36164;&#28304;&#12290;&#30001;&#20110;&#28857;&#23454;&#20363;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#22240;&#26085;&#26399;&#12289;&#22823;&#38470;&#21644;&#20113;&#20379;&#24212;&#21830;&#19981;&#21516;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20998;&#37197;&#36164;&#28304;&#21487;&#33021;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#20294;&#26159;&#65292;&#23578;&#26410;&#35843;&#26597;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#21542;&#22312;&#35206;&#30422;&#19981;&#21516;&#25968;&#25454;&#20013;&#24515;&#21644;&#20113;&#25552;&#20379;&#21830;&#30340;&#28857; VM &#20840;&#29699;&#24066;&#22330;&#19978;&#20197;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65311;&#20026;&#20102;&#25552;&#20379;&#25351;&#23548;&#65292;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#19981;&#21516;&#21306;&#22495;&#12289;&#22823;&#38470;&#21644;&#20113;&#23545;&#20195;&#34920;&#24615; CV &#21644; NLP &#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#21534;&#21520;&#37327;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#23637;&#24403;&#21069;&#30340;&#22521;&#35757;&#36873;&#25321;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training.  This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;Equity-Transformer&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#65292;&#24182;&#29983;&#25104;&#32771;&#34385;&#20844;&#24179;&#24037;&#20316;&#36127;&#36733;&#30340;&#39034;&#24207;&#21160;&#20316;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;Equity-Transformer&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02689</link><description>&lt;p&gt;
&#23558;NP&#22256;&#38590;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20316;&#20026;&#20855;&#26377;&#20844;&#24179;&#32972;&#26223;&#30340;&#39034;&#24207;&#29983;&#25104;&#26469;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. (arXiv:2306.02689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;Equity-Transformer&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#65292;&#24182;&#29983;&#25104;&#32771;&#34385;&#20844;&#24179;&#24037;&#20316;&#36127;&#36733;&#30340;&#39034;&#24207;&#21160;&#20316;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;Equity-Transformer&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#25152;&#26377;&#20195;&#29702;&#21830;&#21327;&#21516;&#35775;&#38382;&#25152;&#26377;&#22478;&#24066;&#30340;&#26368;&#22823;&#26053;&#28216;&#38271;&#24230;&#65292;&#21363;&#23436;&#25104;&#26102;&#38388;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#26377;&#24433;&#21709;&#21147;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#34987;&#35748;&#20026;&#26159;NP&#22256;&#38590;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#21327;&#35843;&#20247;&#22810;&#20195;&#29702;&#21830;&#35206;&#30422;&#25968;&#21315;&#20010;&#22478;&#24066;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#22810;&#20010;&#20195;&#29702;&#21830;&#30340;&#21516;&#26102;&#20915;&#31574;&#24314;&#27169;&#20026;&#39034;&#24207;&#29983;&#25104;&#36807;&#31243;&#65292;&#20801;&#35768;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#22312;&#39034;&#24207;&#36817;&#20284;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;Transformer&#27169;&#22411;Equity-Transformer&#65292;&#23427;&#29983;&#25104;&#32771;&#34385;&#20854;&#20182;&#20195;&#29702;&#21830;&#20043;&#38388;&#20844;&#24179;&#24037;&#20316;&#36127;&#36733;&#30340;&#39034;&#24207;&#21160;&#20316;&#12290;Equity-Transformer&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20854;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#24471;&#21040;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Min-max routing problems aim to minimize the maximum tour length among agents as they collaboratively visit all cities, i.e., the completion time. These problems include impactful real-world applications but are known as NP-hard. Existing methods are facing challenges, particularly in large-scale problems that require the coordination of numerous agents to cover thousands of cities. This paper proposes a new deep-learning framework to solve large-scale min-max routing problems. We model the simultaneous decision-making of multiple agents as a sequential generation process, allowing the utilization of scalable deep-learning models for sequential decision-making. In the sequentially approximated problem, we propose a scalable contextual Transformer model, Equity-Transformer, which generates sequential actions considering an equitable workload among other agents. The effectiveness of Equity-Transformer is demonstrated through its superior performance in two representative min-max routing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#26041;&#27861;&#31934;&#32454;&#21270;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26080;&#28145;&#24230;&#20381;&#36182;&#24615;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.01992</link><description>&lt;p&gt;
&#20851;&#20110;ReLU&#32593;&#32476;&#30340;&#22823;&#23567;&#26080;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Size-Independent Sample Complexity of ReLU Networks. (arXiv:2306.01992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#26041;&#27861;&#31934;&#32454;&#21270;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26080;&#28145;&#24230;&#20381;&#36182;&#24615;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#27867;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23398;&#20064;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#26435;&#37325;&#30697;&#38453;&#19978;&#32473;&#23450;&#33539;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#31867;&#30340;Rademacher&#22797;&#26434;&#24230;&#12290;&#20043;&#21069;Golowich-Rakhlin-Shamir (2020)&#33719;&#24471;&#20102;&#19968;&#20010;&#19981;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#65288;&#19982;Frobenius&#33539;&#25968;&#30340;&#20056;&#31215;&#25104;&#27604;&#20363;&#65289;&#19978;&#30028;&#65292;&#38500;&#20102;&#19968;&#20010;&#24179;&#26041;&#26681;&#28145;&#24230;&#30340;&#22240;&#23376;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#31934;&#32454;&#21270;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#26681;&#26412;&#27809;&#26377;&#26126;&#26174;&#30340;&#28145;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of learning ReLU neural networks from the point of view of generalization. Given norm constraints on the weight matrices, a common approach is to estimate the Rademacher complexity of the associated function class. Previously Golowich-Rakhlin-Shamir (2020) obtained a bound independent of the network size (scaling with a product of Frobenius norms) except for a factor of the square-root depth. We give a refinement which often has no explicit depth-dependence at all.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01276</link><description>&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#20013;&#23545;&#31216;&#25506;&#32034;&#26159;&#20813;&#36153;&#30340;&#65281;
&lt;/p&gt;
&lt;p&gt;
Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20813;&#36153;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#22686;&#24378;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20445;&#30041;&#22870;&#21169;&#30340;&#21464;&#25442;&#26469;&#22686;&#24378;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#12290;&#35813;&#31639;&#27861;&#21487;&#33021;&#20855;&#26377;&#24433;&#21709;&#21147;&#65292;&#22240;&#20026;&#23427;&#31616;&#21333;&#65292;&#26131;&#20110;&#19982;&#29616;&#26377;&#27714;&#35299;&#22120;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;NP&#38590;&#30340;&#36335;&#32447;&#20248;&#21270;&#65292;&#35843;&#24230;&#20248;&#21270;&#21644;&#26032;&#22411;&#20998;&#23376;&#20248;&#21270;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36731;&#26494;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a "free" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2306.01271</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20250;&#21516;&#26102;&#20986;&#29616;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01271
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#19982;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#29615;&#22659;&#20013;&#20986;&#29616;&#24778;&#20154;&#30340;&#24178;&#20928;&#27867;&#21270;&#33021;&#21147;&#31867;&#20284;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#20928;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#24178;&#20928;&#27867;&#21270;&#19981;&#21516;&#30340;&#26159;&#65292;&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#33021;&#22815;&#23454;&#29616;&#20302;&#40065;&#26834;&#35757;&#32451;&#35823;&#24046;&#65292;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#40065;&#26834;&#27867;&#21270;&#36317;&#31163;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#35757;&#32451;&#22914;&#20309;&#23548;&#33268;&#32593;&#32476;&#23398;&#20064;&#32773;&#36827;&#20837;&#21040;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#36843;&#20351;&#23398;&#20064;&#22120;&#25104;&#20026;&#24378;&#39044;&#27979;&#32593;&#32476;&#65292;&#23545;&#25239;&#35757;&#32451;&#23558;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\textit{robust training error}$, there still exists a significant $\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u
&lt;/p&gt;</description></item><item><title>STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.00937</link><description>&lt;p&gt;
STEVE-1: &#19968;&#20010;&#29992;&#20110;Minecraft&#20013;&#25991;&#26412;-&#34892;&#20026;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. (arXiv:2306.00937v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00937
&lt;/p&gt;
&lt;p&gt;
STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#23545;&#25991;&#26412;&#25351;&#20196;&#20570;&#20986;&#21709;&#24212;&#30340;AI&#27169;&#22411;&#23545;&#20110;&#36830;&#32493;&#24615;&#20915;&#31574;&#20219;&#21153;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STEVE-1&#30340;Minecraft&#25351;&#20196;&#35843;&#25972;&#22411;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;DALL-E 2&#20013;&#20351;&#29992;&#30340;unCLIP&#26041;&#27861;&#20063;&#23545;&#21019;&#24314;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#38750;&#24120;&#26377;&#25928;&#12290;STEVE-1&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#35757;&#32451;&#65306;&#39318;&#20808;&#26159;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;VPT&#27169;&#22411;&#36866;&#24212;MineCLIP&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25351;&#20196;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#20197;&#20174;&#25991;&#26412;&#39044;&#27979;&#28508;&#22312;&#20195;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;VPT&#65292;&#36991;&#20813;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#25991;&#26412;&#27880;&#37322;&#12290;&#36890;&#36807;&#21033;&#29992;VPT&#21644;MineCLIP&#31561;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;STEVE-1&#30340;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;60&#32654;&#20803;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Minecraft&#20013;&#36981;&#24490;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#20026;&#24320;&#25918;&#30340;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instructi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18466</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#37117;&#26088;&#22312;&#22312;&#27979;&#35797;&#26102;&#20174;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#20854;&#26631;&#20934;&#35757;&#32451;&#35774;&#32622;&#23545;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;Pile&#8221;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26368;&#36817;&#37051;&#32034;&#24341;&#12290;&#32473;&#23450;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26816;&#32034;&#26597;&#35810;&#30340;&#37051;&#23621;&#65292;&#24182;&#22312;&#23545;&#24212;&#20110;&#36825;&#20123;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26816;&#32034;&#21644;&#35757;&#32451;&#20165;20&#20010;&#37051;&#23621;&#65292;&#27599;&#20010;&#37051;&#23621;&#20165;&#36827;&#34892;&#19968;&#27425;&#26799;&#24230;&#36845;&#20195;&#65292;&#23601;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#8220;Pile&#8221;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20108;&#21313;&#20010;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26174;&#33879;&#32553;&#23567;&#20102;&#23567;&#22411;GPT2&#27169;&#22411;&#21644;GPTNeo&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#26159;&#19987;&#38376;&#23545;&#8220;Pile&#8221;&#36827;&#34892;&#25910;&#25947;&#35757;&#32451;&#30340;&#65292;&#20307;&#31215;&#21364;&#26159;&#21069;&#32773;&#30340;&#21313;&#20493;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#20854;&#26041;&#27861;&#30340;&#25104;&#21151;&#36824;&#21462;&#20915;&#20110;&#20805;&#20998;&#30340;&#32034;&#24341;&#36136;&#37327;&#21644;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#21449;&#29109;&#20272;&#35745;&#22120;&#30340;&#22791;&#36873;&#19979;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20010;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27604;&#26679;&#26412;&#65292;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#39640;&#20449;&#24687;&#22686;&#30410;&#65292;&#20801;&#35768;&#23398;&#20064;&#26356;&#20248;&#31168;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#38544;&#24335;&#27010;&#29575;&#27169;&#22411;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.18435</link><description>&lt;p&gt;
&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#30340;&#20132;&#21449;&#29109;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Entropy Estimators for Sequential Experiment Design with Reinforcement Learning. (arXiv:2305.18435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18435
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#21449;&#29109;&#20272;&#35745;&#22120;&#30340;&#22791;&#36873;&#19979;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20010;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27604;&#26679;&#26412;&#65292;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#39640;&#20449;&#24687;&#22686;&#30410;&#65292;&#20801;&#35768;&#23398;&#20064;&#26356;&#20248;&#31168;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#38544;&#24335;&#27010;&#29575;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#26377;&#25928;&#22320;&#23398;&#20064;&#35774;&#35745;&#23454;&#39564;&#24207;&#21015;&#30340;&#25674;&#38144;&#35774;&#35745;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#30340;&#23545;&#27604;&#20272;&#35745;&#22120;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#23545;&#27604;&#26679;&#26412;&#26469;&#36798;&#21040;&#26080;&#20559;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27169;&#22411;&#20998;&#24067;&#21644;&#28789;&#27963;&#30340;&#25552;&#35758;&#20998;&#24067;&#30340;&#22791;&#36873;&#19979;&#30028;&#20272;&#35745;&#22120;&#12290;&#25552;&#35758;&#20998;&#24067;&#36924;&#36817;&#27169;&#22411;&#21442;&#25968;&#22312;&#23454;&#39564;&#21382;&#21490;&#21644;&#35774;&#35745;&#31574;&#30053;&#26465;&#20214;&#19979;&#32473;&#23450;&#30340;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#19981;&#38656;&#35201;&#23545;&#27604;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39640;&#20449;&#24687;&#22686;&#30410;&#20272;&#35745;&#65292;&#20801;&#35768;&#23398;&#20064;&#26356;&#20248;&#31168;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#38544;&#24335;&#27010;&#29575;&#27169;&#22411;&#20860;&#23481;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#36830;&#32493;&#21644;&#31163;&#25955;&#35774;&#35745;&#20197;&#21450;&#26174;&#24335;&#21644;&#38544;&#24335;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning can effectively learn amortised design policies for designing sequences of experiments. However, current methods rely on contrastive estimators of expected information gain, which require an exponential number of contrastive samples to achieve an unbiased estimation. We propose an alternative lower bound estimator, based on the cross-entropy of the joint model distribution and a flexible proposal distribution. This proposal distribution approximates the true posterior of the model parameters given the experimental history and the design policy. Our estimator requires no contrastive samples, can achieve more accurate estimates of high information gains, allows learning of superior design policies, and is compatible with implicit probabilistic models. We assess our algorithm's performance in various tasks, including continuous and discrete designs and explicit and implicit likelihoods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.16905</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65306;&#36125;&#21494;&#26031;&#25512;&#29702;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference. (arXiv:2305.16905v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#24615;&#36136;&#38459;&#30861;&#20102;&#35299;&#37322;&#24615;&#12290;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#21152;&#24615;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#20351;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#20114;&#21464;&#24471;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65306;a&#65289;&#23427;&#36890;&#36807;&#20272;&#35745;&#23376;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#19981;&#30830;&#23450;&#24615;&#20026;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65307;b&#65289;&#23427;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#32463;&#39564;&#36125;&#21494;&#26031;&#36807;&#31243;&#25191;&#34892;&#29305;&#24449;&#30340;&#38544;&#24335;&#36873;&#25321;&#65307;c&#65289;&#23427;&#21487;&#29992;&#20110;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#65292;&#20316;&#20026;&#31934;&#32454;&#35843;&#25972;&#30340;&#20132;&#20114;&#27169;&#22411;&#20505;&#36873;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;LA-NAM&#65289;&#25552;&#39640;&#20102;NAM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#20132;&#20114;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have found successful applications in many fields, but their black-box nature hinders interpretability. This is addressed by the neural additive model (NAM), in which the network is divided into additive sub-networks, thus making apparent the interaction between input features and predictions. In this paper, we approach the additive structure from a Bayesian perspective and develop a practical Laplace approximation. This enhances interpretability in three primary ways: a) It provides credible intervals for the recovered feature interactions by estimating function-space uncertainty of the sub-networks; b) it yields a tractable estimate of the marginal likelihood, which can be used to perform an implicit selection of features through an empirical Bayes procedure; and c) it can be used to rank feature pairs as candidates for second-order interactions in fine-tuned interaction models. We show empirically that our proposed Laplace-approximated NAM (LA-NAM) improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16368</link><description>&lt;p&gt;
&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65306;&#23398;&#20064;&#20849;&#36717;&#26799;&#24230;&#27861;&#30340;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural incomplete factorization: learning preconditioners for the conjugate gradient method. (arXiv:2305.16368v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31185;&#23398;&#35745;&#31639;&#21644;&#20248;&#21270;&#20013;&#36935;&#21040;&#30340;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#36890;&#36807;&#26367;&#25442;&#19982;&#20849;&#36717;&#26799;&#24230;&#27861;&#19968;&#36215;&#20351;&#29992;&#30340;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65289;&#26174;&#30528;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#21463;&#31232;&#30095;&#30697;&#38453;&#29702;&#35770;&#21551;&#21457;&#30340;&#26032;&#22411;&#28040;&#24687;&#20256;&#36882;&#22359;&#65292;&#23427;&#19982;&#23547;&#25214;&#30697;&#38453;&#30340;&#31232;&#30095;&#20998;&#35299;&#30340;&#30446;&#26631;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#38382;&#39064;&#21644;&#26469;&#33258;&#31185;&#23398;&#35745;&#31639;&#30340;&#30495;&#23454;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#22987;&#32456;&#20248;&#20110;&#26368;&#24120;&#35265;&#30340;&#36890;&#29992;&#39044;&#22788;&#29702;&#22120;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#30340;Cholesky&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a novel data-driven approach to accelerate solving large-scale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14383</link><description>&lt;p&gt;
&#19968;&#20010;&#38477;&#32500;&#20154;&#31867;&#20998;&#31867;&#30340;&#29702;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#20013;&#29616;&#26377;&#30340;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#22312;&#24515;&#29702;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#32423;&#27010;&#25324;&#34892;&#20026;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#31867;&#21035;&#34920;&#31034;&#21487;&#33021;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20154;&#20204;&#19968;&#33324;&#20381;&#36182;&#20110;&#19968;&#32452;&#21487;&#34892;&#20294;&#36275;&#22815;&#30340;&#29305;&#24449;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#27010;&#29575;&#20027;&#25104;&#20998;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#32463;&#27982;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#31867;&#20998;&#31867;&#20013;&#30340;&#32500;&#24230;&#20559;&#24046;&#24182;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20869;&#21033;&#29992;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#39640;&#32500;&#21050;&#28608;&#19979;&#26356;&#22909;&#30340;&#20998;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#26469;&#32763;&#36716;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#21457;&#29616;&#65292;&#20165;&#21024;&#38500;1%&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#32763;&#36716;&#12290;&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#26222;&#36941;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#19968;&#20010;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#21542;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#32763;&#36716;&#65311;&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#36825;&#31181;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22987;&#32456;&#33021;&#22815;&#20135;&#29983;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26426;&#21046;&#26377;&#22810;&#37325;&#20316;&#29992;&#65306;&#65288;1&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24674;&#22797;&#21487;&#33021;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#65307;&#65288;2&#65289;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#26412;&#25991;&#21457;&#29616;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#35757;&#32451;&#38598;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#27604;&#20363;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65307;&#65288;3&#65289;&#25552;&#20379;&#20102;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#23545;&#35782;&#21035;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;</title><link>http://arxiv.org/abs/2305.11857</link><description>&lt;p&gt;
Q-malizing&#27969;&#21644;&#26080;&#31351;&#23567;&#23494;&#24230;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11857
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#27969;&#32593;&#32476;&#20174;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#27491;&#24577;&#20998;&#24067;&#12290;&#19968;&#31181;&#33021;&#22815;&#20174;P&#20256;&#36755;&#21040;&#20219;&#24847;Q&#30340;&#27969;&#27169;&#22411;&#65292;&#20854;&#20013;P&#21644;Q&#37117;&#21487;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#35775;&#38382;&#65292;&#23558;&#22312;&#21508;&#31181;&#24212;&#29992;&#20852;&#36259;&#20013;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#26395;&#36828;&#38236;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#65288;DRE&#65289;&#65292;&#23427;&#38656;&#35201;&#26500;&#24314;&#20013;&#38388;&#23494;&#24230;&#20197;&#22312;P&#21644;Q&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#8220;Q-malizing&#27969;&#8221;&#65292;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32463;&#39564;&#26679;&#26412;&#30340;&#21487;&#36870;&#20256;&#36755;&#20174;P&#21040;Q&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#20256;&#36755;&#25104;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27969;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#19982;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;log&#23494;&#24230;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#65292;&#36890;&#36807;&#35757;&#32451;&#38468;&#21152;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#32593;&#32476;&#20351;&#29992;&#20998;&#31867;&#25439;&#22833;&#26469;&#20272;&#35745;log&#23494;&#24230;&#30340;&#26102;&#38388;&#20559;&#23548;&#25968;&#12290;&#36890;&#36807;&#31215;&#20998;&#26102;&#38388;&#24471;&#20998;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#26041;&#27861;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10267</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
State Representation Learning Using an Unbalanced Atlas. (arXiv:2305.10267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#26041;&#27861;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35828;&#35748;&#20026;&#65292;&#39640;&#32500;&#25968;&#25454;&#36890;&#24120;&#20301;&#20110;&#36739;&#20302;&#32500;&#30340;&#27969;&#24418;&#19978;&#65292;&#24182;&#19988;&#21033;&#29992;&#35813;&#27969;&#24418;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#21487;&#20197;&#20135;&#29983;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#25216;&#26415;&#29992;&#20110;&#38477;&#32500;&#65292;&#20294;&#23427;&#20204;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#12290;&#26368;&#36817;&#30340;MSIMCLR&#26041;&#27861;&#23558;&#27969;&#24418;&#32534;&#30721;&#19982;SimCLR&#30456;&#32467;&#21512;&#65292;&#20294;&#38656;&#35201;&#26497;&#20302;&#30340;&#30446;&#26631;&#32534;&#30721;&#32500;&#24230;&#25165;&#33021;&#32988;&#36807;SimCLR&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#35843;&#25972;&#26102;&#31354;DeepInfomax&#65288;ST-DIM&#65289;&#26694;&#26550;&#20197;&#19982;&#25105;&#20204;&#25552;&#35758;&#30340;UA&#27169;&#24335;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#37319;&#29992;&#20005;&#35880;&#30340;&#31185;&#23398;&#26041;&#27861;&#26469;&#31934;&#24515;&#30740;&#31350;&#21644;&#35774;&#35745;&#20102;&#20351;&#29992;UA&#30340;DeepInfomax&#65288;DIM-UA&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold hypothesis posits that high-dimensional data often lies on a lower-dimensional manifold and that utilizing this manifold as the target space yields more efficient representations. While numerous traditional manifold-based techniques exist for dimensionality reduction, their application in self-supervised learning has witnessed slow progress. The recent MSIMCLR method combines manifold encoding with SimCLR but requires extremely low target encoding dimensions to outperform SimCLR, limiting its applicability. This paper introduces a novel learning paradigm using an unbalanced atlas (UA), capable of surpassing state-of-the-art self-supervised learning approaches. We meticulously investigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA) method by systematically adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align with our proposed UA paradigm, employing rigorous scientific methodologies throughout the process. The efficacy of DIM-UA is demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04228</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20998;&#31867;&#26159;&#31243;&#24207;&#29702;&#35299;&#21644;&#33258;&#21160;&#32534;&#30721;&#20013;&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#30001;&#20110;&#31243;&#24207;&#30340;&#27169;&#31946;&#35821;&#27861;&#21644;&#22797;&#26434;&#35821;&#20041;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25216;&#26415;&#21019;&#24314;&#20195;&#30721;&#34920;&#31034;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;&#12290;&#36825;&#20123;&#25216;&#26415;&#21033;&#29992;&#20195;&#30721;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#21482;&#32771;&#34385;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;AST&#20013;&#33410;&#28857;&#20043;&#38388;&#24050;&#32463;&#23384;&#22312;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20195;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#65288;HDHG&#65289;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HDHGN&#65289;&#22788;&#29702;&#22270;&#24418;&#12290;HDHG&#20445;&#30041;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#24182;&#26356;&#20840;&#38754;&#22320;&#32534;&#30721;&#20102;AST&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;HDHGN&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#33410;&#28857;&#30340;&#29305;&#24449;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#26469;&#23545;AST&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HDHG&#21644;HDHGN&#22312;&#20195;&#30721;&#20998;&#31867;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02614</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#21450;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23547;&#25214;&#40657;&#31665;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#34429;&#28982;&#40657;&#31665;&#20989;&#25968;&#30340;&#35780;&#20272;&#25104;&#26412;&#24448;&#24448;&#24456;&#39640;&#65292;&#20294;&#20943;&#23569;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;BO&#29615;&#22659;&#19979;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#39564;&#35777;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#25552;&#39640;BO&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#65292;&#23558;&#20854;&#20248;&#21270;&#20026;&#25152;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#36890;&#36807;&#20174;&#21160;&#24577;&#36866;&#24212;&#30340;&#26497;&#20540;&#20998;&#24067;&#20013;&#36873;&#25321;&#26410;&#26631;&#31614;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BO&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;BO&#26041;&#27861;&#22312;&#23398;&#20064;&#21518;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;Lipschitz&#32422;&#26463;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#21487;&#20197;&#31616;&#21333;&#26126;&#20102;&#22320;&#25511;&#21046;&#24191;&#27867;&#30340;VAE&#27169;&#22411;&#30340;&#21518;&#39564;&#22349;&#22604;&#31243;&#24230;&#65292;&#24182;&#24102;&#26377;&#20855;&#20307;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12770</link><description>&lt;p&gt;
&#22522;&#20110;&#21453;Lipschitz&#32422;&#26463;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#25511;&#21046;&#21518;&#39564;&#22349;&#22604;
&lt;/p&gt;
&lt;p&gt;
Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network. (arXiv:2304.12770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;Lipschitz&#32422;&#26463;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#21487;&#20197;&#31616;&#21333;&#26126;&#20102;&#22320;&#25511;&#21046;&#24191;&#27867;&#30340;VAE&#27169;&#22411;&#30340;&#21518;&#39564;&#22349;&#22604;&#31243;&#24230;&#65292;&#24182;&#24102;&#26377;&#20855;&#20307;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#19968;&#31181;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#31216;&#20026;&#21518;&#39564;&#22349;&#22604;&#30340;&#38382;&#39064;&#65292;&#24403;&#32534;&#30721;&#22120;&#19982;&#27809;&#26377;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#20808;&#39564;&#37325;&#21512;&#25110;&#22349;&#22604;&#26102;&#23601;&#20250;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;Lipschitz&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#30721;&#22120;&#65292;&#22522;&#20110;&#36825;&#20010;&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31616;&#21333;&#26126;&#20102;&#22320;&#25511;&#21046;&#24191;&#27867;&#30340;VAE&#27169;&#22411;&#30340;&#21518;&#39564;&#22349;&#22604;&#31243;&#24230;&#65292;&#24182;&#24102;&#26377;&#20855;&#20307;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are one of the deep generative models that have experienced enormous success over the past decades. However, in practice, they suffer from a problem called posterior collapse, which occurs when the encoder coincides, or collapses, with the prior taking no information from the latent structure of the input data into consideration. In this work, we introduce an inverse Lipschitz neural network into the decoder and, based on this architecture, provide a new method that can control in a simple and clear manner the degree of posterior collapse for a wide range of VAE models equipped with a concrete theoretical guarantee. We also illustrate the effectiveness of our method through several numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.08172</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#29702;&#35770;&#32467;&#26500;&#36880;&#28176;&#24471;&#21040;&#20102;&#38416;&#26126;&#12290;Imaizumi-Fukumizu&#65288;2019&#65289;&#21644;Suzuki&#65288;2019&#65289;&#25351;&#20986;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#20026;&#38750;&#20809;&#28369;&#20989;&#25968;&#26102;&#65292;DNN&#30340;&#23398;&#20064;&#33021;&#21147;&#20248;&#20110;&#20808;&#21069;&#30340;&#29702;&#35770;&#12290;&#28982;&#32780;&#65292;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#20247;&#22810;&#30740;&#31350;&#23581;&#35797;&#22312;&#27809;&#26377;&#20219;&#20309;&#32479;&#35745;&#35770;&#35777;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#23398;&#30740;&#31350;&#65292;&#25506;&#31350;&#30495;&#27491;&#33021;&#22815;&#24341;&#21457;&#26799;&#24230;&#19979;&#38477;&#30340;DNN&#26550;&#26500;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#24615;&#65292;&#36825;&#19968;&#23581;&#35797;&#20284;&#20046;&#26356;&#36148;&#36817;&#23454;&#38469;DNN&#12290;&#26412;&#25991;&#23558;&#30446;&#26631;&#20989;&#25968;&#38480;&#21046;&#20026;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#65292;&#24182;&#22312;ReLU-DNN&#20013;&#26500;&#36896;&#20102;&#19968;&#20010;&#31232;&#30095;&#19988;&#20855;&#26377;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.05622</link><description>&lt;p&gt;
SAMM&#65288;Segment Any Medical Model&#65289;&#65306;&#29992;&#20110;SAM&#30340;3D Slicer&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM. (arXiv:2304.05622v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05622
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#24037;&#20855;&#65292;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#20998;&#21106;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#34920;&#26126;&#23427;&#21487;&#20197;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#27169;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#20026;&#20102;&#21327;&#21161;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#24320;&#21457;&#65292;&#35780;&#20272;&#21644;&#21033;&#29992;SAM&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Segment Any Medical Model&#65288;SAMM&#65289;&#65292;&#23427;&#26159;SAM&#22312;3D Slicer&#19978;&#30340;&#25193;&#23637;&#12290;3D Slicer&#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#36719;&#20214;&#30340;&#24320;&#28304;&#36719;&#20214;&#12290;&#36825;&#20010;&#24320;&#28304;&#25193;&#23637;&#31243;&#24207;&#21450;&#20854;&#28436;&#31034;&#24050;&#21457;&#24067;&#22312;GitHub&#19978;&#65288;https://github.com/bingogome/samm&#65289;&#12290;SAMM&#22312;&#23436;&#25972;&#21608;&#26399;&#20013;&#23454;&#29616;&#20102;0.6&#31186;&#30340;&#24310;&#36831;&#65292;&#24182;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;&#20986;&#22270;&#20687;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest segmentation dataset at this time. The model has demonstrated that it can create high-quality masks for image segmentation with good promptability and generalizability. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21152;&#36895;&#20840;&#33145;MRI&#25195;&#25551;&#30340;&#26032;&#26041;&#27861;GLADE&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#26469;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.11831</link><description>&lt;p&gt;
GLADE&#65306;&#29992;&#20110;&#38750;&#37197;&#23545;&#36229;&#20998;&#36776;&#29575;&#21508;&#21521;&#24322;&#24615;MRI&#30340;&#26799;&#24230;&#25439;&#22833;&#22686;&#24378;&#36864;&#21270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI. (arXiv:2303.11831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21152;&#36895;&#20840;&#33145;MRI&#25195;&#25551;&#30340;&#26032;&#26041;&#27861;GLADE&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#26469;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#38750;&#37197;&#23545;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21508;&#21521;&#24322;&#24615;3D&#22270;&#20687;&#20013;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;CycleGAN&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21508;&#21521;&#24322;&#24615;&#20307;&#31215;&#39640;&#20998;&#36776;&#29575;&#65288;&#38754;&#20869;&#65289;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#30340;&#34917;&#19969;&#65292;&#24378;&#21046;&#32593;&#32476;&#29983;&#25104;&#22120;&#22686;&#21152;&#20302;&#20998;&#36776;&#29575;&#65288;&#38754;&#22806;&#65289;&#20999;&#29255;&#30340;&#20998;&#36776;&#29575;&#12290;&#36825;&#23558;&#20351;&#22312;&#30701;&#26102;&#38388;&#20869;&#20197;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;&#22270;&#20687;&#36827;&#34892;&#20840;&#33145;&#25195;&#25551;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to synthesise high-resolution isotropic 3D abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a modified CycleGAN architecture with a gradient mapping loss, we leverage disjoint patches from the high-resolution (in-plane) data of an anisotropic volume to enforce the network generator to increase the resolution of the low-resolution (through-plane) slices. This will enable accelerated whole-abdomen scanning with high-resolution isotropic images within short breath-hold times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10181</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Operating critical machine learning models in resource constrained regimes. (arXiv:2303.10181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#26041;&#38754;&#30340;&#26368;&#26032;&#31361;&#30772;&#65292;&#20351;&#20854;&#24471;&#21040;&#24555;&#36895;&#21457;&#23637;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#65292;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#26041;&#38754;&#30340;&#36164;&#28304;&#28040;&#32791;&#26159;&#24040;&#22823;&#30340;&#12290;&#36825;&#20123;&#24040;&#22823;&#30340;&#36164;&#28304;&#25104;&#26412;&#21487;&#33021;&#20250;&#38459;&#30861;&#36825;&#20123;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#21162;&#21147;&#24341;&#20837;&#36164;&#28304;&#25928;&#29575;&#30340;&#27010;&#24565;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#37327;&#21270;&#26469;&#20943;&#36731;&#20869;&#23384;&#28040;&#32791;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#21033;&#29992;&#65292;&#20294;&#21487;&#33021;&#20250;&#20197;&#24615;&#33021;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#29305;&#21035;&#26159;&#22312;&#35786;&#25152;&#31561;&#20851;&#38190;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerated development of machine learning methods, primarily deep learning, are causal to the recent breakthroughs in medical image analysis and computer aided intervention. The resource consumption of deep learning models in terms of amount of training data, compute and energy costs are known to be massive. These large resource costs can be barriers in deploying these models in clinics, globally. To address this, there are cogent efforts within the machine learning community to introduce notions of resource efficiency. For instance, using quantisation to alleviate memory consumption. While most of these methods are shown to reduce the resource utilisation, they could come at a cost in performance. In this work, we probe into the trade-off between resource consumption and performance, specifically, when dealing with models that are used in critical settings such as in clinics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.09167</link><description>&lt;p&gt;
&#22522;&#20110;&#20687;&#32032;&#30340;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#19982;&#21327;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mixed Traffic Control and Coordination from Pixels. (arXiv:2302.09167v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#26159;&#25105;&#20204;&#31038;&#20250;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#25511;&#21046;&#26041;&#27861;&#22312;&#32531;&#35299;&#24403;&#21069;&#25317;&#22581;&#31243;&#24230;&#26041;&#38754;&#24050;&#32463;&#22833;&#25928;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#36890;&#36807;&#26426;&#22120;&#20154;&#36710;&#36742;&#36827;&#34892;&#20132;&#36890;&#25511;&#21046;&#30340;&#24819;&#27861;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#32423;&#21035;&#33258;&#20027;&#24615;&#36710;&#36742;&#30340;&#19981;&#26029;&#28044;&#29616;&#12290;&#36825;&#24341;&#36215;&#20102;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#36710;&#36742;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35843;&#33410;&#20154;&#39550;&#39542;&#36710;&#36742;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;1&#65289;&#22270;&#20687;&#36890;&#36807;&#21355;&#26143;&#22270;&#20687;&#12289;&#36710;&#20869;&#25668;&#20687;&#31995;&#32479;&#21644;&#20132;&#36890;&#30417;&#25511;&#31995;&#32479;&#26222;&#36941;&#23384;&#22312;&#65307;2&#65289;&#22270;&#20687;&#19981;&#38656;&#35201;&#26356;&#26032;&#29616;&#26377;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#21521;&#21487;&#33021;&#19981;&#24895;&#24847;&#37197;&#21512;&#30340;&#20154;&#31867;&#39550;&#39542;&#21592;&#20256;&#36882;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion is a persistent problem in our society. Existing methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that involve global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations as the alternative for mixed traffic control via RL: 1) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; 2) images do not require a compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#21644;&#22810;&#26679;&#21270;&#30340;&#21050;&#28608;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06279</link><description>&lt;p&gt;
Sneaky Spikes: &#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#25581;&#31034;&#38544;&#34109;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data. (arXiv:2302.06279v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#21644;&#22810;&#26679;&#21270;&#30340;&#21050;&#28608;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#21270;DNN&#30340;&#25928;&#26524;&#38656;&#35201;&#36890;&#36807;&#35757;&#32451;&#23545;&#20247;&#22810;&#36229;&#21442;&#25968;&#21644;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#31934;&#32454;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#39640;&#24615;&#33021;&#30340;DNN&#28041;&#21450;&#35768;&#22810;&#21442;&#25968;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#20854;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#29983;&#29289;&#23398;&#21487;&#34892;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#24863;&#30693;&#25968;&#25454;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#26041;&#38754;&#12290;&#23613;&#31649;&#23384;&#22312;&#20248;&#21183;&#65292;SNN&#19982;DNN&#19968;&#26679;&#65292;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#23041;&#32961;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;SNN&#22312;&#29702;&#35299;&#21644;&#23545;&#25239;&#36825;&#20123;&#25915;&#20987;&#26041;&#38754;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#21644;&#22810;&#26679;&#21270;&#30340;&#21050;&#28608;&#30340;SNN&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.  This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse trig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.03596</link><description>&lt;p&gt;
&#24102;&#26377;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graph Generation with Destination-Predicting Diffusion Mixture. (arXiv:2302.03596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#26159;&#29702;&#35299;&#20854;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#30495;&#23454;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#19981;&#36866;&#21512;&#24314;&#27169;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22240;&#20026;&#23398;&#20064;&#21435;&#22122;&#22768;&#26679;&#26412;&#19981;&#33021;&#26126;&#30830;&#22320;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#27979;&#25193;&#25955;&#36807;&#31243;&#30340;&#30446;&#26631;&#65292;&#21363;&#20855;&#26377;&#27491;&#30830;&#25299;&#25169;&#20449;&#24687;&#30340;&#21407;&#22987;&#22270;&#20316;&#20026;&#25968;&#25454;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#65292;&#24314;&#27169;&#20102;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#36807;&#31243;&#35774;&#35745;&#20026;&#19968;&#20010;&#20197;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#32456;&#28857;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#36807;&#31243;&#28151;&#21512;&#65292;&#23427;&#23558;&#36807;&#31243;&#25512;&#21521;&#39044;&#27979;&#30340;&#30446;&#26631;&#65292;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39044;&#27979;&#30446;&#26631;&#30340;&#26032;&#22411;&#26080;&#20223;&#30495;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23558;&#36825;&#31181;&#31574;&#30053;&#32435;&#20837;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of graphs is a major challenge for real-world tasks that require understanding the complex nature of their non-Euclidean structures. Although diffusion models have achieved notable success in graph generation recently, they are ill-suited for modeling the structural information of graphs since learning to denoise the noisy samples does not explicitly capture the graph topology. To tackle this limitation, we propose a novel generative framework that models the topology of graphs by predicting the destination of the diffusion process, which is the original graph that has the correct topology information, as a weighted mean of data. Specifically, we design the generative process as a mixture of diffusion processes conditioned on the endpoint in the data distribution, which drives the process toward the predicted destination, resulting in rapid convergence. We introduce new simulation-free training objectives for predicting the destination, and further discuss the advantages of 
&lt;/p&gt;</description></item><item><title>FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12407</link><description>&lt;p&gt;
FedEBA+&#65306;&#22522;&#20110;&#29109;&#30340;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#21644;&#26377;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12407
&lt;/p&gt;
&lt;p&gt;
FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20844;&#24179;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#23427;&#20351;&#27169;&#22411;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20419;&#36827;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23454;&#29616;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19982;&#21069;&#32773;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedEBA+&#65292;&#23427;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;FedEBA+&#30340;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;FedEBA+&#22312;&#20844;&#24179;&#24615;&#21644;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;SOTA&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.10636</link><description>&lt;p&gt;
&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#33945;&#29256;&#35270;&#39057;&#24314;&#27169;&#25216;&#26415;&#36890;&#36807;&#22312;&#35270;&#39057;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#23548;&#33268;&#39044;&#27979;&#26080;&#25928;&#30340;&#26631;&#35760;/&#24103;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#65292;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#26426;&#21644;&#22823;&#37327;&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#39057;&#34917;&#19969;&#20013;&#30340;&#19981;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65306;MATS&#65306;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#65292;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#25214;&#21040;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#24182;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#26368;&#37325;&#35201;&#21644;&#22240;&#26524;&#24615;&#30340;&#24103;&#65292;&#24182;&#20351;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24471;&#21040;&#26174;&#30528;&#38477;&#20302;&#65292;&#20351;&#24471;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#34880;&#20957;&#32032;&#24207;&#21015;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#22312;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#24207;&#21015;&#30340;&#36215;&#28304;&#12290;</title><link>http://arxiv.org/abs/2207.13842</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#34880;&#20957;&#32032;&#24207;&#21015;&#22312;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences. (arXiv:2207.13842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#34880;&#20957;&#32032;&#24207;&#21015;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#22312;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#24207;&#21015;&#30340;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#30149;&#27602;&#31361;&#21464;&#36805;&#36895;&#65292;&#23545;&#20844;&#20247;&#20581;&#24247;&#65292;&#29305;&#21035;&#26159;&#33030;&#24369;&#32676;&#20307;&#26500;&#25104;&#23041;&#32961;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#22312;&#19981;&#21516;&#29289;&#31181;&#20043;&#38388;&#24341;&#21457;&#36807;&#22823;&#27969;&#34892;&#12290;&#30830;&#23450;&#30149;&#27602;&#30340;&#36215;&#28304;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#38450;&#27490;&#30123;&#24773;&#30340;&#20256;&#25773;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30149;&#27602;&#24207;&#21015;&#30340;&#24555;&#36895;&#20934;&#30830;&#39044;&#27979;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30495;&#23454;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#20197;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#30001;&#20110;&#34880;&#20957;&#32032;&#26159;&#20813;&#30123;&#21453;&#24212;&#20013;&#30340;&#20027;&#35201;&#34507;&#30333;&#36136;&#65292;&#21482;&#20351;&#29992;&#34880;&#20957;&#32032;&#24207;&#21015;&#65292;&#24182;&#20197;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#27979;&#30149;&#27602;&#24207;&#21015;&#36215;&#28304;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#22312;&#36739;&#39640;&#20998;&#31867;&#27700;&#24179;&#19978;&#22823;&#32422;&#26377;99.54&#65285;&#30340;AUCPR&#65292;98.01&#65285;&#30340;F1&#24471;&#20998;&#21644;96.60&#65285;&#30340;MCC&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenza viruses mutate rapidly and can pose a threat to public health, especially to those in vulnerable groups. Throughout history, influenza A viruses have caused pandemics between different species. It is important to identify the origin of a virus in order to prevent the spread of an outbreak. Recently, there has been increasing interest in using machine learning algorithms to provide fast and accurate predictions for viral sequences. In this study, real testing data sets and a variety of evaluation metrics were used to evaluate machine learning algorithms at different taxonomic levels. As hemagglutinin is the major protein in the immune response, only hemagglutinin sequences were used and represented by position-specific scoring matrix and word embedding. The results suggest that the 5-grams-transformer neural network is the most effective algorithm for predicting viral sequence origins, with approximately 99.54% AUCPR, 98.01% F1 score and 96.60% MCC at a higher classification l
&lt;/p&gt;</description></item></channel></rss>