<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#24050;&#30693;&#31639;&#27861;&#20219;&#21153;&#20013;&#26377;&#26102;&#20250;&#21457;&#29616;&#36136;&#24577;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#21363;&#20351;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#23567;&#35843;&#25972;&#65292;&#20063;&#21487;&#33021;&#20986;&#29616;&#24182;&#34892;&#23454;&#29616;&#22810;&#20010;&#31639;&#27861;&#30340;&#24773;&#20917;&#12290;&#36825;&#19968;&#32467;&#35770;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20063;&#21487;&#20197;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.17844</link><description>&lt;p&gt;
&#26102;&#38047;&#19982;&#25259;&#33832;&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#26800;&#35299;&#37322;&#20013;&#30340;&#20004;&#20010;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks. (arXiv:2306.17844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#24050;&#30693;&#31639;&#27861;&#20219;&#21153;&#20013;&#26377;&#26102;&#20250;&#21457;&#29616;&#36136;&#24577;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#21363;&#20351;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#23567;&#35843;&#25972;&#65292;&#20063;&#21487;&#33021;&#20986;&#29616;&#24182;&#34892;&#23454;&#29616;&#22810;&#20010;&#31639;&#27861;&#30340;&#24773;&#20917;&#12290;&#36825;&#19968;&#32467;&#35770;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20063;&#21487;&#20197;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#24050;&#30693;&#30340;&#31639;&#27861;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#33021;&#21542;&#21487;&#38752;&#22320;&#37325;&#26032;&#21457;&#29616;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#24050;&#30693;&#31639;&#27861;&#65311;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#20174;&#32676;&#31639;&#26415;&#21040;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#30340;&#20219;&#21153;&#65292;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#25105;&#20204;&#20197;&#27169;&#22359;&#21152;&#27861;&#20026;&#20856;&#22411;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31639;&#27861;&#21457;&#29616;&#26377;&#26102;&#26356;&#21152;&#22797;&#26434;&#12290;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#21644;&#21021;&#22987;&#21270;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#21487;&#20197;&#23548;&#33268;&#20174;&#22266;&#23450;&#35757;&#32451;&#38598;&#20013;&#21457;&#29616;&#23450;&#24615;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#29978;&#33267;&#26159;&#24182;&#34892;&#23454;&#29616;&#22810;&#20010;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#19968;&#20123;&#35757;&#32451;&#29992;&#20110;&#25191;&#34892;&#27169;&#22359;&#21152;&#27861;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#29087;&#24713;&#30340;&#26102;&#38047;&#31639;&#27861;&#65307;&#20854;&#20182;&#23454;&#29616;&#20102;&#20197;&#21069;&#26410;&#25551;&#36848;&#36807;&#30340;&#12289;&#19981;&#22826;&#30452;&#35266;&#20294;&#21487;&#29702;&#35299;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#25259;&#33832;&#31639;&#27861;&#65292;&#25110;&#32773;&#20854;&#20182;&#26356;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#20063;&#21487;&#20197;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#20419;&#36827;&#20102;&#26032;&#24037;&#20855;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools 
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17833</link><description>&lt;p&gt;
&#37325;&#32622;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#22120;&#65306;&#19968;&#20010;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Resetting the Optimizer in Deep RL: An Empirical Study. (arXiv:2306.17833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17833
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#20284;&#35745;&#31639;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#21253;&#25324;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#35299;&#20915;&#19968;&#31995;&#21015;&#19981;&#21516;&#36845;&#20195;&#20013;&#30446;&#26631;&#20989;&#25968;&#21487;&#33021;&#25913;&#21464;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#29616;&#20195;&#21464;&#31181;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22914;Adam&#12290;&#36825;&#20123;&#20248;&#21270;&#22120;&#20445;&#25345;&#33258;&#24049;&#30340;&#20869;&#37096;&#21442;&#25968;&#65292;&#22914;&#26799;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#65292;&#24182;&#38543;&#26102;&#38388;&#26356;&#26032;&#36825;&#20123;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#20043;&#21069;&#36845;&#20195;&#30340;&#20449;&#24687;&#34987;&#29992;&#26469;&#22312;&#24403;&#21069;&#36845;&#20195;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#20043;&#21069;&#36845;&#20195;&#30340;&#20248;&#21270;&#39118;&#26223;&#19982;&#24403;&#21069;&#36845;&#20195;&#30456;&#24046;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#20250;&#27745;&#26579;&#25152;&#20351;&#29992;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#24433;&#21709;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#26159;&#22312;&#24320;&#22987;&#26032;&#30340;&#36845;&#20195;&#26102;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of approximately solving a sequence of optimization problems where the objective function can change per iteration. The common approach to solving the problem is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first and the second moment of the gradient, and update these parameters over time. Therefore, information obtained in previous iterations is being used to solve the optimization problem in the current iteration. We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration. To hedge against this effect, a simple idea is to reset the internal parameters of the optimizer when starting a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;YOLOv5&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#26410;&#35265;&#36807;&#30446;&#26631;&#30340;&#36793;&#30028;&#26694;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17829</link><description>&lt;p&gt;
&#32852;&#21512;&#38598;&#25104;YOLOv5 - &#19968;&#31181;&#26356;&#22909;&#30340;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm. (arXiv:2306.17829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;YOLOv5&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#26410;&#35265;&#36807;&#30446;&#26631;&#30340;&#36793;&#30028;&#26694;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#31639;&#27861;&#24050;&#32463;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#22914;&#32852;&#21512;&#24179;&#22343;&#65288;FED Avg&#65289;&#25110;&#32852;&#21512;SGD&#65288;FED SGD&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#30340;&#30456;&#20284;&#24615;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;FL&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#30340;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;FL&#35757;&#32451;&#30340;YOLOv5&#27169;&#22411;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#37319;&#29992;&#26080;&#26367;&#25442;&#30340;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#37096;&#20998;&#29992;&#20110;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#30456;&#21516;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;FL&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#26410;&#35265;&#36807;&#30446;&#26631;&#30340;&#36793;&#30028;&#26694;&#26041;&#38754;&#30340;&#21331;&#36234;&#25928;&#29575;&#65292;&#27979;&#35797;&#38598;&#26159;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#23545;&#35937;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has gained significant traction as a privacy-preserving algorithm, but the underlying resembles of federated learning algorithm like Federated averaging (FED Avg) or Federated SGD (FED SGD) to ensemble learning algorithms has not been fully explored. The purpose of this paper is to examine the application of FL to object detection as a method to enhance generalizability, and to compare its performance against a centralized training approach for an object detection algorithm. Specifically, we investigate the performance of a YOLOv5 model trained using FL across multiple clients and employ a random sampling strategy without replacement, so each client holds a portion of the same dataset used for centralized training. Our experimental results showcase the superior efficiency of the FL object detector's global model in generating accurate bounding boxes for unseen objects, with the test set being a mixture of objects from two distinct clients not represented in the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17828</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#27010;&#24565;&#24433;&#21709;&#29702;&#35299;&#19981;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17828
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#21407;&#22240;&#26377;&#21161;&#20110;&#20174;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22521;&#35757;&#25968;&#25454;&#36825;&#19968;&#20027;&#35201;&#19981;&#20844;&#24179;&#26469;&#28304;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#26524;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#20123;&#26679;&#26412;&#65288;1&#65289;&#26469;&#33258;&#19981;&#21516;&#30340;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#65289;&#32676;&#20307;&#65292;&#65288;2&#65289;&#26631;&#35760;&#26041;&#24335;&#19981;&#21516;&#65292;&#25110;&#32773;&#65288;3&#65289;&#26576;&#20123;&#29305;&#24449;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#37027;&#20040;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#34920;&#29616;&#20250;&#21457;&#29983;&#24590;&#26679;&#30340;&#21464;&#21270;&#65311;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#20107;&#23454;&#22320;&#23545;&#22522;&#20110;&#39044;&#23450;&#20041;&#27010;&#24565;&#30340;&#26679;&#26412;&#36827;&#34892;&#24178;&#39044;&#21644;&#25913;&#21464;&#65292;&#37327;&#21270;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30456;&#23545;&#20110;&#27010;&#24565;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27010;&#24565;&#29983;&#25104;&#21453;&#20107;&#23454;&#29256;&#26412;&#30340;&#26679;&#26412;&#65292;&#21363;&#22914;&#26524;&#27010;&#24565;&#21457;&#29983;&#21464;&#21270;&#65292;&#26679;&#26412;&#30340;&#21453;&#20107;&#23454;&#29256;&#26412;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24352;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#36229;&#22270;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;TTSV&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20302;&#20110;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#37051;&#25509;&#24352;&#37327;&#65292;&#24182;&#24212;&#29992;&#20110;&#36229;&#22270;&#20013;&#24515;&#24615;&#21644;&#32858;&#31867;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#33021;&#25552;&#20379;&#19982;&#22270;&#32553;&#20943;&#26041;&#27861;&#20114;&#34917;&#30340;&#20449;&#24687;&#65292;&#36824;&#33021;&#22815;&#25506;&#27979;&#21040;&#39640;&#38454;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.17825</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#38750;&#22343;&#21248;&#36229;&#22270;&#30340;&#24352;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable tensor methods for nonuniform hypergraphs. (arXiv:2306.17825v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24352;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#36229;&#22270;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;TTSV&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20302;&#20110;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#37051;&#25509;&#24352;&#37327;&#65292;&#24182;&#24212;&#29992;&#20110;&#36229;&#22270;&#20013;&#24515;&#24615;&#21644;&#32858;&#31867;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#33021;&#25552;&#20379;&#19982;&#22270;&#32553;&#20943;&#26041;&#27861;&#20114;&#34917;&#30340;&#20449;&#24687;&#65292;&#36824;&#33021;&#22815;&#25506;&#27979;&#21040;&#39640;&#38454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#32447;&#24615;&#20195;&#25968;&#22312;&#30740;&#31350;&#30001;&#36229;&#22270;&#27169;&#25311;&#30340;&#22810;&#26041;&#20132;&#20114;&#26041;&#38754;&#20284;&#20046;&#24456;&#33258;&#28982;&#65292;&#20294;&#36890;&#29992;&#36229;&#22270;&#30340;&#24352;&#37327;&#26041;&#27861;&#21463;&#21040;&#29702;&#35770;&#21644;&#23454;&#38469;&#38480;&#21046;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#37051;&#25509;&#24352;&#37327;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#36229;&#22270;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24418;&#25104;&#21644;&#20998;&#26512;&#23427;&#26159;&#20195;&#20215;&#39640;&#26114;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#36825;&#20010;&#24352;&#37327;&#30340;&#24352;&#37327;&#20056;&#30456;&#21516;&#21521;&#37327;&#65288;TTSV&#65289;&#31639;&#27861;&#65292;&#23558;&#22797;&#26434;&#24230;&#20174;$O(n^r)$&#38477;&#20302;&#21040;$r$&#30340;&#20302;&#27425;&#22810;&#39033;&#24335;&#65292;&#20854;&#20013;$n$&#26159;&#39030;&#28857;&#30340;&#25968;&#37327;&#65292;$r$&#26159;&#26368;&#22823;&#36229;&#36793;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#38544;&#24335;&#30340;&#65292;&#36991;&#20813;&#20102;&#24418;&#25104;$r$&#38454;&#37051;&#25509;&#24352;&#37327;&#12290;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24352;&#37327;&#30340;&#36229;&#22270;&#20013;&#24515;&#24615;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#24352;&#37327;&#24230;&#37327;&#22312;&#25968;&#25454;&#19978;&#19982;&#31867;&#20284;&#30340;&#22270;&#32553;&#20943;&#26041;&#27861;&#25552;&#20379;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#33021;&#22815;&#26816;&#27979;&#21040;&#35768;&#22810;&#29616;&#26377;&#22522;&#20110;&#30697;&#38453;&#30340;&#26041;&#27861;&#26080;&#27861;&#26816;&#27979;&#21040;&#30340;&#39640;&#38454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilinear algebra appears natural for studying the multiway interactions modeled by hypergraphs, tensor methods for general hypergraphs have been stymied by theoretical and practical barriers. A recently proposed adjacency tensor is applicable to nonuniform hypergraphs, but is prohibitively costly to form and analyze in practice. We develop tensor times same vector (TTSV) algorithms for this tensor which improve complexity from $O(n^r)$ to a low-degree polynomial in $r$, where $n$ is the number of vertices and $r$ is the maximum hyperedge size. Our algorithms are implicit, avoiding formation of the order $r$ adjacency tensor. We demonstrate the flexibility and utility of our approach in practice by developing tensor-based hypergraph centrality and clustering algorithms. We also show these tensor measures offer complementary information to analogous graph-reduction approaches on data, and are also able to detect higher-order structure that many existing matrix-based approaches p
&lt;/p&gt;</description></item><item><title>Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.17817</link><description>&lt;p&gt;
Act3D&#65306;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26816;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation. (arXiv:2306.17817v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17817
&lt;/p&gt;
&lt;p&gt;
Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#34920;&#24449;&#38750;&#24120;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#32534;&#30721;&#36974;&#25377;&#24773;&#20917;&#24182;&#31616;&#21270;&#31354;&#38388;&#25512;&#29702;&#12290;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#38656;&#35201;&#23545;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#36827;&#34892;&#39640;&#31354;&#38388;&#31934;&#24230;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;3D&#24863;&#30693;&#32593;&#26684;&#36827;&#34892;&#35745;&#31639;&#65292;&#36825;&#22312;&#22788;&#29702;&#19978;&#38750;&#24120;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25805;&#20316;&#31574;&#30053;&#30452;&#25509;&#22312;2D&#20013;&#36816;&#20316;&#65292;&#25918;&#24323;&#20102;3D&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Act3D&#65292;&#19968;&#31181;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#35270;&#20026;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#25805;&#20316;&#31574;&#30053;Transformer&#12290;&#23427;&#20197;&#19968;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#30340;&#26410;&#25237;&#24433;3D&#29305;&#24449;&#20113;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#31895;-&#31934;&#26041;&#24335;&#22312;&#33258;&#30001;&#31354;&#38388;&#20013;&#36845;&#20195;&#37319;&#26679;3D&#28857;&#32593;&#26684;&#65292;&#20351;&#29992;&#30456;&#23545;&#31354;&#38388;&#27880;&#24847;&#21147;&#23558;&#20854;&#29305;&#24449;&#21270;&#20026;&#29289;&#29702;&#29305;&#24449;&#20113;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#28857;&#36827;&#34892;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#12290;Act3D&#22312;&#24050;&#24314;&#31435;&#30340;&#25805;&#32437;&#22522;&#20934;RLbench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#25104;&#32489;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#20013;&#23454;&#29616;&#20102;10%&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.17815</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20449;&#24515;&#39044;&#27979;&#23454;&#29616;&#20855;&#22791;&#24418;&#24335;&#23433;&#20840;&#20445;&#35777;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction. (arXiv:2306.17815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#38646;&#38454;&#20248;&#21270;&#26159;&#37329;&#34701;&#12289;&#29289;&#29702;&#21644;&#24037;&#31243;&#31561;&#39046;&#22495;&#24212;&#29992;&#30340;&#26680;&#24515;&#22522;&#26412;&#25805;&#20316;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#24418;&#24335;&#20013;&#65292;&#35774;&#35745;&#32773;&#39034;&#24207;&#23581;&#35797;&#20505;&#36873;&#35299;&#65292;&#24182;&#20174;&#31995;&#32479;&#20013;&#25509;&#25910;&#21040;&#20851;&#20110;&#27599;&#20010;&#23581;&#35797;&#20540;&#30340;&#22122;&#22768;&#21453;&#39304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#23581;&#35797;&#35299;&#30340;&#23433;&#20840;&#24615;&#30340;&#21453;&#39304;&#65292;&#24182;&#19988;&#20248;&#21270;&#22120;&#34987;&#38480;&#21046;&#22312;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#20013;&#23581;&#35797;&#30340;&#19981;&#23433;&#20840;&#35299;&#30340;&#25968;&#37327;&#19978;&#12290;&#22312;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#30340;&#26041;&#27861;&#19978;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;SAFEOPT&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#21482;&#35201;&#28385;&#36275;&#23545;&#23433;&#20840;&#32422;&#26463;&#20989;&#25968;&#30340;&#20005;&#26684;&#20551;&#35774;&#65292;&#23601;&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#27010;&#29575;&#22312;&#21453;&#39304;&#22122;&#22768;&#19978;&#36991;&#20813;&#36873;&#25321;&#20219;&#20309;&#19981;&#23433;&#20840;&#30340;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;BO&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box zero-th order optimization is a central primitive for applications in fields as diverse as finance, physics, and engineering. In a common formulation of this problem, a designer sequentially attempts candidate solutions, receiving noisy feedback on the value of each attempt from the system. In this paper, we study scenarios in which feedback is also provided on the safety of the attempted solution, and the optimizer is constrained to limit the number of unsafe solutions that are tried throughout the optimization process. Focusing on methods based on Bayesian optimization (BO), prior art has introduced an optimization scheme -- referred to as SAFEOPT -- that is guaranteed not to select any unsafe solution with a controllable probability over feedback noise as long as strict assumptions on the safety constraint function are met. In this paper, a novel BO-based approach is introduced that satisfies safety requirements irrespective of properties of the constraint function. This s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#25351;&#23548;&#65288;CFG&#65289;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#25512;&#26029;&#26102;&#38388;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32431;&#35821;&#35328;&#24314;&#27169;&#20013;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#22686;&#24378;&#21161;&#25163;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25552;&#31034;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17806</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#19979;&#20445;&#25345;&#35805;&#39064;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stay on topic with Classifier-Free Guidance. (arXiv:2306.17806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#25351;&#23548;&#65288;CFG&#65289;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#25512;&#26029;&#26102;&#38388;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32431;&#35821;&#35328;&#24314;&#27169;&#20013;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#22686;&#24378;&#21161;&#25163;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25552;&#31034;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#25351;&#23548;&#65288;CFG&#65289;&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#20986;&#29616;&#65292;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#25216;&#26415;&#20419;&#36827;&#29983;&#25104;&#30340;&#31435;&#21363;&#36981;&#24490;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;CFG&#21487;&#20197;&#24191;&#27867;&#29992;&#20316;&#32431;&#35821;&#35328;&#24314;&#27169;&#30340;&#25512;&#26029;&#26102;&#38388;&#25216;&#26415;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CFG&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;Pythia&#12289;GPT-2&#21644;LLaMA-family&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;&#38382;&#31572;&#65292;&#25512;&#29702;&#65292;&#20195;&#30721;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#22312;LAMBADA&#19978;&#20351;&#29992;LLaMA-7B&#36229;&#36807;PaLM-540B&#30340;SOTA&#65307;&#65288;2&#65289;&#24102;&#26469;&#20102;&#30456;&#24403;&#20110;&#21452;&#20493;&#21442;&#25968;&#25968;&#30340;&#27169;&#22411;&#30340;&#25913;&#36827;&#65307;&#65288;3&#65289;&#21487;&#20197;&#19982;&#20854;&#20182;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#22914;Chain-of-Thought&#21644;Self-Consistency&#19968;&#36215;&#20351;&#29992;&#65292;&#22312;&#22256;&#38590;&#20219;&#21153;&#20013;&#21462;&#24471;&#36827;&#19968;&#27493;&#25913;&#36827;&#65307;&#65288;4&#65289;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#21161;&#25163;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24418;&#24335;&#39537;&#21160;&#21644;&#20869;&#23481;&#39537;&#21160;&#25552;&#31034;&#20013;&#30340;&#24544;&#23454;&#24230;&#21644;&#36830;&#36143;&#24615;&#65306;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;75&#65285;&#30340;&#29992;&#25143;&#26356;&#21916;&#27426;&#20351;&#29992;CFG&#30340;GPT4All&#32780;&#19981;&#26159;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\&amp;A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\% preference for GPT4All using CFG over baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#27425;&#36125;&#21494;&#26031;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22810;&#22320;&#28857;&#38144;&#21806;&#20132;&#26131;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#25512;&#29702;&#32467;&#26524;&#22312;&#32676;&#32452;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#21830;&#24215;&#29305;&#35768;&#32463;&#33829;&#24215;&#36141;&#20080;&#24773;&#20917;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.17795</link><description>&lt;p&gt;
&#22810;&#22320;&#28857;&#38144;&#21806;&#20132;&#26131;&#39044;&#27979;&#30340;&#23618;&#27425;&#36125;&#21494;&#26031;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Bayesian Regression for Multi-Location Sales Transaction Forecasting. (arXiv:2306.17795v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#27425;&#36125;&#21494;&#26031;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22810;&#22320;&#28857;&#38144;&#21806;&#20132;&#26131;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#25512;&#29702;&#32467;&#26524;&#22312;&#32676;&#32452;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#21830;&#24215;&#29305;&#35768;&#32463;&#33829;&#24215;&#36141;&#20080;&#24773;&#20917;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#33258;&#28982;&#22320;&#24418;&#25104;&#23618;&#27425;&#32467;&#26500;&#12290;&#36739;&#20302;&#23618;&#27425;&#20195;&#34920;&#20010;&#20307;&#25110;&#20107;&#20214;&#12290;&#36825;&#20123;&#20010;&#20307;&#33258;&#28982;&#22320;&#32858;&#38598;&#25104;&#20026;&#22320;&#28857;&#12289;&#26102;&#38388;&#38388;&#38548;&#25110;&#20854;&#20182;&#32858;&#21512;&#65292;&#36890;&#24120;&#26377;&#22810;&#20010;&#23618;&#27425;&#12290;&#20998;&#32452;&#30340;&#23618;&#27425;&#21487;&#20197;&#20132;&#21449;&#21644;&#21512;&#24182;&#65292;&#23601;&#20687;&#20851;&#31995;&#25968;&#25454;&#24211;&#34920;&#19968;&#26679;&#12290;&#38500;&#20102;&#34920;&#31034;&#25968;&#25454;&#30340;&#32467;&#26500;&#22806;&#65292;&#23618;&#27425;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#29305;&#24449;&#21487;&#20197;&#20998;&#37197;&#21040;&#36866;&#24403;&#30340;&#23618;&#27425;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#36866;&#21512;&#20351;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25512;&#29702;&#32467;&#26524;&#22312;&#32676;&#32452;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#23454;&#29616;&#20849;&#20139;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#23618;&#27425;&#36125;&#21494;&#26031;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#21830;&#24215;&#29305;&#35768;&#32463;&#33829;&#24215;&#27599;&#22825;&#30340;&#36141;&#20080;&#24773;&#20917;&#30340;&#24037;&#20316;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#22320;&#28857;&#21644;&#27599;&#21608;&#26576;&#20123;&#22825;&#30340;&#20998;&#32452;&#12290;&#25105;&#20204;&#20351;&#29992;\textsf{stan}&#36719;&#20214;&#21253;&#23637;&#31034;&#20102;&#22312;&#19968;&#24180;&#30340;&#26102;&#38388;&#20869;&#25910;&#38598;&#30340;&#20010;&#21035;&#38144;&#21806;&#20132;&#26131;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The features in many prediction models naturally take the form of a hierarchy. The lower levels represent individuals or events. These units group naturally into locations and intervals or other aggregates, often at multiple levels. Levels of groupings may intersect and join, much as relational database tables do. Besides representing the structure of the data, predictive features in hierarchical models can be assigned to their proper levels. Such models lend themselves to hierarchical Bayes solution methods that ``share'' results of inference between groups by generalizing over the case of individual models for each group versus one model that aggregates all groups into one.  In this paper we show our work-in-progress applying a hierarchical Bayesian model to forecast purchases throughout the day at store franchises, with groupings over locations and days of the week. We demonstrate using the \textsf{stan} package on individual sales transaction data collected over the course of a yea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#24179;&#34913;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#38544;&#31169;&#35774;&#32622;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.17794</link><description>&lt;p&gt;
&#36879;&#36807;&#38754;&#32433;&#30475;&#8220;&#35270;&#35273;&#8221;&#65306;&#24046;&#20998;&#38544;&#31169;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vision Through the Veil: Differential Privacy in Federated Learning for Medical Image Classification. (arXiv:2306.17794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#24179;&#34913;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#38544;&#31169;&#35774;&#32622;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#26222;&#21450;&#38656;&#35201;&#36328;&#22810;&#20010;&#26426;&#26500;&#36827;&#34892;&#25968;&#25454;&#32858;&#21512;&#65292;&#36825;&#24120;&#24120;&#28041;&#21450;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#21512;&#20316;&#27169;&#22411;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#20132;&#25442;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#22266;&#26377;&#28431;&#27934;&#38656;&#35201;&#26356;&#22810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#24046;&#20998;&#38544;&#31169;&#65292;&#19968;&#31181;&#39046;&#20808;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#25972;&#21512;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#38656;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#35814;&#32454;&#30740;&#31350;&#20102;&#20854;&#23545;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#38544;&#31169;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#20102;&#36890;&#36807;&#21512;&#36866;&#30340;&#38544;&#31169;&#35774;&#32622;&#65292;&#20173;&#28982;&#21487;&#20197;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#23454;&#29616;&#36739;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of deep learning applications in healthcare calls for data aggregation across various institutions, a practice often associated with significant privacy concerns. This concern intensifies in medical image analysis, where privacy-preserving mechanisms are paramount due to the data being sensitive in nature. Federated learning, which enables cooperative model training without direct data exchange, presents a promising solution. Nevertheless, the inherent vulnerabilities of federated learning necessitate further privacy safeguards. This study addresses this need by integrating differential privacy, a leading privacy-preserving technique, into a federated learning framework for medical image classification. We introduce a novel differentially private federated learning model and meticulously examine its impacts on privacy preservation and model performance. Our research confirms the existence of a trade-off between model accuracy and privacy settings. However, we demonstr
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#65288;&#30475;&#12289;&#35760;&#20303;&#12289;&#25512;&#29702;&#65289;&#36880;&#27493;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17778</link><description>&lt;p&gt;
&#30475;&#30475;&#12289;&#35760;&#20303;&#21644;&#25512;&#29702;&#65306;&#22522;&#20110;&#26426;&#29702;&#30340;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Look, Remember and Reason: Visual Reasoning with Grounded Rationales. (arXiv:2306.17778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17778
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#65288;&#30475;&#12289;&#35760;&#20303;&#12289;&#25512;&#29702;&#65289;&#36880;&#27493;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36827;&#34892;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#35814;&#32454;&#30740;&#31350;&#12290;&#22312;&#35768;&#22810;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38656;&#35201;&#23558;&#35270;&#35273;&#20449;&#24687;&#32039;&#23494;&#34701;&#21512;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36825;&#20010;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#31181;&#20302;&#32423;&#35270;&#35273;&#33021;&#21147;&#12290;&#23427;&#36890;&#24120;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#8220;&#30475;&#65292;&#35760;&#20303;&#65292;&#25512;&#29702;&#8221;&#30340;&#19977;&#20010;&#27493;&#39588;&#36807;&#31243;&#65306;&#36890;&#36807;&#36880;&#27493;&#36827;&#34892;&#20302;&#32423;&#35270;&#35273;&#36807;&#31243;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#65292;&#30452;&#21040;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#25105;&#20204;&#36981;&#24490;&#30456;&#21516;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#26550;&#26500;&#26356;&#25913;&#65292;&#20351;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#30340;&#21407;&#29702;&#65292;&#20801;&#35768;&#25105;&#20204;&#38598;&#25104;&#20302;&#32423;&#35270;&#35273;&#33021;&#21147;&#65292;&#22914;&#23545;&#35937;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have recently shown human level performance on a variety of reasoning tasks. However, the ability of these models to perform complex visual reasoning has not been studied in detail yet. A key challenge in many visual reasoning tasks is that the visual information needs to be tightly integrated in the reasoning process. We propose to address this challenge by drawing inspiration from human visual problem solving which depends on a variety of low-level visual capabilities. It can often be cast as the three step-process of ``Look, Remember, Reason'': visual information is incrementally extracted using low-level visual routines in a step-by-step fashion until a final answer is reached. We follow the same paradigm to enable existing large language models, with minimal changes to the architecture, to solve visual reasoning problems. To this end, we introduce rationales over the visual input that allow us to integrate low-level visual capabilities, such as object recogni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDS&#30340;&#25197;&#36716;&#24335;&#25193;&#25955;&#37319;&#26679;&#22120;&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25197;&#36716;&#25216;&#26415;&#32467;&#21512;&#21551;&#21457;&#24335;&#36817;&#20284;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22312;&#24191;&#27867;&#30340;&#26465;&#20214;&#20998;&#24067;&#19978;&#25552;&#20379;&#31934;&#30830;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.17775</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#23454;&#29992;&#21644;&#28176;&#36827;&#31934;&#30830;&#26465;&#20214;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Practical and Asymptotically Exact Conditional Sampling in Diffusion Models. (arXiv:2306.17775v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDS&#30340;&#25197;&#36716;&#24335;&#25193;&#25955;&#37319;&#26679;&#22120;&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25197;&#36716;&#25216;&#26415;&#32467;&#21512;&#21551;&#21457;&#24335;&#36817;&#20284;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22312;&#24191;&#27867;&#30340;&#26465;&#20214;&#20998;&#24067;&#19978;&#25552;&#20379;&#31934;&#30830;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20998;&#23376;&#35774;&#35745;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#23601;&#20027;&#35201;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#35757;&#32451;&#25110;&#23481;&#26131;&#20986;&#38169;&#30340;&#21551;&#21457;&#24335;&#36817;&#20284;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#24212;&#35813;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20026;&#24191;&#27867;&#30340;&#26465;&#20214;&#20998;&#24067;&#25552;&#20379;&#31934;&#30830;&#30340;&#26679;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25197;&#36716;&#24335;&#25193;&#25955;&#37319;&#26679;&#22120;(TDS)&#12290;TDS&#26159;&#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;(SMC)&#31639;&#27861;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#25197;&#36716;&#65292;&#19968;&#31181;&#20855;&#26377;&#33391;&#22909;&#35745;&#31639;&#25928;&#29575;&#30340;SMC&#25216;&#26415;&#65292;&#26469;&#32467;&#21512;&#21551;&#21457;&#24335;&#36817;&#20284;&#32780;&#19981;&#24433;&#21709;&#28176;&#36827;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#27169;&#25311;&#23454;&#39564;&#21644;MNIST&#22270;&#20687;&#20462;&#22797;&#20197;&#21450;&#31867;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#21457;&#29616;&#65292;TDS&#25552;&#20379;&#20102;&#35745;&#31639;&#32479;&#35745;&#26435;&#34913;&#65292;&#20351;&#29992;&#26356;&#22810;&#31890;&#23376;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#36817;&#20284;&#32467;&#26524;&#65292;&#20294;&#21516;&#26102;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring task-specific training. To this end, we introduce the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models. The main idea is to use twisting, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and on MNIST image inpainting and class-conditional generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#25490;&#24207;&#26041;&#27861;&#65292;&#20934;&#30830;&#36873;&#25321;&#21644;&#20248;&#20808;&#25490;&#24207;&#25935;&#24863;&#33647;&#29289;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#25239;&#30284;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2306.17771</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#25490;&#24207;&#23454;&#29616;&#31934;&#30830;&#30340;&#25239;&#30284;&#33647;&#29289;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Precision Anti-Cancer Drug Selection via Neural Ranking. (arXiv:2306.17771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17771
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#25490;&#24207;&#26041;&#27861;&#65292;&#20934;&#30830;&#36873;&#25321;&#21644;&#20248;&#20808;&#25490;&#24207;&#25935;&#24863;&#33647;&#29289;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#25239;&#30284;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30284;&#30151;&#27835;&#30103;&#38656;&#35201;&#23545;&#33647;&#29289;&#19982;&#30284;&#32454;&#32990;&#31995;&#22312;&#19981;&#21516;&#30340;&#36951;&#20256;&#21644;&#20998;&#23376;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#39640;&#36890;&#37327;&#31579;&#36873;&#24050;&#34987;&#29992;&#26469;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#33647;&#29289;&#21453;&#24212;&#25968;&#25454;&#65292;&#20419;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#23436;&#20840;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#25429;&#25417;&#21040;&#19981;&#21516;&#29615;&#22659;&#19979;&#22797;&#26434;&#30340;&#33647;&#29289;-&#32454;&#32990;&#31995;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#22320;&#20026;&#27599;&#20010;&#32454;&#32990;&#31995;&#20248;&#20808;&#36873;&#25321;&#26368;&#25935;&#24863;&#30340;&#33647;&#29289;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31070;&#32463;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#30340;&#22810;&#20010;&#32454;&#32990;&#31995;&#30340;&#22823;&#35268;&#27169;&#33647;&#29289;&#21453;&#24212;&#25968;&#25454;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22238;&#24402;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#33647;&#29289;&#36873;&#25321;&#21644;&#20248;&#20808;&#32423;&#30830;&#23450;&#30340;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#33647;&#29289;&#25490;&#24207;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#25490;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#28508;&#22312;&#30340;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized cancer treatment requires a thorough understanding of complex interactions between drugs and cancer cell lines in varying genetic and molecular contexts. To address this, high-throughput screening has been used to generate large-scale drug response data, facilitating data-driven computational models. Such models can capture complex drug-cell line interactions across various contexts in a fully data-driven manner. However, accurately prioritizing the most sensitive drugs for each cell line still remains a significant challenge. To address this, we developed neural ranking approaches that leverage large-scale drug response data across multiple cell lines from diverse cancer types. Unlike existing approaches that primarily utilize regression and classification techniques for drug response prediction, we formulated the objective of drug selection and prioritization as a drug ranking problem. In this work, we proposed two neural listwise ranking methods that learn latent repres
&lt;/p&gt;</description></item><item><title>&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#27604;&#20363;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;Softmax-based&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;Transformer&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#26497;&#38480;&#20998;&#24067;&#21487;&#20197;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#12290;&#36890;&#36807;&#20462;&#25913;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17759</link><description>&lt;p&gt;
&#21463;&#24418;&#29366;&#25913;&#21464;&#30340;Transformer&#65306;&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#26497;&#38480;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit. (arXiv:2306.17759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17759
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#27604;&#20363;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;Softmax-based&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;Transformer&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#26497;&#38480;&#20998;&#24067;&#21487;&#20197;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#12290;&#36890;&#36807;&#20462;&#25913;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#29992;&#20316;&#26816;&#26597;&#32593;&#32476;&#21487;&#35757;&#32451;&#24615;&#30340;&#20195;&#29702;&#12290;&#21463;Transformer&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#27604;&#20363;&#26497;&#38480;&#19979;&#65292;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#20462;&#25913;Softmax-based&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#26497;&#38480;&#20998;&#24067;&#21487;&#20197;&#29992;&#28145;&#24230;&#19982;&#23485;&#24230;&#27604;&#29575;&#20026;&#32034;&#24341;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#25551;&#36848;&#12290;&#20026;&#20102;&#23454;&#29616;&#33391;&#23450;&#20041;&#30340;&#38543;&#26426;&#26497;&#38480;&#65292;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#23558;Softmax&#36755;&#20986;&#23621;&#20013;&#22312;&#21333;&#20301;&#30697;&#38453;&#19978;&#65292;&#24182;&#36890;&#36807;&#23485;&#24230;&#30456;&#20851;&#30340;&#28201;&#24230;&#21442;&#25968;&#23545;Softmax logits&#36827;&#34892;&#32553;&#25918;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#30456;&#24212;&#30340;SDE&#30740;&#31350;&#20102;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#20248;&#38597;&#22320;&#25511;&#21046;&#28418;&#31227;&#21644;&#25193;&#25955;&#30340;&#23610;&#24230;&#12290;&#31283;&#23450;SDE&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#21327;&#26041;&#24046;&#32467;&#26500;&#26159;&#33391; behaved &#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#22823;&#30340;&#28145;&#24230;&#21644;&#23485;&#24230;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and widt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20248;&#21270;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26102;&#24046;(TD)&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#32463;&#20856;&#21453;&#20363;&#20013;&#30830;&#23450;&#20102;&#24433;&#21709;&#31639;&#27861;&#25910;&#25947;&#25110;&#21457;&#25955;&#30340;&#20004;&#20010;&#21147;&#37327;&#65292;&#24182;&#22312;&#32447;&#24615;&#36924;&#36817;&#21644;&#24179;&#26041;&#25439;&#22833;&#20197;&#22806;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;TD&#30340;&#25910;&#25947;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;TD&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25104;&#21151;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.17750</link><description>&lt;p&gt;
TD&#25910;&#25947;&#24615;&#65306;&#19968;&#20010;&#20248;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
TD Convergence: An Optimization Perspective. (arXiv:2306.17750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20248;&#21270;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26102;&#24046;(TD)&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#32463;&#20856;&#21453;&#20363;&#20013;&#30830;&#23450;&#20102;&#24433;&#21709;&#31639;&#27861;&#25910;&#25947;&#25110;&#21457;&#25955;&#30340;&#20004;&#20010;&#21147;&#37327;&#65292;&#24182;&#22312;&#32447;&#24615;&#36924;&#36817;&#21644;&#24179;&#26041;&#25439;&#22833;&#20197;&#22806;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;TD&#30340;&#25910;&#25947;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;TD&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25104;&#21151;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33879;&#21517;&#30340;&#26102;&#24046;(TD)&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#36890;&#36807;&#20248;&#21270;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#31639;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#35770;&#35777;&#20102;TD&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#27599;&#27425;&#36845;&#20195;&#26102;&#35201;&#26368;&#23567;&#21270;&#30340;&#20989;&#25968;&#37117;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;TD&#22312;&#32463;&#20856;&#21453;&#20363;&#20013;&#30340;&#21457;&#25955;&#34892;&#20026;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20915;&#23450;&#31639;&#27861;&#25910;&#25947;&#25110;&#21457;&#25955;&#34892;&#20026;&#30340;&#20004;&#20010;&#21147;&#37327;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#19968;&#20248;&#21270;&#35270;&#35282;&#25512;&#24191;&#21040;&#20102;&#27604;&#32447;&#24615;&#36924;&#36817;&#21644;&#24179;&#26041;&#25439;&#22833;&#26356;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#65292;&#35777;&#26126;&#20102;TD&#30340;&#25910;&#25947;&#24615;&#21462;&#20915;&#20110;&#36825;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;TD&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24050;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20173;&#26080;&#27861;&#20987;&#36133;&#20256;&#32479;&#30340;&#38750;&#28145;&#24230;&#27169;&#22411;&#12290;&#20851;&#38190;&#21407;&#22240;&#22312;&#20110;&#20998;&#23376;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#27169;&#24335;&#65292;&#32780;&#20351;&#29992;&#20998;&#23376;&#25351;&#32441;&#20316;&#20026;&#36755;&#20837;&#30340;&#26641;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.17702</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#28145;&#24230;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#19978;&#32463;&#24120;&#26080;&#27861;&#25171;&#36133;&#38750;&#28145;&#24230;&#23545;&#25163;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Deep Models Often cannot Beat Non-deep Counterparts on Molecular Property Prediction?. (arXiv:2306.17702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24050;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20173;&#26080;&#27861;&#20987;&#36133;&#20256;&#32479;&#30340;&#38750;&#28145;&#24230;&#27169;&#22411;&#12290;&#20851;&#38190;&#21407;&#22240;&#22312;&#20110;&#20998;&#23376;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#27169;&#24335;&#65292;&#32780;&#20351;&#29992;&#20998;&#23376;&#25351;&#32441;&#20316;&#20026;&#36755;&#20837;&#30340;&#26641;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65288;MPP&#65289;&#26159;&#33647;&#29289;&#21457;&#29616;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26368;&#36817;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#32780;&#21463;&#21040;&#20102;&#30456;&#24403;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;MPP&#19978;&#65292;&#28145;&#24230;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#36229;&#36807;&#20256;&#32479;&#30340;&#38750;&#28145;&#24230;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;14&#31181;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#30340;12&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65288;3&#20010;&#38750;&#28145;&#24230;&#27169;&#22411;&#21644;9&#20010;&#28145;&#24230;&#27169;&#22411;&#65289;&#12290;&#36890;&#36807;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;: \textbf{&#65288;&#32599;&#39532;&#25968;&#23383; 1&#65289;} &#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#36229;&#36234;&#38750;&#28145;&#24230;&#27169;&#22411;; \textbf{&#65288;&#32599;&#39532;&#25968;&#23383; 2&#65289;} &#28145;&#24230;&#27169;&#22411;&#22312;MPP&#19978;&#30340;&#22833;&#36133;&#19981;&#33021;&#20165;&#24402;&#22240;&#20110;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#23567;&#12290;&#37325;&#35201;&#30340;&#26159;&#20998;&#23376;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#27169;&#24335;; \textbf{&#65288;&#32599;&#39532;&#25968;&#23383; 3&#65289;} &#29305;&#21035;&#26159;&#20351;&#29992;&#20998;&#23376;&#25351;&#32441;&#20316;&#20026;&#36755;&#20837;&#30340;&#26641;&#27169;&#22411;&#24448;&#24448;&#27604;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20998;&#23376;&#25968;&#25454;&#30340;&#29420;&#29305;&#27169;&#24335;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction (MPP) is a crucial task in the drug discovery pipeline, which has recently gained considerable attention thanks to advances in deep neural networks. However, recent research has revealed that deep models struggle to beat traditional non-deep ones on MPP. In this study, we benchmark 12 representative models (3 non-deep models and 9 deep models) on 14 molecule datasets. Through the most comprehensive study to date, we make the following key observations: \textbf{(\romannumeral 1)} Deep models are generally unable to outperform non-deep ones; \textbf{(\romannumeral 2)} The failure of deep models on MPP cannot be solely attributed to the small size of molecular datasets. What matters is the irregular molecule data pattern; \textbf{(\romannumeral 3)} In particular, tree models using molecular fingerprints as inputs tend to perform better than other competitors. Furthermore, we conduct extensive empirical investigations into the unique patterns of molecule data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36229;&#36234;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#24615;&#21035;&#20445;&#25252;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#27979;&#35797;&#22522;&#20110;&#35821;&#38899;&#29305;&#24449;&#30340;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#19982;&#20154;&#31867;&#25191;&#34892;&#30340;&#22768;&#38899;&#36866;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.17700</link><description>&lt;p&gt;
&#36229;&#36234;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20445;&#25252;&#28436;&#35762;&#32773;&#24615;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond Neural-on-Neural Approaches to Speaker Gender Protection. (arXiv:2306.17700v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36229;&#36234;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#24615;&#21035;&#20445;&#25252;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#27979;&#35797;&#22522;&#20110;&#35821;&#38899;&#29305;&#24449;&#30340;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#19982;&#20154;&#31867;&#25191;&#34892;&#30340;&#22768;&#38899;&#36866;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#20462;&#25913;&#35821;&#38899;&#20197;&#38450;&#27490;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20445;&#25252;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#25511;&#21046;&#20851;&#20110;&#28436;&#35762;&#32773;&#24615;&#21035;&#36825;&#20010;&#38544;&#31169;&#25935;&#24863;&#23646;&#24615;&#30340;&#20449;&#24687;&#30340;&#21487;&#29992;&#24615;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21644;&#27979;&#35797;&#24615;&#21035;&#20445;&#25252;&#31639;&#27861;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159; "&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;"&#65292;&#21363;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21644;&#27979;&#35797;&#25200;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36229;&#36234;&#36825;&#31181;&#20570;&#27861;&#20197;&#21152;&#24378;&#23545;&#24615;&#21035;&#20445;&#25252;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27979;&#35797;&#22522;&#20110;&#35821;&#38899;&#31185;&#23398;&#23478;&#21382;&#21490;&#19978;&#24320;&#21457;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#24615;&#21035;&#25512;&#27979;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#36824;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#20998;&#31867;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#20351;&#29992;&#35821;&#38899;&#29305;&#24449;&#26469;&#27934;&#23519;&#20445;&#25252;&#24615;&#20462;&#25913;&#22914;&#20309;&#25913;&#21464;&#35821;&#38899;&#20449;&#21495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#24615;&#21035;&#20445;&#25252;&#31639;&#27861;&#24212;&#35813;&#19982;&#26032;&#22411;&#30340; "&#35821;&#38899;&#23545;&#25163;"&#65292;&#21363;&#20154;&#31867;&#25191;&#34892;&#30340;&#22768;&#38899;&#36866;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has proposed approaches that modify speech to defend against gender inference attacks. The goal of these protection algorithms is to control the availability of information about a speaker's gender, a privacy-sensitive attribute. Currently, the common practice for developing and testing gender protection algorithms is "neural-on-neural", i.e., perturbations are generated and tested with a neural network. In this paper, we propose to go beyond this practice to strengthen the study of gender protection. First, we demonstrate the importance of testing gender inference attacks that are based on speech features historically developed by speech scientists, alongside the conventionally used neural classifiers. Next, we argue that researchers should use speech features to gain insight into how protective modifications change the speech signal. Finally, we point out that gender-protection algorithms should be compared with novel "vocal adversaries", human-executed voice adaptati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#35270;&#20026;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#25216;&#26415;&#20013;&#30340;Thompson&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#25913;&#36827;&#25506;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17693</link><description>&lt;p&gt;
GFlowNets&#20013;&#30340;Thompson&#25277;&#26679;&#29992;&#20110;&#25913;&#36827;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Thompson sampling for improved exploration in GFlowNets. (arXiv:2306.17693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#35270;&#20026;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#25216;&#26415;&#20013;&#30340;Thompson&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#25913;&#36827;&#25506;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#23545;&#35937;&#20998;&#24067;&#37319;&#26679;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#21160;&#20316;&#31574;&#30053;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;&#20248;&#21270;&#21464;&#20998;&#30028;&#38480;&#30340;&#20998;&#23618;&#37319;&#26679;&#31639;&#27861;&#19981;&#21516;&#65292;GFlowNet&#31639;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#36827;&#34892;&#31163;&#31574;&#30053;&#36816;&#34892;&#65292;&#36825;&#22312;&#21457;&#29616;&#30446;&#26631;&#20998;&#24067;&#30340;&#27169;&#24335;&#26102;&#26377;&#20248;&#21183;&#12290;&#23613;&#31649;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#36873;&#25321;&#19978;&#23384;&#22312;&#28789;&#27963;&#24615;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#31995;&#32479;&#22320;&#25506;&#32034;&#26377;&#25928;&#36873;&#25321;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26368;&#20339;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#36712;&#36857;&#30340;&#36873;&#25321;&#35270;&#20026;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21463;&#22810;&#33218;&#32769;&#34382;&#26426;&#26041;&#27861;&#21551;&#21457;&#30340;&#36125;&#21494;&#26031;&#25216;&#26415;&#26469;&#22788;&#29702;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;Thompson&#37319;&#26679;GFlowNets&#65288;TS-GFN&#65289;&#36890;&#36807;&#32500;&#25252;&#31574;&#30053;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#20174;&#35813;&#21518;&#39564;&#20013;&#37319;&#26679;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#35777;&#26126;TS-GFN&#21487;&#20197;&#25913;&#36827;&#25506;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative flow networks (GFlowNets) are amortized variational inference algorithms that treat sampling from a distribution over compositional objects as a sequential decision-making problem with a learnable action policy. Unlike other algorithms for hierarchical sampling that optimize a variational bound, GFlowNet algorithms can stably run off-policy, which can be advantageous for discovering modes of the target distribution. Despite this flexibility in the choice of behaviour policy, the optimal way of efficiently selecting trajectories for training has not yet been systematically explored. In this paper, we view the choice of trajectories for training as an active learning problem and approach it using Bayesian techniques inspired by methods for multi-armed bandits. The proposed algorithm, Thompson sampling GFlowNets (TS-GFN), maintains an approximate posterior distribution over policies and samples trajectories from this posterior for training. We show in two domains that TS-GFN yi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#26500;&#24314;&#27867;&#21270;&#26102;&#38388;&#25197;&#26354;&#31639;&#23376;&#65292;&#20197;&#23454;&#29616;&#36830;&#32493;&#30340;&#26102;&#38388;&#25197;&#26354;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25197;&#26354;&#36335;&#24452;&#12289;&#23383;&#20856;&#21644;&#31232;&#30095;&#31995;&#25968;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17690</link><description>&lt;p&gt;
&#27867;&#21270;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;&#23383;&#20856;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generalized Time Warping Invariant Dictionary Learning for Time Series Classification and Clustering. (arXiv:2306.17690v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#26500;&#24314;&#27867;&#21270;&#26102;&#38388;&#25197;&#26354;&#31639;&#23376;&#65292;&#20197;&#23454;&#29616;&#36830;&#32493;&#30340;&#26102;&#38388;&#25197;&#26354;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25197;&#26354;&#36335;&#24452;&#12289;&#23383;&#20856;&#21644;&#31232;&#30095;&#31995;&#25968;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#21508;&#31181;&#23383;&#20856;&#23398;&#20064;&#25216;&#26415;&#20013;&#65292;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#24310;&#36831;&#12289;&#32553;&#25918;&#12289;&#36716;&#25442;&#21644;&#20854;&#20182;&#21508;&#31181;&#26102;&#38388;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;DTW&#22312;&#23545;&#40784;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26102;&#26159;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#22240;&#27492;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#25110;&#20449;&#24687;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#27867;&#21270;&#26102;&#38388;&#25197;&#26354;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#30001;&#36830;&#32493;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26500;&#25104;&#65292;&#20197;&#20415;&#23454;&#29616;&#36830;&#32493;&#30340;&#26102;&#38388;&#25197;&#26354;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#31639;&#23376;&#19982;&#23383;&#20856;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23558;&#20854;&#24314;&#27169;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22359;&#22352;&#26631;&#19979;&#38477;&#27861;&#26469;&#32852;&#21512;&#20248;&#21270;&#25197;&#26354;&#36335;&#24452;&#12289;&#23383;&#20856;&#21644;&#31232;&#30095;&#31995;&#25968;&#12290;&#20248;&#21270;&#32467;&#26524;&#34987;&#29992;&#20316;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#25552;&#21462;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dictionary learning is an effective tool for pattern recognition and classification of time series data. Among various dictionary learning techniques, the dynamic time warping (DTW) is commonly used for dealing with temporal delays, scaling, transformation, and many other kinds of temporal misalignments issues. However, the DTW suffers overfitting or information loss due to its discrete nature in aligning time series data. To address this issue, we propose a generalized time warping invariant dictionary learning algorithm in this paper. Our approach features a generalized time warping operator, which consists of linear combinations of continuous basis functions for facilitating continuous temporal warping. The integration of the proposed operator and the dictionary learning is formulated as an optimization problem, where the block coordinate descent method is employed to jointly optimize warping paths, dictionaries, and sparseness coefficients. The optimized results are then used as hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17670</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#36317;&#30340;&#33192;&#32960;&#21367;&#31215;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings. (arXiv:2306.17670v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#26159;&#26500;&#24314;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22914;&#35821;&#38899;&#35782;&#21035;&#31561;&#26102;&#38388;&#20219;&#21153;&#12290;&#22312;SNNs&#20013;&#65292;&#24310;&#36831;&#25351;&#30340;&#26159;&#20174;&#19968;&#20010;&#31070;&#32463;&#20803;&#21040;&#21478;&#19968;&#20010;&#31070;&#32463;&#20803;&#20256;&#25773;&#38656;&#35201;&#30340;&#26102;&#38388;&#12290;&#36825;&#20123;&#24310;&#36831;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24433;&#21709;&#33033;&#20914;&#21040;&#36798;&#26102;&#38388;&#65292;&#24050;&#30693;&#23574;&#23792;&#31070;&#32463;&#20803;&#23545;&#20110;&#37325;&#21472;&#30340;&#36755;&#20837;&#33033;&#20914;&#26377;&#26356;&#24378;&#30340;&#21709;&#24212;&#12290;&#26356;&#27491;&#24335;&#22320;&#35828;&#65292;&#29702;&#35770;&#19978;&#24050;&#32463;&#35777;&#26126;&#21487;&#22609;&#24615;&#24310;&#36831;&#26497;&#22823;&#22686;&#21152;&#20102;SNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#24310;&#36831;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22312;&#28145;&#24230;&#21069;&#39304;SNNs&#20013;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#27169;&#25311;&#36830;&#32493;&#23618;&#20043;&#38388;&#30340;&#24310;&#36831;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#27839;&#26102;&#38388;&#36724;&#30340;&#19968;&#32500;&#21367;&#31215;&#12290;&#21367;&#31215;&#26680;&#20165;&#21253;&#21547;&#23569;&#25968;&#38750;&#38646;&#26435;&#37325; - &#27599;&#20010;&#31361;&#35302;&#19968;&#20010; - &#23427;&#20204;&#30340;&#20301;&#32622;&#23545;&#24212;&#20110;&#24310;&#36831;&#12290;&#36825;&#20123;&#20301;&#32622;&#19982;&#26435;&#37325;&#19968;&#36215;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights - one per synapse - whose positions correspond to the delays. These positions are learned together with the wei
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#39044;&#26465;&#20214;&#22120;&#25913;&#36827;&#20102;L-BFGS&#20248;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#21152;&#24615;&#39044;&#26465;&#20214;&#22120;&#36824;&#20855;&#26377;&#24182;&#34892;&#24615;&#65292;&#20026;&#27169;&#22411;&#24182;&#34892;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.17648</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#31574;&#30053;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. (arXiv:2306.17648v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#39044;&#26465;&#20214;&#22120;&#25913;&#36827;&#20102;L-BFGS&#20248;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#21152;&#24615;&#39044;&#26465;&#20214;&#22120;&#36824;&#20855;&#26377;&#24182;&#34892;&#24615;&#65292;&#20026;&#27169;&#22411;&#24182;&#34892;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#32447;&#24615;&#21152;&#24615;&#21644;&#20056;&#24615;&#39044;&#26465;&#20214;&#31574;&#30053;&#65292;&#29992;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;L-BFGS&#20248;&#21270;&#22120;&#12290;&#38750;&#32447;&#24615;&#39044;&#26465;&#20214;&#22120;&#26159;&#36890;&#36807;&#21033;&#29992;Schwarz&#22495;&#20998;&#35299;&#26694;&#26550;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#32593;&#32476;&#30340;&#21442;&#25968;&#20197;&#36880;&#23618;&#26041;&#24335;&#36827;&#34892;&#20998;&#35299;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21152;&#24615;&#21644;&#20056;&#24615;&#39044;&#26465;&#20214;&#22120;&#37117;&#33021;&#26174;&#33879;&#25913;&#21892;&#26631;&#20934;L-BFGS&#20248;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#27492;&#22806;&#65292;&#21152;&#24615;&#39044;&#26465;&#20214;&#22120;&#26412;&#36136;&#19978;&#26159;&#24182;&#34892;&#30340;&#65292;&#22240;&#27492;&#20026;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to enhance the training of physics-informed neural networks (PINNs). To this aim, we introduce nonlinear additive and multiplicative preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear preconditioners are constructed by utilizing the Schwarz domain-decomposition framework, where the parameters of the network are decomposed in a layer-wise manner. Through a series of numerical experiments, we demonstrate that both, additive and multiplicative preconditioners significantly improve the convergence of the standard L-BFGS optimizer, while providing more accurate solutions of the underlying partial differential equations. Moreover, the additive preconditioner is inherently parallel, thus giving rise to a novel approach to model parallelism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;YOLOv5&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#21644;Federated Averaging&#20316;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#25972;&#20307;&#23458;&#25143;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#30456;&#23545;&#20110;&#20351;&#29992;&#26412;&#22320;&#23458;&#25143;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#12290;</title><link>http://arxiv.org/abs/2306.17645</link><description>&lt;p&gt;
&#20849;&#20139;&#29983;&#20135;&#20013;&#30340;&#32852;&#37030;&#30446;&#26631;&#26816;&#27979;&#29992;&#20110;&#36136;&#37327;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Federated Object Detection for Quality Inspection in Shared Production. (arXiv:2306.17645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;YOLOv5&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#21644;Federated Averaging&#20316;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#25972;&#20307;&#23458;&#25143;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#30456;&#23545;&#20110;&#20351;&#29992;&#26412;&#22320;&#23458;&#25143;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22312;&#20998;&#25955;&#25968;&#25454;&#30340;&#26465;&#20214;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;YOLOv5&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#21644;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#20316;&#20026;FL&#31639;&#27861;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21046;&#36896;&#19994;&#20013;&#30340;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#65292;&#22810;&#20010;&#24037;&#21378;/&#23458;&#25143;&#20849;&#20139;&#25968;&#25454;&#20197;&#35757;&#32451;&#20840;&#23616;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#38750;IID&#25968;&#25454;&#38598;&#19978;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#25972;&#20307;&#23458;&#25143;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#36793;&#30028;&#26694;&#30456;&#23545;&#20110;&#20351;&#29992;&#26412;&#22320;&#23458;&#25143;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21046;&#36896;&#19994;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#21033;&#29992;YOLOv5&#21644;FedAvg&#36827;&#34892;&#32852;&#37030;&#30446;&#26631;&#26816;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a promising approach for training machine learning models on decentralized data without compromising data privacy. In this paper, we propose a FL algorithm for object detection in quality inspection tasks using YOLOv5 as the object detection algorithm and Federated Averaging (FedAvg) as the FL algorithm. We apply this approach to a manufacturing use-case where multiple factories/clients contribute data for training a global object detection model while preserving data privacy on a non-IID dataset. Our experiments demonstrate that our FL approach achieves better generalization performance on the overall clients' test dataset and generates improved bounding boxes around the objects compared to models trained using local clients' datasets. This work showcases the potential of FL for quality inspection tasks in the manufacturing industry and provides valuable insights into the performance and feasibility of utilizing YOLOv5 and FedAvg for federated ob
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20960;&#20309;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#24494;&#20998;&#20960;&#20309;&#30340;&#35282;&#24230;&#23545;&#35299;&#30721;&#22120;&#36827;&#34892;&#20248;&#21270;&#65292;&#36991;&#20813;&#20102;&#21487;&#35270;&#21270;&#32467;&#26524;&#30340;&#22833;&#30495;&#65292;&#20351;&#24471;&#25968;&#25454;&#32467;&#26500;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34987;&#25429;&#25417;&#21040;&#12290;</title><link>http://arxiv.org/abs/2306.17638</link><description>&lt;p&gt;
&#20960;&#20309;&#33258;&#32534;&#30721;&#22120; - &#20320;&#25152;&#35265;&#21363;&#20320;&#25152;&#35299;&#30721;&#30340;
&lt;/p&gt;
&lt;p&gt;
Geometric Autoencoders -- What You See is What You Decode. (arXiv:2306.17638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17638
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20960;&#20309;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#24494;&#20998;&#20960;&#20309;&#30340;&#35282;&#24230;&#23545;&#35299;&#30721;&#22120;&#36827;&#34892;&#20248;&#21270;&#65292;&#36991;&#20813;&#20102;&#21487;&#35270;&#21270;&#32467;&#26524;&#30340;&#22833;&#30495;&#65292;&#20351;&#24471;&#25968;&#25454;&#32467;&#26500;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34987;&#25429;&#25417;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#21270;&#26159;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;&#22823;&#35268;&#27169;&#30340;&#32593;&#32476;&#28145;&#24230;&#21644;&#23485;&#24230;&#21487;&#20197;&#24110;&#21161;&#23637;&#31034;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#28508;&#22312;&#34920;&#31034;&#34987;&#25197;&#26354;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#32593;&#32476;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#20302;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#35823;&#23548;&#24615;&#30340;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35299;&#30721;&#22120;&#30340;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#23884;&#20837;&#30340;&#22833;&#30495;&#30340;&#28145;&#20837;&#35786;&#26029;&#65292;&#20854;&#27425;&#26159;&#19968;&#20010;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32531;&#35299;&#36825;&#31181;&#22833;&#30495;&#12290;&#25105;&#20204;&#30340;&#8220;&#20960;&#20309;&#33258;&#32534;&#30721;&#22120;&#8221;&#36991;&#20813;&#20102;&#35823;&#36896;&#25104;&#23884;&#20837;&#30340;&#25289;&#20280;&#65292;&#20197;&#20415;&#21487;&#35270;&#21270;&#26356;&#20934;&#30830;&#22320;&#34920;&#36798;&#25968;&#25454;&#32467;&#26500;&#12290;&#23427;&#36824;&#26631;&#35760;&#20102;&#26080;&#27861;&#23454;&#29616;&#30340;&#23567;&#22833;&#30495;&#21306;&#22495;&#65292;&#20174;&#32780;&#38450;&#27490;&#20102;&#38169;&#35823;&#35299;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visualization is a crucial step in exploratory data analysis. One possible approach is to train an autoencoder with low-dimensional latent space. Large network depth and width can help unfolding the data. However, such expressive networks can achieve low reconstruction error even when the latent representation is distorted. To avoid such misleading visualizations, we propose first a differential geometric perspective on the decoder, leading to insightful diagnostics for an embedding's distortion, and second a new regularizer mitigating such distortion. Our ``Geometric Autoencoder'' avoids stretching the embedding spuriously, so that the visualization captures the data structure more faithfully. It also flags areas where little distortion could not be achieved, thus guarding against misinterpretation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#21644;&#28145;&#24230;&#22270;&#20687;&#65292;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;RGB-D&#30456;&#26426;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#19987;&#29992;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#36710;&#20869;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17636</link><description>&lt;p&gt;
&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#23454;&#29616;&#19982;RGB-D&#30456;&#26426;&#30456;&#24403;&#30340;&#20998;&#21106;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Achieving RGB-D level Segmentation Performance from a Single ToF Camera. (arXiv:2306.17636v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#21644;&#28145;&#24230;&#22270;&#20687;&#65292;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;RGB-D&#30456;&#26426;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#19987;&#29992;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#36710;&#20869;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#31181;&#27169;&#24577;&#65292;&#36890;&#24120;&#20316;&#20026;RGB&#30340;&#34917;&#20805;&#20449;&#24687;&#30001;RGB-D&#30456;&#26426;&#25552;&#20379;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#65288;IR&#65289;&#21644;&#28145;&#24230;&#22270;&#20687;&#21487;&#20197;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#33719;&#24471;&#19982;RGB-D&#30456;&#26426;&#30456;&#21516;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#34701;&#21512;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#21644;&#28145;&#24230;&#27169;&#24577;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#28145;&#24230;&#19987;&#29992;&#21367;&#31215;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#23545;&#36710;&#20869;&#20998;&#21106;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26356;&#26114;&#36149;&#30340;RGB-D&#26041;&#27861;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth is a very important modality in computer vision, typically used as complementary information to RGB, provided by RGB-D cameras. In this work, we show that it is possible to obtain the same level of accuracy as RGB-D cameras on a semantic segmentation task using infrared (IR) and depth images from a single Time-of-Flight (ToF) camera. In order to fuse the IR and depth modalities of the ToF camera, we introduce a method utilizing depth-specific convolutions in a multi-task learning framework. In our evaluation on an in-car segmentation dataset, we demonstrate the competitiveness of our method against the more costly RGB-D approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#22122;&#22768;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28608;&#27963;&#22122;&#22768;&#33021;&#26368;&#26377;&#25928;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#36755;&#20837;&#22686;&#24378;&#22122;&#22768;&#21017;&#33021;&#26174;&#33879;&#25913;&#21892;&#20998;&#24067;&#22806;&#30340;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.17630</link><description>&lt;p&gt;
&#22122;&#22768;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Noise on Calibration and Generalisation of Neural Networks. (arXiv:2306.17630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#22122;&#22768;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28608;&#27963;&#22122;&#22768;&#33021;&#26368;&#26377;&#25928;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#36755;&#20837;&#22686;&#24378;&#22122;&#22768;&#21017;&#33021;&#26174;&#33879;&#25913;&#21892;&#20998;&#24067;&#22806;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#27880;&#20837;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#23545;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26377;&#25928;&#12290;&#26576;&#20123;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#22914;&#26631;&#31614;&#24179;&#28369;&#21644;MixUp&#65292;&#20063;&#34987;&#35777;&#26126;&#33021;&#25913;&#21892;&#26657;&#20934;&#12290;&#30001;&#20110;&#22122;&#22768;&#21487;&#20197;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#30340;&#19981;&#21516;&#38454;&#27573;&#28155;&#21152;&#65292;&#36825;&#24341;&#21457;&#20102;&#22122;&#22768;&#22312;&#20309;&#26102;&#20309;&#22320;&#26368;&#26377;&#25928;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22122;&#22768;&#31867;&#22411;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#25913;&#36827;&#31243;&#24230;&#20197;&#21450;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#36215;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#21644;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22330;&#26223;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#27880;&#20837;&#31574;&#30053;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28608;&#27963;&#22122;&#22768;&#23545;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#26159;&#26368;&#20855;&#20256;&#36882;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#65292;&#32780;&#36755;&#20837;&#22686;&#24378;&#22122;&#22768;&#22312;&#25913;&#21892;&#20998;&#24067;&#22806;&#26657;&#20934;&#19978;&#24456;&#26174;&#33879;&#65292;&#20294;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#20998;&#24067;&#20869;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise injection and data augmentation strategies have been effective for enhancing the generalisation and robustness of neural networks (NNs). Certain types of noise such as label smoothing and MixUp have also been shown to improve calibration. Since noise can be added in various stages of the NN's training, it motivates the question of when and where the noise is the most effective. We study a variety of noise types to determine how much they improve calibration and generalisation, and under what conditions. More specifically we evaluate various noise-injection strategies in both in-distribution (ID) and out-of-distribution (OOD) scenarios. The findings highlight that activation noise was the most transferable and effective in improving generalisation, while input augmentation noise was prominent in improving calibration on OOD but not necessarily ID data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#23450;&#21046;&#30340;&#24863;&#24212;&#30005;&#21160;&#26426;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#30005;&#26426;&#35774;&#35745;&#23454;&#20363;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#20351;&#24471;&#30005;&#26426;&#35774;&#35745;&#33258;&#21160;&#21270;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#25805;&#20316;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.17626</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#24863;&#24212;&#30005;&#26426;
&lt;/p&gt;
&lt;p&gt;
Design of Induction Machines using Reinforcement Learning. (arXiv:2306.17626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#23450;&#21046;&#30340;&#24863;&#24212;&#30005;&#21160;&#26426;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#30005;&#26426;&#35774;&#35745;&#23454;&#20363;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#20351;&#24471;&#30005;&#26426;&#35774;&#35745;&#33258;&#21160;&#21270;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#25805;&#20316;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30005;&#30913;&#21644;&#28909;&#32422;&#26463;&#30340;&#19981;&#21516;&#65292;&#24863;&#24212;&#30005;&#26426;&#30340;&#35774;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#38144;&#21806;&#24037;&#20855;&#20013;&#24555;&#36895;&#20272;&#31639;&#26426;&#22120;&#30340;&#23610;&#23544;&#23545;&#20110;&#26681;&#25454;&#29305;&#23450;&#35201;&#27714;&#32473;&#23458;&#25143;&#25552;&#20379;&#24555;&#36895;&#25253;&#20215;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#36873;&#25321;&#19981;&#21516;&#30340;&#35774;&#35745;&#21442;&#25968;&#65292;&#22914;&#38271;&#24230;&#12289;&#30452;&#24452;&#12289;&#40831;&#23574;&#39640;&#24230;&#21644;&#32469;&#32452;&#21277;&#25968;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#30340;&#29305;&#23450;&#25197;&#30697;&#12289;&#30005;&#27969;&#21644;&#28201;&#24230;&#12290;&#30005;&#26426;&#35774;&#35745;&#24072;&#36890;&#36807;&#20182;&#20204;&#30340;&#32463;&#39564;&#30693;&#36947;&#22914;&#20309;&#25913;&#21464;&#19981;&#21516;&#30340;&#26426;&#22120;&#35774;&#35745;&#21442;&#25968;&#65292;&#20197;&#28385;&#36275;&#23458;&#25143;&#30340;&#29305;&#23450;&#25805;&#20316;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35774;&#35745;&#23450;&#21046;&#30340;&#24863;&#24212;&#30005;&#21160;&#26426;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#30005;&#26426;&#35774;&#35745;&#28216;&#25103;&#30340;&#19981;&#21516;&#23454;&#20363;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#24403;&#20570;&#20986;&#33391;&#22909;&#25110;&#19981;&#33391;&#35774;&#35745;&#36873;&#25321;&#26102;&#65292;&#20351;&#29992;&#22870;&#21169;&#25110;&#24809;&#32602;&#20989;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#21270;&#30005;&#26426;&#35774;&#35745;&#65292;&#32780;&#26080;&#38656;&#24212;&#29992;&#20219;&#20309;&#20154;&#20026;&#21046;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of induction machine is a challenging task due to different electromagnetic and thermal constraints. Quick estimation of machine's dimensions is important in the sales tool to provide quick quotations to customers based on specific requirements. The key part of this process is to select different design parameters like length, diameter, tooth tip height and winding turns to achieve certain torque, current and temperature of the machine. Electrical machine designers, with their experience know how to alter different machine design parameters to achieve a customer specific operation requirements. We propose a reinforcement learning algorithm to design a customised induction motor. The neural network model is trained off-line by simulating different instances of of electrical machine design game with a reward or penalty function when a good or bad design choice is made. The results demonstrate that the suggested method automates electrical machine design without applying any hu
&lt;/p&gt;</description></item><item><title>Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17624</link><description>&lt;p&gt;
Sphere2Vec&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#29699;&#38754;&#19978;&#36890;&#29992;&#20301;&#32622;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17624
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20026;&#31354;&#38388;&#20013;&#30340;&#28857;&#29983;&#25104;&#36866;&#21512;&#23398;&#20064;&#30340;&#34920;&#31034;&#26159;&#19968;&#20010;&#22522;&#26412;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#32534;&#30721;&#26041;&#26696;&#65288;&#22914;Space2Vec&#21644;NeRF&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#20108;&#32500;/&#19977;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#28857;&#32534;&#30721;&#20026;&#39640;&#32500;&#21521;&#37327;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25152;&#26377;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#20301;&#32622;&#32534;&#30721;&#22120;&#37117;&#26159;&#35774;&#35745;&#29992;&#26469;&#27169;&#25311;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#22312;&#24212;&#29992;&#20110;&#38656;&#35201;&#22312;&#29699;&#38754;&#19978;&#36827;&#34892;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#22320;&#22270;&#25237;&#24433;&#22833;&#30495;&#38382;&#39064;&#65288;2D&#65289;&#21644;&#29699;&#38754;&#21040;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#36817;&#20284;&#35823;&#24046;&#65288;3D&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sphere2Vec&#30340;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#29699;&#38754;&#19978;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;&#36317;&#31163;&#20445;&#25345;&#32534;&#30721;&#30340;&#32479;&#19968;&#35270;&#35282;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating learning-friendly representations for points in space is a fundamental and long-standing problem in ML. Recently, multi-scale encoding schemes (such as Space2Vec and NeRF) were proposed to directly encode any point in 2D/3D Euclidean space as a high-dimensional vector, and has been successfully applied to various geospatial prediction and generative tasks. However, all current 2D and 3D location encoders are designed to model point distances in Euclidean space. So when applied to large-scale real-world GPS coordinate datasets, which require distance metric learning on the spherical surface, both types of models can fail due to the map projection distortion problem (2D) and the spherical-to-Euclidean distance approximation error (3D). To solve these problems, we propose a multi-scale location encoder called Sphere2Vec which can preserve spherical distances when encoding point coordinates on a spherical surface. We developed a unified view of distance-reserving encoding on sph
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25511;&#21046;&#24494;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17598</link><description>&lt;p&gt;
&#24494;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#23548;&#33322;&#20197;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigation of micro-robot swarms for targeted delivery using reinforcement learning. (arXiv:2306.17598v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17598
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25511;&#21046;&#24494;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#26426;&#22120;&#20154;&#22312;&#26377;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24494;&#23567;&#30340;&#23610;&#23544;&#65292;&#21333;&#29420;&#25511;&#21046;&#27599;&#20010;&#26426;&#22120;&#20154;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21333;&#19968;&#25511;&#21046;&#22120;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#25104;&#21151;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Proximal Policy Optimization (PPO)&#21644;Robust Policy Optimization (RPO)&#26469;&#25511;&#21046;&#19968;&#32676;&#24494;&#22411;&#28216;&#27891;&#26426;&#22120;&#20154;&#65292;&#22312;&#21463;&#27700;&#21160;&#21147;&#23398;&#25928;&#24212;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23427;&#20204;&#30340;&#26041;&#21521;&#26397;&#21521;&#19968;&#20010;&#22278;&#24418;&#21560;&#25910;&#30446;&#26631;&#12290;&#25105;&#20204;&#32771;&#26597;&#20102;PPO&#21644;RPO&#22312;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#27979;&#35797;&#20102;&#23427;&#20204;&#22312;&#38543;&#26426;&#30446;&#26631;&#20301;&#32622;&#21644;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#23398;&#20064;&#25511;&#21046;&#19968;&#32676;25&#20010;&#28216;&#27891;&#26426;&#22120;&#20154;&#24182;&#23558;&#23427;&#20204;&#23548;&#33322;&#21040;&#19968;&#20010;&#23637;&#31034;&#30446;&#26631;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Micro robotics is quickly emerging to be a promising technological solution to many medical treatments with focus on targeted drug delivery. They are effective when working in swarms whose individual control is mostly infeasible owing to their minute size. Controlling a number of robots with a single controller is thus important and artificial intelligence can help us perform this task successfully. In this work, we use the Reinforcement Learning (RL) algorithms Proximal Policy Optimization (PPO) and Robust Policy Optimization (RPO) to navigate a swarm of 4, 9 and 16 microswimmers under hydrodynamic effects, controlled by their orientation, towards a circular absorbing target. We look at both PPO and RPO performances with limited state information scenarios and also test their robustness for random target location and size. We use curriculum learning to improve upon the performance and demonstrate the same in learning to navigate a swarm of 25 swimmers and steering the swarm to exempli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#23494;&#24230;&#27867;&#20989;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#30456;&#20114;&#20316;&#29992;&#21160;&#33021;&#27867;&#20989;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23494;&#24230;&#27867;&#20989;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#19968;&#32500;&#31995;&#32479;&#19978;&#30340;&#20248;&#31168;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17587</link><description>&lt;p&gt;
&#29992;&#21464;&#20998;&#21407;&#29702;&#26469;&#35268;&#33539;&#26426;&#22120;&#23398;&#20064;&#30340;&#23494;&#24230;&#27867;&#20989;&#65306;&#38750;&#30456;&#20114;&#20316;&#29992;&#21160;&#33021;&#27867;&#20989;
&lt;/p&gt;
&lt;p&gt;
Variational principle to regularize machine-learned density functionals: the non-interacting kinetic-energy functional. (arXiv:2306.17587v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#23494;&#24230;&#27867;&#20989;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#30456;&#20114;&#20316;&#29992;&#21160;&#33021;&#27867;&#20989;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23494;&#24230;&#27867;&#20989;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#19968;&#32500;&#31995;&#32479;&#19978;&#30340;&#20248;&#31168;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770; (DFT) &#30340;&#25104;&#21151;&#24402;&#21151;&#20110; Kohn &#21644; Sham &#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#20182;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#36741;&#21161;&#22343;&#22330;&#31995;&#32479;&#35745;&#31639;&#38750;&#30456;&#20114;&#20316;&#29992;&#21160;&#33021;&#30340;&#20934;&#30830;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;DFT &#30340;&#20840;&#37096;&#28508;&#21147;&#23558;&#26080;&#27861;&#37322;&#25918;&#65292;&#30452;&#21040;&#25214;&#21040;&#30005;&#23376;&#23494;&#24230;&#19982;&#38750;&#30456;&#20114;&#20316;&#29992;&#21160;&#33021;&#20043;&#38388;&#30340;&#20934;&#30830;&#20851;&#31995;&#12290;&#24050;&#32463;&#23581;&#35797;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#36825;&#20010;&#27867;&#20989;&#65292;&#31867;&#20284;&#20110;&#20132;&#25442;&#20851;&#32852;&#27867;&#20989;&#65292;&#20294;&#30001;&#20110;&#21160;&#33021;&#30340;&#36129;&#29486;&#26356;&#22823;&#19988;&#26356;&#38750;&#23616;&#22495;&#65292;&#25104;&#21151;&#31243;&#24230;&#36739;&#20302;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#35268;&#33539;&#26041;&#27861;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#22522;&#30784;&#26469;&#35757;&#32451;&#23494;&#24230;&#27867;&#20989;&#65292;&#23588;&#20854;&#26159;&#21160;&#33021;&#27867;&#20989;&#12290;&#35813;&#26041;&#27861;&#22312;&#65288;&#26377;&#25928;&#30340;&#65289;&#19968;&#32500;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#27682;&#38142;&#12289;&#38750;&#30456;&#20114;&#20316;&#29992;&#30005;&#23376;&#21644;&#21069;&#20004;&#20010;&#21608;&#26399;&#20803;&#32032;&#30340;&#21407;&#23376;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical density functional theory (DFT) owes its success to the groundbreaking work of Kohn and Sham that introduced the exact calculation of the non-interacting kinetic energy of the electrons using an auxiliary mean-field system. However, the full power of DFT will not be unleashed until the exact relationship between the electron density and the non-interacting kinetic energy is found. Various attempts have been made to approximate this functional, similar to the exchange--correlation functional, with much less success due to the larger contribution of kinetic energy and its more non-local nature. In this work we propose a new and efficient regularization method to train density functionals based on deep neural networks, with particular interest in the kinetic-energy functional. The method is tested on (effectively) one-dimensional systems, including the hydrogen chain, non-interacting electrons, and atoms of the first two periods, with excellent results. For the atomic systems, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17582</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65306;&#35774;&#35745;&#21407;&#21017;&#21644;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#25552;&#31034;&#24037;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#19982;&#39640;&#32423;&#20989;&#25968;&#24211;&#30340;&#21019;&#24314;&#30456;&#32467;&#21512;&#65292;&#20351;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12289;&#27169;&#25311;&#22120;&#21644;&#24418;&#24577;&#12290;&#25105;&#20204;&#37325;&#28857;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#21644;&#23545;&#35805;&#31574;&#30053;&#23545;&#25191;&#34892;&#21508;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#23545;&#35805;&#12289;&#35299;&#26512;XML&#26631;&#35760;&#21644;&#21512;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#20989;&#25968;&#21644;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#38381;&#29615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20174;&#22522;&#26412;&#30340;&#36923;&#36753;&#12289;&#20960;&#20309;&#21644;&#25968;&#23398;&#25512;&#29702;&#21040;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#31354;&#20013;&#23548;&#33322;&#12289;&#25805;&#32437;&#21644;&#20855;&#36523;&#20195;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#21487;&#20197;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#21516;&#26102;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#22823;&#23398;&#24405;&#21462;&#36807;&#31243;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#34920;&#29616;&#19979;&#38477;&#65292;&#32780;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#21487;&#20197;&#37096;&#20998;&#24674;&#22797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17575</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#22823;&#23398;&#24405;&#21462;&#20013;&#30340;&#25972;&#20307;&#35780;&#20272;&#65292;&#20197;&#20998;&#26512;&#35770;&#25991;&#21644;&#25512;&#33616;&#20449;
&lt;/p&gt;
&lt;p&gt;
Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters. (arXiv:2306.17575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17575
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#22823;&#23398;&#24405;&#21462;&#36807;&#31243;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#34920;&#29616;&#19979;&#38477;&#65292;&#32780;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#21487;&#20197;&#37096;&#20998;&#24674;&#22797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#24230;&#36873;&#25321;&#24615;&#30340;&#26426;&#26500;&#20013;&#65292;&#22823;&#23398;&#24405;&#21462;&#37319;&#29992;&#20840;&#38754;&#35780;&#20272;&#36807;&#31243;&#65292;&#32771;&#34385;&#30003;&#35831;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#21253;&#25324;&#38544;&#31169;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#12289;&#25104;&#32489;&#12289;&#35770;&#25991;&#21644;&#25512;&#33616;&#20449;&#65292;&#20197;&#32452;&#25104;&#19968;&#25903;&#20248;&#31168;&#21644;&#22810;&#26679;&#21270;&#30340;&#29677;&#32423;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23454;&#35777;&#35780;&#20272;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#39044;&#27979;&#24405;&#21462;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#25991;&#26412;&#20449;&#24687;&#65288;&#22914;&#20010;&#20154;&#35770;&#25991;&#12289;&#25945;&#24072;&#25512;&#33616;&#20449;&#65289;&#22312;&#27169;&#22411;&#20013;&#20195;&#26367;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;2022-2023&#23398;&#24180;&#22312;&#19968;&#25152;&#20855;&#26377;&#36873;&#25321;&#24615;&#30340;&#32654;&#22269;&#26412;&#31185;&#20837;&#23398;&#21150;&#20844;&#23460;&#30340;14,915&#21517;&#30003;&#35831;&#20154;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;ML&#27169;&#22411;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#24405;&#21462;&#34920;&#29616;&#12290;&#36890;&#36807;TF-IDF&#34920;&#31034;&#21644;&#38544;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#27169;&#22411;&#65292;&#25991;&#26412;&#20449;&#24687;&#30340;&#21253;&#21547;&#37096;&#20998;&#24674;&#22797;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
University admission at many highly selective institutions uses a holistic review process, where all aspects of the application, including protected attributes (e.g., race, gender), grades, essays, and recommendation letters are considered, to compose an excellent and diverse class. In this study, we empirically evaluate how influential protected attributes are for predicting admission decisions using a machine learning (ML) model, and in how far textual information (e.g., personal essay, teacher recommendation) may substitute for the loss of protected attributes in the model. Using data from 14,915 applicants to an undergraduate admission office at a selective U.S. institution in the 2022-2023 cycle, we find that the exclusion of protected attributes from the ML model leads to substantially reduced admission-prediction performance. The inclusion of textual information via both a TF-IDF representation and a Latent Dirichlet allocation (LDA) model partially restores model performance, b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#25991;&#26412;&#25490;&#24207;&#22120;&#65292;&#20855;&#26377;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#30452;&#25509;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#25991;&#26723;&#36755;&#20837;&#25552;&#31034;&#36827;&#34892;&#25991;&#26723;&#25490;&#24207;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#23454;&#29992;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24456;&#38590;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#31934;&#35843;&#22522;&#20934;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#30340;&#28857;&#23545;&#28857;&#21644;&#21015;&#34920;&#25490;&#24207;&#25552;&#31034;&#65292;&#24182;&#35748;&#20026;&#29616;&#25104;&#30340;LLM&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#36825;&#20123;&#25490;&#24207;&#20844;&#24335;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;LLM&#30340;&#35757;&#32451;&#26041;&#24335;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#65288;PRP&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;LLM&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;&#24320;&#28304;LLM&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;&#22312;TREC-DL2020&#19978;&#65292;&#22522;&#20110;20B&#21442;&#25968;&#30340;Flan-UL2&#27169;&#22411;&#30340;PRP&#36229;&#36807;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#21830;&#19994;&#40657;&#30418;GPT-4&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#38468;&#21152;&#25968;&#25454;&#28304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23646;&#20110;&#20808;&#21069;&#36935;&#21040;&#30340;&#22270;&#20687;&#25152;&#23646;&#31867;&#21035;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#22312;&#33976;&#39311;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#20013;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17560</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33976;&#39311;&#21644;&#37325;&#25773;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning using Diffusion Model for Distillation and Replay. (arXiv:2306.17560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#38468;&#21152;&#25968;&#25454;&#28304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23646;&#20110;&#20808;&#21069;&#36935;&#21040;&#30340;&#22270;&#20687;&#25152;&#23646;&#31867;&#21035;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#22312;&#33976;&#39311;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#20013;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#20197;&#22686;&#37327;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#22810;&#20010;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#37327;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#38468;&#21152;&#25968;&#25454;&#26469;&#24110;&#21161;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#38468;&#21152;&#25968;&#25454;&#28304;&#12290;&#19982;&#20381;&#36182;&#20110;&#22806;&#37096;&#12289;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23646;&#20110;&#20808;&#21069;&#36935;&#21040;&#30340;&#22270;&#20687;&#25152;&#23646;&#31867;&#21035;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#36825;&#20351;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#22312;&#33976;&#39311;&#25439;&#22833;&#20013;&#20351;&#29992;&#36825;&#20123;&#38468;&#21152;&#25968;&#25454;&#26679;&#26412;&#65292;&#36824;&#21487;&#20197;&#22312;&#20998;&#31867;&#25439;&#22833;&#20013;&#36827;&#34892;&#37325;&#25773;&#12290;&#22312;CIFAR100&#12289;ImageNet-Subset&#21644;ImageNet&#31561;&#31454;&#20105;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning aims to learn new classes in an incremental fashion without forgetting the previously learned ones. Several research works have shown how additional data can be used by incremental models to help mitigate catastrophic forgetting. In this work, following the recent breakthrough in text-to-image generative models and their wide distribution, we propose the use of a pretrained Stable Diffusion model as a source of additional data for class-incremental learning. Compared to competitive methods that rely on external, often unlabeled, datasets of real images, our approach can generate synthetic samples belonging to the same classes as the previously encountered images. This allows us to use those additional data samples not only in the distillation loss but also for replay in the classification loss. Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and ImageNet demonstrate how this new approach can be used to further improve the performance of s
&lt;/p&gt;</description></item><item><title>TTSWING&#26159;&#19968;&#31181;&#19987;&#20026;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#33719;&#21462;&#35814;&#32454;&#20449;&#24687;&#24182;&#19982;&#36816;&#21160;&#21592;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#12290;&#23545;&#20110;&#20050;&#20051;&#29699;&#20998;&#26512;&#30340;&#21019;&#26032;&#30740;&#31350;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23545;&#31185;&#23398;&#30028;&#26469;&#35828;&#26159;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.17550</link><description>&lt;p&gt;
TTSWING&#65306;&#19968;&#31181;&#29992;&#20110;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TTSWING: a Dataset for Table Tennis Swing Analysis. (arXiv:2306.17550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17550
&lt;/p&gt;
&lt;p&gt;
TTSWING&#26159;&#19968;&#31181;&#19987;&#20026;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#33719;&#21462;&#35814;&#32454;&#20449;&#24687;&#24182;&#19982;&#36816;&#21160;&#21592;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#12290;&#23545;&#20110;&#20050;&#20051;&#29699;&#20998;&#26512;&#30340;&#21019;&#26032;&#30740;&#31350;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23545;&#31185;&#23398;&#30028;&#26469;&#35828;&#26159;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TTSWING&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#38598;&#25104;&#21040;&#23450;&#21046;&#30340;&#29699;&#25293;&#25569;&#25226;&#19978;&#30340;9&#36724;&#20256;&#24863;&#22120;&#33719;&#21462;&#20102;&#35814;&#32454;&#30340;&#25381;&#25293;&#20449;&#24687;&#65292;&#24182;&#38468;&#24102;&#20102;&#36816;&#21160;&#21592;&#30340;&#21311;&#21517;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#27880;&#37322;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25381;&#25293;&#20998;&#26512;&#30740;&#31350;&#12290;TTSWING&#22312;&#20419;&#36827;&#20050;&#20051;&#29699;&#20998;&#26512;&#30340;&#21019;&#26032;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#23545;&#31185;&#23398;&#30028;&#26469;&#35828;&#26159;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;https://github.com/DEPhantom/TTSWING&#19978;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce TTSWING, a novel dataset designed for table tennis swing analysis. This dataset comprises comprehensive swing information obtained through 9-axis sensors integrated into custom-made racket grips, accompanied by anonymized demographic data of the players. We detail the data collection and annotation procedures. Furthermore, we conduct pilot studies utilizing diverse machine learning models for swing analysis. TTSWING holds tremendous potential to facilitate innovative research in table tennis analysis and is a valuable resource for the scientific community. We release the dataset and experimental codes at https://github.com/DEPhantom/TTSWING.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#22320;&#22270;&#26469;&#25913;&#36827;&#21160;&#24577;&#36710;&#36742;&#26816;&#27979;&#65292;&#26080;&#38656;&#20351;&#29992;3D&#22320;&#22270;&#25110;&#20687;&#32032;&#32423;&#22320;&#22270;&#26597;&#35810;&#23545;&#24212;&#12290;&#36890;&#36807;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#21644;&#20108;&#36827;&#21046;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25104;&#21151;&#20248;&#21270;&#20102;&#21021;&#22987;&#30340;&#20505;&#36873;&#29289;&#20307;&#26816;&#27979;&#65292;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17536</link><description>&lt;p&gt;
&#36890;&#36807;&#24694;&#21155;&#24773;&#20917;&#19979;&#30340;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#25913;&#36827;&#21160;&#24577;&#36710;&#36742;&#26816;&#27979;&#8212;&#8212;&#36801;&#31227;&#23545;&#35937;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
DisPlacing Objects: Improving Dynamic Vehicle Detection via Visual Place Recognition under Adverse Conditions. (arXiv:2306.17536v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#22320;&#22270;&#26469;&#25913;&#36827;&#21160;&#24577;&#36710;&#36742;&#26816;&#27979;&#65292;&#26080;&#38656;&#20351;&#29992;3D&#22320;&#22270;&#25110;&#20687;&#32032;&#32423;&#22320;&#22270;&#26597;&#35810;&#23545;&#24212;&#12290;&#36890;&#36807;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#21644;&#20108;&#36827;&#21046;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25104;&#21151;&#20248;&#21270;&#20102;&#21021;&#22987;&#30340;&#20505;&#36873;&#29289;&#20307;&#26816;&#27979;&#65292;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#65292;&#30693;&#36947;&#33258;&#24049;&#25152;&#22788;&#30340;&#20301;&#32622;&#26159;&#21542;&#26377;&#21161;&#20110;&#24863;&#30693;&#21608;&#22260;&#30340;&#29289;&#20307;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20808;&#39564;&#22320;&#22270;&#26469;&#24110;&#21161;&#26816;&#27979;&#22330;&#26223;&#20013;&#30340;&#21160;&#24577;&#29289;&#20307;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;3D&#22320;&#22270;&#25110;&#20687;&#32032;&#32423;&#22320;&#22270;&#26597;&#35810;&#23545;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20808;&#39564;&#22320;&#22270;&#20248;&#21270;&#21021;&#22987;&#30340;&#20505;&#36873;&#29289;&#20307;&#26816;&#27979;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#32463;&#36807;&#31934;&#30830;&#20462;&#27491;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#65288;VPR&#65289;&#26469;&#20026;&#32473;&#23450;&#30340;&#26597;&#35810;&#22270;&#20687;&#26816;&#32034;&#21442;&#32771;&#22320;&#22270;&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#20108;&#36827;&#21046;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#27604;&#36739;&#26597;&#35810;&#21644;&#21442;&#32771;&#22270;&#20687;&#21306;&#22495;&#20197;&#39564;&#35777;&#26597;&#35810;&#26816;&#27979;&#12290;&#24403;&#25105;&#20204;&#30340;&#20998;&#31867;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#65292;&#22312;&#22823;&#32422;1000&#23545;&#26597;&#35810;-&#22320;&#22270;&#22270;&#20687;&#23545;&#19978;&#65292;&#19982;&#29616;&#26377;&#30340;&#29616;&#25104;&#36710;&#36742;&#26816;&#27979;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;&#36710;&#36742;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can knowing where you are assist in perceiving objects in your surroundings, especially under adverse weather and lighting conditions? In this work we investigate whether a prior map can be leveraged to aid in the detection of dynamic objects in a scene without the need for a 3D map or pixel-level map-query correspondences. We contribute an algorithm which refines an initial set of candidate object detections and produces a refined subset of highly accurate detections using a prior map. We begin by using visual place recognition (VPR) to retrieve a reference map image for a given query image, then use a binary classification neural network that compares the query and mapping image regions to validate the query detection. Once our classification network is trained, on approximately 1000 query-map image pairs, it is able to improve the performance of vehicle detection when combined with an existing off-the-shelf vehicle detector. We demonstrate our approach using standard datasets across
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#26045;&#21152;&#30340;&#36816;&#21160;&#32422;&#26463;&#26469;&#25913;&#21892;&#35270;&#35273;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#21160;&#24577;&#36710;&#36742;&#22312;&#23450;&#20301;&#27969;&#31243;&#20013;&#25552;&#20379;&#26377;&#38480;&#23039;&#24577;&#32422;&#26463;&#20449;&#24687;&#65292;&#20248;&#21270;&#23039;&#24577;&#20272;&#35745;&#24182;&#35745;&#31639;&#26410;&#26469;&#23039;&#24577;&#20272;&#35745;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23450;&#20301;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17529</link><description>&lt;p&gt;
&#38145;&#23450;&#65306;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#26045;&#21152;&#30340;&#36816;&#21160;&#32422;&#26463;&#25913;&#21892;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization. (arXiv:2306.17529v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#26045;&#21152;&#30340;&#36816;&#21160;&#32422;&#26463;&#26469;&#25913;&#21892;&#35270;&#35273;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#21160;&#24577;&#36710;&#36742;&#22312;&#23450;&#20301;&#27969;&#31243;&#20013;&#25552;&#20379;&#26377;&#38480;&#23039;&#24577;&#32422;&#26463;&#20449;&#24687;&#65292;&#20248;&#21270;&#23039;&#24577;&#20272;&#35745;&#24182;&#35745;&#31639;&#26410;&#26469;&#23039;&#24577;&#20272;&#35745;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23450;&#20301;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;6&#33258;&#30001;&#24230;&#30340;&#23450;&#20301;&#21644;SLAM&#31995;&#32479;&#20351;&#29992;&#38745;&#24577;&#22320;&#26631;&#65292;&#20294;&#24573;&#30053;&#21160;&#24577;&#29289;&#20307;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#26377;&#29992;&#22320;&#32435;&#20837;&#21040;&#20856;&#22411;&#30340;&#27969;&#31243;&#20013;&#12290;&#22312;&#24050;&#32463;&#32435;&#20837;&#21160;&#24577;&#29289;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20856;&#22411;&#30340;&#26041;&#27861;&#23581;&#35797;&#30528;&#30456;&#23545;&#22797;&#26434;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#36825;&#20123;&#29289;&#20307;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#25110;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25240;&#20013;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#28436;&#31034;&#65292;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#22312;6&#33258;&#30001;&#24230;&#36880;&#24103;PnP-RANSAC&#23450;&#20301;&#27969;&#31243;&#20013;&#25552;&#20379;&#26377;&#38480;&#30340;&#23039;&#24577;&#32422;&#26463;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#21160;&#27169;&#22411;&#23545;&#21021;&#22987;&#23039;&#24577;&#20272;&#35745;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26410;&#26469;&#23039;&#24577;&#20272;&#35745;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#29615;&#22659;&#20013;&#30456;&#23545;&#24103;&#38388;&#20301;&#32622;&#30340;&#36816;&#21160;&#26159;&#21542;&#21463;&#21040;&#21160;&#24577;&#36710;&#36742;&#30340;&#32422;&#26463;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26816;&#27979;&#21644;&#35782;&#21035;&#36866;&#21512;&#30340;&#21160;&#24577;&#36710;&#36742;&#26469;&#23450;&#20041;&#36825;&#20123;&#23039;&#24577;&#32422;&#26463;&#65292;&#20197;&#20462;&#25913;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered based on whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;&#20102;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#26465;&#20214;&#19979;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.17501</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient uniform approximation using Random Vector Functional Link networks. (arXiv:2306.17501v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;&#20102;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#26465;&#20214;&#19979;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;(RVFL)&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#20869;&#37096;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20108;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#36825;&#31181;&#26550;&#26500;&#21482;&#38656;&#35201;&#23398;&#20064;&#22806;&#37096;&#26435;&#37325;&#65292;&#23398;&#20064;&#36807;&#31243;&#21487;&#20197;&#31616;&#21270;&#20026;&#32447;&#24615;&#20248;&#21270;&#20219;&#21153;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#20854;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#23613;&#31649;&#20043;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;&#20197;$L_2$&#26041;&#24335;&#21487;&#20197;&#23454;&#29616;&#36825;&#26679;&#30340;&#36924;&#36817;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#24773;&#20917;&#19979;&#30340;&#21487;&#34892;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#30340;&#19979;&#30028;&#65292;&#21462;&#20915;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#12289;&#26399;&#26395;&#30340;&#20934;&#30830;&#24230;&#21644;&#36755;&#20837;&#32500;&#24230;&#31561;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26041;&#27861;&#26681;&#26893;&#20110;&#27010;&#29575;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Random Vector Functional Link (RVFL) network is a depth-2 neural network with random inner weights and biases. As only the outer weights of such architectures need to be learned, the learning process boils down to a linear optimization task, allowing one to sidestep the pitfalls of nonconvex optimization problems. In this paper, we prove that an RVFL with ReLU activation functions can approximate Lipschitz continuous functions provided its hidden layer is exponentially wide in the input dimension. Although it has been established before that such approximation can be achieved in $L_2$ sense, we prove it for $L_\infty$ approximation error and Gaussian inner weights. To the best of our knowledge, our result is the first of this kind. We give a nonasymptotic lower bound for the number of hidden layer nodes, depending on, among other things, the Lipschitz constant of the target function, the desired accuracy, and the input dimension. Our method of proof is rooted in probability theory an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22768;&#23398;&#32972;&#26223;&#21644;&#38899;&#20301;&#36793;&#30028;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#23616;&#37096;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.17500</link><description>&lt;p&gt;
&#35821;&#38899;&#22768;&#23398;&#32972;&#26223;&#21644;&#24773;&#24863;&#35782;&#21035;&#20851;&#31995;&#30340;&#32463;&#39564;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Empirical Interpretation of the Relationship Between Speech Acoustic Context and Emotion Recognition. (arXiv:2306.17500v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22768;&#23398;&#32972;&#26223;&#21644;&#38899;&#20301;&#36793;&#30028;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#23616;&#37096;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#23545;&#20110;&#33719;&#21462;&#24773;&#24863;&#26234;&#33021;&#21644;&#29702;&#35299;&#35821;&#38899;&#30340;&#35821;&#22659;&#21547;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#36741;&#38899;-&#20803;&#38899;&#65288;CV&#65289;&#38899;&#20301;&#36793;&#30028;&#30340;&#21464;&#21270;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#32447;&#32034;&#20016;&#23500;&#22768;&#23398;&#32972;&#26223;&#65292;&#20174;&#32780;&#24433;&#21709;SER&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35821;&#38899;&#24773;&#24863;&#34987;&#35270;&#20026;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#30340;&#19968;&#20010;&#22768;&#23398;&#29255;&#27573;&#30340;&#21333;&#20010;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#20013;&#30340;&#38899;&#20301;&#36793;&#30028;&#19981;&#26159;&#31163;&#25955;&#30340;&#20107;&#20214;&#65292;&#22240;&#27492;&#24863;&#30693;&#21040;&#30340;&#24773;&#24863;&#29366;&#24577;&#20063;&#24212;&#22312;&#28508;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#31383;&#21475;&#19978;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#25506;&#35752;&#20102;&#22768;&#23398;&#32972;&#26223;&#21644;&#38899;&#20301;&#36793;&#30028;&#23545;SER&#20013;&#30340;&#23616;&#37096;&#26631;&#35760;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20132;&#21449;&#25968;&#25454;&#38598;&#20998;&#26512;&#23454;&#39564;&#30340;&#32467;&#26524;&#25903;&#25345;&#20351;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#30340;&#22909;&#22788;&#12290;&#23454;&#39564;&#20013;&#65292;&#23558;&#38899;&#20301;&#21644;&#35789;&#35821;&#26144;&#23556;&#21040;&#27880;&#24847;&#21147;&#21521;&#37327;&#20197;&#21450;&#22522;&#39057;&#65292;&#20197;&#35266;&#23519;&#37325;&#21472;&#20998;&#24067;&#21644;&#22240;&#27492;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition (SER) is vital for obtaining emotional intelligence and understanding the contextual meaning of speech. Variations of consonant-vowel (CV) phonemic boundaries can enrich acoustic context with linguistic cues, which impacts SER. In practice, speech emotions are treated as single labels over an acoustic segment for a given time duration. However, phone boundaries within speech are not discrete events, therefore the perceived emotion state should also be distributed over potentially continuous time-windows.  This research explores the implication of acoustic context and phone boundaries on local markers for SER using an attention-based approach. The benefits of using a distributed approach to speech emotion understanding are supported by the results of cross-corpora analysis experiments. Experiments where phones and words are mapped to the attention vectors along with the fundamental frequency to observe the overlapping distributions and thereby the relationship
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#21333;&#38544;&#34255;&#23618;&#30340;&#22810;&#21464;&#37327;ReLU&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#21040;&#20309;&#31181;&#35299;&#12290;&#25105;&#20204;&#21457;&#29616;&#32447;&#24615;&#31283;&#23450;&#30340;&#26368;&#23567;&#20540;&#23545;&#24212;&#20110;&#20855;&#26377;&#26377;&#30028;$L^1$&#33539;&#25968;&#30340;&#39044;&#27979;&#22120;&#30340;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#25512;&#24191;&#21040;&#22810;&#21464;&#37327;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#26368;&#23567;&#20540;&#23545;&#24212;&#30340;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#24456;&#20302;&#12290;</title><link>http://arxiv.org/abs/2306.17499</link><description>&lt;p&gt;
&#22810;&#21464;&#37327;&#27973;&#23618;ReLU&#32593;&#32476;&#20013;&#26368;&#23567;&#20540;&#31283;&#23450;&#24615;&#30340;&#38544;&#21547;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks. (arXiv:2306.17499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#21333;&#38544;&#34255;&#23618;&#30340;&#22810;&#21464;&#37327;ReLU&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#21040;&#20309;&#31181;&#35299;&#12290;&#25105;&#20204;&#21457;&#29616;&#32447;&#24615;&#31283;&#23450;&#30340;&#26368;&#23567;&#20540;&#23545;&#24212;&#20110;&#20855;&#26377;&#26377;&#30028;$L^1$&#33539;&#25968;&#30340;&#39044;&#27979;&#22120;&#30340;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#25512;&#24191;&#21040;&#22810;&#21464;&#37327;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#26368;&#23567;&#20540;&#23545;&#24212;&#30340;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#24456;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#35757;&#32451;&#21333;&#38544;&#34255;&#23618;&#30340;&#22810;&#21464;&#37327;ReLU&#32593;&#32476;&#26102;&#25910;&#25947;&#21040;&#21738;&#31181;&#35299;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#26159;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#21160;&#24577;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#22312;&#19968;&#20803;&#24773;&#20917;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#32447;&#24615;&#31283;&#23450;&#30340;&#26368;&#23567;&#20540;&#23545;&#24212;&#20110;&#32593;&#32476;&#20989;&#25968;&#65288;&#39044;&#27979;&#22120;&#65289;&#65292;&#20854;&#20108;&#38454;&#23548;&#25968;&#20855;&#26377;&#26377;&#30028;&#21152;&#26435;$L^1$&#33539;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36793;&#30028;&#38543;&#30528;&#27493;&#38271;&#22686;&#21152;&#32780;&#21464;&#23567;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#36739;&#22823;&#30340;&#27493;&#38271;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#8220;&#26356;&#24179;&#28369;&#8221;&#30340;&#39044;&#27979;&#22120;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#25512;&#24191;&#21040;&#22810;&#21464;&#37327;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#39044;&#27979;&#22120;&#30340;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#12290;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#36793;&#30028;&#30340;&#32039;&#23494;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#35299;&#22312;&#27493;&#38271;&#20989;&#25968;&#20013;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#26368;&#23567;&#20540;&#23545;&#24212;&#30340;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#30340;&#28145;&#24230;&#20998;&#31163;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27973;&#23618;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#36828;&#20302;&#20110;&#31283;&#23450;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother' predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#30340;&#36845;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#27714;&#35299;&#36895;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#24341;&#20837;&#38544;&#24335;&#23618;&#26469;&#35299;&#20915;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#12289;&#25913;&#36827;CNN&#39044;&#26465;&#20214;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#23610;&#23544;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17486</link><description>&lt;p&gt;
&#29992;&#20110;Helmholtz&#26041;&#31243;&#30340;&#22810;&#32593;&#26684;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;&#36890;&#36807;&#32039;&#33268;&#38544;&#24335;&#23618;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers. (arXiv:2306.17486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#30340;&#36845;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#27714;&#35299;&#36895;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#24341;&#20837;&#38544;&#24335;&#23618;&#26469;&#35299;&#20915;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#12289;&#25913;&#36827;CNN&#39044;&#26465;&#20214;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#23610;&#23544;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#22312;&#39640;&#27874;&#25968;&#19979;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36845;&#20195;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#19982;&#39044;&#26465;&#20214;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26356;&#24555;&#19988;&#21487;&#20280;&#32553;&#24615;&#26356;&#22909;&#30340;&#23398;&#20064;&#22411;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#26356;&#20248;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20808;&#21069;&#36825;&#31867;&#31070;&#32463;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;U-Net-like&#32534;&#30721;&#22120;-&#27714;&#35299;&#22120;CNN&#65292;&#20854;&#20013;&#22312;U-Net&#30340;&#26368;&#31895;&#31961;&#32593;&#26684;&#19978;&#21253;&#21547;&#19968;&#20010;&#38544;&#24335;&#23618;&#65292;&#21367;&#31215;&#26680;&#34987;&#21453;&#36716;&#12290;&#36825;&#31181;&#26041;&#27861;&#32531;&#35299;&#20102;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26356;&#22909;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#21442;&#25968;&#25968;&#37327;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;CNN&#39044;&#26465;&#20214;&#22120;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#25193;&#23637;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23610;&#23544;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#21512;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#30340;&#20998;&#32423;&#24378;&#21270;&#23398;&#20064;&#21644;&#22320;&#26631;&#24341;&#23548;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#39640;&#23618;&#31574;&#30053;&#34892;&#21160;&#31354;&#38388;&#36807;&#22823;&#21644;&#20302;&#23618;&#31574;&#30053;&#30340;&#38750;&#31283;&#24577;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17484</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#23450;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#22320;&#26631;&#24341;&#23548;&#30340;&#20027;&#21160;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Landmark Guided Active Exploration with Stable Low-level Policy Learning. (arXiv:2306.17484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#30340;&#20998;&#32423;&#24378;&#21270;&#23398;&#20064;&#21644;&#22320;&#26631;&#24341;&#23548;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#39640;&#23618;&#31574;&#30053;&#34892;&#21160;&#31354;&#38388;&#36807;&#22823;&#21644;&#20302;&#23618;&#31574;&#30053;&#30340;&#38750;&#31283;&#24577;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#20998;&#32423;&#24378;&#21270;&#23398;&#20064;&#65288;GCHRL&#65289;&#36890;&#36807;&#20998;&#23618;&#26694;&#26550;&#23558;&#38271;&#26399;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#39640;&#23618;&#31574;&#30053;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#36807;&#22823;&#65292;&#32473;&#26377;&#25928;&#25506;&#32034;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#12290;&#27492;&#22806;&#65292;&#20302;&#23618;&#31574;&#30053;&#30340;&#21160;&#24577;&#21464;&#24322;&#24615;&#23558;&#38750;&#31283;&#24577;&#24341;&#20837;&#21040;&#39640;&#23618;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#20013;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#39640;&#23618;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#30446;&#26631;&#31354;&#38388;&#20013;&#22522;&#20110;&#30446;&#26631;&#23548;&#21521;&#20215;&#20540;&#20989;&#25968;&#36827;&#34892;&#35268;&#21010;&#30340;&#23376;&#30446;&#26631;&#21069;&#26223;&#24230;&#37327;&#12290;&#22312;&#23376;&#30446;&#26631;&#21069;&#26223;&#24230;&#37327;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#26631;&#24341;&#23548;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#21069;&#26223;&#24230;&#37327;&#21644;&#26032;&#39062;&#24615;&#24230;&#37327;&#26469;&#24341;&#23548;&#26234;&#33021;&#20307;&#36827;&#34892;&#39640;&#25928;&#25506;&#32034;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposes long-horizon tasks into sub-tasks through a hierarchical framework and it has demonstrated promising results across a variety of domains. However, the high-level policy's action space is often excessively large, presenting a significant challenge to effective exploration and resulting in potentially inefficient training. Moreover, the dynamic variability of the low-level policy introduces non-stationarity to the high-level state transition function, significantly impeding the learning of the high-level policy. In this paper, we design a measure of prospect for subgoals by planning in the goal space based on the goal-conditioned value function. Building upon the measure of prospect, we propose a landmark-guided exploration strategy by integrating the measures of prospect and novelty which aims to guide the agent to explore efficiently and improve sample efficiency. To address the non-stationarity arising from the dy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Graphtester&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#22270;&#25968;&#25454;&#38598;&#19978;GNNs&#30340;&#29702;&#35770;&#36793;&#30028;&#12290;&#36890;&#36807;&#20998;&#26512;40&#22810;&#20010;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;GNNs&#24615;&#33021;&#30340;&#19978;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#23545;&#22270;&#21464;&#25442;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Graphtester&#29983;&#25104;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.17482</link><description>&lt;p&gt;
Graphtester&#65306; &#22312;&#22270;&#25968;&#25454;&#38598;&#19978;&#25506;&#32034;GNNs&#30340;&#29702;&#35770;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Graphtester: Exploring Theoretical Boundaries of GNNs on Graph Datasets. (arXiv:2306.17482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Graphtester&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#22270;&#25968;&#25454;&#38598;&#19978;GNNs&#30340;&#29702;&#35770;&#36793;&#30028;&#12290;&#36890;&#36807;&#20998;&#26512;40&#22810;&#20010;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;GNNs&#24615;&#33021;&#30340;&#19978;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#23545;&#22270;&#21464;&#25442;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Graphtester&#29983;&#25104;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22312;&#21487;&#20197;&#21306;&#20998;&#30340;&#32467;&#26500;&#26041;&#38754;&#20063;&#26377;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#32593;&#32476;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29616;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Graphtester&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#24471;&#20998;&#30340;GNNs&#30340;&#29702;&#35770;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;Graphtester&#20998;&#26512;&#20102;40&#22810;&#20010;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#26681;&#25454;&#23618;&#25968;&#30830;&#23450;&#20102;&#21508;&#31181;GNNs&#24615;&#33021;&#30340;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#23545;&#20351;&#29992;&#20301;&#32622;&#33410;&#28857;&#32534;&#30721;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#36866;&#29992;&#24615;&#65292;&#20174;&#32780;&#25193;&#22823;&#20102;&#20854;&#33539;&#22260;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Graphtester&#29983;&#25104;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#22270;&#21464;&#25442;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#65288;&#22914;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35813;&#36719;&#20214;&#21253;&#21487;&#20197;&#20813;&#36153;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful tool for learning from graph-structured data. However, even state-of-the-art architectures have limitations on what structures they can distinguish, imposing theoretical limits on what the networks can achieve on different datasets. In this paper, we provide a new tool called Graphtester for a comprehensive analysis of the theoretical capabilities of GNNs for various datasets, tasks, and scores. We use Graphtester to analyze over 40 different graph datasets, determining upper bounds on the performance of various GNNs based on the number of layers. Further, we show that the tool can also be used for Graph Transformers using positional node encodings, thereby expanding its scope. Finally, we demonstrate that features generated by Graphtester can be used for practical applications such as Graph Transformers, and provide a synthetic dataset to benchmark node and edge features, such as positional encodings. The package is freely availa
&lt;/p&gt;</description></item><item><title>FedBone&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#20998;&#31163;&#23398;&#20064;&#21644;&#26799;&#24230;&#25237;&#24433;&#30340;&#35282;&#24230;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#22823;&#35268;&#27169;&#27169;&#22411;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#22823;&#35268;&#27169;&#27169;&#22411;&#20197;&#21450;&#24573;&#30053;&#26799;&#24230;&#20914;&#31361;&#23545;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17465</link><description>&lt;p&gt;
FedBone: &#36808;&#21521;&#22823;&#35268;&#27169;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedBone: Towards Large-Scale Federated Multi-Task Learning. (arXiv:2306.17465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17465
&lt;/p&gt;
&lt;p&gt;
FedBone&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#20998;&#31163;&#23398;&#20064;&#21644;&#26799;&#24230;&#25237;&#24433;&#30340;&#35282;&#24230;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#22823;&#35268;&#27169;&#27169;&#22411;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#22823;&#35268;&#27169;&#27169;&#22411;&#20197;&#21450;&#24573;&#30053;&#26799;&#24230;&#20914;&#31361;&#23545;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;HFMTL&#65289;&#26159;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#23558;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#24322;&#26500;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#20840;&#38754;&#30340;&#39044;&#27979;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#27169;&#22411;&#26469;&#25552;&#21462;&#39640;&#23618;&#27425;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#27169;&#22411;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;HFML&#26041;&#27861;&#20063;&#24573;&#30053;&#20102;&#22312;&#32852;&#37030;&#32858;&#21512;&#36807;&#31243;&#20013;&#26799;&#24230;&#20914;&#31361;&#23545;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FedBone&#65292;&#23427;&#36890;&#36807;&#20174;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#20998;&#31163;&#23398;&#20064;&#21644;&#26799;&#24230;&#25237;&#24433;&#30340;&#35282;&#24230;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#32452;&#20214;&#65306;&#20113;&#26381;&#21153;&#22120;&#19978;&#30340;&#22823;&#35268;&#27169;&#36890;&#29992;&#27169;&#22411;&#65288;&#31216;&#20026;&#36890;&#29992;&#27169;&#22411;&#65289;&#21644;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65288;&#31216;&#20026;&#23458;&#25143;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous federated multi-task learning (HFMTL) is a federated learning technique that combines heterogeneous tasks of different clients to achieve more accurate, comprehensive predictions. In real-world applications, visual and natural language tasks typically require large-scale models to extract high-level abstract features. However, large-scale models cannot be directly applied to existing federated multi-task learning methods. Existing HFML methods also disregard the impact of gradient conflicts on multi-task optimization during the federated aggregation process. In this work, we propose an innovative framework called FedBone, which enables the construction of large-scale models with better generalization from the perspective of server-client split learning and gradient projection. We split the entire model into two components: a large-scale general model (referred to as the general model) on the cloud server and multiple task-specific models (referred to as the client model) 
&lt;/p&gt;</description></item><item><title>GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.17439</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#30340;&#38024;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17439
&lt;/p&gt;
&lt;p&gt;
GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#25776;&#20889;&#30340;&#20869;&#23481;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTWatermark&#65292;&#19968;&#31181;&#24378;&#22823;&#19988;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#26469;&#33258;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22266;&#23450;&#30340;&#20998;&#32452;&#35774;&#35745;&#65292;&#20197;&#22686;&#24378;&#23545;&#32534;&#36753;&#21644;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24102;&#27700;&#21360;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#12289;&#26816;&#27979;&#27491;&#30830;&#24615;&#21644;&#23545;&#25239;&#35268;&#36991;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#12290;&#22312;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#29983;&#25104;&#36136;&#37327;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#30456;&#24403;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;LLMs&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23558;&#26377;&#38480;&#22810;&#38754;&#20307;&#20998;&#35299;&#19982;&#25345;&#32493;&#21516;&#35843;&#32467;&#21512;&#20351;&#29992;&#26469;&#26816;&#27979;&#36755;&#20837;&#31354;&#38388;&#20013;&#27969;&#24418;&#30340;&#21516;&#35843;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#21508;&#31181;&#35757;&#32451;&#30446;&#30340;&#30340;&#32593;&#32476;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17418</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#12289;&#22810;&#38754;&#20307;&#20998;&#35299;&#21644;&#25345;&#32493;&#21516;&#35843;
&lt;/p&gt;
&lt;p&gt;
ReLU Neural Networks, Polyhedral Decompositions, and Persistent Homolog. (arXiv:2306.17418v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17418
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23558;&#26377;&#38480;&#22810;&#38754;&#20307;&#20998;&#35299;&#19982;&#25345;&#32493;&#21516;&#35843;&#32467;&#21512;&#20351;&#29992;&#26469;&#26816;&#27979;&#36755;&#20837;&#31354;&#38388;&#20013;&#27969;&#24418;&#30340;&#21516;&#35843;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#21508;&#31181;&#35757;&#32451;&#30446;&#30340;&#30340;&#32593;&#32476;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#23548;&#33268;&#36755;&#20837;&#31354;&#38388;&#30340;&#26377;&#38480;&#22810;&#38754;&#20307;&#20998;&#35299;&#21644;&#30456;&#24212;&#30340;&#26377;&#38480;&#23545;&#20598;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#36825;&#20010;&#23545;&#20598;&#22270;&#26159;&#36755;&#20837;&#31354;&#38388;&#30340;&#31895;&#31890;&#21270;&#65292;&#20294;&#23427;&#36275;&#22815;&#31283;&#20581;&#65292;&#21487;&#20197;&#19982;&#25345;&#32493;&#21516;&#35843;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;&#36755;&#20837;&#31354;&#38388;&#20013;&#27969;&#24418;&#30340;&#21516;&#35843;&#20449;&#21495;&#12290;&#36825;&#20010;&#24615;&#36136;&#36866;&#29992;&#20110;&#35768;&#22810;&#35757;&#32451;&#29992;&#36884;&#24191;&#27867;&#30340;&#32593;&#32476;&#65292;&#24182;&#19981;&#23616;&#38480;&#20110;&#25299;&#25169;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#29305;&#24615;&#20196;&#20154;&#24778;&#35766;&#21644;&#26377;&#36259;&#65292;&#24076;&#26395;&#23427;&#20063;&#33021;&#26377;&#25152;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
A ReLU neural network leads to a finite polyhedral decomposition of input space and a corresponding finite dual graph. We show that while this dual graph is a coarse quantization of input space, it is sufficiently robust that it can be combined with persistent homology to detect homological signals of manifolds in the input space from samples. This property holds for a variety of networks trained for a wide range of purposes that have nothing to do with this topological application. We found this feature to be surprising and interesting; we hope it will also be useful.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;Koopman&#31639;&#23376;&#12290; FlowDMD&#31639;&#27861;&#21033;&#29992;&#32806;&#21512;&#27969;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#23398;&#20064;Koopman&#31639;&#23376;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#24182;&#20934;&#30830;&#37325;&#26500;&#29366;&#24577;&#21464;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17396</link><description>&lt;p&gt;
&#29992;&#20110;Koopman&#31639;&#23376;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#21453;&#36716;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed invertible neural network for the Koopman operator learning. (arXiv:2306.17396v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;Koopman&#31639;&#23376;&#12290; FlowDMD&#31639;&#27861;&#21033;&#29992;&#32806;&#21512;&#27969;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#23398;&#20064;Koopman&#31639;&#23376;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#24182;&#20934;&#30830;&#37325;&#26500;&#29366;&#24577;&#21464;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Koopman&#31639;&#23376;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#19968;&#32452;&#21487;&#35266;&#27979;&#20989;&#25968;&#65292;&#23558;&#19968;&#20010;&#26377;&#38480;&#32500;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#36716;&#21270;&#20026;&#19968;&#20010;&#26080;&#31351;&#20294;&#32447;&#24615;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20808;&#21069;&#30693;&#35782;&#25163;&#21160;&#36873;&#25321;&#33021;&#22815;&#35206;&#30422;Koopman&#31639;&#23376;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#21487;&#35266;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#24213;&#23618;&#31995;&#32479;&#20960;&#20046;&#27809;&#26377;&#20449;&#24687;&#25110;&#27809;&#26377;&#20219;&#20309;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#21487;&#35266;&#27979;&#20989;&#25968;&#21487;&#36870;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;FlowDMD&#65292;&#21363;&#22522;&#20110;&#27969;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65292;&#21033;&#29992;&#32806;&#21512;&#27969;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;CF-INN&#65289;&#26694;&#26550;&#12290;FlowDMD&#21033;&#29992;CF-INN&#30340;&#20869;&#22312;&#21487;&#36870;&#29305;&#24615;&#65292;&#23398;&#20064;Koopman&#31639;&#23376;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#24182;&#20934;&#30830;&#37325;&#26500;&#29366;&#24577;&#21464;&#37327;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30456;&#27604;&#24403;&#21069;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Koopman operator theory, a finite-dimensional nonlinear system is transformed into an infinite but linear system using a set of observable functions. However, manually selecting observable functions that span the invariant subspace of the Koopman operator based on prior knowledge is inefficient and challenging, particularly when little or no information is available about the underlying systems. Furthermore, current methodologies tend to disregard the importance of the invertibility of observable functions, which leads to inaccurate results. To address these challenges, we propose the so-called FlowDMD, a Flow-based Dynamic Mode Decomposition that utilizes the Coupling Flow Invertible Neural Network (CF-INN) framework. FlowDMD leverages the intrinsically invertible characteristics of the CF-INN to learn the invariant subspaces of the Koopman operator and accurately reconstruct state variables. Numerical experiments demonstrate the superior performance of our algorithm compared to st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21452;&#21464;&#37327;&#22522;&#20110;&#26799;&#24230;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#19968;&#31181;&#36335;&#24452;&#36319;&#36394;&#20248;&#21270;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.17378</link><description>&lt;p&gt;
&#21452;&#21464;&#37327;&#22522;&#20110;&#26799;&#24230;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global Optimality in Bivariate Gradient-based DAG Learning. (arXiv:2306.17378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21452;&#21464;&#37327;&#22522;&#20110;&#26799;&#24230;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#19968;&#31181;&#36335;&#24452;&#36319;&#36394;&#20248;&#21270;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#31867;&#26032;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#21463;&#21040;&#20102;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#65292;&#23427;&#28304;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#29615;&#26377;&#21521;&#22270;&#27169;&#22411;&#30340;&#32479;&#35745;&#38382;&#39064;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#26631;&#20934;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#38382;&#39064;&#30340;&#38590;&#28857;&#22312;&#20110;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#38750;&#20984;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#20010;&#38382;&#39064;&#24182;&#19981;&#26159;"&#33391;&#24615;"&#30340;&#65292;&#24182;&#19988;&#23384;&#22312;&#30528;&#22810;&#20010;&#34394;&#20551;&#35299;&#65292;&#26631;&#20934;&#26041;&#27861;&#24456;&#23481;&#26131;&#38519;&#20837;&#20854;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36335;&#24452;&#36319;&#36394;&#20248;&#21270;&#31639;&#27861;&#22312;&#21452;&#21464;&#37327;&#24773;&#20917;&#19979;&#20250;&#20840;&#23616;&#25910;&#25947;&#21040;&#24635;&#20307;&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a new class of non-convex optimization problems motivated by the statistical problem of learning an acyclic directed graphical model from data has attracted significant interest. While existing work uses standard first-order optimization schemes to solve this problem, proving the global optimality of such approaches has proven elusive. The difficulty lies in the fact that unlike other non-convex problems in the literature, this problem is not "benign", and possesses multiple spurious solutions that standard approaches can easily get trapped in. In this paper, we prove that a simple path-following optimization scheme globally converges to the global minimum of the population loss in the bivariate setting.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17366</link><description>&lt;p&gt;
$\lambda$-AC&#65306;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21363;&#27169;&#22411;&#22312;&#20915;&#31574;&#21046;&#23450;&#26102;&#24212;&#35813;&#26159;&#20934;&#30830;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#20915;&#31574;&#24863;&#30693;&#25439;&#22833;&#30340;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#31639;&#27861;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#31639;&#27861;&#24605;&#24819;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;MuZero&#31995;&#21015;&#24037;&#20316;&#20013;&#25152;&#24314;&#31435;&#30340;&#32463;&#39564;&#24615;&#35774;&#35745;&#20915;&#31574;&#23545;&#20110;&#30456;&#20851;&#31639;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#19981;&#21516;&#30340;&#20215;&#20540;&#24863;&#30693;&#31639;&#27861;&#23454;&#20363;&#20043;&#38388;&#34892;&#20026;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#22411;&#39537;&#21160;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;$\lambda$-AC&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;(DUN)&#25216;&#26415;&#65292;&#23398;&#20064;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#20197;&#26080;&#20559;&#26041;&#24335;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#20013;&#35774;&#22791;&#24046;&#24322;&#21644;&#32479;&#35745;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31934;&#30830;&#24615;&#21644;&#36136;&#37327;&#24863;&#30693;&#30340;&#32858;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#24322;&#26500;&#24615;&#21644;FL&#29615;&#22659;&#19979;&#30340;&#36136;&#37327;&#24863;&#30693;&#32858;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17362</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#25913;&#36827;&#32852;&#37030;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Improving Federated Aggregation with Deep Unfolding Networks. (arXiv:2306.17362v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;(DUN)&#25216;&#26415;&#65292;&#23398;&#20064;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#20197;&#26080;&#20559;&#26041;&#24335;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#20013;&#35774;&#22791;&#24046;&#24322;&#21644;&#32479;&#35745;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31934;&#30830;&#24615;&#21644;&#36136;&#37327;&#24863;&#30693;&#30340;&#32858;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#24322;&#26500;&#24615;&#21644;FL&#29615;&#22659;&#19979;&#30340;&#36136;&#37327;&#24863;&#30693;&#32858;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#30340;&#24615;&#33021;&#21463;&#21040;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#35774;&#22791;&#24046;&#24322;&#21644;&#32479;&#35745;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65288;DUN&#65289;&#30340;&#25216;&#26415;&#65292;&#23398;&#20064;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#26080;&#20559;&#22320;&#25913;&#21892;&#24322;&#26500;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31934;&#30830;&#24615;&#21644;&#36136;&#37327;&#24863;&#30693;&#30340;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#23427;&#35780;&#20272;&#20102;&#26368;&#20339;&#21152;&#26435;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#20197;&#22312;&#32858;&#21512;&#26041;&#27861;&#19978;&#23450;&#20041;&#36739;&#23569;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#23398;&#20064;&#30340;&#26080;&#20559;&#26435;&#37325;&#30340;&#35299;&#37322;&#33021;&#21147;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#36890;&#36807;&#23558;&#26080;&#20559;&#26435;&#37325;&#34701;&#20837;&#27169;&#22411;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#24322;&#26500;&#21450;FL&#29615;&#22659;&#19979;&#30340;&#36136;&#37327;&#24863;&#30693;&#32858;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Federated learning (FL) is negatively affected by device differences and statistical characteristics between participating clients. To address this issue, we introduce a deep unfolding network (DUN)-based technique that learns adaptive weights that unbiasedly ameliorate the adverse impacts of heterogeneity. The proposed method demonstrates impressive accuracy and quality-aware aggregation. Furthermore, it evaluated the best-weighted normalization approach to define less computational power on the aggregation method. The numerical experiments in this study demonstrate the effectiveness of this approach and provide insights into the interpretability of the unbiased weights learned.  By incorporating unbiased weights into the model, the proposed approach effectively addresses quality-aware aggregation under the heterogeneity of the participating clients and the FL environment. Codes and details are \href{https://github.com/shanikairoshi/Improved_DUN_basedFL_Aggregation}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.17361</link><description>&lt;p&gt;
iSCAN&#65306;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20197;&#34920;&#31034;&#22797;&#26434;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30495;&#27491;&#30340;&#24213;&#23618;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20174;&#35266;&#27979;&#25968;&#25454;&#25110;&#24178;&#39044;&#25968;&#25454;&#20013;&#30830;&#23450;&#23427;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;&#30456;&#20851;SCM&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#21464;&#21270;(&#36716;&#21464;)&#32780;&#19981;&#26159;&#24674;&#22797;&#25972;&#20010;&#24213;&#23618;DAG&#32467;&#26500;&#12290;&#20363;&#23376;&#21253;&#25324;&#20998;&#26512;&#20581;&#24247;&#21644;&#30284;&#30151;&#24739;&#32773;&#20043;&#38388;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#32467;&#26500;&#21464;&#21270;&#65292;&#25110;&#32773;&#22312;&#19981;&#21516;&#32454;&#32990;&#29615;&#22659;&#19979;&#29702;&#35299;&#29983;&#29289;&#36884;&#24452;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#30456;&#21516;&#30340;&#21464;&#37327;&#38598;&#19978;&#35782;&#21035;&#20004;&#20010;&#25110;&#22810;&#20010;&#30456;&#20851;SCM&#20013;&#30340;$\textit{&#21151;&#33021;}$&#26426;&#21046;&#36716;&#21464;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#27599;&#20010;SCM&#30340;&#25972;&#20010;DAG&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#20351;&#29992;&#20102;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#32447;&#24615;&#27169;&#22411;&#65307;&#32780;&#26412;&#25991;&#20013;&#25105;&#20204;&#21017;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the true underlying directed acyclic graph (DAG) structure is often unknown, and determining it from observational or interventional data remains a challenging task. However, in many situations, the end goal is to identify changes (shifts) in causal mechanisms between related SCMs rather than recovering the entire underlying DAG structure. Examples include analyzing gene regulatory network structure changes between healthy and cancerous individuals or understanding variations in biological pathways under different cellular contexts. This paper focuses on identifying $\textit{functional}$ mechanism shifts in two or more related SCMs over the same set of variables -$\textit{without estimating the entire DAG structure of each SCM}$. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we ass
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#27604;&#36739;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20013;&#37319;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#38544;&#31169;&#26426;&#21046;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17338</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;&#25968;&#25454;&#38544;&#31169;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Blockchain-Based Federated Learning and Data Privacy. (arXiv:2306.17338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#27604;&#36739;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20013;&#37319;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#38544;&#31169;&#26426;&#21046;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#35745;&#31639;&#33021;&#21147;&#21644;&#27169;&#22411;&#20256;&#36755;&#36827;&#34892;&#21327;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#24322;&#26500;&#35774;&#22791;&#19978;&#20998;&#24067;&#35757;&#32451;&#25968;&#25454;&#26469;&#20943;&#23569;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#32570;&#28857;&#26159;&#22312;&#23384;&#20648;&#12289;&#20256;&#36755;&#21644;&#20849;&#20139;&#36807;&#31243;&#20013;&#32570;&#20047;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#27844;&#38706;&#65292;&#23545;&#25968;&#25454;&#25152;&#26377;&#32773;&#21644;&#20379;&#24212;&#21830;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#21306;&#22359;&#38142;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25552;&#20379;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#29615;&#22659;&#20013;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#27604;&#36739;&#22312;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20013;&#37319;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#38544;&#31169;&#26426;&#21046;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a decentralized machine learning paradigm that allows multiple clients to collaborate by leveraging local computational power and the models transmission. This method reduces the costs and privacy concerns associated with centralized machine learning methods while ensuring data privacy by distributing training data across heterogeneous devices. On the other hand, federated learning has the drawback of data leakage due to the lack of privacy-preserving mechanisms employed during storage, transfer, and sharing, thus posing significant risks to data owners and suppliers. Blockchain technology has emerged as a promising technology for offering secure data-sharing platforms in federated learning, especially in Industrial Internet of Things (IIoT) settings. This survey aims to compare the performance and security of various data privacy mechanisms adopted in blockchain-based federated learning architectures. We conduct a systematic review of existing literature on secur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20984;&#20998;&#26512;&#21644;ODE&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32534;&#30721;&#38750;&#25193;&#24352;&#31639;&#23376;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#26469;&#38480;&#21046;Lipschitz&#24120;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#26550;&#26500;&#36824;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#23398;&#20064;&#21435;&#22122;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26469;&#20445;&#35777;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2306.17332</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20998;&#26512;&#21644;ODE&#35774;&#35745;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Designing Stable Neural Networks using Convex Analysis and ODEs. (arXiv:2306.17332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20984;&#20998;&#26512;&#21644;ODE&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32534;&#30721;&#38750;&#25193;&#24352;&#31639;&#23376;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#26469;&#38480;&#21046;Lipschitz&#24120;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#26550;&#26500;&#36824;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#23398;&#20064;&#21435;&#22122;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26469;&#20445;&#35777;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#20102;&#19968;&#31181;&#22522;&#20110;ResNet&#39118;&#26684;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32534;&#30721;&#38750;&#25193;&#24352;&#65288;1-Lipschitz&#65289;&#31639;&#23376;&#65292;&#21482;&#35201;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21463;&#21040;&#36866;&#24403;&#30340;&#32422;&#26463;&#12290;&#19982;&#20256;&#32479;&#30340;ResNet&#26550;&#26500;&#30456;&#27604;&#65292;&#21363;&#20351;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21463;&#21040;&#32422;&#26463;&#65292;&#20854;Lipschitz&#24120;&#25968;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20063;&#20250;&#38543;&#32593;&#32476;&#30340;&#28145;&#24230;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#32422;&#26463;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#65292;&#20197;&#30830;&#20445;&#32593;&#32476;&#26159;&#19968;&#20010;&#24179;&#22343;&#31639;&#23376;&#65292;&#20351;&#20854;&#25104;&#20026;Plug-and-Play&#31639;&#27861;&#20013;&#30340;&#23398;&#20064;&#21435;&#22122;&#22120;&#30340;&#33258;&#28982;&#20505;&#36873;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#26041;&#24335;&#26469;&#24378;&#21046;&#35889;&#33539;&#25968;&#32422;&#26463;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#22312;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#20063;&#21487;&#20197;&#35757;&#32451;&#20986;&#24615;&#33021;&#20248;&#36234;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#24212;&#29992;&#20110;&#23545;&#25239;&#24615;&#31283;&#20581;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by classical work on the numerical integration of ordinary differential equations we present a ResNet-styled neural network architecture that encodes non-expansive (1-Lipschitz) operators, as long as the spectral norms of the weights are appropriately constrained. This is to be contrasted with the ordinary ResNet architecture which, even if the spectral norms of the weights are constrained, has a Lipschitz constant that, in the worst case, grows exponentially with the depth of the network. Further analysis of the proposed architecture shows that the spectral norms of the weights can be further constrained to ensure that the network is an averaged operator, making it a natural candidate for a learned denoiser in Plug-and-Play algorithms. Using a novel adaptive way of enforcing the spectral norm constraints, we show that, even with these constraints, it is possible to train performant networks. The proposed architecture is applied to the problem of adversarially robust image cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;$\epsilon$-&#36138;&#24515;&#31574;&#30053;&#24212;&#29992;&#20110;&#24773;&#22659;&#33033;&#20914;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21152;&#26435;&#26680;&#23725;&#22238;&#24402;&#20272;&#35745;&#22120;&#23454;&#29616;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#20272;&#35745;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#19968;&#33268;&#24615;&#21644;&#20381;&#36182;&#20110;RKHS&#32500;&#24230;&#30340;&#27425;&#32447;&#24615;&#21518;&#24724;&#29575;&#65292;&#22312;&#26377;&#38480;&#32500;RKHS&#30340;&#36793;&#38469;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#21518;&#24724;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17329</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;$\epsilon$-&#36138;&#24515;&#31574;&#30053;&#22312;&#24773;&#22659;&#33033;&#20914;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Kernel $\epsilon$-Greedy for Contextual Bandits. (arXiv:2306.17329v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;$\epsilon$-&#36138;&#24515;&#31574;&#30053;&#24212;&#29992;&#20110;&#24773;&#22659;&#33033;&#20914;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21152;&#26435;&#26680;&#23725;&#22238;&#24402;&#20272;&#35745;&#22120;&#23454;&#29616;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#20272;&#35745;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#19968;&#33268;&#24615;&#21644;&#20381;&#36182;&#20110;RKHS&#32500;&#24230;&#30340;&#27425;&#32447;&#24615;&#21518;&#24724;&#29575;&#65292;&#22312;&#26377;&#38480;&#32500;RKHS&#30340;&#36793;&#38469;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#21518;&#24724;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24773;&#22659;&#33033;&#20914;&#20013;&#30340;&#22522;&#20110;&#26680;&#30340;$\epsilon$-&#36138;&#24515;&#31574;&#30053;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#33218;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35748;&#20026;&#24179;&#22343;&#22870;&#21169;&#20989;&#25968;&#20301;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#21152;&#26435;&#26680;&#23725;&#22238;&#24402;&#20272;&#35745;&#22120;&#12290;&#22312;&#23545;&#25506;&#32034;&#27010;&#29575;&#24207;&#21015;$\{\epsilon_t\}_t$&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;$\{\lambda_t\}_t$&#30340;&#19968;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#26680;&#21644;&#30456;&#24212;&#30340;RKHS&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20381;&#36182;&#20110;RKHS&#20869;&#22312;&#32500;&#24230;&#30340;&#27425;&#32447;&#24615;&#21518;&#24724;&#29575;&#12290;&#27492;&#22806;&#65292;&#22312;&#26377;&#38480;&#32500;RKHS&#30340;&#36793;&#38469;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;$\sqrt{T}$&#30340;&#26368;&#20248;&#21518;&#24724;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a kernelized version of the $\epsilon$-greedy strategy for contextual bandits. More precisely, in a setting with finitely many arms, we consider that the mean reward functions lie in a reproducing kernel Hilbert space (RKHS). We propose an online weighted kernel ridge regression estimator for the reward functions. Under some conditions on the exploration probability sequence, $\{\epsilon_t\}_t$, and choice of the regularization parameter, $\{\lambda_t\}_t$, we show that the proposed estimator is consistent. We also show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\sqrt{T}$ under a margin condition for finite-dimensional RKHS.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;DNN&#20998;&#26512;&#30340;&#27169;&#22411;&#26816;&#39564;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#26816;&#39564;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17323</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#26469;&#25193;&#23637;DNN&#20998;&#26512;&#30340;&#27169;&#22411;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version). (arXiv:2306.17323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;DNN&#20998;&#26512;&#30340;&#27169;&#22411;&#26816;&#39564;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#26816;&#39564;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#22522;&#20110;NN&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20351;&#29992;&#25345;&#32493;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#30340;&#21508;&#31181;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#21457;&#29616;&#34920;&#26126;&#65292;&#24494;&#23567;&#30340;NN&#36755;&#20837;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#21644;&#19981;&#21487;&#21462;&#30340;NN&#34892;&#20026;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#20854;&#24418;&#24335;&#20998;&#26512;&#30340;&#24191;&#27867;&#20852;&#36259;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#32473;&#23450;NN&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#20351;&#29992;&#21487;&#28385;&#36275;&#24615;&#27714;&#35299;&#21644;&#32447;&#24615;&#35268;&#21010;&#20026;&#35757;&#32451;&#30340;NN&#25552;&#20379;&#20102;&#31283;&#20581;&#24615;&#21644;/&#25110;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FANNet&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#26816;&#39564;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;NN&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#27169;&#22411;&#26816;&#39564;&#30456;&#20851;&#30340;&#29366;&#24577;&#31354;&#38388;&#29190;&#28856;&#23548;&#33268;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#20351;&#24471;FANNet&#21482;&#36866;&#29992;&#20110;&#23567;&#22411;NN&#12290;&#26412;&#24037;&#20316;&#24320;&#21457;&#20102;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#26102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to their remarkable learning capabilities and performance in real-world applications, the use of machine learning systems based on Neural Networks (NNs) has been continuously increasing. However, various case studies and empirical findings in the literature suggest that slight variations to NN inputs can lead to erroneous and undesirable NN behavior. This has led to considerable interest in their formal analysis, aiming to provide guarantees regarding a given NN's behavior. Existing frameworks provide robustness and/or safety guarantees for the trained NNs, using satisfiability solving and linear programming. We proposed FANNet, the first model checking-based framework for analyzing a broader range of NN properties. However, the state-space explosion associated with model checking entails a scalability problem, making the FANNet applicable only to small NNs. This work develops state-space reduction and input segmentation approaches, to improve the scalability and timing efficienc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#25506;&#35752;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#30340;&#35889;&#20998;&#26512;&#26469;&#29702;&#35299;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2306.17301</link><description>&lt;p&gt;
&#27973;&#23618;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65306;&#19968;&#20010;&#25968;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study. (arXiv:2306.17301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#25506;&#35752;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#30340;&#35889;&#20998;&#26512;&#26469;&#29702;&#35299;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20998;&#26512;&#21644;&#23454;&#39564;&#30340;&#32508;&#21512;&#25968;&#20540;&#30740;&#31350;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#23454;&#38469;&#22240;&#32032;&#20013;&#65292;&#22788;&#29702;&#39640;&#39057;&#29575;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#20102;&#20197;&#19979;&#22522;&#26412;&#35745;&#31639;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22312;&#26377;&#38480;&#30340;&#26426;&#22120;&#31934;&#24230;&#19979;&#21487;&#20197;&#36798;&#21040;&#30340;&#26368;&#20339;&#31934;&#24230;&#65292;&#65288;2&#65289;&#23454;&#29616;&#32473;&#23450;&#31934;&#24230;&#25152;&#38656;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#65288;3&#65289;&#23545;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#30456;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#30340;&#35889;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#23646;&#24615;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a comprehensive numerical study involving analysis and experiments shows why a two-layer neural network has difficulties handling high frequencies in approximation and learning when machine precision and computation cost are important factors in real practice. In particular, the following fundamental computational issues are investigated: (1) the best accuracy one can achieve given a finite machine precision, (2) the computation cost to achieve a given accuracy, and (3) stability with respect to perturbations. The key to the study is the spectral analysis of the corresponding Gram matrix of the activation functions which also shows how the properties of the activation function play a role in the picture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.17279</link><description>&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#29575;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Constraint for Safety-Critical Reinforcement Learning. (arXiv:2306.17279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#27010;&#29575;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#23433;&#20840;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23433;&#20840;&#31574;&#30053;&#25110;&#25511;&#21046;&#22120;&#26159;&#25351;&#20197;&#39640;&#27010;&#29575;&#20445;&#25345;&#20195;&#29702;&#22312;&#32473;&#23450;&#23433;&#20840;&#38598;&#21512;&#20013;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#39057;&#32321;&#25506;&#32034;&#30340;&#32047;&#31215;&#32422;&#26463;&#38382;&#39064;&#21644;&#36825;&#31181;&#27010;&#29575;&#32422;&#26463;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30028;&#38480;&#65292;&#38416;&#26126;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#65288;&#32422;&#26463;&#28385;&#36275;&#65289;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#22312;&#22788;&#29702;&#27010;&#29575;&#32422;&#26463;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25506;&#32034;&#30340;&#37027;&#26679;&#65292;&#28304;&#20110;&#27809;&#26377;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#36825;&#31181;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#65292;&#31216;&#20043;&#20026;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#26799;&#24230;SPG-Actor-Critic
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning safe policies for probabilistic-constrained reinforcement learning (RL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. We establish a connection between this probabilistic-constrained setting and the cumulative-constrained formulation that is frequently explored in the existing literature. We provide theoretical bounds elucidating that the probabilistic-constrained setting offers a better trade-off in terms of optimality and safety (constraint satisfaction). The challenge encountered when dealing with the probabilistic constraints, as explored in this work, arises from the absence of explicit expressions for their gradients. Our prior work provides such an explicit gradient expression for probabilistic constraints which we term Safe Policy Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved gradient SPG-Actor-Critic that lead
&lt;/p&gt;</description></item><item><title>DisasterResponseGPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#24555;&#36895;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#65292;&#21487;&#22312;&#28798;&#23475;&#21709;&#24212;&#22330;&#26223;&#20013;&#21152;&#24555;&#35268;&#21010;&#36807;&#31243;&#12290;DisasterResponseGPT&#29983;&#25104;&#30340;&#34892;&#21160;&#26041;&#26696;&#19982;&#20154;&#24037;&#29983;&#25104;&#30340;&#26041;&#26696;&#30456;&#24403;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#26102;&#20462;&#25913;&#30340;&#20415;&#21033;&#65292;&#26377;&#21487;&#33021;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#23454;&#29616;&#24555;&#36895;&#26356;&#26032;&#21644;&#35843;&#25972;&#65292;&#20174;&#32780;&#38761;&#26032;&#28798;&#23475;&#21709;&#24212;&#34892;&#21160;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.17271</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#21152;&#36895;&#28798;&#23475;&#21709;&#24212;&#22330;&#26223;&#20013;&#30340;&#34892;&#21160;&#26041;&#26696;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios. (arXiv:2306.17271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17271
&lt;/p&gt;
&lt;p&gt;
DisasterResponseGPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#24555;&#36895;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#65292;&#21487;&#22312;&#28798;&#23475;&#21709;&#24212;&#22330;&#26223;&#20013;&#21152;&#24555;&#35268;&#21010;&#36807;&#31243;&#12290;DisasterResponseGPT&#29983;&#25104;&#30340;&#34892;&#21160;&#26041;&#26696;&#19982;&#20154;&#24037;&#29983;&#25104;&#30340;&#26041;&#26696;&#30456;&#24403;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#26102;&#20462;&#25913;&#30340;&#20415;&#21033;&#65292;&#26377;&#21487;&#33021;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#23454;&#29616;&#24555;&#36895;&#26356;&#26032;&#21644;&#35843;&#25972;&#65292;&#20174;&#32780;&#38761;&#26032;&#28798;&#23475;&#21709;&#24212;&#34892;&#21160;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28798;&#23475;&#21709;&#24212;&#22330;&#26223;&#20013;&#65292;&#21046;&#23450;&#34892;&#21160;&#26041;&#26696;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#21152;&#36895;&#35813;&#36807;&#31243;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;DisasterResponseGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#24555;&#36895;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#28798;&#23475;&#21709;&#24212;&#21644;&#35268;&#21010;&#25351;&#21335;&#32435;&#20837;&#21021;&#22987;&#25552;&#31034;&#20013;&#12290;&#22312;DisasterResponseGPT&#20013;&#65292;&#29992;&#25143;&#36755;&#20837;&#24773;&#26223;&#25551;&#36848;&#65292;&#36755;&#20986;&#19968;&#20010;&#34892;&#21160;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#22810;&#20010;&#26041;&#26696;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#21453;&#39304;&#21487;&#20197;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;DisasterResponseGPT&#29983;&#25104;&#30340;&#34892;&#21160;&#26041;&#26696;&#19982;&#20154;&#24037;&#29983;&#25104;&#30340;&#26041;&#26696;&#30456;&#24403;&#65292;&#21516;&#26102;&#22312;&#23454;&#26102;&#20462;&#25913;&#26041;&#38754;&#26356;&#21152;&#20415;&#21033;&#12290;&#35813;&#26041;&#27861;&#26377;&#21487;&#33021;&#36890;&#36807;&#22312;&#25191;&#34892;&#35745;&#21010;&#36807;&#31243;&#20013;&#23454;&#29616;&#24555;&#36895;&#26356;&#26032;&#21644;&#35843;&#25972;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#28798;&#23475;&#21709;&#24212;&#34892;&#21160;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of plans of action in disaster response scenarios is a time-consuming process. Large Language Models (LLMs) offer a powerful solution to expedite this process through in-context learning. This study presents DisasterResponseGPT, an algorithm that leverages LLMs to generate valid plans of action quickly by incorporating disaster response and planning guidelines in the initial prompt. In DisasterResponseGPT, users input the scenario description and receive a plan of action as output. The proposed method generates multiple plans within seconds, which can be further refined following the user's feedback. Preliminary results indicate that the plans of action developed by DisasterResponseGPT are comparable to human-generated ones while offering greater ease of modification in real-time. This approach has the potential to revolutionize disaster response operations by enabling rapid updates and adjustments during the plan's execution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#31995;&#32479;&#26550;&#26500;&#21644;&#20849;&#35782;+&#21019;&#26032;&#31639;&#27861;&#65292;&#21152;&#24555;&#20102;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17267</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#40065;&#26834;&#30340;&#20998;&#23618;&#23398;&#20064;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Fast and Robust State Estimation and Tracking via Hierarchical Learning. (arXiv:2306.17267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#31995;&#32479;&#26550;&#26500;&#21644;&#20849;&#35782;+&#21019;&#26032;&#31639;&#27861;&#65292;&#21152;&#24555;&#20102;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#20195;&#29702;&#32593;&#32476;&#30340;&#23436;&#20840;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#36319;&#36394;&#35299;&#20915;&#26041;&#26696;&#25910;&#25947;&#36895;&#24230;&#24930;&#19988;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#20998;&#23618;&#31995;&#32479;&#26550;&#26500;&#26469;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#24182;&#22686;&#24378;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#35813;&#26550;&#26500;&#20013;&#65292;&#20195;&#29702;&#34987;&#20998;&#25104;&#36739;&#23567;&#30340;&#32593;&#32476;&#65292;&#19968;&#20010;&#21442;&#25968;&#26381;&#21153;&#22120;&#29992;&#20110;&#24110;&#21161;&#32593;&#32476;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;&#32593;&#32476;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#20195;&#20215;&#39640;&#19988;&#36739;&#23569;&#21457;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#38382;&#39064;&#30340;&#20849;&#35782;+&#21019;&#26032;&#31639;&#27861;&#12290;&#22312;&#36825;&#20004;&#20010;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#25512;&#36865;&#21644;&#27714;&#21644;&#20849;&#35782;&#32452;&#20214;&#12290;&#23545;&#20110;&#29366;&#24577;&#20272;&#35745;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21452;&#24179;&#22343;&#20316;&#20026;&#23616;&#37096;&#21019;&#26032;&#32452;&#20214;&#12290;&#22312;&#23384;&#22312;&#26029;&#38142;&#25925;&#38556;&#30340;&#24773;&#20917;&#19979;&#65292;&#29366;&#24577;&#36319;&#36394;&#26356;&#21152;&#22256;&#38590;&#65292;&#32780;&#26631;&#20934;&#30340;&#20849;&#35782;&#21644;&#21019;&#26032;&#26041;&#27861;&#30340;&#38598;&#25104;&#19981;&#20877;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully distributed estimation and tracking solutions to large-scale multi-agent networks suffer slow convergence and are vulnerable to network failures. In this paper, we aim to speed up the convergence and enhance the resilience of state estimation and tracking using a simple hierarchical system architecture wherein agents are clusters into smaller networks, and a parameter server exists to aid the information exchanges among networks. The information exchange among networks is expensive and occurs only once in a while.  We propose two consensus + innovation algorithms for the state estimation and tracking problems, respectively. In both algorithms, we use a novel hierarchical push-sum consensus component. For the state estimation, we use dual averaging as the local innovation component. State tracking is much harder to tackle in the presence of dropping-link failures and the standard integration of the consensus and innovation approaches are no longer applicable. Moreover, dual averag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#22270;&#38745;&#24577;&#30828;&#20214;&#36719;&#20214;&#25512;&#29702;&#32852;&#21512;&#35774;&#35745;&#30340;&#26696;&#20363;&#65292;&#38024;&#23545;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#37096;&#32626;&#22330;&#26223;&#20013;&#36816;&#34892;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#30340;SuperNet&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24310;&#36831;-&#20934;&#30830;&#24615;&#26435;&#34913;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.17266</link><description>&lt;p&gt;
&#23376;&#22270;&#38745;&#24577;&#30828;&#20214;&#36719;&#20214;&#25512;&#29702;&#32852;&#21512;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Subgraph Stationary Hardware-Software Inference Co-Design. (arXiv:2306.17266v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#22270;&#38745;&#24577;&#30828;&#20214;&#36719;&#20214;&#25512;&#29702;&#32852;&#21512;&#35774;&#35745;&#30340;&#26696;&#20363;&#65292;&#38024;&#23545;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#37096;&#32626;&#22330;&#26223;&#20013;&#36816;&#34892;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#30340;SuperNet&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24310;&#36831;-&#20934;&#30830;&#24615;&#26435;&#34913;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21151;&#33021;&#65292;&#21516;&#26102;&#20063;&#20174;&#26356;&#39640;&#36136;&#37327;&#30340;ML&#39044;&#27979;&#21644;&#26356;&#22909;&#30340;&#21450;&#26102;&#24615;&#65288;&#24310;&#36831;&#65289;&#21463;&#30410;&#12290;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#12289;ML&#21644;&#31995;&#32479;&#36719;&#20214;&#39046;&#22495;&#30340;&#30740;&#31350;&#26085;&#30410;&#22686;&#22810;&#65292;&#37325;&#28857;&#26159;&#22312;ML&#27169;&#22411;&#30340;&#24310;&#36831;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21162;&#21147;&#21253;&#25324;&#21387;&#32553;&#12289;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#25552;&#21069;&#36864;&#20986;&#27169;&#22411;&#12289;&#28151;&#21512;DNN&#31934;&#24230;&#20197;&#21450;ML&#25512;&#29702;&#21152;&#36895;&#22120;&#35774;&#35745;&#65292;&#20197;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;&#33021;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20256;&#36882;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#21482;&#23545;&#24310;&#36831;-&#20934;&#30830;&#24615;&#26435;&#34913;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#38745;&#24577;&#28857;&#20135;&#29983;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#65292;&#38024;&#23545;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#37096;&#32626;&#22330;&#26223;&#20013;&#36816;&#34892;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36825;&#37324;&#27809;&#26377;&#19968;&#20010;&#21333;&#19968;&#30340;&#38745;&#24577;&#28857;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#26435;&#37325;&#20849;&#20139;&#30340;SuperNet&#26426;&#21046;&#65292;&#20351;&#24471;&#33021;&#22815;&#26381;&#21153;&#20110;&#20351;&#29992;&#35813;&#26435;&#37325;&#20849;&#20139;&#32467;&#26500;&#20013;&#30340;&#19981;&#21516;&#23376;&#32593;&#30340;&#26597;&#35810;&#27969;&#12290;&#36825;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#25506;&#32034;&#22522;&#20110;&#23376;&#22270;&#30340;&#38745;&#24577;&#30828;&#20214;&#36719;&#20214;&#25512;&#29702;&#32852;&#21512;&#35774;&#35745;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing number of applications depend on Machine Learning (ML) functionality and benefits from both higher quality ML predictions and better timeliness (latency) at the same time. A growing body of research in computer architecture, ML, and systems software literature focuses on reaching better latency-accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and energy, while preserving delivered accuracy. All of them, however, yield improvements for a single static point in the latency-accuracy tradeoff space. We make a case for applications that operate in dynamically changing deployment scenarios, where no single static point is optimal. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses (activates) different SubNets within this weight-shared construct. This creates an opportunity to expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17258</link><description>&lt;p&gt;
&#36973;&#21463;&#33510;&#38590;&#30340;&#28900;&#38754;&#21253;&#26426;
&lt;/p&gt;
&lt;p&gt;
Suffering Toasters. (arXiv:2306.17258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#65292;&#26234;&#33021;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#23450;&#20041;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#30001;&#20110;&#25105;&#20204;&#23545;AI&#33539;&#24335;&#12289;&#26550;&#26500;&#21644;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#33258;&#28982;&#20135;&#29983;&#30340;AI&#24847;&#35782;&#27604;&#20197;&#24448;&#26356;&#26377;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22768;&#31216;&#25152;&#26377;&#24403;&#21069;&#30340;&#26234;&#33021;&#27979;&#35797;&#37117;&#19981;&#36275;&#20197;&#25351;&#20986;&#23384;&#22312;&#25110;&#32570;&#20047;&#35937;&#20154;&#31867;&#30452;&#35273;&#24863;&#30693;&#30340;&#26234;&#33021;&#12290;&#25105;&#20204;&#20511;&#37492;&#31185;&#23398;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#30340;&#26356;&#28165;&#26224;&#23450;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#30340;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#21746;&#23398;&#38382;&#39064;&#36824;&#26159;&#23454;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A widely accepted definition of intelligence in the context of Artificial Intelligence (AI) still eludes us. Due to our exceedingly rapid development of AI paradigms, architectures, and tools, the prospect of naturally arising AI consciousness seems more likely than ever. In this paper, we claim that all current intelligence tests are insufficient to point to the existence or lack of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas in the philosophy of science, psychology, and other areas of research to provide a clearer definition of the problems of artificial intelligence, self-awareness, and agency. We furthermore propose a new heuristic approach to test for artificial self-awareness and outline a possible implementation. Finally, we discuss some of the questions that arise from this new heuristic, be they philosophical or implementation-oriented.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#39044;&#27979;COVID-19&#24739;&#32773;&#20986;&#38498;&#21518;&#22312;&#24613;&#35786;&#23460;&#30340;&#20877;&#35775;&#24773;&#20917;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21161;&#20110;&#21307;&#29983;&#19987;&#27880;&#20110;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.17257</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#24613;&#35786;&#23460;&#20877;&#35775;
&lt;/p&gt;
&lt;p&gt;
Prediction of COVID-19 Patients' Emergency Room Revisit using Multi-Source Transfer Learning. (arXiv:2306.17257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#39044;&#27979;COVID-19&#24739;&#32773;&#20986;&#38498;&#21518;&#22312;&#24613;&#35786;&#23460;&#30340;&#20877;&#35775;&#24773;&#20917;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21161;&#20110;&#21307;&#29983;&#19987;&#27880;&#20110;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#20896;&#29366;&#30149;&#27602;&#30149;&#65288;COVID-19&#65289;&#23548;&#33268;&#20102;&#19968;&#22330;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#22823;&#27969;&#34892;&#12290;&#38500;&#20102;&#20855;&#26377;&#39640;&#20256;&#26579;&#24615;&#22806;&#65292;COVID-19&#30340;&#20020;&#24202;&#36827;&#23637;&#21487;&#20197;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#20174;&#26080;&#30151;&#29366;&#25658;&#24102;&#32773;&#21040;&#20005;&#37325;&#19988;&#28508;&#22312;&#21361;&#21450;&#29983;&#21629;&#30340;&#20581;&#24247;&#24182;&#21457;&#30151;&#12290;&#35768;&#22810;&#24739;&#32773;&#22312;&#20986;&#38498;&#21518;&#30340;&#30701;&#26102;&#38388;&#20869;&#38656;&#35201;&#20877;&#27425;&#23601;&#35786;&#24613;&#35786;&#23460;&#65288;ER&#65289;&#65292;&#36825;&#26497;&#22823;&#22686;&#21152;&#20102;&#21307;&#21153;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#21450;&#26089;&#35782;&#21035;&#27492;&#31867;&#24739;&#32773;&#23545;&#20110;&#24110;&#21161;&#21307;&#29983;&#19987;&#27880;&#20110;&#27835;&#30103;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;2020&#24180;3&#26376;&#33267;2021&#24180;1&#26376;&#26399;&#38388;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#23398;&#20013;&#24515;13&#20010;&#38468;&#23646;&#24613;&#35786;&#23460;&#30340;3,210&#20010;&#24739;&#32773;&#23601;&#35786;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;ScispaCy&#25552;&#21462;&#20020;&#24202;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;&#20986;&#29616;&#26368;&#39057;&#32321;&#30340;1001&#20010;&#27010;&#24565;&#20026;COVID-19&#24739;&#32773;&#22312;&#24613;&#35786;&#23460;&#20013;&#24314;&#31435;&#20102;7&#22825;&#20877;&#35775;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;13&#20010;&#24613;&#35786;&#23460;&#25910;&#38598;&#30340;&#30740;&#31350;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
The coronavirus disease 2019 (COVID-19) has led to a global pandemic of significant severity. In addition to its high level of contagiousness, COVID-19 can have a heterogeneous clinical course, ranging from asymptomatic carriers to severe and potentially life-threatening health complications. Many patients have to revisit the emergency room (ER) within a short time after discharge, which significantly increases the workload for medical staff. Early identification of such patients is crucial for helping physicians focus on treating life-threatening cases. In this study, we obtained Electronic Health Records (EHRs) of 3,210 encounters from 13 affiliated ERs within the University of Pittsburgh Medical Center between March 2020 and January 2021. We leveraged a Natural Language Processing technique, ScispaCy, to extract clinical concepts and used the 1001 most frequent concepts to develop 7-day revisit models for COVID-19 patients in ERs. The research data we collected from 13 ERs may have 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#23610;&#24230;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#30456;&#26426;&#21442;&#25968;&#30340;&#20219;&#24847;&#27979;&#35797;&#22270;&#20687;&#20013;&#39044;&#27979;&#24230;&#37327;&#23610;&#24230;&#30340;ZeroDepth&#26694;&#26550;&#65292;&#36890;&#36807;&#36755;&#20837;&#32423;&#20960;&#20309;&#23884;&#20837;&#21644;&#21464;&#20998;&#28508;&#22312;&#34920;&#31034;&#23454;&#29616;&#20102;&#23610;&#24230;&#20808;&#39564;&#30340;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#38454;&#27573;&#30340;&#35299;&#32806;&#65292;&#22312;&#23460;&#20869;&#21644;&#23460;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17253</link><description>&lt;p&gt;
&#38646;&#23556;&#20987;&#23610;&#24230;&#24863;&#30693;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Zero-Shot Scale-Aware Monocular Depth Estimation. (arXiv:2306.17253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17253
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#23610;&#24230;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#30456;&#26426;&#21442;&#25968;&#30340;&#20219;&#24847;&#27979;&#35797;&#22270;&#20687;&#20013;&#39044;&#27979;&#24230;&#37327;&#23610;&#24230;&#30340;ZeroDepth&#26694;&#26550;&#65292;&#36890;&#36807;&#36755;&#20837;&#32423;&#20960;&#20309;&#23884;&#20837;&#21644;&#21464;&#20998;&#28508;&#22312;&#34920;&#31034;&#23454;&#29616;&#20102;&#23610;&#24230;&#20808;&#39564;&#30340;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#38454;&#27573;&#30340;&#35299;&#32806;&#65292;&#22312;&#23460;&#20869;&#21644;&#23460;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#23384;&#22312;&#23610;&#24230;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#23610;&#24230;&#30417;&#30563;&#26469;&#20135;&#29983;&#24230;&#37327;&#39044;&#27979;&#12290;&#21363;&#20415;&#22914;&#27492;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#23558;&#26159;&#20960;&#20309;&#29305;&#23450;&#30340;&#65292;&#23398;&#20064;&#21040;&#30340;&#23610;&#24230;&#26080;&#27861;&#30452;&#25509;&#36328;&#39046;&#22495;&#20256;&#36882;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#30456;&#23545;&#28145;&#24230;&#19978;&#65292;&#25918;&#24323;&#23610;&#24230;&#20197;&#25552;&#39640;&#38646;&#23556;&#20987;&#36716;&#31227;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ZeroDepth&#65292;&#19968;&#31181;&#26032;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#39046;&#22495;&#21644;&#30456;&#26426;&#21442;&#25968;&#30340;&#20219;&#24847;&#27979;&#35797;&#22270;&#20687;&#20013;&#39044;&#27979;&#24230;&#37327;&#23610;&#24230;&#12290;&#36825;&#26159;&#36890;&#36807;&#20004;&#20010;&#26041;&#38754;&#23454;&#29616;&#30340;&#65306;&#65288;i&#65289;&#20351;&#29992;&#36755;&#20837;&#32423;&#20960;&#20309;&#23884;&#20837;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#23545;&#35937;&#19978;&#30340;&#23610;&#24230;&#20808;&#39564;&#65307;&#65288;ii&#65289;&#36890;&#36807;&#21464;&#20998;&#28508;&#22312;&#34920;&#31034;&#26469;&#35299;&#32806;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#38454;&#27573;&#65292;&#35813;&#34920;&#31034;&#20197;&#21333;&#24103;&#20449;&#24687;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#23460;&#22806;&#65288;KITTI&#12289;DDAD&#12289;nuScenes&#65289;&#21644;&#23460;&#20869;&#65288;NYUv2&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;ZeroDepth&#65292;&#24182;&#22312;&#20004;&#31181;&#35774;&#32622;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monocular depth estimation is scale-ambiguous, and thus requires scale supervision to produce metric predictions. Even so, the resulting models will be geometry-specific, with learned scales that cannot be directly transferred across domains. Because of that, recent works focus instead on relative depth, eschewing scale in favor of improved up-to-scale zero-shot transfer. In this work we introduce ZeroDepth, a novel monocular depth estimation framework capable of predicting metric scale for arbitrary test images from different domains and camera parameters. This is achieved by (i) the use of input-level geometric embeddings that enable the network to learn a scale prior over objects; and (ii) decoupling the encoder and decoder stages, via a variational latent representation that is conditioned on single frame information. We evaluated ZeroDepth targeting both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, and achieved a new state-of-the-art in both settings using the sa
&lt;/p&gt;</description></item><item><title>TemperatureGAN&#26159;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20351;&#29992;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#25968;&#25454;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.17248</link><description>&lt;p&gt;
TemperatureGAN: &#21306;&#22495;&#22823;&#27668;&#28201;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures. (arXiv:2306.17248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17248
&lt;/p&gt;
&lt;p&gt;
TemperatureGAN&#26159;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20351;&#29992;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#25968;&#25454;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29983;&#25104;&#22120;&#23545;&#20110;&#20272;&#35745;&#27668;&#20505;&#23545;&#21508;&#20010;&#39046;&#22495;&#30340;&#24433;&#21709;&#38750;&#24120;&#26377;&#29992;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#27668;&#20505;&#39118;&#38505;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#33021;&#28304;&#31995;&#32479;&#65292;&#38656;&#35201;&#20934;&#30830;&#65288;&#19982;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#26377;&#32479;&#35745;&#30456;&#20284;&#24615;&#65289;&#12289;&#21487;&#38752;&#65288;&#19981;&#20135;&#29983;&#38169;&#35823;&#26679;&#26412;&#65289;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21271;&#32654;&#38470;&#22320;&#25968;&#25454;&#21516;&#21270;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;TemperatureGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#26376;&#20221;&#12289;&#20301;&#32622;&#21644;&#26102;&#38388;&#27573;&#20026;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#27599;&#23567;&#26102;&#20998;&#36776;&#29575;&#29983;&#25104;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#25351;&#26631;&#26469;&#34913;&#37327;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;TemperatureGAN&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#24050;&#30693;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic generators are useful for estimating climate impacts on various sectors. Projecting climate risk in various sectors, e.g. energy systems, requires generators that are accurate (statistical resemblance to ground-truth), reliable (do not produce erroneous examples), and efficient. Leveraging data from the North American Land Data Assimilation System, we introduce TemperatureGAN, a Generative Adversarial Network conditioned on months, locations, and time periods, to generate 2m above ground atmospheric temperatures at an hourly resolution. We propose evaluation methods and metrics to measure the quality of generated samples. We show that TemperatureGAN produces high-fidelity examples with good spatial representation and temporal dynamics consistent with known diurnal cycles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#23376;&#22270;&#32467;&#26500;&#21644;&#35789;&#27719;&#35774;&#35745;&#23545;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23376;&#22270;&#30340;&#20998;&#27573;&#26041;&#26696;Subcover&#65292;&#24182;&#36890;&#36807;&#20004;&#27493;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Subcover&#30340;&#25913;&#36827;&#23376;&#22270;&#37492;&#21035;&#33021;&#21147;&#20351;&#24471;FCD&#24471;&#20998;&#30456;&#23545;&#25552;&#39640;&#20102;30&#65285;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;Subcover&#26377;&#28508;&#21147;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#23545;&#25913;&#36827;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.17246</link><description>&lt;p&gt;
&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#30340;&#24418;&#24335;&#65292;&#22522;&#26412;&#27169;&#24335;&#23545;&#23398;&#20064;&#20998;&#23376;&#20998;&#24067;&#30340;&#24433;&#21709;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The power of motifs as inductive bias for learning molecular distributions. (arXiv:2306.17246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#23376;&#22270;&#32467;&#26500;&#21644;&#35789;&#27719;&#35774;&#35745;&#23545;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23376;&#22270;&#30340;&#20998;&#27573;&#26041;&#26696;Subcover&#65292;&#24182;&#36890;&#36807;&#20004;&#27493;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Subcover&#30340;&#25913;&#36827;&#23376;&#22270;&#37492;&#21035;&#33021;&#21147;&#20351;&#24471;FCD&#24471;&#20998;&#30456;&#23545;&#25552;&#39640;&#20102;30&#65285;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;Subcover&#26377;&#28508;&#21147;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#23545;&#25913;&#36827;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#26377;&#25928;&#25506;&#32034;&#24191;&#38420;&#30340;&#21270;&#23398;&#31354;&#38388;&#21644;&#20248;&#21270;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23427;&#21487;&#20197;&#20419;&#36827;&#26032;&#22411;&#27835;&#30103;&#20998;&#23376;&#30340;&#35774;&#35745;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23545;&#20110;&#23398;&#20064;&#23567;&#22270;&#20998;&#24067;&#30340;&#29305;&#23450;&#24402;&#32435;&#20559;&#35265;&#30340;&#22909;&#22788;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20197;&#23567;&#22411;&#33647;&#29289;&#20998;&#23376;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#23376;&#22270;&#32467;&#26500;&#21644;&#35789;&#27719;&#35774;&#35745;&#23545;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23376;&#22270;&#30340;&#20998;&#27573;&#26041;&#26696;Subcover&#65292;&#24182;&#36890;&#36807;&#20004;&#27493;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Subcover&#23545;&#21270;&#23398;&#24847;&#20041;&#19978;&#30340;&#23376;&#22270;&#30340;&#26356;&#22909;&#37492;&#21035;&#65292;&#20351;FCD&#24471;&#20998;&#30456;&#23545;&#25552;&#39640;&#20102;30&#65285;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;Subcover&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#25913;&#36827;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for molecules holds great potential for efficiently exploring the vast chemical space and thus streamlining the drug discovery process by facilitating the design of new therapeutic molecules. Deep generative models have shown promising results for molecule generation, but the benefits of specific inductive biases for learning distributions over small graphs are unclear. Our study aims to investigate the impact of subgraph structures and vocabulary design on distribution learning, using small drug molecules as a case study. To this end, we introduce Subcover, a new subgraph-based fragmentation scheme, and evaluate it through a two-step variational auto-encoder. Our results show that Subcover's improved identification of chemically meaningful subgraphs leads to a relative improvement of the FCD score by 30%, outperforming previous methods. Our findings highlight the potential of Subcover to enhance the performance and scalability of existing methods, contributing to the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#31181;&#22330;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#32467;&#21512;&#20102;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#31283;&#20581;&#22320;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17210</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scattering Spectra Models for Physics. (arXiv:2306.17210v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#31181;&#22330;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#32467;&#21512;&#20102;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#31283;&#20581;&#22320;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#23478;&#24120;&#24120;&#38656;&#35201;&#27010;&#29575;&#27169;&#22411;&#26469;&#36827;&#34892;&#21442;&#25968;&#25512;&#26029;&#25110;&#29983;&#25104;&#19968;&#20010;&#22330;&#30340;&#26032;&#23454;&#29616;&#12290;&#38024;&#23545;&#39640;&#24230;&#38750;&#39640;&#26031;&#22330;&#30340;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25955;&#23556;&#35889;&#27169;&#22411;&#29992;&#20110;&#24179;&#31283;&#22330;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29289;&#29702;&#23398;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#22330;&#30340;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#32479;&#35745;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#21363;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#12290;&#22312;&#20171;&#32461;&#21033;&#29992;&#26059;&#36716;&#21644;&#32553;&#25918;&#19979;&#22330;&#30340;&#35268;&#24459;&#24615;&#36827;&#34892;&#26377;&#29992;&#30340;&#32500;&#24230;&#32422;&#31616;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#22810;&#23610;&#24230;&#29289;&#29702;&#22330;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#21253;&#25324;&#22235;&#38454;&#31354;&#38388;&#30697;&#12290;&#36825;&#20123;&#25955;&#23556;&#35889;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#32500;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicists routinely need probabilistic models for a number of tasks such as parameter inference or the generation of new realizations of a field. Establishing such models for highly non-Gaussian fields is a challenge, especially when the number of samples is limited. In this paper, we introduce scattering spectra models for stationary fields and we show that they provide accurate and robust statistical descriptions of a wide range of fields encountered in physics. These models are based on covariances of scattering coefficients, i.e. wavelet decomposition of a field coupled with a point-wise modulus. After introducing useful dimension reductions taking advantage of the regularity of a field under rotation and scaling, we validate these models on various multi-scale physical fields and demonstrate that they reproduce standard statistics, including spatial moments up to 4th order. These scattering spectra provide us with a low-dimensional structured representation that captures key prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38477;&#32500;&#21644;&#32858;&#31867;&#22312;&#35266;&#27979;&#21040;&#30340;&#29615;&#22659;&#29366;&#24577;&#31354;&#38388;&#19978;&#35745;&#31639;&#19968;&#20010;&#25277;&#35937;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807; passvie automata learning &#23398;&#20064;&#21040;&#22522;&#20110;&#35266;&#27979;&#21040;&#30340; agent &#21644;&#29615;&#22659;&#20132;&#20114;&#30340;&#38543;&#26426;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.17204</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#38543;&#26426;&#21160;&#24577;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Environment Models with Continuous Stochastic Dynamics. (arXiv:2306.17204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38477;&#32500;&#21644;&#32858;&#31867;&#22312;&#35266;&#27979;&#21040;&#30340;&#29615;&#22659;&#29366;&#24577;&#31354;&#38388;&#19978;&#35745;&#31639;&#19968;&#20010;&#25277;&#35937;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807; passvie automata learning &#23398;&#20064;&#21040;&#22522;&#20110;&#35266;&#27979;&#21040;&#30340; agent &#21644;&#29615;&#22659;&#20132;&#20114;&#30340;&#38543;&#26426;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#34892;&#20026;&#30340;&#33258;&#21160;&#26426;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#25511;&#21046;&#20219;&#21153;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#25511;&#21046;&#38382;&#39064;&#26469;&#35828;&#65292;&#33258;&#21160;&#26426;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#20197;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#33258;&#21160;&#26426;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#21040;&#20855;&#26377;&#22797;&#26434;&#21644;&#36830;&#32493;&#21160;&#24577;&#30340;&#29615;&#22659;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving control tasks in complex environments automatically through learning offers great potential. While contemporary techniques from deep reinforcement learning (DRL) provide effective solutions, their decision-making is not transparent. We aim to provide insights into the decisions faced by the agent by learning an automaton model of environmental behavior under the control of an agent. However, for most control problems, automata learning is not scalable enough to learn a useful model. In this work, we raise the capabilities of automata learning such that it is possible to learn models for environments that have complex and continuous dynamics.  The core of the scalability of our method lies in the computation of an abstract state-space representation, by applying dimensionality reduction and clustering on the observed environmental state space. The stochastic transitions are learned via passive automata learning from observed interactions of the agent and the environment. In an i
&lt;/p&gt;</description></item><item><title>Diff-Foley&#26159;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#21516;&#27493;&#30340;&#35270;&#39057;&#21040;&#38899;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24335;&#38899;&#35270;&#39057;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#22312;&#22768;&#35889;&#22270;&#28508;&#22312;&#31354;&#38388;&#19978;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#21516;&#27493;&#24615;&#21644;&#38899;&#35270;&#39057;&#20851;&#32852;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17203</link><description>&lt;p&gt;
Diff-Foley: &#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#21516;&#27493;&#30340;&#35270;&#39057;&#21040;&#38899;&#39057;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models. (arXiv:2306.17203v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17203
&lt;/p&gt;
&lt;p&gt;
Diff-Foley&#26159;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#21516;&#27493;&#30340;&#35270;&#39057;&#21040;&#38899;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24335;&#38899;&#35270;&#39057;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#22312;&#22768;&#35889;&#22270;&#28508;&#22312;&#31354;&#38388;&#19978;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#21516;&#27493;&#24615;&#21644;&#38899;&#35270;&#39057;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21040;&#38899;&#39057;&#65288;V2A&#65289;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#22312;&#30452;&#25509;&#20174;&#26080;&#22768;&#35270;&#39057;&#29983;&#25104;&#38899;&#39057;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#39057;/&#30005;&#24433;&#21046;&#20316;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;V2A&#26041;&#27861;&#22312;&#26102;&#38388;&#21516;&#27493;&#21644;&#38899;&#35270;&#39057;&#20851;&#32852;&#26041;&#38754;&#23384;&#22312;&#29983;&#25104;&#36136;&#37327;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-Foley&#65292;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#30340;&#21516;&#27493;&#35270;&#39057;&#21040;&#38899;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30340;&#21516;&#27493;&#24615;&#21644;&#38899;&#35270;&#39057;&#20851;&#32852;&#24615;&#30340;&#39640;&#36136;&#37327;&#38899;&#39057;&#12290;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#24335;&#38899;&#35270;&#39057;&#39044;&#35757;&#32451;&#65288;CAVP&#65289;&#26469;&#23398;&#20064;&#26356;&#20855;&#26102;&#38388;&#21644;&#35821;&#20041;&#23545;&#40784;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#22312;&#22768;&#35889;&#22270;&#28508;&#22312;&#31354;&#38388;&#19978;&#20351;&#29992;&#32463;CAVP&#23545;&#40784;&#30340;&#35270;&#35273;&#29305;&#24449;&#35757;&#32451;LDM&#12290;CAVP&#23545;&#40784;&#30340;&#29305;&#24449;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20351;LDM&#33021;&#22815;&#25429;&#25417;&#21040;&#26356;&#24494;&#22937;&#30340;&#38899;&#35270;&#39057;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#8220;&#21452;&#37325;&#24341;&#23548;&#8221;&#36827;&#19968;&#27493;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;Diff-Foley&#22312;&#24403;&#21069;&#22823;&#35268;&#27169;V2A&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;V2A&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonst
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32972;&#26223;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#22522;&#22240;&#34920;&#36798;&#20998;&#31867;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#20110;&#19981;&#21253;&#21547;&#32972;&#26223;&#29983;&#29289;&#32593;&#32476;&#20449;&#24687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30284;&#30151;&#31867;&#22411;&#30340;&#20998;&#31867;&#20013;&#37117;&#21462;&#24471;&#20102;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2306.17202</link><description>&lt;p&gt;
&#19968;&#31181;&#23558;&#32972;&#26223;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#22522;&#22240;&#34920;&#36798;&#20998;&#31867;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65306;&#22312;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An end-to-end framework for gene expression classification by integrating a background knowledge graph: application to cancer prognosis prediction. (arXiv:2306.17202v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32972;&#26223;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#22522;&#22240;&#34920;&#36798;&#20998;&#31867;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#20110;&#19981;&#21253;&#21547;&#32972;&#26223;&#29983;&#29289;&#32593;&#32476;&#20449;&#24687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30284;&#30151;&#31867;&#22411;&#30340;&#20998;&#31867;&#20013;&#37117;&#21462;&#24471;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#25968;&#25454;&#21487;&#20197;&#20998;&#20026;&#21407;&#22987;&#25968;&#25454;&#65288;&#22914;&#22522;&#22240;&#34920;&#36798;&#65289;&#21644;&#20108;&#32423;&#25968;&#25454;&#65288;&#22914;&#36890;&#36335;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65289;&#12290;&#21033;&#29992;&#20108;&#32423;&#25968;&#25454;&#26469;&#22686;&#24378;&#23545;&#21407;&#22987;&#25968;&#25454;&#30340;&#20998;&#26512;&#30340;&#26041;&#27861;&#24456;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#20108;&#32423;&#25968;&#25454;&#21253;&#21547;&#20102;&#21407;&#22987;&#25968;&#25454;&#20013;&#19981;&#21253;&#21547;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20840;&#38754;&#22788;&#29702;&#20108;&#32423;&#25968;&#25454;&#65292;&#20026;&#21407;&#22987;&#25968;&#25454;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#21644;&#29983;&#29289;&#32593;&#32476;&#30340;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#20013;&#12290;&#20132;&#21449;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#19981;&#21253;&#21547;&#32972;&#26223;&#29983;&#29289;&#32593;&#32476;&#20449;&#24687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#30284;&#30151;&#31867;&#22411;&#36827;&#34892;&#30340;&#30149;&#20154;&#20998;&#32452;&#23454;&#39564;&#26174;&#31034;&#20102;&#35768;&#22810;&#32452;&#30340;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#30340;&#25913;&#21892;&#12290;&#20934;&#30830;&#24615;&#36739;&#39640;&#30340;&#30284;&#30151;&#31867;&#22411;&#30340;&#21487;&#35270;&#21270;&#36890;&#36807;&#23500;&#38598;&#20998;&#26512;&#37492;&#23450;&#20102;&#36129;&#29486;&#22522;&#22240;&#21644;&#36890;&#36335;&#12290;&#24050;&#30693;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26032;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological data may be separated into primary data, such as gene expression, and secondary data, such as pathways and protein-protein interactions. Methods using secondary data to enhance the analysis of primary data are promising, because secondary data have background information that is not included in primary data. In this study, we proposed an end-to-end framework to integrally handle secondary data to construct a classification model for primary data. We applied this framework to cancer prognosis prediction using gene expression data and a biological network. Cross-validation results indicated that our model achieved higher accuracy compared with a deep neural network model without background biological network information. Experiments conducted in patient groups by cancer type showed improvement in ROC-area under the curve for many groups. Visualizations of high accuracy cancer types identified contributing genes and pathways by enrichment analysis. Known biomarkers and novel bi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#38745;&#33033;&#22270;&#26696;&#22686;&#24378;&#30340;&#27531;&#24046;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#24213;&#21521;&#19978;&#30340;&#37329;&#23383;&#22612;&#32467;&#26500;&#21644;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#26469;&#25552;&#39640;&#25351;&#38745;&#33033;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#24120;&#29992;&#30340;&#35782;&#21035;&#27969;&#31243;&#20013;&#33021;&#22815;&#20943;&#23569;&#39640;&#36798;5%&#30340;&#24179;&#22343;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17200</link><description>&lt;p&gt;
&#38024;&#23545;&#34880;&#31649;&#22270;&#26696;&#22686;&#24378;&#30340;&#27531;&#24046;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Residual Feature Pyramid Network for Enhancement of Vascular Patterns. (arXiv:2306.17200v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17200
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#38745;&#33033;&#22270;&#26696;&#22686;&#24378;&#30340;&#27531;&#24046;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#24213;&#21521;&#19978;&#30340;&#37329;&#23383;&#22612;&#32467;&#26500;&#21644;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#26469;&#25552;&#39640;&#25351;&#38745;&#33033;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#24120;&#29992;&#30340;&#35782;&#21035;&#27969;&#31243;&#20013;&#33021;&#22815;&#20943;&#23569;&#39640;&#36798;5%&#30340;&#24179;&#22343;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#34880;&#31649;&#21644;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#30340;&#20302;&#23545;&#27604;&#24230;&#21644;&#19981;&#22343;&#21248;&#24615;&#65292;&#25351;&#38745;&#33033;&#35782;&#21035;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#25439;&#23475;&#65292;&#24120;&#24120;&#23548;&#33268;&#34880;&#31649;&#22270;&#26696;&#30340;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#38745;&#33033;&#22686;&#24378;&#25216;&#26415;&#65292;ResFPN&#65288;&#27531;&#24046;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#19982;&#35782;&#21035;&#27969;&#31243;&#26080;&#20851;&#12290;&#20351;&#29992;&#26032;&#39062;&#30340;&#32467;&#26500;&#26816;&#27979;&#22359;&#65288;SDBlock&#65289;&#65292;&#36890;&#36807;&#33258;&#24213;&#21521;&#19978;&#30340;&#37329;&#23383;&#22612;&#32467;&#26500;&#25552;&#21462;&#19981;&#21516;&#23485;&#24230;&#30340;&#34880;&#31649;&#32467;&#26500;&#12290;&#21033;&#29992;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65288;FAM&#65289;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#34880;&#31649;&#32467;&#26500;&#32452;&#21512;&#36215;&#26469;&#65292;&#24182;&#35757;&#32451;&#25152;&#25552;&#20986;&#30340;ResFPN&#26469;&#26816;&#27979;&#19981;&#21516;&#23610;&#24230;&#19978;&#30340;&#34880;&#31649;&#12290;&#36890;&#36807;&#22686;&#24378;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#65292;&#24120;&#29992;&#30340;&#35782;&#21035;&#27969;&#31243;&#30340;&#24179;&#22343;&#35782;&#21035;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;&#39640;&#36798;5%&#12290;&#21363;&#20351;&#22312;&#20132;&#21449;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#29992;&#20110;&#35757;&#32451;ResFPN&#30340;&#25968;&#25454;&#38598;&#19982;&#29992;&#20110;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#36825;&#20123;&#25913;&#36827;&#20173;&#28982;&#25345;&#32493;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of finger vein recognition systems gets degraded due to low and uneven contrast between veins and surroundings, often resulting in poor detection of vein patterns. We propose a finger-vein enhancement technique, ResFPN (Residual Feature Pyramid Network), as a generic preprocessing method agnostic to the recognition pipeline. A bottom-up pyramidal architecture using the novel Structure Detection block (SDBlock) facilitates extraction of veins of varied widths. Using a feature aggregation module (FAM), we combine these vein-structures, and train the proposed ResFPN for detection of veins across scales. With enhanced presentations, our experiments indicate a reduction upto 5% in the average recognition errors for commonly used recognition pipeline over two publicly available datasets. These improvements are persistent even in cross-dataset scenario where the dataset used to train the ResFPN is different from the one used for recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#36741;&#21161;&#22270;&#20687;&#26469;&#25351;&#23548;&#22810;&#27874;&#27573;&#25104;&#20687;&#36870;&#38382;&#39064;&#30340;&#31354;&#38388;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#25552;&#21319;&#37325;&#24314;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17197</link><description>&lt;p&gt;
&#24341;&#23548;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#22810;&#27874;&#27573;&#25104;&#20687;&#36870;&#38382;&#39064;&#30340;&#31354;&#38388;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Guided Deep Generative Model-based Spatial Regularization for Multiband Imaging Inverse Problems. (arXiv:2306.17197v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#36741;&#21161;&#22270;&#20687;&#26469;&#25351;&#23548;&#22810;&#27874;&#27573;&#25104;&#20687;&#36870;&#38382;&#39064;&#30340;&#31354;&#38388;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#25552;&#21319;&#37325;&#24314;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27874;&#27573;&#25104;&#20687;&#20013;&#37319;&#29992;&#27169;&#22411;&#21270;&#20844;&#24335;&#65292;&#35299;&#20915;&#36870;&#38382;&#39064;&#38656;&#35201;&#23450;&#20041;&#31354;&#38388;&#21644;&#35889;&#27491;&#21017;&#21270;&#12290;&#22823;&#37096;&#20998;&#25991;&#29486;&#20013;&#65292;&#35889;&#20449;&#24687;&#30452;&#25509;&#20174;&#35266;&#23519;&#20013;&#25552;&#21462;&#65292;&#20197;&#24471;&#21040;&#25968;&#25454;&#39537;&#21160;&#30340;&#35889;&#20808;&#39564;&#12290;&#30456;&#21453;&#65292;&#31354;&#38388;&#27491;&#21017;&#21270;&#30340;&#36873;&#25321;&#36890;&#24120;&#24402;&#32467;&#20026;&#20351;&#29992;&#20256;&#32479;&#24809;&#32602;&#39033;&#65288;&#22914;&#24635;&#21464;&#24046;&#65289;&#65292;&#20197;&#20419;&#36827;&#37325;&#24314;&#22270;&#20687;&#30340;&#39044;&#26399;&#29305;&#24449;&#65288;&#22914;&#20998;&#27573;&#24120;&#25968;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#36741;&#21161;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#33719;&#21462;&#30340;&#25968;&#25454;&#26469;&#24471;&#21040;&#23450;&#21046;&#30340;&#25968;&#25454;&#39537;&#21160;&#31354;&#38388;&#27491;&#21017;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27491;&#21017;&#21270;&#34987;&#26500;&#24605;&#20026;&#19968;&#20010;&#33021;&#22815;&#32534;&#30721;&#36741;&#21161;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#31354;&#38388;&#35821;&#20041;&#29305;&#24449;&#30340;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
When adopting a model-based formulation, solving inverse problems encountered in multiband imaging requires to define spatial and spectral regularizations. In most of the works of the literature, spectral information is extracted from the observations directly to derive data-driven spectral priors. Conversely, the choice of the spatial regularization often boils down to the use of conventional penalizations (e.g., total variation) promoting expected features of the reconstructed image (e.g., piecewise constant). In this work, we propose a generic framework able to capitalize on an auxiliary acquisition of high spatial resolution to derive tailored data-driven spatial regularizations. This approach leverages on the ability of deep learning to extract high level features. More precisely, the regularization is conceived as a deep generative network able to encode spatial semantic features contained in this auxiliary image of high spatial resolution. To illustrate the versatility of this a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17194</link><description>&lt;p&gt;
&#20851;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#21487;&#21033;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25163;&#22914;&#20309;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#27880;&#20837;&#29305;&#23450;&#30340;&#25351;&#20196;&#36319;&#38543;&#31034;&#20363;&#26469;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#65292;&#20174;&#32780;&#26377;&#24847;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#27880;&#20837;&#25552;&#21450;&#30446;&#26631;&#20869;&#23481;&#30340;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#24341;&#35825;&#19979;&#28216;&#27169;&#22411;&#23637;&#31034;&#27492;&#31867;&#34892;&#20026;&#26469;&#23454;&#29616;&#20869;&#23481;&#27880;&#20837;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AutoPoison&#12290;&#23427;&#20351;&#29992;&#20102;&#19968;&#20010;&#39044;&#35328;&#27169;&#22411;&#26469;&#23558;&#22810;&#26679;&#25915;&#20987;&#30446;&#26631;&#33258;&#28982;&#32780;&#36830;&#36143;&#22320;&#27880;&#20837;&#21040;&#27602;&#21270;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#25915;&#20987;&#65306;&#20869;&#23481;&#27880;&#20837;&#21644;&#36807;&#24230;&#25298;&#32477;&#25915;&#20987;&#65292;&#27599;&#20010;&#25915;&#20987;&#37117;&#26088;&#22312;&#35825;&#23548;&#29305;&#23450;&#30340;&#21487;&#21033;&#29992;&#34892;&#20026;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#27880;&#20837;&#26041;&#26696;&#30340;&#24378;&#24230;&#21644;&#38544;&#34109;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#27602;&#21270;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;AutoPoison&#20801;&#35768;&#23545;&#25163;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#32467;&#26524;&#26159;&#21542;&#26222;&#36866;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27880;&#20837;&#35821;&#20041;&#20445;&#25345;&#30340;&#26356;&#25913;&#26469;&#25193;&#22823;&#27979;&#35797;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#20934;&#30830;&#29575;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#20351;&#29992;&#20102;&#19968;&#20123;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#24674;&#22797;&#21040;&#20043;&#21069;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.17193</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Limits of Machine Learning for Automatic Vulnerability Detection. (arXiv:2306.17193v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17193
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#32467;&#26524;&#26159;&#21542;&#26222;&#36866;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27880;&#20837;&#35821;&#20041;&#20445;&#25345;&#30340;&#26356;&#25913;&#26469;&#25193;&#22823;&#27979;&#35797;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#20934;&#30830;&#29575;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#20351;&#29992;&#20102;&#19968;&#20123;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#24674;&#22797;&#21040;&#20043;&#21069;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#32467;&#26524;&#38750;&#24120;&#26377;&#24076;&#26395;&#65306;&#20165;&#32473;&#23450;&#20989;&#25968;$f$&#30340;&#28304;&#20195;&#30721;&#65292;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#39640;&#36798;70%&#30340;&#20934;&#30830;&#29575;&#21028;&#26029;$f$&#26159;&#21542;&#23384;&#22312;&#23433;&#20840;&#28431;&#27934;&#12290;&#20294;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#26222;&#36866;&#65292;&#32780;&#19981;&#20165;&#38480;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#27880;&#20837;&#35821;&#20041;&#20445;&#25345;&#30340;&#26356;&#25913;&#26469;&#25193;&#22823;&#27979;&#35797;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#19979;&#38477;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#20351;&#29992;&#20102;&#19968;&#20123;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#22686;&#21152;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#22312;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#26524;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#24674;&#22797;&#21040;&#20043;&#21069;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#22797;&#21046;&#24182;&#32487;&#32493;&#20102;&#36825;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function $f$, models trained by machine learning techniques can decide if $f$ contains a security flaw with up to 70% accuracy.  But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model's accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels.  In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;SHAP&#26041;&#27861;&#23454;&#29616;&#23545;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#20998;&#31867;&#21644;&#35299;&#37322;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.17190</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;Shapley Additive Explanation (SHAP) &#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;(DDoS)&#25915;&#20987;&#26816;&#27979;&#30340;&#20998;&#31867;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Classification and Explanation of Distributed Denial-of-Service (DDoS) Attack Detection using Machine Learning and Shapley Additive Explanation (SHAP) Methods. (arXiv:2306.17190v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;SHAP&#26041;&#27861;&#23454;&#29616;&#23545;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#20998;&#31867;&#21644;&#35299;&#37322;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DDoS&#25915;&#20987;&#28041;&#21450;&#20351;&#29992;&#22823;&#37327;&#26469;&#33258;&#22810;&#20010;&#28304;&#30340;&#35831;&#27714;&#25110;&#27969;&#37327;&#26469;&#28153;&#27809;&#30446;&#26631;&#31995;&#32479;&#65292;&#20174;&#32780;&#24178;&#25200;&#30446;&#26631;&#26381;&#21153;&#22120;&#12289;&#26381;&#21153;&#25110;&#32593;&#32476;&#30340;&#27491;&#24120;&#27969;&#37327;&#12290;&#21306;&#20998;&#21512;&#27861;&#27969;&#37327;&#21644;&#24694;&#24847;&#27969;&#37327;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#32593;&#32476;&#27969;&#37327;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20294;&#23454;&#26045;&#27169;&#22411;&#35299;&#37322;&#20197;&#23545;&#27969;&#37327;&#27969;&#36827;&#34892;&#20998;&#31867;&#65288;&#26159;&#33391;&#24615;&#30340;&#36824;&#26159;&#24694;&#24847;&#30340;&#65289;&#26159;&#22686;&#21152;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#21487;&#20197;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24110;&#21161;&#23545;DDoS&#27969;&#37327;&#36827;&#34892;&#20998;&#31867;&#21644;&#35782;&#21035;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#19981;&#20165;&#23545;DDoS&#25915;&#20987;&#30340;&#21512;&#27861;&#27969;&#37327;&#21644;&#24694;&#24847;&#27969;&#37327;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;SHAP&#26041;&#27861;&#35299;&#37322;&#20998;&#31867;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
DDoS attacks involve overwhelming a target system with a large number of requests or traffic from multiple sources, disrupting the normal traffic of a targeted server, service, or network. Distinguishing between legitimate traffic and malicious traffic is a challenging task. It is possible to classify legitimate traffic and malicious traffic and analysis the network traffic by using machine learning and deep learning techniques. However, an inter-model explanation implemented to classify a traffic flow whether is benign or malicious is an important investigation of the inner working theory of the model to increase the trustworthiness of the model. Explainable Artificial Intelligence (XAI) can explain the decision-making of the machine learning models that can be classified and identify DDoS traffic. In this context, we proposed a framework that can not only classify legitimate traffic and malicious traffic of DDoS attacks but also use SHAP to explain the decision-making of the classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20960;&#20010;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#38750;&#24120;&#39640;&#65292;&#19988;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38376;&#38480;&#65292;&#38376;&#38480;&#20043;&#21518;&#27169;&#22411;&#24615;&#33021;&#36805;&#36895;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2306.17189</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Steganographic Capacity of Deep Learning Models. (arXiv:2306.17189v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20960;&#20010;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#38750;&#24120;&#39640;&#65292;&#19988;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38376;&#38480;&#65292;&#38376;&#38480;&#20043;&#21518;&#27169;&#22411;&#24615;&#33021;&#36805;&#36895;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26222;&#36941;&#24212;&#29992;&#65292;&#24517;&#28982;&#20250;&#26377;&#20154;&#35797;&#22270;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#31181;&#22522;&#20110;&#38544;&#20889;&#30340;&#25915;&#20987;&#20013;&#65292;&#20449;&#24687;&#21487;&#20197;&#34987;&#38544;&#34255;&#22312;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#28982;&#21518;&#29992;&#20110;&#20998;&#21457;&#24694;&#24847;&#36719;&#20214;&#25110;&#20854;&#20182;&#24694;&#24847;&#30446;&#30340;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20960;&#20010;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#38382;&#39064;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#27169;&#22411;&#12290;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#24471;&#21040;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#19981;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25913;&#21464;&#30340;&#35757;&#32451;&#21442;&#25968;&#30340;&#20302;&#20301;&#27604;&#29305;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34987;&#27979;&#35797;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#20986;&#20046;&#24847;&#26009;&#22320;&#39640;&#65292;&#24182;&#19988;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38376;&#38480;&#65292;&#38376;&#38480;&#20043;&#21518;&#27169;&#22411;&#24615;&#33021;&#36805;&#36895;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning and deep learning models become ubiquitous, it is inevitable that there will be attempts to exploit such models in various attack scenarios. For example, in a steganographic-based attack, information could be hidden in a learning model, which might then be used to distribute malware, or for other malicious purposes. In this research, we consider the steganographic capacity of several learning models. Specifically, we train a Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Transformer model on a challenging malware classification problem. For each of the resulting models, we determine the number of low-order bits of the trained parameters that can be altered without significantly affecting the performance of the model. We find that the steganographic capacity of the learning models tested is surprisingly high, and that in each case, there is a clear threshold after which model performance rapidly degrades.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2306.17184</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#38382;&#39064;&#65311;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26041;&#38754;&#35777;&#26126;&#20102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#20851;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#25104;&#21151;&#30340;&#20005;&#26684;&#29702;&#35770;&#35299;&#37322;&#23578;&#26410;&#34987;&#25552;&#20986;&#65292;&#22240;&#20026;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#25511;&#21046;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#30340;&#32452;&#21512;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21487;&#20197;&#29992;&#26469;&#27169;&#25311;&#33521;&#35821;&#21477;&#23376;&#30340;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#19988;&#38169;&#35823;&#29575;&#20026;&#38646;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#31361;&#20986;&#20102;&#23884;&#20837;&#23618;&#21644;&#20840;&#36830;&#25509;&#32452;&#20214;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has revolutionized the field of natural language processing, with neural language models proving to be very effective for next-word prediction. However, a rigorous theoretical explanation for their success in the context of formal language theory has not yet been developed, as it is unclear why neural language models can learn the combinatorial rules that govern the next-word prediction task. In this paper, we study a class of formal languages that can be used to model real-world examples of English sentences. We construct neural language models can solve the next-word prediction task in this context with zero error. Our proof highlights the different roles of the embedding layer and the fully connected component within the neural language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28404;&#32423;&#25968;&#25454;&#21644;&#21608;&#26399;&#39044;&#27979;&#20449;&#21495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#30340;&#39640;&#39057;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#39118;&#38505;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.17179</link><description>&lt;p&gt;
&#39640;&#39057;&#24066;&#22330;&#20570;&#24066;&#30340;&#25972;&#21512;&#28404;&#31574;&#30053;&#21644;&#21608;&#26399;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Integrating Tick-level Data and Periodical Signal for High-frequency Market Making. (arXiv:2306.17179v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28404;&#32423;&#25968;&#25454;&#21644;&#21608;&#26399;&#39044;&#27979;&#20449;&#21495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#30340;&#39640;&#39057;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#39118;&#38505;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#39640;&#39057;&#20132;&#26131;&#20013;&#30340;&#24066;&#22330;&#20570;&#24066;&#38382;&#39064;&#12290;&#24066;&#22330;&#20570;&#24066;&#26159;&#37329;&#34701;&#24066;&#22330;&#20013;&#25552;&#20379;&#27969;&#21160;&#24615;&#30340;&#20851;&#38190;&#21151;&#33021;&#65292;&#28041;&#21450;&#36890;&#36807;&#20080;&#21334;&#36164;&#20135;&#25552;&#20379;&#27969;&#21160;&#24615;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#24066;&#22330;&#30340;&#26085;&#30410;&#22797;&#26434;&#21270;&#21644;&#28404;&#32423;&#20132;&#26131;&#25152;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#20351;&#24471;&#24320;&#21457;&#26377;&#25928;&#30340;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28404;&#32423;&#25968;&#25454;&#19982;&#21608;&#26399;&#39044;&#27979;&#20449;&#21495;&#34701;&#21512;&#65292;&#20197;&#24320;&#21457;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#30340;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#12290;&#25105;&#20204;&#22522;&#20110;&#19981;&#21516;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27169;&#25311;&#22330;&#26223;&#21644;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24471;&#21040;&#30340;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#39118;&#38505;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of market making in high-frequency trading. Market making is a critical function in financial markets that involves providing liquidity by buying and selling assets. However, the increasing complexity of financial markets and the high volume of data generated by tick-level trading makes it challenging to develop effective market making strategies. To address this challenge, we propose a deep reinforcement learning approach that fuses tick-level data with periodic prediction signals to develop a more accurate and robust market making strategy. Our results of market making strategies based on different deep reinforcement learning algorithms under the simulation scenarios and real data experiments in the cryptocurrency markets show that the proposed framework outperforms existing methods in terms of profitability and risk management.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26368;&#20248;&#20132;&#26131;&#25191;&#34892;&#30340;&#38382;&#39064;&#65292;&#22312;&#32771;&#34385;&#22810;&#20010;&#20132;&#26131;&#25152;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#40784;&#25968;&#25454;&#25552;&#21462;&#20132;&#21449;&#20132;&#26131;&#25152;&#20449;&#21495;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#23545;&#26368;&#20248;&#25191;&#34892;&#36807;&#31243;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17178</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26368;&#20248;&#20132;&#26131;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Optimal Execution Using Reinforcement Learning. (arXiv:2306.17178v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26368;&#20248;&#20132;&#26131;&#25191;&#34892;&#30340;&#38382;&#39064;&#65292;&#22312;&#32771;&#34385;&#22810;&#20010;&#20132;&#26131;&#25152;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#40784;&#25968;&#25454;&#25552;&#21462;&#20132;&#21449;&#20132;&#26131;&#25152;&#20449;&#21495;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#23545;&#26368;&#20248;&#25191;&#34892;&#36807;&#31243;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#20110;&#26368;&#20248;&#35746;&#21333;&#25191;&#34892;&#65292;&#21363;&#22823;&#35746;&#21333;&#34987;&#20998;&#25104;&#22810;&#20010;&#23567;&#35746;&#21333;&#20197;&#26368;&#22823;&#21270;&#25191;&#34892;&#19981;&#36275;&#12290;&#22522;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25152;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#23545;&#22810;&#20010;&#20132;&#26131;&#25152;&#30340;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#26469;&#25552;&#21462;&#20132;&#21449;&#20132;&#26131;&#25152;&#20449;&#21495;&#12290;&#19982;&#22823;&#22810;&#25968;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#21333;&#19968;&#20132;&#26131;&#25152;&#20449;&#24687;&#19981;&#21516;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20132;&#21449;&#20132;&#26131;&#25152;&#20449;&#21495;&#23545;&#26368;&#20248;&#25191;&#34892;&#38382;&#39064;&#20013;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20132;&#21449;&#20132;&#26131;&#25152;&#20449;&#21495;&#21487;&#20197;&#20026;&#21152;&#23494;&#36135;&#24065;&#30340;&#26368;&#20248;&#25191;&#34892;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#20197;&#20419;&#36827;&#26368;&#20248;&#25191;&#34892;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is about optimal order execution, where a large order is split into several small orders to maximize the implementation shortfall. Based on the diversity of cryptocurrency exchanges, we attempt to extract cross-exchange signals by aligning data from multiple exchanges for the first time. Unlike most previous studies that focused on using single-exchange information, we discuss the impact of cross-exchange signals on the agent's decision-making in the optimal execution problem. Experimental results show that cross-exchange signals can provide additional information for the optimal execution of cryptocurrency to facilitate the optimal execution process.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#22120;&#30340;&#20225;&#19994;&#30913;&#30424;&#39537;&#21160;&#21047;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#38656;&#35201;&#21047;&#26032;&#30340;&#30913;&#30424;&#65292;&#24182;&#25552;&#21069;&#39044;&#27979;&#20854;&#20581;&#24247;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#21487;&#38752;&#24615;&#21644;&#21151;&#29575;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17169</link><description>&lt;p&gt;
&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#22120;&#30340;&#20225;&#19994;&#30913;&#30424;&#39537;&#21160;&#21047;&#26032;
&lt;/p&gt;
&lt;p&gt;
Enterprise Disk Drive Scrubbing Based on Mondrian Conformal Predictors. (arXiv:2306.17169v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#22120;&#30340;&#20225;&#19994;&#30913;&#30424;&#39537;&#21160;&#21047;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#38656;&#35201;&#21047;&#26032;&#30340;&#30913;&#30424;&#65292;&#24182;&#25552;&#21069;&#39044;&#27979;&#20854;&#20581;&#24247;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#21487;&#38752;&#24615;&#21644;&#21151;&#29575;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#30424;&#21047;&#26032;&#26159;&#19968;&#31181;&#36890;&#36807;&#20174;&#30913;&#30424;&#35835;&#21462;&#25968;&#25454;&#26469;&#35299;&#20915;&#35835;&#38169;&#35823;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#19968;&#27425;&#24615;&#21047;&#26032;&#25972;&#20010;&#23384;&#20648;&#25968;&#32452;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#36755;&#20837;/&#36755;&#20986;&#25805;&#20316;&#26399;&#38388;&#12290;&#27492;&#22806;&#65292;&#21047;&#26032;&#26102;&#36830;&#32493;&#20174;&#30913;&#30424;&#35835;&#21462;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#30913;&#30424;&#30340;&#30952;&#25439;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#23481;&#37327;&#30340;&#30913;&#30424;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#30913;&#30424;&#21047;&#26032;&#26041;&#27861;&#65292;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#30340;&#25972;&#20307;&#21487;&#38752;&#24615;&#21644;&#21151;&#29575;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#23384;&#20648;&#27744;&#20013;&#27599;&#20010;&#30913;&#30424;&#30340;&#20581;&#24247;&#29366;&#24577;&#65292;&#25552;&#21069;n&#22825;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#24320;&#28304;&#25968;&#25454;&#38598;&#26469;&#35782;&#21035;&#38656;&#35201;&#21047;&#26032;&#30340;&#29305;&#23450;&#30913;&#30424;&#12290;&#23545;&#20110;&#39044;&#27979;&#20026;&#19981;&#20581;&#24247;&#30340;&#30913;&#30424;&#65292;&#25105;&#20204;&#26631;&#35760;&#23427;&#20204;&#36827;&#34892;&#26367;&#25442;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#25805;&#20316;&#12290;&#23545;&#20110;&#20581;&#24247;&#30340;&#39537;&#21160;&#22120;&#65292;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#38598;&#21512;&#21644;&#25968;&#37327;&#35780;&#20272;&#30340;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Disk scrubbing is a process aimed at resolving read errors on disks by reading data from the disk. However, scrubbing the entire storage array at once can adversely impact system performance, particularly during periods of high input/output operations. Additionally, the continuous reading of data from disks when scrubbing can result in wear and tear, especially on larger capacity disks, due to the significant time and energy consumption involved. To address these issues, we propose a selective disk scrubbing method that enhances the overall reliability and power efficiency in data centers. Our method employs a Machine Learning model based on Mondrian Conformal prediction to identify specific disks for scrubbing, by proactively predicting the health status of each disk in the storage pool, forecasting n-days in advance, and using an open-source dataset. For disks predicted as non-healthy, we mark them for replacement without further action. For healthy drives, we create a set and quanti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16703</link><description>&lt;p&gt;
&#24377;&#24615;&#32422;&#26463;&#19979;&#30340;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#31105;&#27490;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#22240;&#20026;&#21333;&#20010;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65288;&#22914;Per-FedAvg&#65289;&#12290;&#20803;&#23398;&#20064;&#23398;&#20064;&#36866;&#29992;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20849;&#20139;&#21021;&#22987;&#21442;&#25968;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23558;&#21021;&#22987;&#21270;&#24555;&#36895;&#35843;&#25972;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#27169;&#22411;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#37319;&#26679;&#26356;&#26032;&#30340;&#38543;&#26426;&#24615;&#65292;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#26412;&#22320;&#36866;&#24212;&#21516;&#19968;&#23458;&#25143;&#31471;&#26102;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#21516;&#36866;&#24212;&#26041;&#21521;&#30340;&#27874;&#21160;&#38459;&#30861;&#20102;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21382;&#21490;&#26412;&#22320;&#35843;&#25972;&#30340;&#27169;&#22411;&#26469;&#38480;&#21046;&#20869;&#24490;&#29615;&#30340;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#21160;&#24577;&#20998;&#36776;&#29575;&#27169;&#22411;&#23398;&#20064;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#20998;&#36776;&#29575;&#30340;&#31890;&#23376;&#29615;&#22659;&#34920;&#31034;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#30340;&#21160;&#24577;&#21644;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#22312;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#65288;Translated from Abstract&#65289;</title><link>http://arxiv.org/abs/2306.16700</link><description>&lt;p&gt;
&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#21160;&#24577;&#20998;&#36776;&#29575;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic-Resolution Model Learning for Object Pile Manipulation. (arXiv:2306.16700v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#21160;&#24577;&#20998;&#36776;&#29575;&#27169;&#22411;&#23398;&#20064;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#20998;&#36776;&#29575;&#30340;&#31890;&#23376;&#29615;&#22659;&#34920;&#31034;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#30340;&#21160;&#24577;&#21644;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#22312;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#65288;Translated from Abstract&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#23398;&#20064;&#36825;&#31181;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20351;&#29992;&#20160;&#20040;&#22330;&#26223;&#34920;&#31034;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#22266;&#23450;&#32500;&#24230;&#25110;&#20998;&#36776;&#29575;&#30340;&#34920;&#31034;&#65292;&#36825;&#23545;&#31616;&#21333;&#20219;&#21153;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#65292;&#23545;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#26080;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23398;&#20064;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#21160;&#24577;&#21644;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#25928;&#29575;&#21644;&#25928;&#26524;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21160;&#24577;&#20998;&#36776;&#29575;&#30340;&#31890;&#23376;&#29615;&#22659;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23398;&#20064;&#20102;&#32479;&#19968;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20801;&#35768;&#36830;&#32493;&#36873;&#25321;&#25277;&#35937;&#23618;&#27425;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#20195;&#29702;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#27599;&#20010;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#27493;&#39588;&#30340;&#26368;&#20339;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#22312;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#25105;&#20204;&#22312;&#28921;&#39274;, &#20892;&#19994;&#31561;&#39046;&#22495;&#32463;&#24120;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agric
&lt;/p&gt;</description></item><item><title>DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16058</link><description>&lt;p&gt;
DUET: 2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16058
&lt;/p&gt;
&lt;p&gt;
DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;(MSSL)&#22522;&#20110;&#23398;&#20064;&#30456;&#23545;&#20110;&#19968;&#32452;&#36755;&#20837;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#21464;&#24615;&#20174;&#34920;&#31034;&#20013;&#37096;&#20998;&#25110;&#23436;&#20840;&#31227;&#38500;&#19982;&#21464;&#25442;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#38656;&#35201;&#36825;&#20123;&#20449;&#24687;&#30340;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36896;&#25104;&#25439;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;2D&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#34920;&#31034;&#65292;&#31216;&#20026;DUET&#65292;&#23427;&#20204;&#26159;&#20197;&#30697;&#38453;&#32467;&#26500;&#32452;&#32455;&#30340;2D&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20316;&#29992;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#25442;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;DUET&#34920;&#31034;&#20445;&#30041;&#26377;&#20851;&#36755;&#20837;&#21464;&#25442;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#34920;&#36798;&#33021;&#21147;&#12290;&#19982;SimCLR&#65288;Chen&#31561;&#65292;2020&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#19981;&#21464;&#24615;&#65289;&#21644;ESSL&#65288;Dangovski&#31561;&#65292;2022&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#31561;&#21464;&#24615;&#65289;&#30456;&#27604;&#65292;DUET&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#24615;&#20351;&#24471;&#29983;&#25104;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#21487;&#25511;&#24615;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;SimCLR&#25110;ESSL&#21017;&#26080;&#27861;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;DUET&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#36974;&#25377;&#24773;&#20917;&#12290;&#36890;&#36807;&#36880;&#27493;&#24341;&#20837;&#36974;&#25377;&#65292;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#31616;&#21333;&#12289;&#21487;&#36776;&#21035;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#36880;&#28176;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#36974;&#25377;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.15574</link><description>&lt;p&gt;
&#36879;&#36807;&#36855;&#38654;&#30475;&#28165;&#26970;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#37319;&#29992;&#28176;&#36827;&#36974;&#25377;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
See Through the Fog: Curriculum Learning with Progressive Occlusion in Medical Imaging. (arXiv:2306.15574v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#36974;&#25377;&#24773;&#20917;&#12290;&#36890;&#36807;&#36880;&#27493;&#24341;&#20837;&#36974;&#25377;&#65292;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#31616;&#21333;&#12289;&#21487;&#36776;&#21035;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#36880;&#28176;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#36974;&#25377;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#30340;&#22797;&#26434;&#22270;&#20687;&#19978;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#36825;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#36974;&#25377;&#21307;&#23398;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#24341;&#20837;&#36974;&#25377;&#65292;&#20174;&#28165;&#26224;&#12289;&#26080;&#36974;&#25377;&#30340;&#22270;&#20687;&#24320;&#22987;&#65292;&#36880;&#28176;&#36807;&#28193;&#21040;&#36974;&#25377;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#30340;&#22270;&#20687;&#12290;&#36825;&#31181;&#26377;&#24207;&#30340;&#23398;&#20064;&#36807;&#31243;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#20351;&#27169;&#22411;&#39318;&#20808;&#25484;&#25569;&#31616;&#21333;&#12289;&#21487;&#36776;&#21035;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#22312;&#27492;&#22522;&#30784;&#19978;&#36880;&#28176;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#36974;&#25377;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#20840;&#26032;&#30340;&#36974;&#25377;&#29983;&#25104;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;Wasserstein&#35838;&#31243;&#23398;&#20064;&#65288;WCL&#65289;&#12289;&#20449;&#24687;&#33258;&#36866;&#24212;&#23398;&#20064;&#65288;IAL&#65289;&#21644;&#27979;&#22320;&#32447;&#35838;&#31243;&#23398;&#20064;&#65288;Geodesic Curriculum Learn&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning models have revolutionized medical image interpretation, offering substantial improvements in diagnostic accuracy. However, these models often struggle with challenging images where critical features are partially or fully occluded, which is a common scenario in clinical practice. In this paper, we propose a novel curriculum learning-based approach to train deep learning models to handle occluded medical images effectively. Our method progressively introduces occlusion, starting from clear, unobstructed images and gradually moving to images with increasing occlusion levels. This ordered learning process, akin to human learning, allows the model to first grasp simple, discernable patterns and subsequently build upon this knowledge to understand more complicated, occluded scenarios. Furthermore, we present three novel occlusion synthesis methods, namely Wasserstein Curriculum Learning (WCL), Information Adaptive Learning (IAL), and Geodesic Curriculum Learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;(IFL-LSTP)&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30830;&#20445;&#20998;&#24067;&#24335;&#23384;&#20648;&#30340;&#36127;&#36733;&#24179;&#34913;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#21010;&#20998;&#30340;&#26102;&#31354;&#25509;&#36817;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12857</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Partitioning Method of Large-Scale Public Safety Spatio-Temporal Data based on Information Loss Constraints. (arXiv:2306.12857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;(IFL-LSTP)&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30830;&#20445;&#20998;&#24067;&#24335;&#23384;&#20648;&#30340;&#36127;&#36733;&#24179;&#34913;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#21010;&#20998;&#30340;&#26102;&#31354;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26102;&#31354;&#25968;&#25454;&#30340;&#23384;&#20648;&#12289;&#31649;&#29702;&#21644;&#24212;&#29992;&#22312;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#20844;&#20849;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#29420;&#29305;&#26102;&#31354;&#20998;&#24067;&#29305;&#24449;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#25968;&#25454;&#26102;&#31354;&#25509;&#36817;&#24230;&#21644;&#20998;&#24067;&#24335;&#23384;&#20648;&#36127;&#36733;&#24179;&#34913;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;(IFL-LSTP)&#12290;&#35813;IFL-LSTP&#27169;&#22411;&#38024;&#23545;&#22823;&#35268;&#27169;&#26102;&#31354;&#28857;&#25968;&#25454;&#65292;&#23558;&#26102;&#31354;&#21010;&#20998;&#27169;&#22359;(STPM)&#21644;&#22270;&#21010;&#20998;&#27169;&#22359;(GPM)&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#25552;&#39640;&#21010;&#20998;&#25928;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#30830;&#20445;&#20998;&#24067;&#24335;&#23384;&#20648;&#30340;&#36127;&#36733;&#24179;&#34913;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#21010;&#20998;&#30340;&#26102;&#31354;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The storage, management, and application of massive spatio-temporal data are widely applied in various practical scenarios, including public safety. However, due to the unique spatio-temporal distribution characteristics of re-al-world data, most existing methods have limitations in terms of the spatio-temporal proximity of data and load balancing in distributed storage. There-fore, this paper proposes an efficient partitioning method of large-scale public safety spatio-temporal data based on information loss constraints (IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal point da-ta by combining the spatio-temporal partitioning module (STPM) with the graph partitioning module (GPM). This approach can significantly reduce the scale of data while maintaining the model's accuracy, in order to improve the partitioning efficiency. It can also ensure the load balancing of distributed storage while maintaining spatio-temporal proximity of the data partitioning res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.06247</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Set-Valued Feedback. (arXiv:2306.06247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#39044;&#27979;&#21333;&#20010;&#26631;&#31614;&#65292;&#20294;&#25509;&#25910;&#21040;&#19968;&#20010;&#26631;&#31614;&#30340;&#38598;&#21512;&#20316;&#20026;&#21453;&#39304;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#23398;&#20064;&#22120;&#27809;&#26377;&#36755;&#20986;&#21253;&#21547;&#22312;&#21453;&#39304;&#38598;&#21512;&#20013;&#30340;&#26631;&#31614;&#65292;&#21017;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#20855;&#26377;&#21333;&#26631;&#31614;&#21453;&#39304;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#19981;&#21516;&#65292;&#22312;&#23454;&#29616;&#35774;&#32622;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#26102;&#65292;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;\textit{&#19981;&#31561;&#20215;}&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;&#38598;&#21512;&#23567;&#30707;&#21644;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#65292;&#20005;&#26684;&#25551;&#36848;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#22312;&#24735;&#24615;&#35774;&#32622;&#19979;&#20005;&#26684;&#25551;&#36848;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#26159;&#25105;&#20204;&#36890;&#29992;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variant of online multiclass classification where the learner predicts a single label but receives a \textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension tightly characterizes online learnability in the agnostic setting. Finally, we show that practical learning settings like online multilabel ranking and online multilabel classification are specific instances of our general online learning framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#23646;&#24615;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#19968;&#31181;&#26356;&#22909;&#30340;&#65292;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#65292;&#20197;&#27492;&#35299;&#20915;&#29616;&#26377;&#26631;&#37327;&#21270;&#26041;&#26696;&#25152;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04620</link><description>&lt;p&gt;
&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;GFlowNets&#29992;&#20110;&#21487;&#25511;&#22810;&#30446;&#26631;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular Design. (arXiv:2306.04620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#23646;&#24615;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#19968;&#31181;&#26356;&#22909;&#30340;&#65292;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#65292;&#20197;&#27492;&#35299;&#20915;&#29616;&#26377;&#26631;&#37327;&#21270;&#26041;&#26696;&#25152;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20110;&#21407;&#23376;&#20998;&#23376;&#35774;&#35745;&#30340;&#20851;&#27880;&#36880;&#28176;&#21152;&#21095;&#12290;&#22312;&#35774;&#35745;&#33647;&#29289;&#24212;&#29992;&#30340;&#26032;&#21270;&#21512;&#29289;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#20248;&#21270;&#20998;&#23376;&#30340;&#22810;&#20010;&#23646;&#24615;&#65292;&#22914;&#19982;&#30446;&#26631;&#30340;&#32467;&#21512;&#33021;&#12289;&#21487;&#21512;&#25104;&#24615;&#12289;&#27602;&#24615;&#12289;EC50&#31561;&#31561;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#37319;&#29992;&#26631;&#37327;&#21270;&#26041;&#26696;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#20197;&#20559;&#22909;&#20026;&#26465;&#20214;&#30340;&#21333;&#20010;&#30446;&#26631;&#65292;&#20294;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#20855;&#26377;&#20985;&#24418;Pareto&#21069;&#27839;&#30340;&#38382;&#39064;&#19978;&#65292;&#36825;&#31181;&#20943;&#23569;&#26041;&#26696;&#21487;&#33021;&#20250;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#20542;&#21521;&#20110;&#28369;&#21521;&#30446;&#26631;&#31354;&#38388;&#30340;&#26497;&#31471;&#28857;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21478;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#20844;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#33719;&#24471;&#26356;&#21487;&#25511;&#30340;&#26465;&#20214;&#27169;&#22411;&#65292;&#20197;&#20415;&#27839;&#25972;&#20010;Pareto&#21069;&#27839;&#22343;&#21248;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, in-silico molecular design has received much attention from the machine learning community. When designing a new compound for pharmaceutical applications, there are usually multiple properties of such molecules that need to be optimised: binding energy to the target, synthesizability, toxicity, EC50, and so on. While previous approaches have employed a scalarization scheme to turn the multi-objective problem into a preference-conditioned single objective, it has been established that this kind of reduction may produce solutions that tend to slide towards the extreme points of the objective space when presented with a problem that exhibits a concave Pareto front. In this work we experiment with an alternative formulation of goal-conditioned molecular generation to obtain a more controllable conditional model that can uniformly explore solutions along the entire Pareto front.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#19981;&#30830;&#23450;&#21306;&#22495;&#21644;&#21033;&#29992;&#26377;&#25215;&#35834;&#21306;&#22495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;COCO&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04262</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Adjusting Weighted Expected Improvement for Bayesian Optimization. (arXiv:2306.04262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#19981;&#30830;&#23450;&#21306;&#22495;&#21644;&#21033;&#29992;&#26377;&#25215;&#35834;&#21306;&#22495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;COCO&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#65292;&#23545;&#23567;&#22411;&#35780;&#20272;&#39044;&#31639;&#30340;&#40657;&#31665;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#31867;&#12290;BO&#31649;&#36947;&#26412;&#36523;&#39640;&#24230;&#21487;&#37197;&#32622;&#65292;&#28041;&#21450;&#21021;&#22987;&#35774;&#35745;&#12289;&#20195;&#29702;&#27169;&#22411;&#21644;&#33719;&#21462;&#21151;&#33021;&#65288;AF&#65289;&#30340;&#35768;&#22810;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#22914;&#20309;&#20026;&#25163;&#22836;&#38382;&#39064;&#36873;&#25321;&#21512;&#36866;&#30340;&#32452;&#20214;&#30340;&#29702;&#35299;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;AF&#30340;&#23450;&#20041;&#65292;&#20854;&#20027;&#35201;&#30446;&#30340;&#26159;&#24179;&#34913;&#23545;&#39640;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#21644;&#23545;&#22909;&#35299;&#20915;&#26041;&#26696;&#26377;&#39640;&#25215;&#35834;&#30340;&#21306;&#22495;&#20043;&#38388;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#35753;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#22522;&#20110;BO&#30340;&#25910;&#25947;&#20934;&#21017;&#12290;&#22312;COCO&#22522;&#20934;&#24179;&#21488;&#30340;&#26080;&#22122;&#22768;&#40657;&#31665;BBOB&#20989;&#25968;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#32447;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#20219;&#20309;&#26102;&#38388;&#24615;&#33021;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;SP&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient algorithms for optimizing black-box problems with small evaluation budgets. The BO pipeline itself is highly configurable with many different design choices regarding the initial design, surrogate model, and acquisition function (AF). Unfortunately, our understanding of how to select suitable components for a problem at hand is very limited. In this work, we focus on the definition of the AF, whose main purpose is to balance the trade-off between exploring regions with high uncertainty and those with high promise for good solutions. We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let the exploration-exploitation trade-off self-adjust in a data-driven manner, based on a convergence criterion for BO. On the noise-free black-box BBOB functions of the COCO benchmarking platform, our method exhibits a favorable any-time performance compared to handcrafted baselines and serves as a robust def
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01589</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#21407;&#23376;&#27169;&#25311;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#32780;&#29983;&#25104;&#21442;&#32771;&#35745;&#31639;&#26159;&#35745;&#31639;&#19978;&#35201;&#27714;&#24456;&#39640;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25551;&#36848;&#21270;&#23398;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#26680;&#22343;&#20540;&#23884;&#20837;&#12290;&#25105;&#20204;&#20174;&#39044;&#20808;&#22312;OC20&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#30340;GNN&#20013;&#25552;&#21462;&#29305;&#24449;&#26144;&#23556;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20174;&#20652;&#21270;&#36807;&#31243;&#30340;&#31995;&#32479;&#29305;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#36890;&#36807;&#28789;&#27963;&#30340;&#26680;&#20989;&#25968;&#26469;&#22686;&#24378;&#65292;&#35813;&#26680;&#20989;&#25968;&#21253;&#25324;&#21270;&#23398;&#29289;&#31181;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36880;&#28176;&#22797;&#26434;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#65292;&#25913;&#36827;&#20102;&#20381;&#36182;GNNs&#25110;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15912</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#30340;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#20540;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21333;&#20010;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#12290;&#25105;&#20204;&#23558;ReLU&#21333;&#20803;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#23545;&#24212;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#31216;&#20026;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29305;&#24449;&#28608;&#27963;&#38598;&#19982;ReLU&#32593;&#32476;&#20013;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#25216;&#26415;&#22914;&#20309;&#35268;&#33539;&#21270;&#21644;&#31283;&#23450;SGD&#20248;&#21270;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;ReLU&#32593;&#32476;&#20197;&#25913;&#36827;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#20854;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#20102;&#19981;&#37027;&#20040;&#31934;&#24515;&#36873;&#25321;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21644;&#26356;&#22823;&#30340;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#31283;&#23450;&#24615;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#31216;&#20026;GNSTODE&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.12334</link><description>&lt;p&gt;
&#37319;&#29992;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#21160;&#24577;&#29289;&#29702;&#31995;&#32479;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs. (arXiv:2305.12334v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#31216;&#20026;GNSTODE&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;&#30495;&#23454;&#30340;&#29289;&#29702;&#19990;&#30028;&#65292;&#22240;&#27492;&#23398;&#20064;&#27169;&#25311;&#22797;&#26434;&#30340;&#31890;&#23376;&#31995;&#32479;&#26159;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#19990;&#30028;&#30340;&#22797;&#26434;&#35268;&#24459;&#32473;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22914;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#20043;&#38388;&#30340;&#19981;&#21516;&#31354;&#38388;&#20381;&#36182;&#24615;&#20197;&#21450;&#19981;&#21516;&#26102;&#38388;&#25139;&#20043;&#38388;&#31890;&#23376;&#31995;&#32479;&#29366;&#24577;&#30340;&#19981;&#21516;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#36825;&#20123;&#22240;&#32032;&#20915;&#23450;&#20102;&#31890;&#23376;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#21644;&#29289;&#29702;&#31995;&#32479;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#30340;&#29289;&#29702;&#27861;&#21017;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#8212;&#8212;&#20855;&#26377;&#26102;&#31354;&#24314;&#27169;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#65288;GNSTODE&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The great learning ability of deep learning models facilitates us to comprehend the real physical world, making learning to simulate complicated particle systems a promising endeavour. However, the complex laws of the physical world pose significant challenges to the learning based simulations, such as the varying spatial dependencies between interacting particles and varying temporal dependencies between particle system states in different time stamps, which dominate particles' interacting behaviour and the physical systems' evolution patterns. Existing learning based simulation methods fail to fully account for the complexities, making them unable to yield satisfactory simulations. To better comprehend the complex physical laws, this paper proposes a novel learning based simulation model- Graph Networks with Spatial-Temporal neural Ordinary Equations (GNSTODE)- that characterizes the varying spatial and temporal dependencies in particle systems using a united end-to-end framework. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;</title><link>http://arxiv.org/abs/2305.05448</link><description>&lt;p&gt;
&#20511;&#21161;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#40065;&#26834;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#33021;&#26377;&#35768;&#22810;&#25554;&#20540;&#35299;; &#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#25351;&#29305;&#23450;&#20248;&#21270;&#26041;&#27861;&#23545;&#20247;&#22810;&#25554;&#20540;&#35299;&#20043;&#19968;&#30340;&#38544;&#21547;&#21916;&#22909;&#12290;&#24050;&#32463;&#24314;&#31435;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#22312;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#26102;&#20542;&#21521;&#20110;&#20855;&#26377;&#20302;&#31209;&#21644;/&#25110;&#31232;&#30095;&#35299;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#20174;&#26576;&#31181;&#31243;&#24230;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24179;&#26041;&#25439;&#22833;&#30446;&#26631;&#29702;&#35770;&#36890;&#24120;&#38656;&#35201;&#21487;&#35757;&#32451;&#26435;&#37325;&#30340;&#38750;&#24120;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#20013;&#20026;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#32780;&#21021;&#22987;&#21270;&#30340;&#26356;&#22823;&#35268;&#27169;&#30340;&#26435;&#37325;&#30683;&#30462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32435;&#20837;&#24182;&#20998;&#26512;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#26435;&#37325;&#21521;&#37327;&#20197;&#26497;&#22352;&#26631;&#21442;&#25968;&#21270;&#65292;&#23548;&#33268;&#33258;&#28982;&#30340;&#26435;&#37325;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#26435;&#37325;&#21521;&#37327;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;&#20559;&#24046;&#19982;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#32463;&#39564;&#33539;&#25968;&#27491;&#21017;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.02506</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#35299;&#23494;&#24230;&#30340;&#23383;&#31526;&#20018;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;&#25193;&#23637;&#23450;&#21521;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#30340;&#24517;&#35201;&#24615;&#12290;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#37117;&#23450;&#20041;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#31232;&#30095;&#32467;&#26500;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#26377;&#20851;&#27010;&#29575;&#26144;&#23556;&#30340;&#39532;&#23572;&#21487;&#22827;&#33539;&#30068;&#30340;&#24037;&#20316;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#33539;&#30068;&#65292;&#20854;&#24577;&#23556;&#23558;&#20998;&#21035;&#30001;&#27599;&#20010;&#26679;&#26412;&#31354;&#38388;&#20998;&#35299;&#30340;&#32852;&#21512;&#23494;&#24230;&#19982;&#20174;&#26679;&#26412;&#21040;&#36820;&#22238;&#20540;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#32452;&#21512;&#12290;&#36825;&#26159;&#36808;&#21521;&#26368;&#36817;&#30340;&#33539;&#30068;&#35770;&#27010;&#29575;&#27979;&#24230;&#25551;&#36848;&#21644;&#36890;&#24120;&#22312;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#20998;&#35299;&#23494;&#24230;&#30340;&#25805;&#20316;&#23450;&#20041;&#20043;&#38388;&#30340;&#32553;&#23567;&#24046;&#36317;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;</title><link>http://arxiv.org/abs/2305.00152</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19979;&#30340;&#27169;&#22411;&#36873;&#25321;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20851;&#20110;&#36716;&#31227;&#23398;&#20064;&#25110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24050;&#30693;&#20551;&#35774;&#31867;&#25110;&#27169;&#22411;&#30340;&#24773;&#20917;&#65307;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#36825;&#32463;&#24120;&#20986;&#29616;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24635;&#20307;&#33539;&#30068;&#19979;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#32771;&#34385;&#35843;&#25972;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#27491;&#30830;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#30456;&#20851;&#28304;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#19982;&#27169;&#22411;&#36873;&#25321;&#26377;&#20851;&#30340;&#36817;&#20284;&#19982;&#20272;&#35745;&#35823;&#24046;&#30340;&#36890;&#24120;&#26435;&#34913;&#20043;&#22806;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#24102;&#26469;&#20102;&#26032;&#30340;&#22797;&#26434;&#24230;&#65292;&#21363;&#28304;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#31227;&#36317;&#31163;&#65292;&#36825;&#20010;&#36317;&#31163;&#38543;&#30528;&#20551;&#35774;&#31867;&#30340;&#36873;&#25321;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#20998;&#31867;&#38382;&#39064;&#12290;&#29305;&#21035;&#30340;&#65292;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#29616;&#35937;&#65306;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21363;&#27809;&#26377;&#20998;&#24067;&#24335;&#20449;&#24687;&#26102;&#21487;&#36798;&#21040;&#30340;&#36895;&#29575;&#65292;&#21487;&#20197;&#20219;&#24847;&#24930;&#20110;oracle&#36895;&#29575;&#65292;&#21363;&#22312;&#32473;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.05294</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#36873;&#25321;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24378;&#20581;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20581;&#30340;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#21019;&#24314;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#12289;&#28508;&#22312;&#20132;&#20114;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#38598;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#65288;M&#65289;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Tigramite Python&#21253;&#20013;&#23454;&#29616;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;PC1&#25110;PCMCI&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#25512;&#26029;&#22240;&#26524;&#22270;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#23558;&#21097;&#20313;&#22240;&#26524;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;ML&#27169;&#22411;&#65288;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65289;&#39044;&#27979;&#30446;&#26631;&#20043;&#21069;&#65292;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#35199;&#22826;&#24179;&#27915;&#28909;&#24102;&#22320;&#21306;&#30340;&#22320;&#38663;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04970</link><description>&lt;p&gt;
GRIL&#65306;&#19968;&#31181;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#22522;&#20110;&#21521;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning. (arXiv:2304.04970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21442;&#25968;&#25345;&#20037;&#24615;&#21516;Topology Data Analysis (TDA)&#30456;&#20851;&#65292;&#21487;&#30740;&#31350;&#25968;&#25454;&#20013;&#38544;&#34255;&#30528;&#30340;&#36830;&#36890;&#20998;&#37327;&#21644;&#24490;&#29615;&#31561;&#25299;&#25169;&#29305;&#24449;&#12290;&#24050;&#24212;&#29992;&#20110;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20016;&#23500;&#25299;&#25169;&#29305;&#24449;&#30340;&#34920;&#31034;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30740;&#31350;&#21452;&#36807;&#28388;&#20989;&#25968;&#35825;&#23548;&#30340;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#27169;&#22359;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#34920;&#31034;&#20449;&#24687;&#21152;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21521;&#37327;&#34920;&#31034;&#31216;&#20026;Generalized Rank Invariant Landscape \textsc{Gril}&#65292;&#24182;&#23558;&#20854;&#35777;&#26126;&#20026;&#22312;Lipschitz&#31283;&#23450;&#26465;&#20214;&#19979;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#30784;&#36807;&#28388;&#20989;&#25968;&#30340;&#32534;&#30721;&#21487;&#20197;&#23481;&#26131;&#22320;&#34701;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#21521;&#37327;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
$1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks (GNNs). To enrich the representations of topological features, here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape \textsc{Gril} for $2$-parameter persistence modules. We show that this vector representation is $1$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.02836</link><description>&lt;p&gt;
&#38271;&#26399;&#30340;&#22810;&#27169;&#24335;&#21464;&#21387;&#22120;&#25972;&#21512;EHR&#20013;&#25104;&#20687;&#21644;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#65292;&#29992;&#20110;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#22797;&#25104;&#20687;&#21644;&#21307;&#30103;&#32972;&#26223;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65289;&#32435;&#20837;&#39044;&#27979;&#24615;&#23396;&#31435;&#24615;&#32954;&#37096;&#32467;&#33410;&#65288;SPN&#65289;&#35786;&#26029;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20687;&#25104;&#20687;&#21644;&#35786;&#26029;&#20195;&#30721;&#36825;&#26679;&#30340;&#20020;&#24202;&#24120;&#35268;&#27169;&#24335;&#21487;&#33021;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#19981;&#35268;&#21017;&#37319;&#26679;&#65292;&#36825;&#26159;&#38271;&#26399;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#23558;&#37325;&#22797;&#25104;&#20687;&#19982;&#26085;&#24120;&#25910;&#38598;&#30340;EHR&#20013;&#30340;&#38271;&#26399;&#20020;&#24202;&#29305;&#24449;&#30456;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;SPN&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#35299;&#32544;&#32538;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#36317;&#31163;&#32553;&#25918;&#33258;&#27880;&#24847;&#21147;&#26469;&#32852;&#21512;&#23398;&#20064;&#20020;&#24202;&#29305;&#24449;&#34920;&#36798;&#21644;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#26159;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;2,668&#20010;&#25195;&#25551;&#21644;1,149&#21517;&#24535;&#24895;&#32773;&#30340;&#38271;&#26399;&#33016;&#37096;CT&#12289;&#36134;&#21333;&#20195;&#30721;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#35760;&#24405;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#22797;&#26434;&#24230;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;Bernoulli&#20998;&#24067;&#26368;&#20339;&#33218;&#35782;&#21035;&#31561;&#20219;&#21153;&#26102;&#26080;&#27861;&#23454;&#29616;&#32479;&#19968;&#26368;&#20339;&#21487;&#36798;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09468</link><description>&lt;p&gt;
&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#30340;&#22797;&#26434;&#24230;&#23384;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Existence of a Complexity in Fixed Budget Bandit Identification. (arXiv:2303.09468v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#22797;&#26434;&#24230;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;Bernoulli&#20998;&#24067;&#26368;&#20339;&#33218;&#35782;&#21035;&#31561;&#20219;&#21153;&#26102;&#26080;&#27861;&#23454;&#29616;&#32479;&#19968;&#26368;&#20339;&#21487;&#36798;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#65292;&#31639;&#27861;&#25353;&#39034;&#24207;&#35266;&#23519;&#26469;&#33258;&#22810;&#20010;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#30452;&#21040;&#32473;&#23450;&#26368;&#32456;&#26102;&#38388;&#12290;&#28982;&#21518;&#65292;&#23427;&#22238;&#31572;&#20851;&#20110;&#20998;&#24067;&#38598;&#30340;&#26597;&#35810;&#12290;&#19968;&#20010;&#22909;&#30340;&#31639;&#27861;&#23558;&#26377;&#23567;&#30340;&#38169;&#35823;&#27010;&#29575;&#12290;&#34429;&#28982;&#36825;&#20010;&#27010;&#29575;&#38543;&#30528;&#26368;&#32456;&#26102;&#38388;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#19979;&#38477;&#65292;&#20294;&#23545;&#20110;&#22823;&#22810;&#25968;&#26631;&#35782;&#20219;&#21153;&#65292;&#26368;&#20339;&#21487;&#36798;&#29575;&#24182;&#38750;&#31934;&#30830;&#24050;&#30693;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#22266;&#23450;&#39044;&#31639;&#20219;&#21153;&#25509;&#21463;&#22797;&#26434;&#24230;&#65288;&#23450;&#20041;&#20026;&#21333;&#20010;&#31639;&#27861;&#22312;&#25152;&#26377;&#36172;&#21338;&#38382;&#39064;&#20013;&#23454;&#29616;&#30340;&#38169;&#35823;&#27010;&#29575;&#30340;&#19979;&#38480;&#65289;&#65292;&#21017;&#35813;&#22797;&#26434;&#24230;&#30001;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#38750;&#33258;&#36866;&#24212;&#25277;&#26679;&#36807;&#31243;&#30830;&#23450;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20960;&#20010;&#22266;&#23450;&#39044;&#31639;&#35782;&#21035;&#20219;&#21153;&#65292;&#21253;&#25324;&#20855;&#26377;&#20004;&#20010;&#33218;&#30340;&#20271;&#21162;&#21033;&#26368;&#20339;&#33218;&#35782;&#21035;&#65292;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#22797;&#26434;&#24230;&#65306;&#27809;&#26377;&#21333;&#20010;&#31639;&#27861;&#33021;&#22815;&#38543;&#22788;&#23454;&#29616;&#26368;&#20339;&#21487;&#33021;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by a single algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20307;&#32946;&#21338;&#24425;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#26657;&#20934;&#24615;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#27492;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06021</link><description>&lt;p&gt;
&#20307;&#32946;&#21338;&#24425;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#39044;&#27979;&#27169;&#22411;&#24212;&#20248;&#21270;&#20934;&#30830;&#24615;&#36824;&#26159;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?. (arXiv:2303.06021v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20307;&#32946;&#21338;&#24425;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#26657;&#20934;&#24615;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#27492;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#26368;&#36817;&#23545;&#20307;&#32946;&#21338;&#24425;&#36827;&#34892;&#20102;&#32852;&#37030;&#21512;&#27861;&#21270;&#65292;&#36825;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26102;&#20195;&#30456;&#36935;&#12290;&#22914;&#26524;&#21338;&#24425;&#32773;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#20182;&#20204;&#21487;&#20197;&#35748;&#35782;&#21040;&#20309;&#26102;&#20070;maker&#30340;&#36180;&#29575;&#23545;&#20182;&#20204;&#26377;&#21033;&#12290;&#30001;&#20110;&#20307;&#32946;&#21338;&#24425;&#20165;&#22312;&#32654;&#22269;&#30340;&#24066;&#22330;&#19978;&#23601;&#26159;&#19968;&#20010;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#65292;&#22240;&#27492;&#25214;&#21040;&#36825;&#26679;&#30340;&#26426;&#20250;&#21487;&#33021;&#20250;&#38750;&#24120;&#26377;&#21033;&#21487;&#22270;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20307;&#32946;&#36187;&#26524;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#20934;&#30830;&#24230;&#26469;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20551;&#35774;&#65292;&#23545;&#20110;&#20307;&#32946;&#21338;&#24425;&#38382;&#39064;&#65292;&#27169;&#22411;&#26657;&#20934;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#36187;&#23395;&#30340;NBA&#25968;&#25454;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#20010;&#36187;&#23395;&#19978;&#20351;&#29992;&#24050;&#21457;&#24067;&#30340;&#36180;&#29575;&#36827;&#34892;&#21338;&#24425;&#23454;&#39564;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#21338;&#24425;&#31995;&#32479;&#65292;&#25105;&#20204;&#34920;&#26126;&#20248;&#21270;&#26657;&#20934;&#30340;&#39044;&#27979;&#27169;&#22411;&#27604;&#20248;&#21270;&#20934;&#30830;&#24230;&#24179;&#22343;&#24102;&#26469;&#26356;&#39640;&#30340;&#22238;&#25253;&#29575;&#65288;&#25237;&#36164;&#22238;&#25253;&#29575;&#20026;$110.42&#65285;$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to accurately predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of forecasting models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. Evaluating various betting systems, we show that optimising the forecasting model for calibration leads to greater returns than optimising for accuracy, on average (return on investment of $110.42\%$ v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02045</link><description>&lt;p&gt;
&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#20351;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;&#35777;&#25454;&#26469;&#21442;&#25968;&#21270;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;&#26126;&#30830;&#32771;&#34385;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#38169;&#35823;&#26631;&#35760;&#30340;&#31867;&#21035;&#30340;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#20250;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;$\mathcal{I}$-EDL&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#26469;&#34913;&#37327;&#27599;&#20010;&#26679;&#26412;&#25152;&#25658;&#24102;&#30340;&#35777;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#26681;&#25454;&#36825;&#20010;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#21160;&#24577;&#22320;&#37325;&#26032;&#21152;&#26435;&#30446;&#26631;&#25439;&#22833;&#39033;&#65292;&#20351;&#32593;&#32476;&#26356;&#21152;&#19987;&#27880;&#20110;&#19981;&#30830;&#23450;&#31867;&#21035;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36890;&#36807;&#20248;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#38454;ANIL&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#21040;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#22522;&#20110;&#23545;&#26080;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.01335</link><description>&lt;p&gt;
&#21021;&#38454;ANIL&#22312;&#23384;&#22312;&#35823;&#25351;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#24773;&#20917;&#19979;&#23398;&#20064;&#32447;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
First-order ANIL learns linear representations despite misspecified latent dimension. (arXiv:2303.01335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#38454;ANIL&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#21040;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#22522;&#20110;&#23545;&#26080;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#20803;&#23398;&#20064;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20803;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#23547;&#25214;&#36215;&#22987;&#28857;&#65292;&#20174;&#35813;&#36215;&#22987;&#28857;&#24320;&#22987;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36805;&#36895;&#36866;&#24212;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#32463;&#39564;&#19978;&#24314;&#35758;&#36825;&#26679;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#34892;&#20026;&#30340;&#29702;&#35770;&#35777;&#25454;&#26377;&#38480;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24182;&#27809;&#26377;&#20005;&#26684;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#20173;&#20250;&#23398;&#20064;&#21040;&#20849;&#20139;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#26412;&#25991;&#22312;&#26080;&#38480;&#25968;&#37327;&#30340;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#65292;&#20351;&#29992;&#32447;&#24615;&#21452;&#23618;&#32593;&#32476;&#32467;&#26500;&#30340;&#21021;&#38454;ANIL&#25104;&#21151;&#22320;&#23398;&#20064;&#21040;&#20102;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#21363;&#20351;&#26159;&#22312;&#21442;&#25968;&#21270;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#65292;&#21363;&#32593;&#32476;&#30340;&#23485;&#24230;&#22823;&#20110;
&lt;/p&gt;
&lt;p&gt;
Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialisation points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been rigorously shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with a misspecified network parameterisation; having a width larger than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#22312;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26550;&#26500;&#19978;&#30340;&#20462;&#35746;&#21644;&#24212;&#29992;&#12290;LRP&#36890;&#36807;&#21487;&#35270;&#21270;&#30456;&#20851;&#24615;&#28909;&#22270;&#25581;&#31034;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#21407;&#22240;&#65292;&#20294;&#38656;&#35201;&#27880;&#24847;&#20854;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20154;&#24037;&#21046;&#21697;&#12290;</title><link>http://arxiv.org/abs/2302.12317</link><description>&lt;p&gt;
&#20107;&#23454;&#36824;&#26159;&#20154;&#24037;&#21046;&#21697;&#65311;&#22312;&#19981;&#21516;&#30340;ANN&#26550;&#26500;&#19978;&#20462;&#35746;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures. (arXiv:2302.12317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#22312;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26550;&#26500;&#19978;&#30340;&#20462;&#35746;&#21644;&#24212;&#29992;&#12290;LRP&#36890;&#36807;&#21487;&#35270;&#21270;&#30456;&#20851;&#24615;&#28909;&#22270;&#25581;&#31034;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#21407;&#22240;&#65292;&#20294;&#38656;&#35201;&#27880;&#24847;&#20854;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20154;&#24037;&#21046;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25581;&#31034;&#21508;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26550;&#26500;&#30340;&#35265;&#35299;&#12290;LRP&#32463;&#24120;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#12290;&#20854;&#30446;&#30340;&#26159;&#29702;&#35299;&#36755;&#20837;&#26679;&#26412;&#30340;&#21738;&#20123;&#37096;&#20998;&#20855;&#26377;&#26368;&#39640;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#39044;&#27979;&#20135;&#29983;&#26368;&#22823;&#24433;&#21709;&#12290;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#36861;&#28335;&#30456;&#20851;&#24615;&#65292;&#24182;&#20026;&#27599;&#20010;&#36755;&#20837;&#20687;&#32032;&#20998;&#37197;&#29305;&#23450;&#30340;&#24471;&#20998;&#12290;&#28982;&#21518;&#23558;&#30456;&#20851;&#24615;&#24471;&#20998;&#32452;&#21512;&#24182;&#26174;&#31034;&#20026;&#28909;&#22270;&#65292;&#20351;&#20154;&#31867;&#23545;&#20998;&#31867;&#27169;&#22411;&#26377;&#30452;&#35266;&#30340;&#35270;&#35273;&#29702;&#35299;&#12290;&#25171;&#24320;&#40657;&#30418;&#20197;&#35814;&#32454;&#20102;&#35299;&#20998;&#31867;&#24341;&#25806;&#23545;&#20110;&#39046;&#22495;&#19987;&#23478;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#33719;&#24471;&#23545;ANN&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#22312;&#33719;&#24471;&#30340;&#30456;&#20851;&#24615;&#22270;&#20013;&#23384;&#22312;&#27169;&#22411;&#26412;&#36136;&#19978;&#30340;&#20154;&#24037;&#21046;&#21697;&#38519;&#38449;&#65292;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#36827;&#34892;&#26377;&#25928;&#30340;&#35299;&#37322;&#65292;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#21046;&#21697;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#32463;&#36807;&#35757;&#32451;&#30340;ANN&#26550;&#26500;&#19978;&#24212;&#29992;&#21644;&#20462;&#35746;LRP&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer-wise relevance propagation (LRP) is a widely used and powerful technique to reveal insights into various artificial neural network (ANN) architectures. LRP is often used in the context of image classification. The aim is to understand, which parts of the input sample have highest relevance and hence most influence on the model prediction. Relevance can be traced back through the network to attribute a certain score to each input pixel. Relevance scores are then combined and displayed as heat maps and give humans an intuitive visual understanding of classification models. Opening the black box to understand the classification engine in great detail is essential for domain experts to gain trust in ANN models. However, there are pitfalls in terms of model-inherent artifacts included in the obtained relevance maps, that can easily be missed. But for a valid interpretation, these artifacts must not be ignored. Here, we apply and revise LRP on various ANN architectures trained as class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#35774;&#35745;&#30340;&#29366;&#24577;&#38450;&#24481;&#25216;&#26415;MalProtect&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10739</link><description>&lt;p&gt;
MalProtect&#65306;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#26597;&#35810;&#25915;&#20987;&#30340;&#29366;&#24577;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection. (arXiv:2302.10739v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#35774;&#35745;&#30340;&#29366;&#24577;&#38450;&#24481;&#25216;&#26415;MalProtect&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26597;&#35810;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#26597;&#35810;&#20250;&#34987;&#19981;&#26029;&#25200;&#21160;&#65292;&#20197;&#26399;&#36798;&#21040;&#29305;&#23450;&#30340;&#20998;&#31867;&#30446;&#30340;&#65292;&#32780;&#19988;&#27809;&#26377;&#20851;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#20309;&#20102;&#35299;&#65292;&#20165;&#20973;&#20854;&#36755;&#20986;&#12290;&#36828;&#31243;&#25176;&#31649;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#21644;&#38754;&#21521;&#26381;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#30340;&#26222;&#36941;&#23384;&#22312;&#24847;&#21619;&#30528;&#26597;&#35810;&#25915;&#20987;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#26500;&#25104;&#20102;&#30495;&#27491;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#29366;&#24577;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#30417;&#35270;&#21644;&#20998;&#26512;&#31995;&#32479;&#25509;&#25910;&#21040;&#30340;&#26597;&#35810;&#24207;&#21015;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#24182;&#38450;&#27490;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29366;&#24577;&#38450;&#24481;&#26426;&#21046;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26426;&#21046;&#20165;&#20381;&#36182;&#20110;&#30456;&#20284;&#24615;&#25110;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#22312;&#20854;&#20182;&#39046;&#22495;&#26377;&#25928;&#12290;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26041;&#27861;&#26412;&#36136;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26816;&#27979;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#26126;&#26174;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MalProtect&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#35774;&#35745;&#30340;&#29366;&#24577;&#38450;&#24481;&#25216;&#26415;&#12290;MalProtect&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20851;&#20110;&#24694;&#24847;&#21644;&#33391;&#24615;&#26597;&#35810;&#20998;&#24067;&#30340;&#30693;&#35782;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MalProtect&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#20102;&#26597;&#35810;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML models are known to be vulnerable to adversarial query attacks. In these attacks, queries are iteratively perturbed towards a particular class without any knowledge of the target model besides its output. The prevalence of remotely-hosted ML classification models and Machine-Learning-as-a-Service platforms means that query attacks pose a real threat to the security of these systems. To deal with this, stateful defenses have been proposed to detect query attacks and prevent the generation of adversarial examples by monitoring and analyzing the sequence of queries received by the system. Several stateful defenses have been proposed in recent years. However, these defenses rely solely on similarity or out-of-distribution detection methods that may be effective in other domains. In the malware detection domain, the methods to generate adversarial examples are inherently different, and therefore we find that such detection mechanisms are significantly less effective. Hence, in this paper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19979;&#27700;&#24179;&#24378;&#20984;&#24615;&#30340;&#21452;&#23618;&#20248;&#21270;&#22343;&#20540;&#20056;&#27861;&#26041;&#27861;(sl-BAMM)&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#23618;&#30446;&#26631;&#27714;&#24179;&#22343;&#20540;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;BLO&#20013;&#39640;&#25928;&#32780;&#31616;&#21333;&#30340;&#27714;&#35299;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#35770;&#25991;&#30340;&#20998;&#26512;&#19981;&#38656;&#35201;&#24378;&#26799;&#24230;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#36866;&#29992;&#33539;&#22260;&#26356;&#24191;&#27867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03407</link><description>&lt;p&gt;
&#26080;&#38656;&#36739;&#24378;&#19979;&#27700;&#24179;&#20984;&#24615;&#30340;&#21452;&#23618;&#20248;&#21270;&#30340;&#22343;&#20540;&#20056;&#27861;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity. (arXiv:2302.03407v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19979;&#27700;&#24179;&#24378;&#20984;&#24615;&#30340;&#21452;&#23618;&#20248;&#21270;&#22343;&#20540;&#20056;&#27861;&#26041;&#27861;(sl-BAMM)&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#23618;&#30446;&#26631;&#27714;&#24179;&#22343;&#20540;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;BLO&#20013;&#39640;&#25928;&#32780;&#31616;&#21333;&#30340;&#27714;&#35299;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#35770;&#25991;&#30340;&#20998;&#26512;&#19981;&#38656;&#35201;&#24378;&#26799;&#24230;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#36866;&#29992;&#33539;&#22260;&#26356;&#24191;&#27867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#39046;&#22495;&#21452;&#23618;&#20248;&#21270;(BLO)&#30340;&#20027;&#27969;&#25216;&#26415;&#12290;&#29616;&#26377;&#24037;&#20316;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#35201;&#20040;&#26377;&#38480;&#30340;&#19979;&#27700;&#24179;&#20984;&#24615;(LLSC)&#26465;&#20214;&#65292;&#35201;&#20040;&#35299;&#20915;&#19968;&#31995;&#21015;&#39640;&#31934;&#24230;&#30340;&#36817;&#20284;&#23376;&#38382;&#39064;&#65292;&#25110;&#32773;&#20004;&#32773;&#20860;&#26377;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19978;&#19979;&#23618;&#30446;&#26631;&#27714;&#24179;&#22343;&#20540;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;BLO&#30340;&#21333;&#24490;&#29615;&#22343;&#20540;&#20056;&#27861;&#21452;&#23618;(sl-BAMM)&#26041;&#27861;&#65292;&#24182;&#25670;&#33073;&#20102;&#26377;&#38480;&#30340;LLSC&#32422;&#26463;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;sl-BAMM&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;KKT&#24179;&#31283;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#27604;&#36739;&#20248;&#21183;&#22312;&#20110;&#19981;&#38656;&#35201;&#24378;&#26799;&#24230;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#27861;&#24635;&#26159;&#38656;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#23433;&#20840;&#22320;&#25429;&#25417;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#19978;&#23618;&#30446;&#26631;&#30456;&#23545;&#20110;&#19979;&#23618;&#21464;&#37327;&#26159;&#20108;&#27425;&#30340;&#24773;&#20917;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning fields. The validity of existing works heavily rely on either a restrictive Lower-Level Strong Convexity (LLSC) condition or on solving a series of approximation subproblems with high accuracy or both. In this work, by averaging the upper and lower level objectives, we propose a single loop Bi-level Averaged Method of Multipliers (sl-BAMM) for BLO that is simple yet efficient for large-scale BLO and gets rid of the limited LLSC restriction. We further provide non-asymptotic convergence analysis of sl-BAMM towards KKT stationary points, and the comparative advantage of our analysis lies in the absence of strong gradient boundedness assumption, which is always required by others. Thus our theory safely captures a wider variety of applications in deep learning, especially where the upper-level objective is quadratic w.r.t. the lower-level variable. Experimental results demonstrate the superiorit
&lt;/p&gt;</description></item><item><title>UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.13741</link><description>&lt;p&gt;
UPop&#65306;&#29992;&#20110;&#21387;&#32553;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13741
&lt;/p&gt;
&lt;p&gt;
UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20854;&#20013;&#35270;&#35273;&#21644;&#35821;&#35328;&#26159;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#20004;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#37325;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;Transformer&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#27169;&#22411;&#21387;&#32553;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21387;&#32553;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;&#35821;&#35328;Transformer&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UPop&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;1&#65289;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#22312;&#36830;&#32493;&#20248;&#21270;&#31354;&#38388;&#20013;&#32479;&#19968;&#25628;&#32034;&#22810;&#27169;&#24577;&#23376;&#32593;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#21387;&#32553;&#27169;&#24577;&#21644;&#32467;&#26500;&#20043;&#38388;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65307;2&#65289;&#28176;&#36827;&#24335;&#25628;&#32034;&#21644;&#24494;&#35843;&#23376;&#32593;&#65292;&#20174;&#32780;&#20445;&#25345;&#25628;&#32034;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#25910;&#25947;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \textit{e}.\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive \textbf{P}runing (\textbf{\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21512;&#25104;&#25511;&#21046;&#29702;&#35770;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26377;&#30028;&#36951;&#25022;&#30340;&#25512;&#33616;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#31934;&#30830;&#30693;&#35782;&#12289;&#28508;&#22312;&#21327;&#21464;&#37327;&#30340;&#23384;&#22312;&#12289;&#19981;&#22343;&#21248;&#30340;&#29992;&#25143;&#21040;&#36798;&#36895;&#29575;&#21644;&#29992;&#25143;&#36873;&#25321;&#36864;&#20986;&#31169;&#20154;&#25968;&#25454;&#36319;&#36394;&#31561;&#23454;&#36341;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12571</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#25511;&#21046;&#29702;&#35770;&#23454;&#29616;&#26377;&#30028;&#65288;O(1)&#65289;&#36951;&#25022;&#30340;&#25512;&#33616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bounded (O(1)) Regret Recommendation Learning via Synthetic Controls Oracle. (arXiv:2301.12571v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12571
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#25511;&#21046;&#29702;&#35770;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26377;&#30028;&#36951;&#25022;&#30340;&#25512;&#33616;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#31934;&#30830;&#30693;&#35782;&#12289;&#28508;&#22312;&#21327;&#21464;&#37327;&#30340;&#23384;&#22312;&#12289;&#19981;&#22343;&#21248;&#30340;&#29992;&#25143;&#21040;&#36798;&#36895;&#29575;&#21644;&#29992;&#25143;&#36873;&#25321;&#36864;&#20986;&#31169;&#20154;&#25968;&#25454;&#36319;&#36394;&#31561;&#23454;&#36341;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#25506;&#32034;&#31995;&#32479;&#20013;&#65292;&#24403;&#20855;&#26377;&#22266;&#23450;&#20559;&#22909;&#30340;&#29992;&#25143;&#37325;&#22797;&#21040;&#36798;&#26102;&#65292;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#23558;&#31995;&#32479;&#24314;&#27169;&#20026;&#32447;&#24615;&#24773;&#22659;&#24191;&#21578;&#24102;&#26469;O(1)&#30340;&#26377;&#30028;&#36951;&#25022;&#12290;&#36825;&#20010;&#32467;&#26524;&#21487;&#33021;&#23545;&#25512;&#33616;&#31995;&#32479;&#20855;&#26377;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#36890;&#24120;&#26159;&#30701;&#26242;&#30340;&#65292;&#21363;&#25506;&#32034;&#26412;&#36523;&#21487;&#33021;&#22312;&#28508;&#22312;&#30340;&#38271;&#26399;&#38750;&#31283;&#24577;&#24320;&#22987;&#20043;&#21069;&#24456;&#24555;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#32447;&#24615;&#27169;&#22411;&#30340;&#31934;&#30830;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#28508;&#22312;&#21327;&#21464;&#37327;&#30340;&#23384;&#22312;&#65292;&#19981;&#22343;&#21248;&#30340;&#29992;&#25143;&#21040;&#36798;&#36895;&#29575;&#65292;&#23545;&#24517;&#35201;&#31561;&#32423;&#26465;&#20214;&#30340;&#35299;&#37322;&#20197;&#21450;&#29992;&#25143;&#36873;&#25321;&#36864;&#20986;&#31169;&#20154;&#25968;&#25454;&#36319;&#36394;&#31561;&#37117;&#38656;&#35201;&#22312;&#23454;&#38469;&#30340;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#20197;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#28982;&#23454;&#29616;&#20102;&#26377;&#30028;&#36951;&#25022;&#12290;&#38500;&#20102;&#35777;&#26126;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25152;&#20570;&#30340;&#20851;&#38190;&#21306;&#21035;&#24615;&#20551;&#35774;&#26159;&#26377;&#25928;&#21512;&#25104;&#25511;&#21046;&#29702;&#35770;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online exploration systems where users with fixed preferences repeatedly arrive, it has recently been shown that O(1), i.e., bounded regret, can be achieved when the system is modeled as a linear contextual bandit. This result may be of interest for recommender systems, where the popularity of their items is often short-lived, as the exploration itself may be completed quickly before potential long-run non-stationarities come into play. However, in practice, exact knowledge of the linear model is difficult to justify. Furthermore, potential existence of unobservable covariates, uneven user arrival rates, interpretation of the necessary rank condition, and users opting out of private data tracking all need to be addressed for practical recommender system applications. In this work, we conduct a theoretical study to address all these issues while still achieving bounded regret. Aside from proof techniques, the key differentiating assumption we make here is the presence of effective Sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WDC&#20135;&#21697;&#20316;&#20026;&#19968;&#20010;&#22810;&#32500;&#23454;&#20307;&#21305;&#37197;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#35282;&#33853;&#26696;&#20363;&#25968;&#37327;&#12289;&#26410;&#35265;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#24320;&#21457;&#38598;&#22823;&#23567;&#31561;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#35780;&#20272;&#23454;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09521</link><description>&lt;p&gt;
WDC&#20135;&#21697;&#65306;&#19968;&#20010;&#22810;&#32500;&#23454;&#20307;&#21305;&#37197;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
WDC Products: A Multi-Dimensional Entity Matching Benchmark. (arXiv:2301.09521v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WDC&#20135;&#21697;&#20316;&#20026;&#19968;&#20010;&#22810;&#32500;&#23454;&#20307;&#21305;&#37197;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#35282;&#33853;&#26696;&#20363;&#25968;&#37327;&#12289;&#26410;&#35265;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#24320;&#21457;&#38598;&#22823;&#23567;&#31561;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#35780;&#20272;&#23454;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#20219;&#21153;&#30340;&#38590;&#24230;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#32452;&#21512;&#65292;&#20363;&#22914;&#35282;&#33853;&#26696;&#20363;&#23545;&#30340;&#25968;&#37327;&#12289;&#35757;&#32451;&#38598;&#20013;&#26410;&#35265;&#36807;&#30340;&#23454;&#20307;&#30340;&#27604;&#20363;&#20197;&#21450;&#24320;&#21457;&#38598;&#30340;&#22823;&#23567;&#12290;&#30446;&#21069;&#30340;&#23454;&#20307;&#21305;&#37197;&#22522;&#20934;&#36890;&#24120;&#20195;&#34920;&#36825;&#20123;&#32500;&#24230;&#19978;&#30340;&#21333;&#20010;&#28857;&#65292;&#25110;&#32773;&#25552;&#20379;&#20102;&#27839;&#30528;&#21333;&#20010;&#32500;&#24230;&#35780;&#20272;&#21305;&#37197;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;WDC&#20135;&#21697;&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#20307;&#21305;&#37197;&#22522;&#20934;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#21305;&#37197;&#31995;&#32479;&#22312;&#19977;&#20010;&#32500;&#24230;&#32452;&#21512;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#21516;&#26102;&#20381;&#36182;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#36825;&#19977;&#20010;&#32500;&#24230;&#26159;(i) &#35282;&#33853;&#26696;&#20363;&#30340;&#25968;&#37327; (ii) &#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450; (iii) &#24320;&#21457;&#38598;&#30340;&#22823;&#23567;&#65288;&#35757;&#32451;&#38598;&#21152;&#39564;&#35777;&#38598;&#65289;&#12290;&#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#30446;&#21069;&#20219;&#20309;&#24050;&#26377;&#30340;&#33521;&#35821;&#22522;&#20934;&#25152;&#19981;&#21253;&#21547;&#30340;&#32500;&#24230;&#65292;&#20294;&#23545;&#20110;&#35780;&#20272;&#23454;&#20307;&#21305;&#37197;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of an entity matching task depends on a combination of multiple factors such as the amount of corner-case pairs, the fraction of entities in the test set that have not been seen during training, and the size of the development set. Current entity matching benchmarks usually represent single points in the space along such dimensions or they provide for the evaluation of matching methods along a single dimension, for instance the amount of training data. This paper presents WDC Products, an entity matching benchmark which provides for the systematic evaluation of matching systems along combinations of three dimensions while relying on real-world data. The three dimensions are (i) amount of corner-cases (ii) generalization to unseen entities, and (iii) development set size (training set plus validation set). Generalization to unseen entities is a dimension not covered by any of the existing English-language benchmarks yet but is crucial for evaluating the robustness of enti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26679;&#26412;&#22806;&#20445;&#35777;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#27169;&#22411;&#37197;&#32622;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#23545;&#20915;&#31574;&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.08649</link><description>&lt;p&gt;
&#20855;&#26377;&#26679;&#26412;&#22806;&#20445;&#35777;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation with Out-of-Sample Guarantees. (arXiv:2301.08649v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26679;&#26412;&#22806;&#20445;&#35777;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#27169;&#22411;&#37197;&#32622;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#23545;&#20915;&#31574;&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#36807;&#21435;&#30340;&#35266;&#23519;&#25968;&#25454;&#35780;&#20272;&#20915;&#31574;&#31574;&#30053;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#31574;&#30053;&#30340;&#32467;&#26524;&#20197;&#25439;&#22833;&#65288;&#21363;&#22833;&#25928;&#25110;&#36127;&#22870;&#21169;&#65289;&#26469;&#34913;&#37327;&#65292;&#20027;&#35201;&#38382;&#39064;&#26159;&#22312;&#20197;&#19981;&#21516;&#19988;&#21487;&#33021;&#26410;&#30693;&#30340;&#31574;&#30053;&#19979;&#35266;&#23519;&#21040;&#36807;&#21435;&#25968;&#25454;&#26102;&#23545;&#20854;&#26679;&#26412;&#22806;&#25439;&#22833;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#20351;&#29992;&#26679;&#26412;&#20998;&#21106;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#32472;&#21046;&#36825;&#26679;&#30340;&#25512;&#26029;&#65292;&#24182;&#23545;&#25972;&#20010;&#25439;&#22833;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#20854;&#22343;&#20540;&#36827;&#34892;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#20445;&#35777;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#23545;&#36807;&#21435;&#31574;&#30053;&#30340;&#27169;&#22411;&#37197;&#32622;&#38169;&#35823;&#65292;&#21253;&#25324;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#35780;&#20272;&#26041;&#27861;&#21487;&#29992;&#20110;&#22312;&#19968;&#23450;&#21487;&#20449;&#27169;&#22411;&#20551;&#35774;&#33539;&#22260;&#20869;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#23545;&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finite-sample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy - including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions.
&lt;/p&gt;</description></item><item><title>&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2301.06535</link><description>&lt;p&gt;
&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65306;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#30340;&#39640;&#38454;&#20132;&#20114;&#30340;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06535
&lt;/p&gt;
&lt;p&gt;
&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#25968;&#25454;&#39537;&#21160;&#30340;&#21327;&#21464;&#37327;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#27604;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#26696;&#20363;&#22522;&#30784;&#25277;&#26679;&#26694;&#26550;&#19982;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#26041;&#26696;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#33258;&#28982;&#22320;&#32771;&#34385;&#21040;&#25130;&#23614;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#26102;&#38388;&#36755;&#20837;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;CBNNs&#36890;&#36807;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#21051;&#20107;&#20214;&#21457;&#29983;&#30340;&#27010;&#29575;&#26469;&#20272;&#35745;&#21361;&#38505;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#20351;&#29992;&#20004;&#20010;&#26102;&#38388;&#20381;&#36182;&#25351;&#26631;&#27604;&#36739;CBNNs&#19982;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#22797;&#26434;&#22522;&#32447;&#39118;&#38505;&#21644;&#26102;&#38388;&#21464;&#21270;&#20132;&#20114;&#30340;&#27169;&#25311;&#26469;&#35780;&#20272;&#25152;&#26377;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;CBNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#39034;&#24207;&#33258;&#21160;&#32534;&#30721;&#22120;&#20174;&#31070;&#32463;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#65292;&#21457;&#29616;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20026;&#22522;&#30784;&#30340; SAES &#22312;&#20934;&#30830;&#29575;&#21644;&#32500;&#24230;&#26041;&#38754;&#20248;&#20110;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340; SAES&#12290;</title><link>http://arxiv.org/abs/2212.03771</link><description>&lt;p&gt;
&#34920;&#29616;&#21147;&#26550;&#26500;&#22686;&#24378;&#22522;&#20110;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#20154;&#32676;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expressive architectures enhance interpretability of dynamics-based neural population models. (arXiv:2212.03771v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03771
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#39034;&#24207;&#33258;&#21160;&#32534;&#30721;&#22120;&#20174;&#31070;&#32463;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#65292;&#21457;&#29616;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20026;&#22522;&#30784;&#30340; SAES &#22312;&#20934;&#30830;&#29575;&#21644;&#32500;&#24230;&#26041;&#38754;&#20248;&#20110;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340; SAES&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#35760;&#24405;&#30340;&#31070;&#32463;&#27963;&#21160;&#20013;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20026;&#35782;&#21035;&#21644;&#35299;&#37322;&#29983;&#29289;&#35745;&#31639;&#20013;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#27169;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#20165;&#20973;&#31070;&#32463;&#26041;&#24046;&#26080;&#27861;&#21807;&#19968;&#30830;&#23450;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#24212;&#35813;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#21644;&#20302;&#32500;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39034;&#24207;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAEs&#65289;&#22312;&#20174;&#27169;&#25311;&#30340;&#31070;&#32463;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37319;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20026;&#21160;&#21147;&#23398;&#22522;&#30784;&#30340;SAEs&#26080;&#27861;&#22312;&#30495;&#23454;&#30340;&#28508;&#22312;&#29366;&#24577;&#32500;&#24230;&#19978;&#25512;&#26029;&#20986;&#20934;&#30830;&#30340;&#21457;&#23556;&#29575;&#65292;&#24182;&#19988;&#26356;&#22823;&#30340;RNNs&#20381;&#36182;&#20110;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#20026;&#22522;&#30784;&#30340;SAEs&#22312;&#30495;&#23454;&#30340;&#28508;&#22312;&#29366;&#24577;&#32500;&#24230;&#19978;&#25512;&#26029;&#20986;&#20934;&#30830;&#30340;&#29575;&#65292;&#21516;&#26102;&#36824;&#24674;&#22797;&#20102;&#28508;&#22312;&#36712;&#36857;&#21644;&#22266;&#23450;&#28857;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks that can recover latent dynamics from recorded neural activity may provide a powerful avenue for identifying and interpreting the dynamical motifs underlying biological computation. Given that neural variance alone does not uniquely determine a latent dynamical system, interpretable architectures should prioritize accurate and low-dimensional latent dynamics. In this work, we evaluated the performance of sequential autoencoders (SAEs) in recovering latent chaotic attractors from simulated neural datasets. We found that SAEs with widely-used recurrent neural network (RNN)-based dynamics were unable to infer accurate firing rates at the true latent state dimensionality, and that larger RNNs relied upon dynamical features not present in the data. On the other hand, SAEs with neural ordinary differential equation (NODE)-based dynamics inferred accurate rates at the true latent state dimensionality, while also recovering latent trajectories and fixed point structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;HBFP&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21442;&#25968;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#35757;&#32451;&#20013;&#31639;&#26415;&#25805;&#20316;&#30340;&#26356;&#23567;&#32534;&#30721;&#12290;&#20351;&#29992;&#20998;&#26512;&#27169;&#22411;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;HBFP&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#31639;&#26415;&#23494;&#24230;&#22686;&#21152;&#39640;&#36798;$21.3\times$&#12290;</title><link>http://arxiv.org/abs/2211.10737</link><description>&lt;p&gt;
&#25552;&#39640;&#20934;&#30830;&#24615;: &#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;&#22359;&#28014;&#28857;&#26041;&#27861;&#29992;&#20110;DNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training. (arXiv:2211.10737v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;HBFP&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21442;&#25968;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#35757;&#32451;&#20013;&#31639;&#26415;&#25805;&#20316;&#30340;&#26356;&#23567;&#32534;&#30721;&#12290;&#20351;&#29992;&#20998;&#26512;&#27169;&#22411;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;HBFP&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#31639;&#26415;&#23494;&#24230;&#22686;&#21152;&#39640;&#36798;$21.3\times$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#12289;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#21069;&#25152;&#26410;&#26377;&#22686;&#38271;&#23548;&#33268;&#20102;&#23545;&#35745;&#31639;&#30340;&#24040;&#22823;&#38656;&#27714;&#21644;&#23545;&#26368;&#23567;&#32534;&#30721;&#30340;&#25628;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#24352;&#20351;&#29992;&#28151;&#21512;&#22359;&#28014;&#28857;(HBFP)&#26469;&#26368;&#23567;&#21270;&#21152;&#36895;&#22120;&#20013;&#30340;&#30789;&#37197;&#22791;&#65292;&#36890;&#36807;&#23558;&#22823;&#37096;&#20998;&#35757;&#32451;&#20013;&#30340;&#31639;&#26415;&#25805;&#20316;&#36716;&#25442;&#20026;8&#20301;&#23450;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#24037;&#20855;&#23545;HBFP&#35774;&#35745;&#31354;&#38388;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#21508;&#23618;&#21644;&#21508;&#20010;&#26102;&#20195;&#20013;&#26356;&#23567;&#32534;&#30721;&#30340;&#26426;&#20250;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Accuracy Boosters&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;HBFP&#25216;&#26415;&#65292;&#21482;&#22312;&#26368;&#21518;&#19968;&#20010;&#26102;&#20195;&#21644;&#31532;&#19968;&#20010;/&#26368;&#21518;&#19968;&#23618;&#20013;&#20351;&#29992;6&#20301;&#23614;&#25968;&#65292;&#22312;&#35757;&#32451;&#20013;&#30340;&#20854;&#20182;&#31639;&#26415;&#25805;&#20316;&#20013;&#20351;&#29992;4&#20301;&#23614;&#25968;&#36798;&#21040;$99.7\%$&#12290;&#20351;&#29992;&#20998;&#26512;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Accuracy Boosters&#21487;&#20197;&#20351;HBFP&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#31639;&#26415;&#23494;&#24230;&#22686;&#21152;&#39640;&#36798;$21.3\times$&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented growth in DNN model complexity, size, and amount of training data has led to a commensurate increase in demand for computing and a search for minimal encoding. Recent research advocates Hybrid Block Floating Point (HBFP) to minimize silicon provisioning in accelerators by converting the majority of arithmetic operations in training to 8-bit fixed point. In this paper, we perform a full-scale exploration of the HBFP design space using mathematical tools to study the interplay among various parameters and identify opportunities for even smaller encodings across layers and epochs. Based on our findings, we propose Accuracy Boosters, an epoch-driven mixed-mantissa HBFP technique that uses 6-bit mantissas only in the last epoch and first/last layers, and 4-bit mantissas for $99.7\%$ of all other arithmetic operations in training. Using analytic models, we show Accuracy Boosters enable increasing arithmetic density for an HBFP training accelerator by up to $21.3\times$ comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#26694;&#26550;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;GEC&#65292;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23558;RL&#38382;&#39064;&#21010;&#20998;&#20026;&#20302;GEC&#21644;&#39640;GEC&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#23637;&#31034;&#20102;&#20302;GEC&#31867;&#21035;&#30340;&#20016;&#23500;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2211.01962</link><description>&lt;p&gt;
GEC: &#19968;&#31181;&#22312;MDP&#12289;POMDP&#21644;&#26356;&#22810;&#24773;&#20917;&#19979;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond. (arXiv:2211.01962v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#26694;&#26550;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;GEC&#65292;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23558;RL&#38382;&#39064;&#21010;&#20998;&#20026;&#20302;GEC&#21644;&#39640;GEC&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#23637;&#31034;&#20102;&#20302;GEC&#31867;&#21035;&#30340;&#20016;&#23500;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#26222;&#36941;&#26694;&#26550;&#19979;&#30340;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#21644;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSR&#65289;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#20026;&#20102;&#25214;&#21040;&#36171;&#20104;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24191;&#20041;eluder&#31995;&#25968;&#65288;GEC&#65289;&#65292;&#23427;&#34920;&#24449;&#20102;&#22312;&#32447;&#20132;&#20114;&#24335;&#20915;&#31574;&#20013;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GEC&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#26356;&#26032;&#31574;&#30053;&#24615;&#33021;&#30340;&#35823;&#24046;&#19982;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#35780;&#20272;&#30340;&#26679;&#26412;&#20869;&#35757;&#32451;&#35823;&#24046;&#65292;&#26469;&#34913;&#37327;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;GEC&#30340;RL&#38382;&#39064;&#24418;&#25104;&#20102;&#19968;&#20010;&#38750;&#24120;&#20016;&#23500;&#30340;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#20302;Bellman eluder&#32500;&#24230;&#38382;&#39064;&#12289;&#21452;&#32447;&#24615;&#31867;&#12289;&#20302;&#35777;&#20154;&#31209;&#38382;&#39064;&#12289;PO-&#21452;&#32447;&#24615;&#31867;&#21644;&#24191;&#20041;&#27491;&#21017;PSR&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sample efficient reinforcement learning (RL) under the general framework of interactive decision making, which includes Markov decision process (MDP), partially observable Markov decision process (POMDP), and predictive state representation (PSR) as special cases. Toward finding the minimum assumption that empowers sample efficient learning, we propose a novel complexity measure, generalized eluder coefficient (GEC), which characterizes the fundamental tradeoff between exploration and exploitation in online interactive decision making. In specific, GEC captures the hardness of exploration by comparing the error of predicting the performance of the updated policy with the in-sample training error evaluated on the historical data. We show that RL problems with low GEC form a remarkably rich class, which subsumes low Bellman eluder dimension problems, bilinear class, low witness rank problems, PO-bilinear class, and generalized regular PSR, where generalized regular PSR, a new tr
&lt;/p&gt;</description></item><item><title>ERL-Re$^2$&#25552;&#20986;&#20102;&#21452;&#23610;&#24230;&#29366;&#24577;&#34920;&#31034;&#21644;&#31574;&#30053;&#34920;&#31034;&#30340;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#24573;&#35270;&#20849;&#20139;&#30693;&#35782;&#21644;&#35821;&#20041;&#32423;&#34892;&#20026;&#36827;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.17375</link><description>&lt;p&gt;
ERL-Re$^2$: &#20855;&#26377;&#20849;&#20139;&#29366;&#24577;&#34920;&#31034;&#21644;&#20010;&#20307;&#31574;&#30053;&#34920;&#31034;&#30340;&#39640;&#25928;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation. (arXiv:2210.17375v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17375
&lt;/p&gt;
&lt;p&gt;
ERL-Re$^2$&#25552;&#20986;&#20102;&#21452;&#23610;&#24230;&#29366;&#24577;&#34920;&#31034;&#21644;&#31574;&#30053;&#34920;&#31034;&#30340;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#24573;&#35270;&#20849;&#20139;&#30693;&#35782;&#21644;&#35821;&#20041;&#32423;&#34892;&#20026;&#36827;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;Deep RL&#65289;&#21644;&#36827;&#21270;&#31639;&#27861;&#65288;EA&#65289;&#26159;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#21407;&#29702;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#21363;&#22522;&#20110;&#26799;&#24230;&#19982;&#22522;&#20110;&#38750;&#26799;&#24230;&#12290;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#36890;&#36807;&#34701;&#21512;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#26469;&#25972;&#21512;Deep RL&#21644;EA&#20197;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;Deep RL&#21644;EA&#32467;&#21512;&#30340;&#24037;&#20316;&#23384;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#32570;&#28857;&#65306;1&#65289;RL&#20195;&#29702;&#21644;EA&#20195;&#29702;&#20998;&#21035;&#23398;&#20064;&#20182;&#20204;&#30340;&#31574;&#30053;&#65292;&#24573;&#35270;&#20102;&#26377;&#29992;&#30340;&#20849;&#20139;&#30693;&#35782;&#30340;&#39640;&#25928;&#20849;&#20139;&#65307;2&#65289;&#21442;&#25968;&#32423;&#21035;&#30340;&#31574;&#30053;&#20248;&#21270;&#19981;&#33021;&#20445;&#35777;EA&#20391;&#30340;&#34892;&#20026;&#36827;&#21270;&#30340;&#35821;&#20041;&#32423;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#21452;&#23610;&#24230;&#29366;&#24577;&#34920;&#31034;&#21644;&#31574;&#30053;&#34920;&#31034;&#30340;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL-Re$^2$&#65289;&#65292;&#36825;&#26159;&#23545;&#21069;&#36848;&#20004;&#20010;&#32570;&#28857;&#30340;&#19968;&#31181;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;ERL-Re$^2$&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21452;&#23610;&#24230;&#34920;&#31034;&#65306;&#25152;&#26377;EA&#21644;RL&#31574;&#30053;&#20849;&#20139;&#30456;&#21516;&#30340;&#38750;&#32447;&#24615;&#29366;&#24577;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#20010;&#20307;&#32447;&#24615;&#31574;&#30053;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual} linear policy representat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;DiffSBDD&#26469;&#29983;&#25104;&#20855;&#26377;&#20146;&#21644;&#21147;&#21644;&#29305;&#24322;&#24615;&#30340;&#26032;&#22411;&#33647;&#29289;&#37197;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSBDD&#22312;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#24615;&#23545;&#25509;&#24471;&#20998;&#30340;&#22810;&#26679;&#21270;&#33647;&#29289;&#26679;&#37197;&#20307;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13695</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#19982;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structure-based Drug Design with Equivariant Diffusion Models. (arXiv:2210.13695v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;DiffSBDD&#26469;&#29983;&#25104;&#20855;&#26377;&#20146;&#21644;&#21147;&#21644;&#29305;&#24322;&#24615;&#30340;&#26032;&#22411;&#33647;&#29289;&#37197;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSBDD&#22312;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#24615;&#23545;&#25509;&#24471;&#20998;&#30340;&#22810;&#26679;&#21270;&#33647;&#29289;&#26679;&#37197;&#20307;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#65288;SBDD&#65289;&#26088;&#22312;&#35774;&#35745;&#19982;&#39044;&#23450;&#30340;&#34507;&#30333;&#38774;&#28857;&#20855;&#26377;&#39640;&#20146;&#21644;&#21147;&#21644;&#29305;&#24322;&#24615;&#30340;&#23567;&#20998;&#23376;&#37197;&#20307;&#12290;&#26412;&#25991;&#23558;SBDD&#34920;&#36848;&#20026;&#19968;&#20010;&#19977;&#32500;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;DiffSBDD&#65292;&#36825;&#26159;&#19968;&#20010;SE(3)-&#31561;&#21464;&#30340;&#19977;&#32500;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#34507;&#30333;&#21475;&#34955;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#26032;&#22411;&#37197;&#20307;&#12290;&#20840;&#38754;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DiffSBDD&#22312;&#29983;&#25104;&#20855;&#26377;&#20855;&#26377;&#31454;&#20105;&#24615;&#23545;&#25509;&#24471;&#20998;&#30340;&#26032;&#39062;&#21644;&#22810;&#26679;&#30340;&#33647;&#29289;&#26679;&#37197;&#20307;&#26041;&#38754;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25193;&#25955;&#26694;&#26550;&#22312;&#33647;&#29289;&#35774;&#35745;&#27963;&#21160;&#20013;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#28789;&#27963;&#24615;&#65292;&#20363;&#22914;&#21363;&#25554;&#21363;&#29992;&#30340;&#24615;&#36136;&#20248;&#21270;&#21644;&#20174;&#23616;&#37096;&#20998;&#23376;&#35774;&#35745;&#24102;&#26377;&#20462;&#34917;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design (SBDD) aims to design small-molecule ligands that bind with high affinity and specificity to pre-determined protein targets. In this paper, we formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an SE(3)-equivariant 3D-conditional diffusion model that generates novel ligands conditioned on protein pockets. Comprehensive in silico experiments demonstrate the efficiency and effectiveness of DiffSBDD in generating novel and diverse drug-like ligands with competitive docking scores. We further explore the flexibility of the diffusion framework for a broader range of tasks in drug design campaigns, such as off-the-shelf property optimization and partial molecular design with inpainting.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#39044;&#27979;&#20174;&#19987;&#23478;&#20013;&#35299;&#20915;&#38544;&#31169;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32431;&#24046;&#20998;&#38544;&#31169;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#23545;&#20110;&#24858;&#34850;&#25932;&#25163;&#65292;&#22312;&#39640;&#32500;&#33539;&#22260;&#20869;&#30340;&#36951;&#25022;&#21487;&#20197;&#36798;&#21040;&#20122;&#32447;&#24615;&#27700;&#24179;&#65292;&#19982;&#33258;&#36866;&#24212;&#23545;&#25163;&#21644;&#38750;&#33258;&#36866;&#24212;&#23545;&#25163;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#36951;&#25022;&#26368;&#20248;&#24615;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2210.13537</link><description>&lt;p&gt;
&#20174;&#19987;&#23478;&#36827;&#34892;&#31169;&#23494;&#22312;&#32447;&#39044;&#27979;: &#20998;&#31163;&#21644;&#26356;&#24555;&#30340;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Private Online Prediction from Experts: Separations and Faster Rates. (arXiv:2210.13537v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#39044;&#27979;&#20174;&#19987;&#23478;&#20013;&#35299;&#20915;&#38544;&#31169;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32431;&#24046;&#20998;&#38544;&#31169;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#23545;&#20110;&#24858;&#34850;&#25932;&#25163;&#65292;&#22312;&#39640;&#32500;&#33539;&#22260;&#20869;&#30340;&#36951;&#25022;&#21487;&#20197;&#36798;&#21040;&#20122;&#32447;&#24615;&#27700;&#24179;&#65292;&#19982;&#33258;&#36866;&#24212;&#23545;&#25163;&#21644;&#38750;&#33258;&#36866;&#24212;&#23545;&#25163;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#36951;&#25022;&#26368;&#20248;&#24615;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#39044;&#27979;&#20174;&#19987;&#23478;&#20013;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#32463;&#26377;&#20960;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#38750;&#33258;&#36866;&#24212;&#23545;&#25163;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#23545;&#20110;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#36951;&#25022;&#30028;&#38480;&#20026;$\tilde{O}(\sqrt{T \log d} + \log d/\varepsilon)$&#65292;&#23545;&#20110;&#24858;&#34850;&#25932;&#25163;&#23454;&#29616;&#20102;&#36951;&#25022;&#30028;&#38480;&#20026;$\tilde{O}(\sqrt{T \log d} + T^{1/3} \log d/\varepsilon)$&#65288;&#20854;&#20013;$d$&#26159;&#19987;&#23478;&#25968;&#37327;&#65289;&#12290;&#23545;&#20110;&#32431;DP&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#39640;&#32500;&#33539;&#22260;$d \ge T$ &#30340;&#24858;&#34850;&#25932;&#25163;&#20013;&#33719;&#24471;&#20122;&#32447;&#24615;&#36951;&#25022;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#23545;&#25163;&#30340;&#26032;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#38750;&#31169;&#23494;&#35774;&#32622;&#19981;&#21516;&#65292;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#36866;&#24212;&#23545;&#25163;&#21644;&#38750;&#33258;&#36866;&#24212;&#23545;&#25163;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#36951;&#25022;&#26368;&#20248;&#24615;&#20998;&#31163;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#20063;&#23637;&#31034;&#20102;&#19968;&#31181;&#22312;&#33258;&#36866;&#24212;&#23545;&#25163;&#21644;&#38750;&#33258;&#36866;&#24212;&#23545;&#25163;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online prediction from experts is a fundamental problem in machine learning and several works have studied this problem under privacy constraints. We propose and analyze new algorithms for this problem that improve over the regret bounds of the best existing algorithms for non-adaptive adversaries. For approximate differential privacy, our algorithms achieve regret bounds of $\tilde{O}(\sqrt{T \log d} + \log d/\varepsilon)$ for the stochastic setting and $\tilde{O}(\sqrt{T \log d} + T^{1/3} \log d/\varepsilon)$ for oblivious adversaries (where $d$ is the number of experts). For pure DP, our algorithms are the first to obtain sub-linear regret for oblivious adversaries in the high-dimensional regime $d \ge T$. Moreover, we prove new lower bounds for adaptive adversaries. Our results imply that unlike the non-private setting, there is a strong separation between the optimal regret for adaptive and non-adaptive adversaries for this problem. Our lower bounds also show a separation between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#23450;&#20301;&#21644;&#36890;&#20449;&#30340;&#21327;&#20316;&#12290;&#35813;&#31639;&#27861;&#26088;&#22312;&#23454;&#29616;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#31227;&#21160;&#25509;&#20837;&#32593;&#32476;&#65292;&#20197;&#25903;&#25345;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2210.00945</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#38752;&#39640;&#25928;&#31227;&#21160;&#25509;&#20837;&#30340;&#22810;UAV&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Agent Deep Reinforcement Learning for Reliable and Energy-Efficient Mobile Access via Multi-UAV Control. (arXiv:2210.00945v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#23450;&#20301;&#21644;&#36890;&#20449;&#30340;&#21327;&#20316;&#12290;&#35813;&#31639;&#27861;&#26088;&#22312;&#23454;&#29616;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#31227;&#21160;&#25509;&#20837;&#32593;&#32476;&#65292;&#20197;&#25903;&#25345;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#23450;&#20301;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30340;&#21327;&#20316;&#65292;&#21363;UAV&#20316;&#20026;&#31227;&#21160;&#22522;&#31449;&#12290;&#35813;&#31639;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24314;&#31435;&#21487;&#38752;&#30340;&#31227;&#21160;&#25509;&#20837;&#32593;&#32476;&#65292;&#29992;&#20110;&#36710;&#32852;&#32593;&#65288;C-V2X&#65289;&#36890;&#20449;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#36136;&#37327;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#23454;&#29616;&#12290;&#21487;&#38752;&#30340;&#31227;&#21160;&#25509;&#20837;&#26381;&#21153;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#23454;&#29616;&#65292;&#21363;i&#65289;&#39640;&#25928;&#33021;&#28304;&#28040;&#32791;&#30340;UAV&#36816;&#34892;&#21644;ii&#65289;&#21487;&#38752;&#30340;&#26080;&#32447;&#36890;&#20449;&#26381;&#21153;&#12290;&#23545;&#20110;&#39640;&#25928;&#33021;&#28304;&#28040;&#32791;&#30340;UAV&#36816;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MADRL&#31639;&#27861;&#30340;&#22870;&#21169;&#21253;&#21547;UAV&#33021;&#28304;&#28040;&#32791;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#21487;&#38752;&#30340;&#26080;&#32447;&#36890;&#20449;&#26381;&#21153;&#65292;&#29992;&#25143;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#35201;&#27714;&#34987;&#35270;&#20026;&#22870;&#21169;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;60GHz&#27627;&#31859;&#27874;&#26080;&#32447;&#30005;&#36827;&#34892;&#31227;&#21160;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a novel multi-agent deep reinforcement learning (MADRL)-based positioning algorithm for multiple unmanned aerial vehicles (UAVs) collaboration (i.e., UAVs work as mobile base stations). The primary objective of the proposed algorithm is to establish dependable mobile access networks for cellular vehicle-to-everything (C-V2X) communication, thereby facilitating the realization of high-quality intelligent transportation systems (ITS). The reliable mobile access services can be achieved in following two ways, i.e., i) energy-efficient UAV operation and ii) reliable wireless communication services. For energy-efficient UAV operation, the reward of our proposed MADRL algorithm contains the features for UAV energy consumption models in order to realize efficient operations. Furthermore, for reliable wireless communication services, the quality of service (QoS) requirements of individual users are considered as a part of rewards and 60GHz mmWave radio is used for mobile a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.11549</link><description>&lt;p&gt;
MAGIC: &#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#23454;&#29616;&#22522;&#20110;&#25513;&#30721;&#30340;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#24102;&#26377;&#24378;&#27491;&#21017;&#21270;&#22120;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#26469;&#25511;&#21046;&#23545;&#21333;&#20010;&#22270;&#20687;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;MAGIC&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#32467;&#26500;&#21270;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36755;&#20837;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#20854;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20445;&#35777;&#21512;&#25104;&#30340;&#21487;&#20449;&#24230;&#12290;&#19982;&#30446;&#21069;&#20351;&#29992;&#22797;&#26434;&#21407;&#35821;&#26469;&#30417;&#30563;&#36807;&#31243;&#25110;&#20351;&#29992;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#24369;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGIC&#36890;&#36807;&#22312;&#36755;&#20837;&#19978;&#32858;&#21512;&#26799;&#24230;&#65292;&#30001;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#25512;&#21160;&#12290;MAGIC&#20197;&#21333;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#24378;&#28872;&#30340;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22312;&#37325;&#22797;&#29289;&#20307;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#32473;&#29992;&#25143;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#36712;&#36857;&#20013;&#23398;&#20064;&#20316;&#29992;Koopman&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20989;&#25968;&#36873;&#25321;&#21644;&#35266;&#27979;&#19981;&#23436;&#25972;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.09977</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#36712;&#36857;&#20013;&#23398;&#20064;&#20316;&#29992;Koopman&#29983;&#25104;&#22120;&#30340;&#21452;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Bilinear Models of Actuated Koopman Generators from Partially-Observed Trajectories. (arXiv:2209.09977v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#36712;&#36857;&#20013;&#23398;&#20064;&#20316;&#29992;Koopman&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20989;&#25968;&#36873;&#25321;&#21644;&#35266;&#27979;&#19981;&#23436;&#25972;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36924;&#36817;&#28508;&#22312;&#30340;Koopman&#31639;&#23376;&#25110;&#29983;&#25104;&#22120;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#39044;&#27979;&#12289;&#29305;&#24449;&#23398;&#20064;&#12289;&#29366;&#24577;&#20272;&#35745;&#21644;&#25511;&#21046;&#24037;&#20855;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#25511;&#21046;&#20223;&#23556;&#31995;&#32479;&#30340;Koopman&#29983;&#25104;&#22120;&#22312;&#36755;&#20837;&#26041;&#38754;&#20063;&#20855;&#26377;&#20223;&#23556;&#20381;&#36182;&#24615;&#65292;&#36827;&#32780;&#23548;&#33268;&#26041;&#20415;&#30340;&#26377;&#38480;&#32500;&#21452;&#32447;&#24615;&#36817;&#20284;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36817;&#20284;&#20855;&#26377;&#20316;&#29992;&#30340;Koopman&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#36873;&#25321;&#29992;&#20110;&#36924;&#36817;Koopman&#29983;&#25104;&#22120;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#23545;&#20110;&#38750;&#27979;&#24230;&#20445;&#25345;&#30340;&#31995;&#32479;&#30446;&#21069;&#27809;&#26377;&#26222;&#36866;&#30340;&#36873;&#25321;&#26041;&#24335;&#12290;&#20854;&#27425;&#65292;&#22914;&#26524;&#25105;&#20204;&#27809;&#26377;&#35266;&#27979;&#21040;&#23436;&#25972;&#30340;&#29366;&#24577;&#65292;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36275;&#22815;&#20016;&#23500;&#30340;&#36825;&#31867;&#20989;&#25968;&#38598;&#21512;&#26469;&#25551;&#36848;&#21160;&#24577;&#12290;&#36825;&#26159;&#22240;&#20026;&#36890;&#24120;&#24773;&#20917;&#19979;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#25551;&#36848;&#21160;&#24577;&#25152;&#38656;&#30340;&#36275;&#22815;&#20016;&#23500;&#30340;&#20989;&#25968;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven models for nonlinear dynamical systems based on approximating the underlying Koopman operator or generator have proven to be successful tools for forecasting, feature learning, state estimation, and control. It has become well known that the Koopman generators for control-affine systems also have affine dependence on the input, leading to convenient finite-dimensional bilinear approximations of the dynamics. Yet there are still two main obstacles that limit the scope of current approaches for approximating the Koopman generators of systems with actuation. First, the performance of existing methods depends heavily on the choice of basis functions over which the Koopman generator is to be approximated; and there is currently no universal way to choose them for systems that are not measure preserving. Secondly, if we do not observe the full state, we may not gain access to a sufficiently rich collection of such functions to describe the dynamics. This is because the commonly u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#32456;&#36523;&#23398;&#20064;&#33539;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIMIP&#30340;&#26041;&#27861;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24314;&#27169;MIP&#23454;&#20363;&#65292;&#36890;&#36807;&#24212;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2208.12226</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lifelong Learning for Neural powered Mixed Integer Programming. (arXiv:2208.12226v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#32456;&#36523;&#23398;&#20064;&#33539;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIMIP&#30340;&#26041;&#27861;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24314;&#27169;MIP&#23454;&#20363;&#65292;&#36890;&#36807;&#24212;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#36890;&#24120;&#36890;&#36807;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#26469;&#27714;&#35299;&#12290;&#26368;&#36817;&#65292;&#23398;&#20064;&#27169;&#20223;&#19987;&#23478;&#24378;&#20998;&#25903;&#21551;&#21457;&#24335;&#24555;&#36895;&#36924;&#36817;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#20943;&#23569;&#35299;&#20915;MIP&#38382;&#39064;&#30340;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#20998;&#25903;&#26041;&#27861;&#20551;&#35774;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#22312;&#21333;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#21487;&#29992;&#12290;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#65292;&#22914;&#26524;&#35757;&#32451;&#25968;&#25454;&#38543;&#26102;&#38388;&#36830;&#32493;&#25552;&#20379;&#65292;&#29616;&#26377;&#25216;&#26415;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36804;&#20170;&#20026;&#27490;&#26410;&#34987;&#25506;&#32034;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#32456;&#36523;&#23398;&#20064;&#33539;&#20363;&#12290;&#20026;&#20102;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LIMIP&#65292;&#23427;&#21033;&#29992;&#20102;&#23558;MIP&#23454;&#20363;&#24314;&#27169;&#20026;&#19968;&#20010;&#20108;&#20998;&#22270;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#20351;&#29992;&#20108;&#20998;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23558;&#20854;&#26144;&#23556;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#22312;&#36825;&#20010;&#20016;&#23500;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed Integer programs (MIPs) are typically solved by the Branch-and-Bound algorithm. Recently, Learning to imitate fast approximations of the expert strong branching heuristic has gained attention due to its success in reducing the running time for solving MIPs. However, existing learning-to-branch methods assume that the entire training data is available in a single session of training. This assumption is often not true, and if the training data is supplied in continual fashion over time, existing techniques suffer from catastrophic forgetting. In this work, we study the hitherto unexplored paradigm of Lifelong Learning to Branch on Mixed Integer Programs. To mitigate catastrophic forgetting, we propose LIMIP, which is powered by the idea of modeling an MIP instance in the form of a bipartite graph, which we map to an embedding space using a bipartite Graph Attention Network. This rich embedding space avoids catastrophic forgetting through the application of knowledge distillation an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25130;&#26029;&#26607;&#35199;&#38543;&#26426;&#25200;&#21160;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#31639;&#27861;&#20855;&#26377;&#31283;&#23450;&#24615;&#19982;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.00290</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#25130;&#26029;&#26607;&#35199;&#38543;&#26426;&#25200;&#21160;&#30340;&#28176;&#36827;&#24179;&#28369;&#20989;&#25968;&#31639;&#27861;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization. (arXiv:2208.00290v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25130;&#26029;&#26607;&#35199;&#38543;&#26426;&#25200;&#21160;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#31639;&#27861;&#20855;&#26377;&#31283;&#23450;&#24615;&#19982;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#19968;&#20010;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26159;&#22122;&#22768;&#25104;&#26412;&#26679;&#26412;&#30340;&#26399;&#26395;&#65292;&#32780;&#21482;&#26377;&#21518;&#32773;&#23545;&#20219;&#20309;&#32473;&#23450;&#30340;&#21442;&#25968;&#36827;&#34892;&#35266;&#27979;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37319;&#29992;&#24102;&#26377;&#38543;&#26426;&#25200;&#21160;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#26696;&#65292;&#36825;&#20123;&#25200;&#21160;&#20351;&#29992;&#20174;delta&#29699;&#20013;&#24471;&#21040;&#30340;&#25130;&#26029;&#26607;&#35199;&#20998;&#24067;&#24418;&#25104;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;&#32780;&#21442;&#25968;&#32500;&#25968;&#24456;&#39640;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#20174;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#30830;&#23450;&#22320;&#25910;&#25947;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#38598;&#21512;&#65292;&#24182;&#33719;&#24471;&#20102;&#25910;&#25947;&#30340;&#28176;&#36817;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36991;&#20813;&#20102;&#19981;&#31283;&#23450;&#30340;&#24179;&#34913;&#28857;&#65292;&#24847;&#21619;&#30528;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24314;&#31435;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#20445;&#35777;&#25910;&#25947;&#29575;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#26159;&#23454;&#25968;&#20540;&#30340;&#21644;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a stochastic gradient algorithm for minimizing a smooth objective function that is an expectation over noisy cost samples, and only the latter are observed for any given parameter. Our algorithm employs a gradient estimation scheme with random perturbations, which are formed using the truncated Cauchy distribution from the delta sphere. We analyze the bias and variance of the proposed gradient estimator. Our algorithm is found to be particularly useful in the case when the objective function is non-convex, and the parameter dimension is high. From an asymptotic convergence analysis, we establish that our algorithm converges almost surely to the set of stationary points of the objective function and obtains the asymptotic convergence rate. We also show that our algorithm avoids unstable equilibria, implying convergence to local minima. Further, we perform a non-asymptotic convergence analysis of our algorithm. In particular, we establish here a non-asymptotic b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#29616;&#20195;GPU&#24352;&#37327;&#26680;&#24515;&#12289;&#26412;&#22320;CUDA&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20197;&#21450;&#20855;&#26377;&#23439;&#21333;&#20803;&#21152;&#36895;&#30340;&#28210;&#26579;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20132;&#20114;&#24335;&#30340;&#20307;&#31215;&#31070;&#32463;&#34920;&#31034;&#20809;&#32447;&#36861;&#36394;&#12290;&#36825;&#31181;&#31070;&#32463;&#34920;&#31034;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21644;&#32039;&#20945;&#24615;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#31163;&#26680;&#35757;&#32451;&#31574;&#30053;&#20197;&#25903;&#25345;&#26497;&#22823;&#35268;&#27169;&#30340;&#20307;&#31215;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.11620</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#20132;&#20114;&#24335;&#20307;&#31215;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Volume Visualization via Multi-Resolution Hash Encoding based Neural Representation. (arXiv:2207.11620v3 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#29616;&#20195;GPU&#24352;&#37327;&#26680;&#24515;&#12289;&#26412;&#22320;CUDA&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20197;&#21450;&#20855;&#26377;&#23439;&#21333;&#20803;&#21152;&#36895;&#30340;&#28210;&#26579;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20132;&#20114;&#24335;&#30340;&#20307;&#31215;&#31070;&#32463;&#34920;&#31034;&#20809;&#32447;&#36861;&#36394;&#12290;&#36825;&#31181;&#31070;&#32463;&#34920;&#31034;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21644;&#32039;&#20945;&#24615;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#31163;&#26680;&#35757;&#32451;&#31574;&#30053;&#20197;&#25903;&#25345;&#26497;&#22823;&#35268;&#27169;&#30340;&#20307;&#31215;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#21387;&#32553;&#20307;&#31215;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#21270;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#31181;&#20307;&#31215;&#31070;&#32463;&#34920;&#31034;&#36804;&#20170;&#21482;&#34987;&#24212;&#29992;&#20110;&#31163;&#32447;&#25968;&#25454;&#22788;&#29702;&#21644;&#38750;&#20132;&#20114;&#24335;&#28210;&#26579;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#29616;&#20195;GPU&#24352;&#37327;&#26680;&#24515;&#12289;&#26412;&#22320;CUDA&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20197;&#21450;&#20855;&#26377;&#23439;&#21333;&#20803;&#21152;&#36895;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#28210;&#26579;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20132;&#20114;&#22320;Ray Tracing&#20307;&#31215;&#31070;&#32463;&#34920;&#31034;&#65288;10-60&#24103;/&#31186;&#65289;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#34920;&#31034;&#20063;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#65288;PSNR &gt; 30dB&#65289;&#21644;&#32039;&#20945;&#24615;&#65288;&#22823;&#23567;&#20943;&#23567;&#20102;10-1000&#20493;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#28210;&#26579;&#24490;&#29615;&#20869;&#23436;&#20840;&#36339;&#36807;&#39044;&#35757;&#32451;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#65292;&#23558;&#25972;&#20010;&#35757;&#32451;&#27493;&#39588;&#36866;&#24212;&#20110;&#28210;&#26579;&#24490;&#29615;&#20013;&#12290;&#20026;&#20102;&#25903;&#25345;&#26497;&#22823;&#35268;&#27169;&#30340;&#20307;&#31215;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31163;&#26680;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#30340;&#20307;&#31215;&#31070;&#32463;&#34920;&#31034;&#35757;&#32451;&#33021;&#22815;&#28508;&#22312;&#22320;&#25193;&#23637;&#21040;&#20351;&#29992;&#20165;&#19968;&#20010;N&#36827;&#34892;TeraScale&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown great potential in compressing volume data for visualization. However, due to the high cost of training and inference, such volumetric neural representations have thus far only been applied to offline data processing and non-interactive rendering. In this paper, we demonstrate that by simultaneously leveraging modern GPU tensor cores, a native CUDA neural network framework, and a well-designed rendering algorithm with macro-cell acceleration, we can interactively ray trace volumetric neural representations (10-60fps). Our neural representations are also high-fidelity (PSNR &gt; 30dB) and compact (10-1000x smaller). Additionally, we show that it is possible to fit the entire training step inside a rendering loop and skip the pre-training process completely. To support extreme-scale volume data, we also develop an efficient out-of-core training strategy, which allows our volumetric neural representation training to potentially scale up to terascale using only an N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#36141;&#20080;&#39034;&#24207;&#20197;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#36141;&#20080;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2207.06225</link><description>&lt;p&gt;
&#38024;&#23545;&#19979;&#19968;&#27425;&#36141;&#20080;&#39044;&#27979;&#30340;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation Model for Next Purchase Prediction. (arXiv:2207.06225v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#36141;&#20080;&#39034;&#24207;&#20197;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#36141;&#20080;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20379;&#24403;&#20195;&#25968;&#23383;&#33829;&#38144;&#20307;&#39564;&#26102;&#65292;&#25512;&#33616;&#30340;&#26102;&#25928;&#24615;&#21644;&#19978;&#19979;&#25991;&#20934;&#30830;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#30340;&#36807;&#21435;&#36141;&#20080;&#35760;&#24405;&#21521;&#29992;&#25143;&#25512;&#33616;&#30456;&#20851;&#20294;&#19981;&#21463;&#26102;&#38388;&#24433;&#21709;&#30340;&#29289;&#21697;&#12290;&#36825;&#20123;&#25512;&#33616;&#21482;&#26159;&#31526;&#21512;&#29992;&#25143;&#30340;&#19968;&#33324;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#29992;&#25143;&#22312;&#36141;&#20080;&#20043;&#21069;&#30340;&#20855;&#20307;&#38656;&#27714;&#12290;&#30456;&#21453;&#65292;&#32771;&#34385;&#20132;&#26131;&#12289;&#36141;&#20080;&#25110;&#20307;&#39564;&#39034;&#24207;&#26469;&#34913;&#37327;&#29992;&#25143;&#28436;&#21270;&#20559;&#22909;&#30340;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25512;&#33616;&#65306;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#33021;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#24403;&#21069;&#38656;&#27714;&#30340;&#34892;&#20026;&#65292;&#36824;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20221;&#21253;&#21547;&#36229;&#36807;2.7&#30334;&#19975;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#21644;46K&#20010;&#25345;&#21345;&#20154;&#30340;&#29983;&#20135;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#24182;&#25490;&#21517;&#20102;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#23545;&#21407;&#22987;&#30340;&#20132;&#26131;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#28982;&#21518;&#25552;&#20132;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timeliness and contextual accuracy of recommendations are increasingly important when delivering contemporary digital marketing experiences. Conventional recommender systems (RS) suggest relevant but time-invariant items to users by accounting for their past purchases. These recommendations only map to customers' general preferences rather than a customer's specific needs immediately preceding a purchase. In contrast, RSs that consider the order of transactions, purchases, or experiences to measure evolving preferences can offer more salient and effective recommendations to customers: Sequential RSs not only benefit from a better behavioral understanding of a user's current needs but also better predictive power. In this paper, we demonstrate and rank the effectiveness of a sequential recommendation system by utilizing a production dataset of over 2.7 million credit card transactions for 46K cardholders. The method first employs an autoencoder on raw transaction data and submits observ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29992;&#25143;&#32423;&#38544;&#31169;&#26465;&#20214;&#19979;&#23545;&#30452;&#26041;&#22270;&#20272;&#35745;&#30340;&#36129;&#29486;&#36793;&#30028;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#22495;&#35774;&#32622;&#19979;&#36873;&#25321;&#26368;&#20339;&#29992;&#25143;&#36129;&#29486;&#36793;&#30028;&#65292;&#24182;&#36817;&#20284;&#36798;&#21040;&#26368;&#20339;&#36129;&#29486;&#36793;&#30028;&#30340;&#20004;&#20493;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2206.03008</link><description>&lt;p&gt;
&#22522;&#20110;&#29992;&#25143;&#32423;&#38544;&#31169;&#30340;&#30452;&#26041;&#22270;&#20272;&#35745;&#30340;&#36129;&#29486;&#36793;&#30028;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithms for bounding contribution for histogram estimation under user-level privacy. (arXiv:2206.03008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29992;&#25143;&#32423;&#38544;&#31169;&#26465;&#20214;&#19979;&#23545;&#30452;&#26041;&#22270;&#20272;&#35745;&#30340;&#36129;&#29486;&#36793;&#30028;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#22495;&#35774;&#32622;&#19979;&#36873;&#25321;&#26368;&#20339;&#29992;&#25143;&#36129;&#29486;&#36793;&#30028;&#65292;&#24182;&#36817;&#20284;&#36798;&#21040;&#26368;&#20339;&#36129;&#29486;&#36793;&#30028;&#30340;&#20004;&#20493;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#32423;&#24046;&#24322;&#38544;&#31169;&#26465;&#20214;&#19979;&#30340;&#30452;&#26041;&#22270;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#20445;&#25252;&#20219;&#20309;&#21333;&#20010;&#29992;&#25143;&#30340;&#25152;&#26377;&#26465;&#30446;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25968;&#25454;&#25968;&#37327;&#23545;&#27599;&#20010;&#29992;&#25143;&#21487;&#33021;&#19981;&#21516;&#30340;&#24322;&#26500;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#33719;&#24471;&#24046;&#24322;&#38544;&#31169;&#65292;&#22312;&#30452;&#26041;&#22270;&#20013;&#27880;&#20837;&#30340;&#22122;&#22768;&#37327;&#19982;&#26368;&#22823;&#29992;&#25143;&#36129;&#29486;&#25104;&#27604;&#20363;&#65292;&#32780;&#36825;&#21487;&#33021;&#20250;&#34987;&#23569;&#25968;&#31163;&#32676;&#20540;&#25918;&#22823;&#12290;&#19968;&#31181;&#24212;&#23545;&#26041;&#27861;&#26159;&#38480;&#21046;&#27599;&#20010;&#29992;&#25143;&#23545;&#30452;&#26041;&#22270;&#30340;&#36129;&#29486;&#65288;&#25110;&#38480;&#21046;&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23558;&#29992;&#25143;&#38480;&#21046;&#22312;&#23567;&#30340;&#36129;&#29486;&#19978;&#65292;&#23558;&#20250;&#20002;&#24323;&#22823;&#37327;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#22495;&#35774;&#32622;&#19979;&#36873;&#25321;&#26368;&#20339;&#29992;&#25143;&#36129;&#29486;&#36793;&#30028;&#30340;&#31639;&#27861;&#12290;&#24403;&#22495;&#30340;&#22823;&#23567;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#36129;&#29486;&#36793;&#30028;&#31574;&#30053;&#65292;&#20960;&#20046;&#21487;&#20197;&#22312;&#20107;&#21518;&#19982;&#26368;&#20339;&#36129;&#29486;&#36793;&#30028;&#36798;&#21040;&#20004;&#20493;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of histogram estimation under user-level differential privacy, where the goal is to preserve the privacy of all entries of any single user. We consider the heterogeneous scenario where the quantity of data can be different for each user. In this scenario, the amount of noise injected into the histogram to obtain differential privacy is proportional to the maximum user contribution, which can be amplified by few outliers. One approach to circumvent this would be to bound (or limit) the contribution of each user to the histogram. However, if users are limited to small contributions, a significant amount of data will be discarded. In this work, we propose algorithms to choose the best user contribution bound for histogram estimation under both bounded and unbounded domain settings. When the size of the domain is bounded, we propose a user contribution bounding strategy that almost achieves a two-approximation with respect to the best contribution bound in hindsight. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26500;&#24314;&#30340;&#26631;&#31614;&#39044;&#27979;&#38598;&#21512;&#65292;&#31934;&#30830;&#22320;&#26435;&#34913;&#20102;&#30495;&#23454;&#26631;&#31614;&#19981;&#22312;&#39044;&#27979;&#38598;&#21512;&#20013;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2201.12006</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#25913;&#36827;&#19987;&#23478;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Expert Predictions with Conformal Prediction. (arXiv:2201.12006v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26500;&#24314;&#30340;&#26631;&#31614;&#39044;&#27979;&#38598;&#21512;&#65292;&#31934;&#30830;&#22320;&#26435;&#34913;&#20102;&#30495;&#23454;&#26631;&#31614;&#19981;&#22312;&#39044;&#27979;&#38598;&#21512;&#20013;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#25215;&#35834;&#24110;&#21161;&#19987;&#23478;&#26356;&#39640;&#25928;&#20934;&#30830;&#22320;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#36890;&#24120;&#35201;&#27714;&#19987;&#23478;&#29702;&#35299;&#20309;&#26102;&#25918;&#24323;&#33258;&#24049;&#30340;&#20915;&#31574;&#26435;&#20197;&#21450;&#20309;&#26102;&#34892;&#20351;&#33258;&#24049;&#30340;&#20915;&#31574;&#26435;&#12290;&#21542;&#21017;&#65292;&#19987;&#23478;&#21487;&#33021;&#26356;&#36866;&#21512;&#33258;&#34892;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#23427;&#19981;&#38656;&#35201;&#19987;&#23478;&#29702;&#35299;&#20309;&#26102;&#20449;&#20219;&#31995;&#32479;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#19981;&#25552;&#20379;&#21333;&#19968;&#30340;&#26631;&#31614;&#39044;&#27979;&#24182;&#35753;&#19987;&#23478;&#20915;&#23450;&#20309;&#26102;&#20449;&#20219;&#36825;&#20123;&#39044;&#27979;&#65292;&#32780;&#26159;&#25552;&#20379;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26500;&#24314;&#30340;&#26631;&#31614;&#39044;&#27979;&#38598;&#21512;&#65292;&#24182;&#24378;&#21046;&#35201;&#27714;&#19987;&#23478;&#20174;&#36825;&#20123;&#38598;&#21512;&#20013;&#39044;&#27979;&#26631;&#31614;&#12290;&#36890;&#36807;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#31934;&#30830;&#22320;&#26435;&#34913;&#30495;&#23454;&#26631;&#31614;&#19981;&#22312;&#39044;&#27979;&#38598;&#21512;&#20013;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#30830;&#23450;&#36755;&#20986;&#39044;&#27979;&#30340;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated decision support systems promise to help human experts solve multiclass classification tasks more efficiently and accurately. However, existing systems typically require experts to understand when to cede agency to the system or when to exercise their own agency. Otherwise, the experts may be better off solving the classification tasks on their own. In this work, we develop an automated decision support system that, by design, does not require experts to understand when to trust the system to improve performance. Rather than providing (single) label predictions and letting experts decide when to trust these predictions, our system provides sets of label predictions constructed using conformal prediction$\unicode{x2014}$prediction sets$\unicode{x2014}$and forcefully asks experts to predict labels from these sets. By using conformal prediction, our system can precisely trade-off the probability that the true label is not in the prediction set, which determines how frequently ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#35299;&#20915;AMP&#31867;&#22411;&#31639;&#27861;&#25910;&#25947;&#24615;&#38382;&#39064;&#30340;&#20805;&#20998;&#32479;&#35745;&#35760;&#24518;&#22411;AMP&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20805;&#20998;&#32479;&#35745;&#32422;&#26463;&#21644;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20449;&#21495;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2112.15327</link><description>&lt;p&gt;
&#20805;&#20998;&#32479;&#35745;&#35760;&#24518;&#22411;AMP&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sufficient-Statistic Memory AMP. (arXiv:2112.15327v4 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.15327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#35299;&#20915;AMP&#31867;&#22411;&#31639;&#27861;&#25910;&#25947;&#24615;&#38382;&#39064;&#30340;&#20805;&#20998;&#32479;&#35745;&#35760;&#24518;&#22411;AMP&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20805;&#20998;&#32479;&#35745;&#32422;&#26463;&#21644;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20449;&#21495;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#31867;&#22411;&#30340;&#31639;&#27861;&#22312;&#26576;&#20123;&#22823;&#22411;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#30340;&#20449;&#21495;&#37325;&#26500;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;AMP&#31867;&#22411;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#23427;&#20204;&#30340;&#21160;&#24577;&#21487;&#20197;&#36890;&#36807;&#29366;&#24577;&#28436;&#21270;&#27491;&#30830;&#22320;&#25551;&#36848;&#12290;&#34429;&#28982;&#29366;&#24577;&#28436;&#21270;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#20294;&#20854;&#25910;&#25947;&#24615;&#24182;&#19981;&#20445;&#35777;&#12290;&#20026;&#20102;&#22312;&#21407;&#21017;&#19978;&#35299;&#20915;AMP&#31867;&#22411;&#31639;&#27861;&#30340;&#29366;&#24577;&#28436;&#21270;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27491;&#30830;&#21333;&#20301;&#19981;&#21464;&#30340;&#24863;&#30693;&#30697;&#38453;&#12289;Lipschitz&#36830;&#32493;&#30340;&#26412;&#22320;&#22788;&#29702;&#22120;&#21644;&#20805;&#20998;&#32479;&#35745;&#32422;&#26463;&#19979;&#30340;&#20805;&#20998;&#32479;&#35745;&#35760;&#24518;&#22411;AMP&#65288;SS-MAMP&#65289;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SS-MAMP&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;L&#24102;&#29366;&#30340;&#19988;&#25910;&#25947;&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#20248;&#30340;&#26694;&#26550;&#65288;&#20174;&#26412;&#22320;MMSE/LMMSE&#35282;&#24230;&#65289;&#32473;&#23450;Lipschitz&#36830;&#32493;&#30340;AMP&#31867;&#22411;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate message passing (AMP) type algorithms have been widely used in the signal reconstruction of certain large random linear systems. A key feature of the AMP-type algorithms is that their dynamics can be correctly described by state evolution. While state evolution is a useful analytic tool, its convergence is not guaranteed. To solve the convergence problem of the state evolution of AMP-type algorithms in principle, this paper proposes a sufficient-statistic memory AMP (SS-MAMP) algorithm framework under the conditions of right-unitarily invariant sensing matrices, Lipschitz-continuous local processors and the sufficient-statistic constraint (i.e., the current message of each local processor is a sufficient statistic of the signal vector given the current and all preceding messages). We show that the covariance matrices of SS-MAMP are L-banded and convergent, which is an optimal framework (from the local MMSE/LMMSE perspective) for AMP-type algorithms given the Lipschitz-conti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20302;&#28201;&#33976;&#39311;&#65288;LTD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20462;&#25913;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#29983;&#25104;&#36719;&#26631;&#31614;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#24120;&#29992;&#30340;&#29420;&#28909;&#21521;&#37327;&#26631;&#31614;&#24102;&#26469;&#30340;&#23398;&#20064;&#22256;&#38590;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.02331</link><description>&lt;p&gt;
&#20302;&#28201;&#33976;&#39311;&#65306;&#29992;&#20110;&#31283;&#20581;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LTD: Low Temperature Distillation for Robust Adversarial Training. (arXiv:2111.02331v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20302;&#28201;&#33976;&#39311;&#65288;LTD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20462;&#25913;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#29983;&#25104;&#36719;&#26631;&#31614;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#24120;&#29992;&#30340;&#29420;&#28909;&#21521;&#37327;&#26631;&#31614;&#24102;&#26469;&#30340;&#23398;&#20064;&#22256;&#38590;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#28982;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#21457;&#29616;&#20102;&#36825;&#20010;&#24046;&#36317;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#24120;&#29992;&#30340;&#29420;&#28909;&#21521;&#37327;&#20316;&#20026;&#26631;&#31614;&#65292;&#36825;&#38459;&#30861;&#20102;&#22270;&#20687;&#35782;&#21035;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#29992;&#29420;&#28909;&#21521;&#37327;&#34920;&#31034;&#27169;&#31946;&#22270;&#20687;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#24471;&#21040;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#20302;&#28201;&#33976;&#39311;&#65288;LTD&#65289;&#65292;&#23427;&#20351;&#29992;&#20462;&#25913;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#29983;&#25104;&#36719;&#26631;&#31614;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;LTD&#22312;&#25945;&#24072;&#27169;&#22411;&#20013;&#20351;&#29992;&#30456;&#23545;&#36739;&#20302;&#30340;&#28201;&#24230;&#65292;&#32780;&#23545;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#20351;&#29992;&#22266;&#23450;&#20294;&#19981;&#21516;&#30340;&#28201;&#24230;&#12290;&#36825;&#20010;&#20462;&#25913;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#36935;&#21040;&#24050;&#32463;&#22312;&#20808;&#21069;&#24037;&#20316;&#20013;&#35299;&#20915;&#30340;&#26799;&#24230;&#25513;&#30721;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been widely used to enhance the robustness of neural network models against adversarial attacks. Despite the popularity of neural network models, a significant gap exists between the natural and robust accuracy of these models. In this paper, we identify one of the primary reasons for this gap is the common use of one-hot vectors as labels, which hinders the learning process for image recognition. Representing ambiguous images with one-hot vectors is imprecise and may lead the model to suboptimal solutions. To overcome this issue, we propose a novel method called Low Temperature Distillation (LTD) that generates soft labels using the modified knowledge distillation framework. Unlike previous approaches, LTD uses a relatively low temperature in the teacher model and fixed, but different temperatures for the teacher and student models. This modification boosts the model's robustness without encountering the gradient masking problem that has been addressed in defe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2110.05367</link><description>&lt;p&gt;
&#22312;&#19981;&#20135;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. (arXiv:2110.05367v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#36890;&#24120;&#24314;&#31435;&#19968;&#20010;&#23567;&#22411;&#30340;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#31532;&#20108;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26377;&#38480;&#19988;&#38598;&#20013;&#20851;&#27880;&#65292;&#31532;&#20108;&#38454;&#27573;&#39044;&#35757;&#32451;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#24536;&#35760;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#20005;&#37325;&#25439;&#23475;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;GLUE&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#35777;&#22320;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20013;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;GEnder Equality Prompt (GEEP)&#65292;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#19988;&#36951;&#24536;&#36739;&#23569;&#12290; GEEP&#20250;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;GEEP&#19981;&#20165;&#22312;&#24615;&#21035;&#20844;&#24179;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#65292;&#32780;&#19988;&#22312;GLUE&#19978;&#36951;&#24536;&#36739;&#23569;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model's downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;&#65292;&#35813;&#35268;&#21017;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#26799;&#24230;&#26469;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#24191;&#27867;&#30340;&#31639;&#27861;&#24212;&#29992;&#12290;&#36825;&#19968;&#24037;&#20316;&#19981;&#20165;&#32479;&#19968;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#36824;&#24110;&#21161;&#25105;&#20204;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.04562</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
The Bayesian Learning Rule. (arXiv:2107.04562v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.04562
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;&#65292;&#35813;&#35268;&#21017;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#26799;&#24230;&#26469;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#24191;&#27867;&#30340;&#31639;&#27861;&#24212;&#29992;&#12290;&#36825;&#19968;&#24037;&#20316;&#19981;&#20165;&#32479;&#19968;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#36824;&#24110;&#21161;&#25105;&#20204;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#20010;&#31216;&#20026;&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;&#30340;&#21333;&#19968;&#31639;&#27861;&#30340;&#29305;&#20363;&#12290;&#36825;&#20010;&#35268;&#21017;&#26159;&#20174;&#36125;&#21494;&#26031;&#21407;&#29702;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#21487;&#20197;&#20174;&#20248;&#21270;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#24418;&#27169;&#22411;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#30340;&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#32463;&#20856;&#31639;&#27861;&#22914;&#23725;&#22238;&#24402;&#12289;&#29275;&#39039;&#27861;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#20197;&#21450;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;RMSprop&#21644;Dropout&#12290;&#25512;&#23548;&#36825;&#20123;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#20272;&#35745;&#30340;&#20505;&#36873;&#20998;&#24067;&#26469;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#12290;&#19981;&#21516;&#30340;&#20505;&#36873;&#20998;&#24067;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#23545;&#33258;&#28982;&#26799;&#24230;&#30340;&#36827;&#19968;&#27493;&#36924;&#36817;&#21017;&#20250;&#20135;&#29983;&#36825;&#20123;&#31639;&#27861;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#32479;&#19968;&#12289;&#27867;&#21270;&#21644;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#36824;&#24110;&#21161;&#25105;&#20204;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;WGANs&#65289;&#65292;&#24182;&#20351;&#29992;GroupSort&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#37492;&#21035;&#22120;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#36924;&#36817;&#35823;&#24046;&#26377;&#24433;&#21709;&#65292;&#24182;&#19988;WGANs&#23545;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#35201;&#27714;&#39640;&#20110;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#22312;&#37492;&#21035;&#22120;&#19981;&#36275;&#22815;&#24378;&#22823;&#26102;&#65292;&#20302;&#23481;&#37327;&#30340;&#29983;&#25104;&#22120;&#21487;&#33021;&#27604;&#36807;&#24230;&#28145;&#23618;&#21644;&#23485;&#24230;&#30340;&#29983;&#25104;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2103.10060</link><description>&lt;p&gt;
&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36817;&#20284;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Approximating Probability Distributions by using Wasserstein Generative Adversarial Networks. (arXiv:2103.10060v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;WGANs&#65289;&#65292;&#24182;&#20351;&#29992;GroupSort&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#37492;&#21035;&#22120;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#36924;&#36817;&#35823;&#24046;&#26377;&#24433;&#21709;&#65292;&#24182;&#19988;WGANs&#23545;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#35201;&#27714;&#39640;&#20110;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#22312;&#37492;&#21035;&#22120;&#19981;&#36275;&#22815;&#24378;&#22823;&#26102;&#65292;&#20302;&#23481;&#37327;&#30340;&#29983;&#25104;&#22120;&#21487;&#33021;&#27604;&#36807;&#24230;&#28145;&#23618;&#21644;&#23485;&#24230;&#30340;&#29983;&#25104;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;GroupSort&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#37492;&#21035;&#22120;&#30340;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;WGANs&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#26631;&#20998;&#24067;&#30340;&#36924;&#36817;&#35823;&#24046;&#30028;&#38480;&#21462;&#20915;&#20110;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#65288;&#23481;&#37327;&#65289;&#20197;&#21450;&#35757;&#32451;&#20013;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#38024;&#23545;&#29983;&#25104;&#30340;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#24314;&#31435;&#20102;&#37327;&#21270;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#26681;&#25454;&#29702;&#35770;&#32467;&#26524;&#65292;WGANs&#23545;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#35201;&#27714;&#27604;&#29983;&#25104;&#22120;&#26356;&#39640;&#65292;&#36825;&#19982;&#19968;&#20123;&#24050;&#26377;&#32467;&#26524;&#19968;&#33268;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22914;&#26524;&#37492;&#21035;&#22120;&#19981;&#36275;&#22815;&#24378;&#22823;&#65292;&#19982;&#36807;&#24230;&#28145;&#23618;&#21644;&#23485;&#24230;&#65288;&#39640;&#23481;&#37327;&#65289;&#30340;&#29983;&#25104;&#22120;&#30456;&#27604;&#65292;&#20302;&#23481;&#37327;&#30340;&#29983;&#25104;&#22120;&#30340;&#32467;&#26524;&#21487;&#33021;&#26356;&#24046;&#12290;&#20351;&#29992;Swiss roll&#21644;MNIST&#25968;&#25454;&#38598;&#24471;&#21040;&#30340;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studied here are Wasserstein generative adversarial networks (WGANs) with GroupSort neural networks as their discriminators. It is shown that the error bound of the approximation for the target distribution depends on the width and depth (capacity) of the generators and discriminators and the number of samples in training. A quantified generalization bound is established for the Wasserstein distance between the generated and target distributions. According to the theoretical results, WGANs have a higher requirement for the capacity of discriminators than that of generators, which is consistent with some existing results. More importantly, the results with overly deep and wide (high-capacity) generators may be worse than those with low-capacity generators if discriminators are insufficiently strong. Numerical results obtained using Swiss roll and MNIST datasets confirm the theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2012.14331</link><description>&lt;p&gt;
&#19968;&#31181;&#25972;&#21512;&#21644;&#20998;&#31867;&#27491;&#24577;&#20998;&#24067;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.14331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#27491;&#24577;&#27010;&#29575;&#20998;&#24067;&#22312;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#35745;&#31639;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#38656;&#35201;&#22312;&#29305;&#23450;&#21306;&#22495;&#20869;&#23545;&#36825;&#20123;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#65292;&#36825;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#20013;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#36890;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12289;&#26631;&#20934;&#25968;&#20540;&#26041;&#27861;&#25110;&#36719;&#20214;&#26469;&#35745;&#31639;&#36825;&#20123;&#31215;&#20998;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25968;&#23398;&#32467;&#26524;&#21644;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#20197;&#19979;&#20869;&#23481;&#65306;&#65288;i&#65289;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#20219;&#24847;&#22495;&#20869;&#27861;&#21521;&#30340;&#27010;&#29575;&#65292;&#65288;ii&#65289;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#12289;&#32047;&#31215;&#20998;&#24067;&#21644;&#36870;&#32047;&#31215;&#20998;&#24067;&#65292;&#65288;iii&#65289;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#31867;&#35823;&#24046;&#12289;&#36125;&#21494;&#26031;&#26368;&#20248;&#36776;&#21035;&#25351;&#25968;&#20197;&#21450;&#20854;&#19982;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#30340;&#20851;&#31995;&#65292;&#65288;iv&#65289;&#27492;&#31867;&#38382;&#39064;&#30340;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#65288;v&#65289;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#29983;&#29289;&#21644;&#24515;&#29702;&#23398;&#26469;&#28436;&#31034;&#36825;&#20123;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#22788;&#29702;&#22810;&#36830;&#25509;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#31216;&#20026;&#25968;&#25454;&#20016;&#23500;/&#20849;&#20139;&#12290;&#25105;&#20204;&#36890;&#36807;&#20984;&#20989;&#25968;&#26469;&#25551;&#36848;&#20844;&#20849;&#21442;&#25968;&#21644;&#20010;&#20307;&#21442;&#25968;&#30340;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#36845;&#20195;&#20272;&#35745;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/1806.04047</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#20016;&#23500;&#21270;&#65306;&#21487;&#35299;&#37322;&#12289;&#24555;&#36895;&#21644;&#25968;&#25454;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient. (arXiv:1806.04047v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1806.04047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#22788;&#29702;&#22810;&#36830;&#25509;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#31216;&#20026;&#25968;&#25454;&#20016;&#23500;/&#20849;&#20139;&#12290;&#25105;&#20204;&#36890;&#36807;&#20984;&#20989;&#25968;&#26469;&#25551;&#36848;&#20844;&#20849;&#21442;&#25968;&#21644;&#20010;&#20307;&#21442;&#25968;&#30340;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#36845;&#20195;&#20272;&#35745;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#22810;&#20010;&#36830;&#25509;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#29305;&#24615;&#65292;&#35813;&#38382;&#39064;&#34987;&#31216;&#20026;&#25968;&#25454;&#20016;&#23500;/&#20849;&#20139;&#12290;&#20219;&#21153;&#38388;&#30340;&#36830;&#25509;&#30001;&#36328;&#20219;&#21153;&#30340;&#8220;&#20844;&#20849;&#21442;&#25968;&#8221;&#25429;&#25417;&#65292;&#35813;&#21442;&#25968;&#36890;&#36807;&#20219;&#21153;&#32423;&#30340;&#8220;&#20010;&#20307;&#21442;&#25968;&#8221;&#36827;&#34892;&#32454;&#21270;&#12290;&#20219;&#20309;&#20984;&#20989;&#25968;&#65292;&#22914;&#33539;&#25968;&#65292;&#37117;&#21487;&#20197;&#34920;&#24449;&#20844;&#20849;&#21442;&#25968;&#21644;&#20010;&#20307;&#21442;&#25968;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#21246;&#21202;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20309;&#26465;&#20214;&#19979;&#20026;&#25152;&#26377;&#21442;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30340;&#38750;&#28176;&#36827;&#36793;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#27719;&#38598;&#26679;&#26412;&#20013;&#21463;&#30410;&#20110;&#20844;&#20849;&#21442;&#25968;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#36845;&#20195;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#34917;&#20805;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#20998;&#26512;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#25968;&#25454;&#20016;&#23500;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of multi-task learning in the high dimensional setting. In particular, we introduce an estimator and investigate its statistical and computational properties for the problem of multiple connected linear regressions known as Data Enrichment/Sharing. The between-tasks connections are captured by a cross-tasks \emph{common parameter}, which gets refined by per-task \emph{individual parameters}. Any convex function, e.g., norm, can characterize the structure of both common and individual parameters. We delineate the sample complexity of our estimator and provide a high probability non-asymptotic bound for estimation error of all parameters under a geometric condition. We show that the recovery of the common parameter benefits from \emph{all} of the pooled samples. We propose an iterative estimation algorithm with a geometric convergence rate and supplement our theoretical analysis with experiments on synthetic data. Overall, we present a first thorough statistical a
&lt;/p&gt;</description></item></channel></rss>