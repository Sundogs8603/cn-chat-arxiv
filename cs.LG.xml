<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#25512;&#29702;&#25216;&#26415;&#30340;&#22312;&#32447;&#37325;&#26032;&#26657;&#20934;&#31243;&#24207;&#25193;&#23637;&#26368;&#26032;&#25216;&#26415;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.02722</link><description>&lt;p&gt;
&#22312;&#32447;&#31526;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#29992;&#20110;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02722
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#25512;&#29702;&#25216;&#26415;&#30340;&#22312;&#32447;&#37325;&#26032;&#26657;&#20934;&#31243;&#24207;&#25193;&#23637;&#26368;&#26032;&#25216;&#26415;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24615;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#65288;PEPF&#65289;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#20154;&#20204;&#38656;&#35201;&#27491;&#30830;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25903;&#25345;&#22312;&#19981;&#26029;&#22686;&#21152;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#22797;&#26434;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#36816;&#33829;&#12290;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26368;&#36817;&#34987;&#35777;&#26126;&#33021;&#22815;&#32988;&#36807;PEPF&#22522;&#20934;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#20851;&#38190;&#30340;&#21487;&#38752;&#24615;&#22686;&#24378;&#65292;&#22240;&#20026;&#22312;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#30340;&#21508;&#20010;&#27493;&#39588;&#19978;&#26410;&#33021;&#36890;&#36807;&#35206;&#30422;&#29575;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PEPF&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#25512;&#29702;&#25216;&#26415;&#30340;&#22312;&#32447;&#37325;&#26032;&#26657;&#20934;&#31243;&#24207;&#65292;&#25193;&#23637;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#24066;&#22330;&#22320;&#21306;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#23567;&#26102;&#35206;&#30422;&#29575;&#21644;&#31283;&#23450;&#27010;&#29575;&#24471;&#20998;&#30340;&#26085;&#21069;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02722v1 Announce Type: new  Abstract: Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#36229;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#25928;&#29575;&#36864;&#21270;&#65292;&#20934;&#30830;&#24230;&#39640;&#19988;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2404.00173</link><description>&lt;p&gt;
&#27604;&#36739;&#36229;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#39044;&#27979;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#25928;&#29575;&#36864;&#21270;
&lt;/p&gt;
&lt;p&gt;
Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00173
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#36229;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#25928;&#29575;&#36864;&#21270;&#65292;&#20934;&#30830;&#24230;&#39640;&#19988;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#26469;&#34920;&#31034;&#22810;&#23618;&#32467;&#26500;ITO/PEDOT:PSS/P3HT:PCBM/Al&#32858;&#21512;&#29289;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#65288;OSCs&#65289;&#30340;&#21151;&#29575;&#36716;&#25442;&#25928;&#29575;&#65288;PCE&#65289;&#25152;&#36973;&#21463;&#30340;&#26102;&#38388;&#36864;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;996&#26465;&#25968;&#25454;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#20851;&#20110;&#21046;&#36896;&#36807;&#31243;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;7&#20010;&#21464;&#37327;&#65292;&#36229;&#36807;180&#22825;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#19968;&#31995;&#21015;&#33258;&#21160;&#21270;ML&#21327;&#35758;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21629;&#20196;&#34892;&#30028;&#38754;&#39034;&#24207;&#22320;&#38024;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#24211;&#25191;&#34892;&#65292;&#20174;&#32780;&#36731;&#26494;&#22320;&#36890;&#36807;&#35814;&#23613;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#36229;&#20248;&#21270;&#21644;&#38543;&#26426;&#21270;ML&#27169;&#22411;&#30340;&#31181;&#23376;&#65292;&#20197;&#33719;&#24471;&#26368;&#20339;&#27169;&#22411;&#12290;&#25152;&#36798;&#21040;&#30340;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;&#24191;&#27867;&#36229;&#36807;0.90&#30340;&#31995;&#25968;&#30830;&#23450;&#20540;&#65288;R2&#65289;&#65292;&#32780;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#12289;&#24179;&#26041;&#35823;&#24046;&#65288;SSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00173v1 Announce Type: new  Abstract: This work presents a set of optimal machine learning (ML) models to represent the temporal degradation suffered by the power conversion efficiency (PCE) of polymeric organic solar cells (OSCs) with a multilayer structure ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996 entries, which includes up to 7 variables regarding both the manufacturing process and environmental conditions for more than 180 days. Then, we relied on a software framework that brings together a conglomeration of automated ML protocols that execute sequentially against our database by simply command-line interface. This easily permits hyper-optimizing and randomizing seeds of the ML models through exhaustive benchmarking so that optimal models are obtained. The accuracy achieved reaches values of the coefficient determination (R2) widely exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared error (SSE), and mean absolute er
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20287</link><description>&lt;p&gt;
&#22522;&#20934;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Counterfactual Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#22312;&#29702;&#35299;&#21464;&#37327;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#29983;&#25104;&#26080;&#20559;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22270;&#20687;&#29983;&#25104;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#35780;&#20272;&#23545;&#29031;&#29983;&#25104;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23545;&#29031;&#24773;&#26223;&#26159;&#27809;&#26377;&#21487;&#35266;&#27979;&#22522;&#20934;&#20107;&#23454;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#39062;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20391;&#37325;&#20110;&#35780;&#20272;&#23545;&#29031;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20363;&#22914;&#32452;&#25104;&#12289;&#26377;&#25928;&#24615;&#12289;&#24178;&#39044;&#30340;&#26368;&#23567;&#24615;&#21644;&#22270;&#20687;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#33539;&#24335;&#30340;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#31867;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#36951;&#24536;&#65292;&#35299;&#20915;&#20808;&#21069;&#38024;&#23545;&#38745;&#24577;&#27169;&#22411;&#26550;&#26500;&#24773;&#20917;&#19979;&#23384;&#22312;&#30340;&#36807;&#22810;&#21442;&#25968;&#20998;&#37197;&#25110;&#36866;&#24212;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18886</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#36866;&#37197;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#25105;&#25193;&#23637;&#20197;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#36951;&#24536;&#65292;&#35299;&#20915;&#20808;&#21069;&#38024;&#23545;&#38745;&#24577;&#27169;&#22411;&#26550;&#26500;&#24773;&#20917;&#19979;&#23384;&#22312;&#30340;&#36807;&#22810;&#21442;&#25968;&#20998;&#37197;&#25110;&#36866;&#24212;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#20174;&#36830;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#33258;&#21160;&#20915;&#23450;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18886v1 Announce Type: new  Abstract: Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts. However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decid
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.18517</link><description>&lt;p&gt;
&#38024;&#23545;&#27491;&#21017;&#21270;&#38750;&#36127;&#23610;&#24230;&#19981;&#21464;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Regularized Nonnegative Scale-invariant Low-rank Approximation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18517
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#38750;&#36127;&#20302;&#31209;&#36924;&#36817;&#65292;&#22914;&#31232;&#30095;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#25110;&#31232;&#30095;&#30340;&#38750;&#36127;Tucker&#20998;&#35299;&#65292;&#26159;&#20855;&#26377;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#38477;&#32500;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#36341;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#22240;&#32032;&#29305;&#24615;&#20197;&#21450;&#32570;&#20047;&#25903;&#25345;&#36825;&#20123;&#36873;&#25321;&#30340;&#29702;&#35770;&#65292;&#27491;&#21017;&#21270;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#39640;&#25928;&#31639;&#27861;&#30340;&#35774;&#35745;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#20855;&#26377;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#30410;&#21644;&#26377;&#23475;&#25928;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18517v1 Announce Type: new  Abstract: Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability. However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices. This paper aims at improving upon these issues. By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects. This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17083</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20462;&#21098;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study in Dataset Pruning for Image Super-Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#65292;&#20381;&#36182;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#23613;&#31649;&#25552;&#20379;&#20016;&#23500;&#30340;&#35757;&#32451;&#32032;&#26448;&#65292;&#20294;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20462;&#21098;&#20316;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#38598;&#32553;&#20943;&#21040;&#22522;&#20110;&#20854;&#25439;&#22833;&#20540;&#32780;&#36873;&#25321;&#30340;&#19968;&#32452;&#26680;&#24515;&#35757;&#32451;&#26679;&#26412;&#12290;&#36890;&#36807;&#20165;&#23558;&#35757;&#32451;&#37325;&#28857;&#25918;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#19978;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25439;&#22833;&#20540;&#26368;&#39640;&#30340;&#26679;&#26412;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#25110;&#29978;&#33267;&#36229;&#36807;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#20540;&#30340;&#21069;5&#65285;&#26679;&#26412;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25490;&#38500;&#36825;&#20123;&#26679;&#26412;&#24182;&#35843;&#25972;&#36873;&#25321;&#20197;&#20559;&#22909;&#26356;&#23481;&#26131;&#30340;&#26679;&#26412;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p
&lt;/p&gt;</description></item><item><title>FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.15769</link><description>&lt;p&gt;
FusionINN&#65306;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#29992;&#20110;&#33041;&#32959;&#30244;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15769
&lt;/p&gt;
&lt;p&gt;
FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;&#22810;&#20010;&#28304;&#22270;&#20687;&#21512;&#24182;&#20026;&#21333;&#20010;&#34701;&#21512;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#19987;&#23478;&#65292;&#20165;&#20381;&#36182;&#34701;&#21512;&#22270;&#20687;&#21487;&#33021;&#19981;&#36275;&#20197;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#65292;&#22240;&#20026;&#34701;&#21512;&#26426;&#21046;&#28151;&#21512;&#20102;&#26469;&#33258;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38590;&#20197;&#35299;&#37322;&#28508;&#22312;&#30340;&#32959;&#30244;&#30149;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FusionINN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#36807;&#31243;&#23558;&#20854;&#20998;&#35299;&#22238;&#28304;&#22270;&#20687;&#12290;FusionINN&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#28508;&#22312;&#22270;&#20687;&#19982;&#34701;&#21512;&#22270;&#20687;&#19968;&#36215;&#65292;&#20197;&#20419;&#36827;&#20998;&#35299;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#20174;&#32780;&#20445;&#35777;&#26080;&#25439;&#30340;&#19968;&#23545;&#19968;&#20687;&#32032;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#34701;&#21512;&#22270;&#20687;&#30340;&#21487;&#20998;&#35299;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#21629;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15769v1 Announce Type: cross  Abstract: Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.15476</link><description>&lt;p&gt;
&#23398;&#20064;&#25512;&#26029;&#29983;&#25104;&#35270;&#35273;&#27010;&#24565;&#30340;&#27169;&#26495;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Infer Generative Template Programs for Visual Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15476
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#21487;&#20197;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#28789;&#27963;&#25484;&#25569;&#35270;&#35273;&#27010;&#24565;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#23398;&#20064;&#22914;&#20309;&#20197;&#19968;&#31181;&#36890;&#29992;&#26041;&#24335;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#65306;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#30340;&#31243;&#24207;&#34920;&#36798;&#24335;&#65292;&#25351;&#23450;&#20102;&#36755;&#20837;&#27010;&#24565;&#20013;&#24120;&#35265;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#22810;&#20010;&#19982;&#27010;&#24565;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#36890;&#36807;&#35299;&#26512;&#36827;&#34892;&#23569;&#26679;&#26412;&#29983;&#25104;&#21644;&#20849;&#20998;&#21106;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#20174;&#21253;&#21547;&#27010;&#24565;&#20998;&#32452;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#35273;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65306;2D&#24067;&#23616;&#12289;Omniglot&#23383;&#31526;&#21644;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#22312;&#26377;&#38480;&#39046;&#22495;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#20102;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15476v1 Announce Type: cross  Abstract: People grasp flexible visual concepts from a few examples. We explore a neurosymbolic system that learns how to infer programs that capture visual concepts in a domain-general fashion. We introduce Template Programs: programmatic expressions from a domain-specific language that specify structural and parametric patterns common to an input concept. Our framework supports multiple concept-related tasks, including few-shot generation and co-segmentation through parsing. We develop a learning paradigm that allows us to train networks that infer Template Programs directly from visual datasets that contain concept groupings. We run experiments across multiple visual domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our method outperforms task-specific alternatives, and performs competitively against domain-specific approaches for the limited domains where they exist.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RewardBench, &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13787</link><description>&lt;p&gt;
RewardBench&#65306;&#35780;&#20272;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RewardBench: Evaluating Reward Models for Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RewardBench, &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#65288;RMs&#65289;&#26159;&#25104;&#21151;RLHF&#30340;&#20851;&#38190;&#65292;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#32780;&#30456;&#23545;&#36739;&#23569;&#30340;&#30740;&#31350;&#20851;&#27880;&#23545;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#20102;&#35299;&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#25216;&#26415;&#21450;&#20854;&#23884;&#20837;&#20160;&#20040;&#20215;&#20540;&#30340;&#26426;&#20250;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#33021;&#21147;&#25551;&#36848;&#12289;&#35757;&#32451;&#26041;&#27861;&#25110;&#24320;&#28304;&#22870;&#21169;&#27169;&#22411;&#30340;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RewardBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#65292;&#20197;&#22686;&#24378;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;RewardBench&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#36328;&#23545;&#35805;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#30340;&#25552;&#31034;-&#36194;-&#36755;&#19977;&#20803;&#32452;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#12289;&#32467;&#26500;&#21270;&#21644;&#36229;&#20998;&#24067;&#26597;&#35810;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;RMs&#21019;&#24314;&#20102;&#29305;&#23450;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26377;&#24494;&#22937;&#20294;&#21487;&#39564;&#35777;&#30340;&#21407;&#22240;&#65288;&#20363;&#22914;&#38169;&#35823;&#12289;&#19981;&#27491;&#30830;&#30340;&#20107;&#23454;&#65289;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#19968;&#20010;&#31572;&#26696;&#24212;&#35813;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13787v1 Announce Type: new  Abstract: Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11103</link><description>&lt;p&gt;
ProgGen:&#36890;&#36807;&#33258;&#21453;&#22823;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11103
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36328;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#25552;&#21462;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;NER&#65289;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#20855;&#26377;&#36866;&#24230;NER&#33021;&#21147;&#30340;LLMs&#26469;&#29983;&#25104;&#20248;&#31168;&#30340;NER&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#22522;&#26412;&#30340;&#31867;&#26465;&#20214;&#25552;&#31034;&#65292;&#32780;&#26159;&#25351;&#23548;LLMs&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#65292;&#20174;&#32780;&#29983;&#25104;&#20855;&#26377;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#65288;&#20363;&#22914;&#24433;&#35780;&#30340;&#31867;&#21035;&#21644;&#24773;&#24863;&#65289;&#30340;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#20808;&#29983;&#25104;&#23454;&#20307;&#26415;&#35821;&#65292;&#28982;&#21518;&#22260;&#32469;&#36825;&#20123;&#23454;&#20307;&#24320;&#21457;NER&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#26377;&#25928;&#35268;&#36991;&#20102;LLMs&#23545;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#36890;&#29992;&#21644;&#19987;&#19994;&#39046;&#22495;&#23637;&#24320;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.08802</link><description>&lt;p&gt;
&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Governance of Generative Artificial Intelligence for Companies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#29305;&#21035;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#36805;&#36895;&#36827;&#20837;&#20225;&#19994;&#65292;&#20294;&#32570;&#20047;&#20805;&#20998;&#30340;&#27835;&#29702;&#65292;&#24102;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23613;&#31649;&#23545;GenAI&#20855;&#26377;&#21464;&#38761;&#24615;&#36136;&#21644;&#30417;&#31649;&#25514;&#26045;&#30340;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#28041;&#21450;&#32452;&#32455;&#27835;&#29702;&#65292;&#21253;&#25324;&#25216;&#26415;&#21644;&#19994;&#21153;&#35270;&#35282;&#12290;&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#12290;&#23427;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#65292;&#36824;&#36890;&#36807;&#21046;&#23450;&#36866;&#29992;&#20110;&#20225;&#19994;&#20869;&#30340;GenAI&#27835;&#29702;&#26694;&#26550;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35814;&#32454;&#25551;&#36848;&#20102;&#33539;&#22260;&#12289;&#30446;&#26631;&#21644;&#27835;&#29702;&#26426;&#21046;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;GenAI&#27835;&#29702;&#30340;&#26041;&#27861;&#65292;&#20026;&#20225;&#19994;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#37319;&#29992;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;&#23545;&#20110;&#25216;&#26415;&#20154;&#21592;&#26469;&#35828;&#65292;&#20063;&#26377;&#21161;&#20110;&#25299;&#23485;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08802v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. This review paper fills this gap by surveying recent works. It goes beyond mere summarization by developing a framework for GenAI governance within companies. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities and mitigate risks associated with GenAI integration. This research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of responsible AI adoption. It is also valuable for a technical audience to broaden their perspective as inc
&lt;/p&gt;</description></item><item><title>SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.07968</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#26159;&#21542;&#24418;&#25104;&#26143;&#24418;&#21306;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Deep Neural Network Solutions Form a Star Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07968
&lt;/p&gt;
&lt;p&gt;
SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Entezari&#31561;&#20154;&#65288;2022&#65289;&#25512;&#27979;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21487;&#36798;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#20984;&#30340;&#65292;&#32771;&#34385;&#21040;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23485;&#26494;&#30340;&#35266;&#28857;&#65306;SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Starlight&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#32473;&#23450;&#23398;&#20064;&#20219;&#21153;&#30340;&#26143;&#24418;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#20010;&#26143;&#24418;&#27169;&#22411;&#19982;&#20854;&#20182;&#29420;&#31435;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32447;&#24615;&#30456;&#36830;&#30340;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07968v1 Announce Type: cross  Abstract: Entezari et al. (2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dataset Condensation&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.07245</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation for Time Series Classification via Dual Domain Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dataset Condensation&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21508;&#31181;&#30740;&#31350;&#39046;&#22495;&#20013;&#34987;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#31649;&#29702;&#22823;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;&#8220;&#25968;&#25454;&#38598;&#21387;&#32553;&#8221;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#29983;&#25104;&#19968;&#20010;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#22312;&#35832;&#22914;&#20998;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#19982;&#23436;&#25972;&#30495;&#23454;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22270;&#20687;&#21644;&#22270;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#39057;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07245v1 Announce Type: new  Abstract: Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset \textit{\textbf{Cond}}ensation for \textit{\textbf{T}}ime \textit{\textbf{S}}eries \textit{\textbf{
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Mixture of Experts&#30340;Di-SkilL&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#19987;&#23478;&#21644;&#20854;&#30456;&#20851;&#19978;&#19979;&#25991;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#20284;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.06966</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#30340;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#33719;&#21462;&#22810;&#26679;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06966
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Mixture of Experts&#30340;Di-SkilL&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#19987;&#23478;&#21644;&#20854;&#30456;&#20851;&#19978;&#19979;&#25991;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#20284;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#33719;&#21462;&#33391;&#22909;&#34920;&#29616;&#31574;&#30053;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#24120;&#29992;&#30340;&#39640;&#26031;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;RL&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Di-SkilL&#30340;RL&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#65292;&#20854;&#20013;&#27599;&#20010;&#19987;&#23478;&#23558;&#25216;&#33021;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#36816;&#21160;&#21407;&#35821;&#12290;Di-SkilL&#20248;&#21270;&#27599;&#20010;&#19987;&#23478;&#21450;&#20854;&#30456;&#20851;&#19978;&#19979;&#25991;&#20998;&#24067;&#20197;&#36798;&#21040;&#26368;&#22823;&#29109;&#30446;&#26631;&#65292;&#28608;&#21169;&#22312;&#30456;&#20284;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#12290;&#27599;&#20010;&#19987;&#23478;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20351;&#24471;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#20351;&#27599;&#20010;&#19987;&#23478;&#33021;&#22815;&#19987;&#27880;&#20110;&#20854;&#22312;&#19978;&#19979;&#25991;&#31354;&#38388;&#30340;&#26368;&#20339;&#34920;&#29616;&#23376;&#21306;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#29615;&#22659;&#26410;&#30693;&#19978;&#19979;&#25991;&#27010;&#29575;&#31354;&#38388;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#30828;&#24615;&#19981;&#36830;&#32493;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#26469;&#34920;&#31034;&#27599;&#20010;&#19987;&#23478;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06966v1 Announce Type: new  Abstract: Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose \textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;&#27169;&#22411;&#26550;&#26500;&#20013;&#20351;&#29992;&#26410;&#32463;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.04847</link><description>&lt;p&gt;
&#20351;&#29992;&#26410;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#35299;&#20915;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Inverse Problems with Model Mismatch using Untrained Neural Networks within Model-based Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04847
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;&#27169;&#22411;&#26550;&#26500;&#20013;&#20351;&#29992;&#26410;&#32463;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#8220;&#23637;&#24320;&#36845;&#20195;&#8221;&#65288;LU&#65289;&#21644;&#8220;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#8221;&#65288;DEQ&#65289;&#25193;&#23637;&#65292;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#65288;IP&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#20248;&#21270;&#36845;&#20195;&#23637;&#24320;&#20026;&#19968;&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#38469;&#19978;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#20989;&#25968;&#12290;&#23613;&#31649;&#36825;&#20123;&#26550;&#26500;&#30446;&#21069;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#22788;&#20110;&#26368;&#21069;&#27839;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27491;&#21521;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#22312;&#35768;&#22810;&#29289;&#29702;&#24212;&#29992;&#20013;&#21463;&#38480;&#20110;&#27169;&#22411;&#31616;&#21270;&#25110;&#20202;&#22120;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#35299;&#20915;&#27491;&#21521;&#27169;&#22411;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27491;&#21521;&#27169;&#22411;&#27531;&#24046;&#22359;&#65292;&#20197;&#21305;&#37197;&#27599;&#20010;&#23454;&#20363;&#22312;&#27979;&#37327;&#22495;&#20013;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#24050;&#30693;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26550;&#26500;&#65288;LU&#21644;DEQ&#65289;&#20013;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#22312;&#36739;&#36731;&#30340;&#26465;&#20214;&#19979;&#25910;&#25947;&#12290;&#23454;&#39564;&#34920;&#26126;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04847v1 Announce Type: new  Abstract: Model-based deep learning methods such as \emph{loop unrolling} (LU) and \emph{deep equilibrium model} (DEQ) extensions offer outstanding performance in solving inverse problems (IP). These methods unroll the optimization iterations into a sequence of neural networks that in effect learn a regularization function from data. While these architectures are currently state-of-the-art in numerous applications, their success heavily relies on the accuracy of the forward model. This assumption can be limiting in many physical applications due to model simplifications or uncertainties in the apparatus. To address forward model mismatch, we introduce an untrained forward model residual block within the model-based architecture to match the data consistency in the measurement domain for each instance. We propose two variants in well-known model-based architectures (LU and DEQ) and prove convergence under mild conditions. The experiments show signi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03849</link><description>&lt;p&gt;
MedMamba: &#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba
&lt;/p&gt;
&lt;p&gt;
MedMamba: Vision Mamba for Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#38750;&#24120;&#22522;&#30784;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CNN&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#26080;&#27861;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#32780;Transformers&#21463;&#21040;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;Mamba&#34920;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20132;&#20114;&#20316;&#29992;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba&#65288;MedMamba&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;Conv-SSM&#27169;&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#23637;&#31034;MedMamba&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03849v1 Announce Type: cross  Abstract: Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02871</link><description>&lt;p&gt;
&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum Mixed-State Self-Attention Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#29420;&#29305;&#33021;&#21147;&#20026;&#22797;&#26434;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#24335;&#35782;&#21035;&#25361;&#25112;&#25552;&#20379;&#26032;&#39062;&#30340;&#35270;&#35282;&#21644;&#26041;&#27861;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#23427;&#23558;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;QMSAN&#27169;&#22411;&#37319;&#29992;&#22522;&#20110;&#28151;&#21512;&#24577;&#30340;&#37327;&#23376;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#39046;&#22495;&#20869;&#26597;&#35810;&#21644;&#38190;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#39640;&#25928;&#30452;&#25509;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376; posit
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02871v1 Announce Type: cross  Abstract: The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks. Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. Additionally, we propose an innovative quantum posit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02630</link><description>&lt;p&gt;
FedHCDR: &#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#22791;&#21463;&#20851;&#27880;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#25968;&#25454;&#26469;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CDR&#26041;&#27861;&#38656;&#35201;&#36328;&#39046;&#22495;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#65292;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#12290;&#22240;&#27492;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;FedCDR&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHCDR&#65292;&#19968;&#31181;&#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#65288;HSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#29305;&#24449;&#35299;&#32806;&#20026;&#39046;&#22495;&#29420;&#26377;&#21644;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#36229;&#22270;&#28388;&#27874;&#22120;&#26469;&#36827;&#34892;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;EBMs&#23884;&#20837;&#21040;&#25193;&#25955;&#27493;&#39588;&#20013;&#24182;&#24341;&#20837;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#25239;&#24615;&#33021;&#37327;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01666</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#25913;&#36827;&#23545;&#25239;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Energy-Based Model via Diffusion Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01666
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;EBMs&#23884;&#20837;&#21040;&#25193;&#25955;&#27493;&#39588;&#20013;&#24182;&#24341;&#20837;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#25239;&#24615;&#33021;&#37327;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#32780;&#39640;&#25928;&#30340;&#20284;&#28982;&#24230;&#20272;&#35745;&#21364;&#40092;&#20026;&#20154;&#30693;&#12290;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#22320;&#21442;&#25968;&#21270;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#65292;&#20294;&#35757;&#32451;&#38590;&#24230;&#24456;&#22823;&#12290;&#23545;&#25239;&#24615;&#30340;EBMs&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#22120;&#24418;&#25104;&#19968;&#20010;&#26497;&#23567;&#26497;&#22823;&#35757;&#32451;&#28216;&#25103;&#65292;&#20197;&#36991;&#20813;&#20256;&#32479;EBMs&#20013;&#20351;&#29992;&#26114;&#36149;&#30340;MCMC&#37319;&#26679;&#65292;&#20294;&#23545;&#25239;&#24615;&#30340;EBMs&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;EBMs&#23884;&#20837;&#21040;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#65292;&#23558;&#19968;&#20010;&#38271;&#29983;&#25104;&#36807;&#31243;&#20998;&#25104;&#20960;&#20010;&#36739;&#23567;&#30340;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#31216;&#30340;Jeffrey&#25955;&#24230;&#24182;&#24341;&#20837;&#19968;&#20010;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#26469;&#35757;&#32451;&#29983;&#25104;&#22120;&#65292;&#20197;&#35299;&#20915;&#23545;&#25239;&#24615;EBMs&#20013;&#23384;&#22312;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;EBMs&#30456;&#27604;&#65292;&#22312;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01666v1 Announce Type: new  Abstract: Generative models have shown strong generation ability while efficient likelihood estimation is less explored. Energy-based models~(EBMs) define a flexible energy function to parameterize unnormalized densities efficiently but are notorious for being difficult to train. Adversarial EBMs introduce a generator to form a minimax training game to avoid expensive MCMC sampling used in traditional EBMs, but a noticeable gap between adversarial EBMs and other strong generative models still exists. Inspired by diffusion-based models, we embedded EBMs into each denoising step to split a long-generated process into several smaller steps. Besides, we employ a symmetric Jeffrey divergence and introduce a variational posterior distribution for the generator's training to address the main challenges that exist in adversarial EBMs. Our experiments show significant improvement in generation compared to existing adversarial EBMs, while also providing a u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.01472</link><description>&lt;p&gt;
WARDEN&#65306;&#22810;&#26041;&#21521;&#32972;&#38376;&#27700;&#21360;&#29992;&#20110;Embedding-as-a-Service&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Embedding as a Service&#65288;EaaS&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;EaaS&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#65307;&#28982;&#32780;&#65292;&#36890;&#36807;&#21521;&#25991;&#26412;&#23884;&#20837;&#28155;&#21152;&#32972;&#38376;&#27700;&#21360;&#65292;&#24182;&#38543;&#21518;&#39564;&#35777;&#25915;&#20987;&#27169;&#22411;&#30340;&#21457;&#24067;&#21518;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#26368;&#36817;&#29992;&#20110;EaaS&#30340;&#27700;&#21360;&#31574;&#30053;EmbMarker&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CSE&#65288;Cluster&#12289;Selection&#12289;Elimination&#65289;&#25915;&#20987;&#65292;&#23427;&#33021;&#22815;&#31227;&#38500;&#32972;&#38376;&#27700;&#21360;&#21516;&#26102;&#20445;&#25345;&#23884;&#20837;&#30340;&#39640;&#25928;&#24615;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#27700;&#21360;&#26041;&#27861;&#26159;&#21487;&#20197;&#34987;&#31361;&#30772;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#23041;&#32961;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#20351;&#27700;&#21360;&#30340;&#31227;&#38500;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;WARDEN&#26174;&#33879;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01472v1 Announce Type: cross  Abstract: Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the ste
&lt;/p&gt;</description></item><item><title>HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.00425</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#28966;&#28857;&#23545;&#27604;&#35299;&#30721;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#65306;HALC
&lt;/p&gt;
&lt;p&gt;
HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00425
&lt;/p&gt;
&lt;p&gt;
HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#37322;&#22810;&#27169;&#24577;&#29615;&#22659;&#26041;&#38754;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#21463;&#21040;&#23545;&#35937;&#24187;&#35273;&#65288;OH&#65289;&#30340;&#22256;&#25200;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;HALC&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;LVLMs&#20013;&#30340;OH&#12290;HALC&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#29420;&#29305;&#30340;&#32454;&#31890;&#24230;&#26368;&#20339;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#25805;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HALC&#38598;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#65288;&#23616;&#37096;&#65289;&#65292;&#22312;&#36816;&#34892;&#26102;&#32416;&#27491;&#20135;&#29983;&#24187;&#35273;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;&#19968;&#31181;&#19987;&#38376;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65288;&#20840;&#23616;&#65289;&#65292;&#20197;&#26174;&#30528;&#20943;&#23569;OH&#65292;&#21516;&#26102;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;HALC&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#20102;HALC&#22312;&#20943;&#23569;OH&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00425v1 Announce Type: cross  Abstract: While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19442</link><description>&lt;p&gt;
&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65306;&#28044;&#29616;&#12289;&#25910;&#25947;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19442
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#32447;&#24615;&#22238;&#24402;&#30340;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#36873;&#25321;&#19979;&#65292;&#26799;&#24230;&#27969;&#21160;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#20013;&#20986;&#29616;&#20102;&#26377;&#36259;&#30340;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#37117;&#19987;&#27880;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#30340;&#21333;&#20010;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#8212;&#8212;&#28909;&#36523;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#25439;&#22833;&#20943;&#23569;&#36895;&#24230;&#36739;&#24930;&#65292;&#27880;&#24847;&#21147;&#22836;&#36880;&#28176;&#20542;&#21521;&#20110;&#21508;&#33258;&#30340;&#20219;&#21153;&#65307;&#28044;&#29616;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27599;&#20010;&#22836;&#36873;&#25321;&#19968;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#25439;&#22833;&#36805;&#36895;&#20943;&#23569;&#65307;&#21644;&#25910;&#25947;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27880;&#24847;&#21147;&#21442;&#25968;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#23398;&#20064;&#26497;&#38480;&#27169;&#22411;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19442v1 Announce Type: cross  Abstract: We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting "task allocation" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#20026;&#30005;&#32593;&#20648;&#33021;&#20013;&#30340;&#36864;&#24441;&#30005;&#27744;&#35774;&#35745;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#65292;&#24320;&#21457;&#20102;&#22235;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#20581;&#24247;&#35780;&#20272;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18859</link><description>&lt;p&gt;
&#21033;&#29992;&#23454;&#39564;&#65292;&#25968;&#25454;&#20998;&#26512;&#21644;&#20581;&#24247;&#35780;&#20272;&#20351;&#20108;&#27425;&#21033;&#29992;&#30005;&#27744;&#30001;&#32791;&#23613;&#21040;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Taking Second-life Batteries from Exhausted to Empowered using Experiments, Data Analysis, and Health Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#20026;&#30005;&#32593;&#20648;&#33021;&#20013;&#30340;&#36864;&#24441;&#30005;&#27744;&#35774;&#35745;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#65292;&#24320;&#21457;&#20102;&#22235;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#20581;&#24247;&#35780;&#20272;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20026;&#37096;&#32626;&#22312;&#30005;&#32593;&#20648;&#33021;&#24212;&#29992;&#20013;&#30340;&#36864;&#24441;&#30005;&#27744;&#65288;BMS$_2$&#65289;&#35774;&#35745;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#12290;&#22312;15&#20010;&#26376;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#32534;&#21046;&#12289;&#20998;&#26512;&#24182;&#20844;&#24320;&#20998;&#20139;&#20102;&#19968;&#20010;&#20108;&#27425;&#21033;&#29992;&#65288;SL&#65289;&#30005;&#27744;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#26045;&#20102;&#19968;&#20010;&#24490;&#29615;&#21327;&#35758;&#65292;&#27169;&#25311;&#20102;&#22312;3 V-4 V&#30005;&#21387;&#33539;&#22260;&#20869;&#30340;&#30005;&#32593;&#20648;&#33021;&#36127;&#33655;&#26354;&#32447;&#12290;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#22235;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#35780;&#20272;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;BMS$_2$&#29305;&#24449;&#21644;&#21021;&#22987;&#23481;&#37327;&#65292;&#25152;&#36873;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#20302;&#20110;2.3%&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22312;&#32447;&#20581;&#24247;&#35780;&#20272;&#31639;&#27861;&#65292;&#22312;&#32447;&#37096;&#32626;&#36807;&#31243;&#20013;&#38480;&#21046;&#20102;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18859v1 Announce Type: new  Abstract: The reuse of retired electric vehicle (EV) batteries in electric grid energy storage emerges as a promising strategy to address environmental concerns and boost economic value. This study concentrates on devising health monitoring algorithms for retired batteries (BMS$_2$) deployed in grid storage applications. Over 15 months of testing, we compile, analyze, and publicly share a dataset of second-life (SL) batteries, implementing a cycling protocol simulating grid energy storage load profiles within a 3 V-4 V voltage window. Four machine learning-based health estimation models, relying on BMS$_2$ features and initial capacity, are developed and compared, with the selected model achieving a Mean Absolute Percentage Error (MAPE) below 2.3% on test data. Additionally, an adaptive online health estimation algorithm is proposed by integrating a clustering-based method, limiting estimation errors during online deployment. These results constit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;</title><link>https://arxiv.org/abs/2402.18377</link><description>&lt;p&gt;
&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Generalization in Dynamical Systems Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#25214;&#21040;&#22312;&#32463;&#39564;&#29616;&#35937;&#32972;&#21518;&#30340;&#25511;&#21046;&#26041;&#31243;&#21644;&#21160;&#21147;&#35268;&#21017;&#12290;&#20256;&#32479;&#19978;&#65292;&#31185;&#23398;&#27169;&#22411;&#26159;&#36890;&#36807;&#20154;&#31867;&#27934;&#23519;&#21644;&#23454;&#39564;&#21608;&#26399;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#29992;&#26469;&#30452;&#25509;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#37325;&#26500;&#21160;&#21147;&#31995;&#32479;&#65288;DS&#65289;&#12290;&#26368;&#20808;&#36827;&#30340;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#65288;DSR&#65289;&#26041;&#27861;&#22312;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;DS&#30340;&#19981;&#21464;&#21644;&#38271;&#26399;&#29305;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#27867;&#21270;&#21040;&#26410;&#35266;&#23519;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#25105;&#20204;&#26399;&#26395;&#20174;&#20219;&#20309;&#21487;&#34892;&#30340;&#31185;&#23398;&#29702;&#35770;&#20013;&#33719;&#24471;&#30340;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;DSR&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;DSR&#20013;&#30340;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#27867;&#21270;&#65288;OODG&#65289;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#32771;&#34385;&#30340;OODG&#26377;&#26681;&#26412;&#21306;&#21035;&#12290;&#25105;&#20204;&#20171;&#32461;&#22522;&#20110;&#25299;&#25169;&#27010;&#24565;&#21644;&#31526;&#21495;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#24182;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18377v1 Announce Type: new  Abstract: In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergo
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#35774;&#32622;&#19968;&#20010;&#24658;&#23450;&#20294;&#36739;&#22823;&#30340;&#27493;&#38271;&#65292;&#22312;&#21021;&#22987;&#38663;&#33633;&#21518;&#21487;&#20197;&#23454;&#29616;&#36739;&#24555;&#30340;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#27493;&#39588;&#21518;&#21487;&#20197;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.15926</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#30340;&#22823;&#27493;&#26799;&#24230;&#19979;&#38477;&#65306;&#25439;&#22833;&#30340;&#38750;&#21333;&#35843;&#24615;&#25552;&#39640;&#20102;&#20248;&#21270;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#35774;&#32622;&#19968;&#20010;&#24658;&#23450;&#20294;&#36739;&#22823;&#30340;&#27493;&#38271;&#65292;&#22312;&#21021;&#22987;&#38663;&#33633;&#21518;&#21487;&#20197;&#23454;&#29616;&#36739;&#24555;&#30340;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#27493;&#39588;&#21518;&#21487;&#20197;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#19982;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#32467;&#21512;&#20351;&#29992;&#30340;&#24658;&#23450;&#27493;&#38271;&#24773;&#20917;&#65292;&#20854;&#20013;&#24658;&#23450;&#27493;&#38271;$\eta$&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#25439;&#22833;&#22312;&#21021;&#22987;&#38454;&#27573;&#20250;&#38663;&#33633;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GD&#22312;$\mathcal{O}(\eta)$&#27493;&#20869;&#36805;&#36895;&#36864;&#20986;&#36825;&#31181;&#21021;&#22987;&#38663;&#33633;&#38454;&#27573;&#65292;&#24182;&#22312;&#39069;&#22806;&#30340;$t$&#27493;&#20043;&#21518;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{\mathcal{O}}(1 / (\eta t) )$&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#32473;&#23450;$T$&#27493;&#30340;&#39044;&#31639;&#65292;&#20351;&#29992;&#31215;&#26497;&#30340;&#27493;&#38271;$\eta:= \Theta( T)$&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#65292;GD&#21487;&#20197;&#23454;&#29616;&#19968;&#20010;$\tilde{\mathcal{O}}(1/T^2)$&#30340;&#21152;&#36895;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#26415;&#22810;&#25165;&#22810;&#33402;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65288;&#20854;&#20013;&#38656;&#35201;&#25351;&#25968;&#23614;&#37096;&#26469;&#23454;&#29616;$\tilde{\mathcal{O}}(1/T^2)$&#30340;&#21152;&#36895;&#65289;&#12289;&#31070;&#32463;&#20999;&#32447;&#26680;&#21306;&#22495;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#22120;&#65292;&#20197;&#21450;&#20855;&#26377;&#22823;&#27493;&#38271;&#30340;&#22312;&#32447;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15926v1 Announce Type: new  Abstract: We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\mathcal{O}(\eta)$ steps -- and subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under sui
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.15374</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier detection by ensembling uncertainty with negative objectness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#30417;&#30563;&#24335;&#35270;&#35273;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#21151;&#33021;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#40723;&#21169;&#26631;&#20934;&#23553;&#38381;&#38598;&#27169;&#22411;&#22312;&#36127;&#35757;&#32451;&#25968;&#25454;&#20013;&#20135;&#29983;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#28151;&#28102;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#36127;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#65292;&#36825;&#20123;logits&#23545;&#24212;&#20110;K&#20010;&#22522;&#26412;&#30495;&#23454;&#31867;&#21035;&#21644;&#19968;&#20010;&#24322;&#24120;&#31867;&#21035;&#12290;&#36825;&#31181;&#35774;&#32622;&#20801;&#35768;&#25105;&#20204;&#21046;&#23450;&#19968;&#31181;&#26032;&#22855;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#20316;&#20026;&#20998;&#24067;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#31867;&#21035;&#30340;&#21518;&#39564;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36127;&#23545;&#35937;&#24615;&#12290;&#29616;&#22312;&#65292;&#24322;&#24120;&#20540;&#21487;&#20197;&#36890;&#36807;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#36127;&#25968;&#25454;&#30456;&#20284;&#20043;&#22788;&#29420;&#31435;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23884;&#20837;&#21040;&#19968;&#20010;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#65292;&#35813;&#32467;&#26500;&#20855;&#26377;K+2&#20010;&#31867;&#21035;&#30340;&#25513;&#30721;&#32423;&#21035;&#35782;&#21035;&#12290;&#35757;&#32451;&#36807;&#31243;&#40723;&#21169;&#26032;&#39062;&#30340;K+2-th&#31867;&#21035;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15374v1 Announce Type: cross  Abstract: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn n
&lt;/p&gt;</description></item><item><title>&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.15010</link><description>&lt;p&gt;
&#27861;&#35821;&#21307;&#29992;&#21475;&#32617;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#21270;&#26377;&#22810;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Important Is Tokenization in French Medical Masked Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15010
&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23376;&#35789;&#30340;&#26631;&#35760;&#21270;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#20854;&#25104;&#21151;&#30340;&#30830;&#20999;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#65292;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#12290;&#36825;&#22312;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#31649;&#29702;&#24418;&#24577;&#32032;&#32452;&#21512;&#30340;&#29305;&#23450;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.14760</link><description>&lt;p&gt;
&#27867;&#21270;&#22870;&#21169;&#24314;&#27169;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalizing Reward Modeling for Out-of-Distribution Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;(PL)&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26088;&#22312;&#20351;LLMs&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20197;&#24448;&#26377;&#20851;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#30340;&#30740;&#31350;&#24050;&#22312;&#20998;&#24067;&#20869;&#30340;PL&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#38590;&#24230;&#65292;&#20026;&#27599;&#20010;&#36935;&#21040;&#30340;&#20998;&#24067;&#31163;&#25955;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;(OOD) PL&#20013;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#22686;&#24378;LLMs&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#23454;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;OOD PL&#38382;&#39064;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#24341;&#23548;&#31574;&#30053;&#23398;&#20064;&#20197;&#20351;&#20043;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36935;&#21040;&#27979;&#35797;&#20998;&#24067;&#26102;&#65292;&#20803;&#27979;&#35797;&#36807;&#31243;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#20960;&#20309;&#20284;&#28982;&#35299;&#20915;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#23384;&#22312;&#20005;&#37325;&#30340;&#27424;&#37319;&#26679;&#65292;&#20063;&#33021;&#23454;&#29616;&#65292;&#19988;&#22312;&#20154;&#21475;&#35268;&#27169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14220</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#20960;&#20309;&#20998;&#24067;&#20272;&#35745;&#26410;&#30693;&#20154;&#21475;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Estimating Unknown Population Sizes Using the Hypergeometric Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#20960;&#20309;&#20284;&#28982;&#35299;&#20915;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#23384;&#22312;&#20005;&#37325;&#30340;&#27424;&#37319;&#26679;&#65292;&#20063;&#33021;&#23454;&#29616;&#65292;&#19988;&#22312;&#20154;&#21475;&#35268;&#27169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#36229;&#20960;&#20309;&#20998;&#24067;&#25551;&#36848;&#20174;&#21010;&#20998;&#20026;&#22810;&#20010;&#31867;&#21035;&#30340;&#31163;&#25955;&#20803;&#32032;&#24635;&#20307;&#20013;&#36827;&#34892;&#26080;&#25918;&#22238;&#25277;&#26679;&#12290;&#22312;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#31354;&#30333;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#30340;&#25361;&#25112;&#65292;&#24403;&#24635;&#20307;&#35268;&#27169;&#21644;&#20854;&#26500;&#25104;&#31867;&#21035;&#30340;&#22823;&#23567;&#22343;&#26410;&#30693;&#26102;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#20960;&#20309;&#20284;&#28982;&#35299;&#20915;&#36825;&#19968;&#20272;&#35745;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#23384;&#22312;&#20005;&#37325;&#30340;&#27424;&#37319;&#26679;&#20063;&#33021;&#23454;&#29616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#37322;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20854;&#20013;&#22320;&#38754;&#30495;&#23454;&#20540;&#26159;&#26377;&#26465;&#20214;&#30340;&#36830;&#32493;&#28508;&#21464;&#37327;&#28151;&#21512;&#20998;&#24067;&#65292;&#27604;&#22914;&#21327;&#21516;&#36807;&#28388;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#12290;&#23454;&#35777;&#25968;&#25454;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#21475;&#35268;&#27169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#29992;&#20110;&#24314;&#27169;&#35745;&#25968;&#25968;&#25454;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14220v1 Announce Type: new  Abstract: The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an 
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#36755;&#20986;&#23618;&#27979;&#35797;&#35823;&#24046;&#30340;&#20005;&#26684;&#28176;&#36817;&#29305;&#24615;&#65292;&#24182;&#23545;&#20351;&#29992;&#39640;&#26031;&#24425;&#34425;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;</title><link>https://arxiv.org/abs/2402.13999</link><description>&lt;p&gt;
&#28145;&#24230;&#32467;&#26500;&#21270;&#65288;&#38543;&#26426;&#65289;&#29305;&#24449;&#23398;&#20064;&#30340;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of Learning with Deep Structured (Random) Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13999
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#36755;&#20986;&#23618;&#27979;&#35797;&#35823;&#24046;&#30340;&#20005;&#26684;&#28176;&#36817;&#29305;&#24615;&#65292;&#24182;&#23545;&#20351;&#29992;&#39640;&#26031;&#24425;&#34425;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#22823;&#31867;&#29305;&#24449;&#26144;&#23556;&#65292;&#25105;&#20204;&#22312;&#36755;&#20837;&#32500;&#24230;&#12289;&#38544;&#34255;&#23618;&#23485;&#24230;&#21644;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#25104;&#27604;&#20363;&#22686;&#38271;&#30340;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#25552;&#20379;&#20102;&#19982;&#23398;&#20064;&#36755;&#20986;&#23618;&#30456;&#20851;&#30340;&#27979;&#35797;&#35823;&#24046;&#30340;&#20005;&#26684;&#28176;&#36817;&#29305;&#24615;&#21051;&#30011;&#12290;&#36825;&#19968;&#29305;&#24449;&#20197;&#29305;&#24449;&#30340;&#24635;&#20307;&#21327;&#26041;&#24046;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37096;&#20998;&#21463;&#21040;&#20351;&#29992;&#39640;&#26031;&#24425;&#34425;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#21363;&#20855;&#26377;&#38543;&#26426;&#20294;&#32467;&#26500;&#21270;&#26435;&#37325;&#30340;&#28145;&#23618;&#38750;&#32447;&#24615;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#20854;&#25353;&#34892;&#30340;&#21327;&#26041;&#24046;&#36827;&#19968;&#27493;&#20801;&#35768;&#20381;&#36182;&#20110;&#20043;&#21069;&#23618;&#30340;&#26435;&#37325;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20197;&#26435;&#37325;&#30697;&#38453;&#20026;&#22522;&#30784;&#30340;&#29305;&#24449;&#21327;&#26041;&#24046;&#30340;&#38381;&#21512;&#24418;&#24335;&#20844;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#33021;&#22815;&#25429;&#25417;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20855;&#26377;&#26377;&#38480;&#23485;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13999v1 Announce Type: cross  Abstract: For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. This characterization is formulated in terms of the population covariance of the features. Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#40065;&#26834;&#24179;&#22343;&#35268;&#21017;&#26469;&#25269;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#24335;&#23458;&#25143;&#65292;&#21516;&#26102;&#24378;&#35843;&#23458;&#25143;&#23376;&#37319;&#26679;&#21644;&#26412;&#22320;&#27493;&#39588;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.12780</link><description>&lt;p&gt;
&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#23458;&#25143;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Byzantine Clients in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12780
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#40065;&#26834;&#24179;&#22343;&#35268;&#21017;&#26469;&#25269;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#24335;&#23458;&#25143;&#65292;&#21516;&#26102;&#24378;&#35843;&#23458;&#25143;&#23376;&#37319;&#26679;&#21644;&#26412;&#22320;&#27493;&#39588;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26367;&#25442;&#26631;&#20934;$\mathsf{FedAvg}$&#31639;&#27861;&#20013;&#26381;&#21153;&#22120;&#31471;&#30340;&#31616;&#21333;&#24179;&#22343;&#25805;&#20316;&#20026;\emph{&#40065;&#26834;&#24179;&#22343;&#35268;&#21017;}&#26469;&#20351;&#32852;&#37030;&#23398;&#20064;(FL)&#25269;&#24481;&#25308;&#21344;&#24237;&#24335;(adversarial)&#23458;&#25143;&#30340;&#21487;&#33021;&#24615;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#37096;&#20998;&#24573;&#30053;&#20102;\emph{&#23458;&#25143;&#23376;&#37319;&#26679;}&#21644;\emph{&#26412;&#22320;&#27493;&#39588;}&#23545;FL&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#28145;&#20837;&#20998;&#26512;&#26469;&#39564;&#35777;&#36825;&#19968;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12780v1 Announce Type: new  Abstract: The possibility of adversarial (a.k.a., {\em Byzantine}) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\mathsf{FedAvg}$ algorithm by a \emph{robust averaging rule}. While a significant amount of work has been devoted to studying the convergence of federated {\em robust averaging} (which we denote by $\mathsf{FedRo}$), prior work has largely ignored the impact of {\em client subsampling} and {\em local steps}, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12348</link><description>&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35780;&#20272;&#25581;&#31034;LLM&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#30340;GTBench
&lt;/p&gt;
&lt;p&gt;
GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25972;&#21512;&#21040;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#30340;&#25112;&#30053;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#32431;&#36923;&#36753;&#21644;&#25112;&#30053;&#25512;&#29702;&#26469;&#19982;&#23545;&#25163;&#31454;&#20105;&#30340;&#26827;&#30424;&#28216;&#25103;&#21644;&#32440;&#29260;&#28216;&#25103;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;GTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;10&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20840;&#38754;&#30340;&#28216;&#25103;&#20998;&#31867;&#27861;&#65306;&#23436;&#25972;&#20449;&#24687;&#19982;&#19981;&#23436;&#25972;&#20449;&#24687;&#65292;&#21160;&#24577;&#19982;&#38745;&#24577;&#65292;&#20197;&#21450;&#27010;&#29575;&#19982;&#30830;&#23450;&#24615;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#34920;&#24449;LLMs&#30340;&#21338;&#24328;&#35770;&#25512;&#29702;&#65307;&#65288;2&#65289;LLM&#23545;&#25239;LLM&#30340;&#27604;&#36187;&#20316;&#20026;&#25512;&#29702;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;LLMs&#22312;&#21508;&#31181;&#28216;&#25103;&#22330;&#26223;&#19979;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#65307;&#20363;&#22914;&#65292;LLMs&#22312;&#23436;&#25972;&#21644;&#30830;&#23450;&#24615;&#28216;&#25103;&#20013;&#22833;&#36133;&#65292;&#20294;&#23427;&#20204;&#22312;&#27010;&#29575;&#28216;&#25103;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.12284</link><description>&lt;p&gt;
&#20026;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20248;&#21270;&#26497;&#23567;&#21270;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Refining Minimax Regret for Unsupervised Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12284
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#23545;&#23545;&#25163;&#26368;&#22823;&#21270;&#26576;&#20010;&#30446;&#26631;&#29983;&#25104;&#30340;&#29615;&#22659;&#37197;&#32622;&#65288;&#20851;&#21345;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36951;&#25022;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#30446;&#26631;&#65292;&#29702;&#35770;&#19978;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#26497;&#23567;&#21270;&#36951;&#25022;&#65288;MMR&#65289;&#31574;&#30053;&#65307;&#29305;&#21035;&#26159;&#65292;&#20195;&#29702;&#30340;&#26368;&#22823;&#36951;&#25022;&#26159;&#26377;&#30028;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#20195;&#29702;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#36798;&#21040;&#20102;&#36825;&#20010;&#36951;&#25022;&#19978;&#30028;&#65292;&#23545;&#25163;&#23558;&#21482;&#20250;&#23545;&#26080;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#36951;&#25022;&#30340;&#20851;&#21345;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26368;&#22823;&#21270;&#36951;&#25022;&#30340;&#20851;&#21345;&#20043;&#22806;&#21487;&#33021;&#23384;&#22312;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#65292;&#20294;&#23398;&#20064;&#20572;&#28382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#65292;&#35299;&#20915;&#36825;&#20010;&#30446;&#26631;&#23558;&#23548;&#33268;MMR&#31574;&#30053;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;BLP&#31574;&#30053;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#37117;&#19982;&#23436;&#32654;&#36125;&#21494;&#26031;&#31574;&#30053;&#19968;&#33268;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;PrivateLoRA&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#26469;&#25269;&#24481;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.11989</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;Latent&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;PrivateLoRA&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#26469;&#25269;&#24481;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#65292;&#33258;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36866;&#24212;Latent&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#20197;&#29983;&#25104;&#29305;&#23450;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;LoRA&#36866;&#24212;&#30340;LDM&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#26029;&#65288;MI&#65289;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#21028;&#26029;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#23646;&#20110;&#31169;&#20154;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#38754;&#20020;&#20005;&#37325;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;&#20026;&#20102;&#25269;&#24481;MI&#25915;&#20987;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#38544;&#31169;&#20445;&#25252;&#30340;LoRA&#65288;PrivateLoRA&#65289;&#12290;PrivateLoRA&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;MI&#22686;&#30410;&#26469;&#35757;&#32451;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#65292;&#32780;LDM&#21017;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#20043;&#21644;&#26469;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;PrivateLoRA&#23384;&#22312;&#31283;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#26799;&#24230;&#35268;&#27169;&#30340;&#22823;&#24133;&#27874;&#21160;&#32780;&#22952;&#30861;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11989v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, w
&lt;/p&gt;</description></item><item><title>MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11756</link><description>&lt;p&gt;
MARS&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;LLMs&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11756
&lt;/p&gt;
&lt;p&gt;
MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#19981;&#20934;&#30830;&#25110;&#35823;&#23548;&#24615;&#36755;&#20986;&#30340;&#20542;&#21521;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#20272;&#35745;&#29983;&#25104;&#24335;LLM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#26159;&#22686;&#24378;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;SOTA&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;&#65288;MARS&#65289;&#30340;&#26367;&#20195;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#30340;UE&#26041;&#27861;&#12290;MARS&#26159;&#19968;&#31181;&#32771;&#34385;&#22312;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#23558;MARS&#25972;&#21512;&#21040;UE&#26041;&#27861;&#20013;&#20250;&#22312;UE&#24615;&#33021;&#19978;&#24102;&#26469;&#26222;&#36941;&#21644;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#38381;&#21367;&#24335;&#38382;&#31572;&#26469;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
&lt;/p&gt;</description></item><item><title>k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.11399</link><description>&lt;p&gt;
k-SemStamp&#65306;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35821;&#20041;&#27700;&#21360;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11399
&lt;/p&gt;
&lt;p&gt;
k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27700;&#21360;&#29983;&#25104;&#31639;&#27861;&#22312;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#20197;&#20415;&#36827;&#34892;&#20107;&#21518;&#26816;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25913;&#20889;&#25915;&#20987;&#65292;&#20294;SemStamp (Hou&#31561;&#20154;&#65292;2023)&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#24212;&#29992;&#27700;&#21360;&#65292;&#24182;&#23637;&#31034;&#20986;&#24456;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;SemStamp&#21033;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#26469;&#21033;&#29992;&#20219;&#24847;&#36229;&#24179;&#38754;&#23545;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#20998;&#21306;&#65292;&#23548;&#33268;&#22312;&#40065;&#26834;&#24615;&#21644;&#36895;&#24230;&#20043;&#38388;&#23384;&#22312;&#27425;&#20248;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;k-SemStamp&#65292;&#36825;&#26159;SemStamp&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#29256;&#65292;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20316;&#20026;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#20102;&#35299;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#26469;&#20998;&#21306;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;k-SemStamp&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#25512;&#36827;&#20102;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11399v1 Announce Type: new  Abstract: Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;LowPopArt&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37327;B(Q)&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26631;&#20934;&#65292;&#20197;&#21450;&#20004;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;Arm&#38598;&#30340;&#20302;&#31209;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11156</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#22522;&#20110;Arm&#38598;&#30340;&#20302;&#31209;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;LowPopArt&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37327;B(Q)&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26631;&#20934;&#65292;&#20197;&#21450;&#20004;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;Arm&#38598;&#30340;&#20302;&#31209;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20302;&#31209;&#30697;&#38453;&#36857;&#22238;&#24402;&#21644;&#30456;&#20851;&#30340;&#20302;&#31209;&#30697;&#38453;&#36172;&#21338;&#38382;&#39064;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21327;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LowPopArt&#30340;&#26032;&#22411;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#20381;&#36182;&#20110;&#19968;&#20010;&#26032;&#39062;&#25968;&#37327;B(Q)&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#35813;&#25968;&#37327;&#34920;&#24449;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#20854;&#20013;Q&#26159;&#27979;&#37327;&#20998;&#24067;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#38382;&#39064;&#20013;&#21487;&#20197;&#25552;&#20379;&#27604;&#32463;&#20856;&#30340;&#26680;&#33539;&#25968;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;Koltchinskii&#31561;&#20154;&#65292;2011&#65289;&#26356;&#32039;&#23494;&#30340;&#24674;&#22797;&#20445;&#35777;&#12290;&#20026;&#20102;&#22312;&#20174;&#20219;&#24847;&#32473;&#23450;&#30340;&#27979;&#37327;&#38598;&#21512;A&#20013;&#36827;&#34892;&#26377;&#38480;&#27979;&#37327;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#39640;&#25928;&#20272;&#35745;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26631;&#20934;&#65292;&#35813;&#26631;&#20934;&#20197;&#35745;&#31639;&#25928;&#29575;&#26368;&#23567;&#21270;B(Q)&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#20272;&#35745;&#22120;&#21644;&#23454;&#39564;&#35774;&#35745;&#25512;&#23548;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;Arm&#38598;&#30340;&#20302;&#31209;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#65292;&#20854;&#20139;&#26377;&#25913;&#36827;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11156v1 Announce Type: cross  Abstract: We study low-rank matrix trace regression and the related problem of low-rank matrix bandits. Assuming access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called LowPopArt and provide its recovery guarantee that depends on a novel quantity denoted by B(Q) that characterizes the hardness of the problem, where Q is the covariance matrix of the measurement distribution. We show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (Koltchinskii et al., 2011) in several problems. To perform efficient estimation with a limited number of measurements from an arbitrarily given measurement set A, we also propose a novel experimental design criterion that minimizes B(Q) with computational efficiency. We leverage our novel estimator and design of experiments to derive two low-rank linear bandit algorithms for general arm sets that enjoy improved 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10820</link><description>&lt;p&gt;
&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Offline Reinforcement Learning via Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10820
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#30446;&#26631;&#26465;&#20214;&#19979;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#27425;&#20248;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#19988;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;RL&#38382;&#39064;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#34920;&#31034;&#24674;&#22797;&#20248;&#21270;&#30340;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23548;&#33268;&#35813;&#23646;&#24615;&#30340;&#26032;&#20248;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20540;&#20989;&#25968;&#20197;&#28436;&#21592;-&#35780;&#35770;&#32773;&#30340;&#26041;&#24335;&#25351;&#23548;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#36825;&#31181;&#26041;&#27861;&#34987;&#25105;&#20204;&#31216;&#20026;MetricRL&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31163;&#32447;RL&#22522;&#32447;&#22312;&#20174;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#21644;&#22810;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10820v1 Announce Type: new  Abstract: In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning. To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets. Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10634</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#38477;&#37319;&#26679;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10634
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#19982;&#31354;&#38388;&#20013;&#20256;&#24863;&#22120;&#28857;&#30456;&#20851;&#32852;&#12289;&#20855;&#26377;&#30456;&#20114;&#20851;&#31995;&#30340;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#65292;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#21253;&#25324;&#20026;&#27599;&#20010;&#28857;&#39044;&#27979;&#26410;&#26469;&#35266;&#27979;&#20540;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20026;&#22270;&#26469;&#23454;&#29616;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#36755;&#20837;&#22987;&#32456;&#21487;&#29992;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#26102;&#26080;&#27861;&#25429;&#25417;&#38544;&#34255;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#26102;&#31354;&#38477;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#38543;&#30528;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#31895;&#21270;&#65292;&#33719;&#24471;&#19968;&#32452;&#25429;&#25417;&#24322;&#36136;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#30340;&#34920;&#31034;&#12290;&#22312;&#35266;&#27979;&#20540;&#21644;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#32452;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10614</link><description>&lt;p&gt;
&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#20197;&#29983;&#25104;&#21487;&#25511;&#30340;&#20855;&#26377;&#20105;&#35758;&#24615;&#30340;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20195;&#34920;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#23588;&#20854;&#26159;&#23569;&#25968;&#32676;&#20307;&#65292;&#24182;&#20135;&#29983;&#25903;&#25345;&#20854;&#22810;&#26679;&#21270;&#29978;&#33267;&#26377;&#20105;&#35758;&#35266;&#28857;&#30340;&#22768;&#26126;&#23545;&#20110;&#21019;&#36896;&#19968;&#20010;&#21253;&#23481;&#30340;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30340;&#25511;&#21046;&#24615;&#26469;&#25903;&#25345;&#29983;&#25104;&#20869;&#23481;&#30340;&#31435;&#22330;&#65292;&#20854;&#20013;&#24448;&#24448;&#21253;&#21547;&#19981;&#19968;&#33268;&#12289;&#20013;&#31435;&#25110;&#26377;&#20559;&#35265;&#30340;&#22768;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;LLMs&#22312;&#29983;&#25104;&#25903;&#25345;&#29992;&#25143;&#22312;&#25552;&#31034;&#20013;&#23450;&#20041;&#30340;&#35770;&#28857;&#30340;&#22768;&#26126;&#26102;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#25345;&#26377;&#30456;&#21453;&#31435;&#22330;&#30340;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#23545;&#20110;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#26159;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Debate &amp; Tuning&#65288;&#8220;DEBATunE&#8221;&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#24494;&#35843;LLMs&#29983;&#25104;&#36890;&#36807;&#36777;&#35770;&#33719;&#24471;&#30340;&#22768;&#26126;&#12290;&#20026;&#20102;&#26816;&#39564;DEBATunE&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#36804;&#20170;&#20026;&#27490;&#28085;&#30422;710&#20010;&#20105;&#35758;&#24615;&#20027;&#39064;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;</title><link>https://arxiv.org/abs/2402.10110</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65306;LLM&#25351;&#20196;&#35843;&#33410;&#30340;&#23398;&#29983;&#36873;&#25321;&#25968;&#25454;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#33410;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35828;&#38750;&#24120;&#20851;&#38190;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#36394;&#21644;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#37117;&#33268;&#21147;&#20110;&#25913;&#36827;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#19982;&#27491;&#22312;&#24494;&#35843;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65292;&#36890;&#36807;&#32467;&#21512;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#12290;&#36825;&#31181;&#24072;&#29983;&#21512;&#20316;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#19988;&#19982;&#23398;&#29983;LLM&#20860;&#23481;&#30340;&#25351;&#20196;&#21709;&#24212;&#23545;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#21644;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#24120;&#33021;&#25913;&#21892;LLM&#24494;&#35843;&#21644;&#33258;&#25105;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;</title><link>https://arxiv.org/abs/2402.08948</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36755;&#20837;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#65292;&#20854;&#20013;&#36755;&#20837;&#20998;&#24067;&#26159;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#36755;&#20986;&#20165;&#20381;&#36182;&#20110;&#36755;&#20837;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#19978;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Abbe&#31561;&#20154;(2022&#24180;)&#20013;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#26465;&#20214;&#20960;&#20046;&#26159;&#20805;&#20998;&#30340;&#65292;&#21363;&#27604;&#24517;&#35201;&#26465;&#20214;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08948v1 Announce Type: new Abstract: In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We propose a basis-free generalization of the merged-staircase property in Abbe et al. (2022) and establish a necessary condition for the SGD-learnability. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.08595</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#65306;&#20851;&#20110;&#22522;&#30784;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Homomorphism Counts for Graph Neural Networks: All About That Basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#23398;&#20064;&#22270;&#19978;&#19981;&#21464;&#20989;&#25968;&#30340;&#26550;&#26500;&#12290;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#36136;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#19982;&#20854;&#34920;&#36798;&#33021;&#21147;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;&#23427;&#20204;&#26080;&#27861;&#35745;&#25968;&#22270;&#20013;&#30340;&#26576;&#20123;&#27169;&#24335;&#65288;&#20363;&#22914;&#24490;&#29615;&#65289;&#26159;&#36825;&#20123;&#38480;&#21046;&#30340;&#26680;&#24515;&#65292;&#22240;&#20026;&#35768;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#20381;&#36182;&#20110;&#35745;&#25968;&#36825;&#20123;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20004;&#31181;&#31361;&#20986;&#30340;&#33539;&#20363;&#26088;&#22312;&#36890;&#36807;&#20016;&#23500;&#22270;&#29305;&#24449;&#30340;&#23376;&#22270;&#25110;&#21516;&#24577;&#27169;&#24335;&#35745;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#37117;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#27169;&#24335;&#30340;&#8220;&#22522;&#30784;&#8221;&#20013;&#30340;&#21516;&#24577;&#35745;&#25968;&#32435;&#20837;&#32771;&#34385;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20135;&#29983;&#20102;&#26356;&#21152;&#34920;&#36798;&#21147;&#30340;&#26550;&#26500;&#65292;&#32780;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the "basis" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#31867;&#21035;&#39044;&#27979;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#27010;&#24565;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#37325;&#26032;&#26657;&#20934;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#39044;&#27979;&#20445;&#35777;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07821</link><description>&lt;p&gt;
&#35770;&#35745;&#31639;&#26377;&#25928;&#30340;&#22810;&#31867;&#21035;&#26657;&#20934;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Computationally Efficient Multi-Class Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#31867;&#21035;&#39044;&#27979;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#27010;&#24565;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#37325;&#26032;&#26657;&#20934;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#39044;&#27979;&#20445;&#35777;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#22810;&#31867;&#21035;&#26631;&#35760;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#35760;&#21487;&#20197;&#22312;[1,k]&#33539;&#22260;&#20869;&#21462;&#20540;&#65292;&#32780;&#39044;&#27979;&#22120;&#39044;&#27979;&#30340;&#26159;&#26631;&#35760;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#22522;&#30784;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#22810;&#31867;&#21035;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#32473;&#20986;&#23545;&#26377;&#24847;&#20041;&#30340;&#39044;&#27979;&#30340;&#24378;&#22823;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#65311;&#20808;&#21069;&#30340;&#26657;&#20934;&#27010;&#24565;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#65306;&#23427;&#20204;&#35201;&#20040;&#22312;k&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#35201;&#20040;&#38656;&#35201;&#27714;&#35299;&#35745;&#31639;&#38590;&#39064;&#65292;&#35201;&#20040;&#32473;&#20986;&#30340;&#20445;&#35777;&#30456;&#24403;&#24369;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#25152;&#26377;&#36825;&#20123;&#26399;&#26395;&#30340;&#26657;&#20934;&#27010;&#24565;&#65306;&#25105;&#20204;&#22312;&#22810;&#31867;&#21035;&#39044;&#27979;&#20013;&#21046;&#23450;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#26032;&#30340;&#37325;&#26032;&#26657;&#20934;&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#20010;&#23450;&#20041;&#19979;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#26657;&#20934;&#39044;&#27979;&#22120;&#12290;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#20026;&#22810;&#31867;&#21035;&#39044;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36335;&#24452;&#31215;&#20998;&#26041;&#27861;&#25506;&#32034;&#20102;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#20013;&#30340;&#27979;&#35797;&#39118;&#38505;&#65292;&#24182;&#22312;&#23567;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#35745;&#31639;&#32431;&#26799;&#24230;&#27969;&#21160;&#21644;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#30340;&#27979;&#35797;&#39118;&#38505;&#26354;&#32447;&#20043;&#38388;&#24046;&#24322;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19968;&#20010;&#24369;&#29305;&#24449;&#27169;&#22411;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#38543;&#26426;&#39033;&#23545;&#21160;&#21147;&#23398;&#30340;&#20462;&#27491;&#25928;&#26524;&#65292;&#24182;&#19982;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#25311;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07626</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#20013;&#30340;&#27979;&#35797;&#39118;&#38505;&#21450;&#20854;&#24369;&#29305;&#24449;&#30340;&#31934;&#30830;&#35299;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36335;&#24452;&#31215;&#20998;&#26041;&#27861;&#25506;&#32034;&#20102;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#20013;&#30340;&#27979;&#35797;&#39118;&#38505;&#65292;&#24182;&#22312;&#23567;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#35745;&#31639;&#32431;&#26799;&#24230;&#27969;&#21160;&#21644;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#30340;&#27979;&#35797;&#39118;&#38505;&#26354;&#32447;&#20043;&#38388;&#24046;&#24322;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19968;&#20010;&#24369;&#29305;&#24449;&#27169;&#22411;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#38543;&#26426;&#39033;&#23545;&#21160;&#21147;&#23398;&#30340;&#20462;&#27491;&#25928;&#26524;&#65292;&#24182;&#19982;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#25311;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#29702;&#35770;&#20013;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#27979;&#35797;&#39118;&#38505;&#12290;&#21033;&#29992;&#36335;&#24452;&#31215;&#20998;&#20844;&#24335;&#65292;&#22312;&#23567;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#32431;&#26799;&#24230;&#27969;&#21160;&#21644;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#30340;&#27979;&#35797;&#39118;&#38505;&#26354;&#32447;&#20043;&#38388;&#24046;&#24322;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#36890;&#29992;&#29702;&#35770;&#24212;&#29992;&#21040;&#19968;&#20010;&#31616;&#21333;&#30340;&#24369;&#29305;&#24449;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#21452;&#23792;&#29616;&#35937;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#20102;&#21160;&#21147;&#23398;&#20013;&#22686;&#21152;&#30340;&#38543;&#26426;&#39033;&#38543;&#26102;&#38388;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#20462;&#27491;&#12290;&#20998;&#26512;&#32467;&#26524;&#19982;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#25311;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the test risk of continuous-time stochastic gradient flow dynamics in learning theory. Using a path integral formulation we provide, in the regime of a small learning rate, a general formula for computing the difference between test risk curves of pure gradient and stochastic gradient flows. We apply the general theory to a simple model of weak features, which displays the double descent phenomenon, and explicitly compute the corrections brought about by the added stochastic term in the dynamics, as a function of time and model parameters. The analytical results are compared to simulations of discrete-time stochastic gradient descent and show good agreement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07366</link><description>&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22312;&#36825;&#31181;&#33539;&#24335;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#20013;&#22830;&#26381;&#21153;&#22120;&#21017;&#36127;&#36131;&#32858;&#21512;&#21644;&#35843;&#24230;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#28041;&#21450;&#23458;&#25143;&#31471;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26469;&#35757;&#32451;&#20182;&#20204;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#26469;&#36991;&#20813;&#36825;&#20123;&#32570;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#21644;&#21387;&#32553;&#38382;&#39064;&#24314;&#27169;&#20026;&#31232;&#30095;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20998;&#32452;&#31232;&#30095;&#20808;&#39564;&#20197;&#23454;&#29616;&#32467;&#26500;&#21270;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; BFL &#31639;&#27861;&#65292;&#21517;&#20026; EMTDAMP&#65292;&#20854;&#20013;&#23558;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#21644; Turbo Deep &#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#20013;&#22830;&#26381;&#21153;&#22120;&#32858;&#21512;&#26412;&#22320;&#21518;&#39564;&#20998;&#24067;&#20197;&#23454;&#29616;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26410;&#30693;&#22122;&#22768;&#27700;&#24179;&#30340;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#19982;&#24050;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#32500;&#24230;&#36739;&#22823;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26377;&#30028;&#22870;&#21169;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07341</link><description>&lt;p&gt;
&#23545;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#22122;&#22768;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#21450;&#20854;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26410;&#30693;&#22122;&#22768;&#27700;&#24179;&#30340;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#19982;&#24050;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#32500;&#24230;&#36739;&#22823;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26377;&#30028;&#22870;&#21169;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#36143;&#20915;&#31574;&#20013;&#65292;&#36866;&#24212;&#26410;&#30693;&#22122;&#22768;&#27700;&#24179;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26377;&#25928;&#30340;&#25506;&#32034;&#36890;&#24120;&#38656;&#35201;&#23545;&#22122;&#22768;&#27700;&#24179;&#26377;&#19968;&#23450;&#30340;&#20102;&#35299;&#65292;&#32780;&#22122;&#22768;&#27700;&#24179;&#36890;&#24120;&#21482;&#33021;&#31895;&#30053;&#22320;&#25351;&#23450;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#26377;&#20004;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#35813;&#32622;&#20449;&#21306;&#38388;&#22312;&#26410;&#30693;&#30340;&#20122;&#39640;&#26031;&#21442;&#25968;&#963;_*^2&#19978;&#26159;&#8220;&#21322;&#33258;&#36866;&#24212;&#8221;&#30340;&#65292;&#24847;&#21619;&#30528;&#65288;&#24402;&#19968;&#21270;&#30340;&#65289;&#32622;&#20449;&#23485;&#24230;&#19982;&#8730;&#65288;d&#963;_*^2 + &#963;_0^2&#65289;&#25104;&#27491;&#27604;&#65292;&#20854;&#20013;d&#20026;&#32500;&#24230;&#65292;&#963;_0^2&#20026;&#25351;&#23450;&#30340;&#65288;&#24050;&#30693;&#65289;&#20122;&#39640;&#26031;&#21442;&#25968;&#65292;&#20854;&#20540;&#21487;&#33021;&#27604;&#963;_*^2&#22823;&#24471;&#22810;&#12290;&#30456;&#27604;&#20110;Abbasi-Yadkori&#31561;&#20154;&#65288;2011&#65289;&#30340;&#26631;&#20934;&#32622;&#20449;&#21306;&#38388;&#30340;&#8730;&#65288;d&#963;_0^2&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#24403;d&#36739;&#22823;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#23548;&#33268;&#20102;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#26377;&#30028;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. We report significant progress in addressing this issue in linear bandits in two respects. First, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-Gaussian parameter $\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$ where $d$ is the dimension and $\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\sigma_*^2$. This is a significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that this leads to an improved regret bound in linear bandits. Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numeri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#20998;&#25903;&#31574;&#30053;&#20013;&#30340;&#23481;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#28040;&#24687;&#20256;&#36882;GNN (MP-GNN) &#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#21478;&#19968;&#31181;GNN&#32467;&#26500; second-order folklore GNN (2-FGNN) &#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.07099</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#25903;&#31574;&#30053;&#20013;&#30340;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Capacity of Graph Neural Networks for Branching Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#20998;&#25903;&#31574;&#30053;&#20013;&#30340;&#23481;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#28040;&#24687;&#20256;&#36882;GNN (MP-GNN) &#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#21478;&#19968;&#31181;GNN&#32467;&#26500; second-order folklore GNN (2-FGNN) &#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILPs&#65289;&#30340;&#23646;&#24615;&#21644;&#21551;&#21457;&#24335;&#65292;&#24182;&#21152;&#36895;MILP&#27714;&#35299;&#22120;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GNNs&#22312;&#34920;&#31034;&#25552;&#20379;&#20998;&#25903;&#38480;&#30028;&#31639;&#27861;&#20013;&#39640;&#25928;&#31574;&#30053;&#30340;&#24378;&#20998;&#25903;&#65288;SB&#65289;&#24471;&#20998;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20013;&#32463;&#24120;&#20351;&#29992;&#26368;&#31616;&#21333;&#30340;&#28040;&#24687;&#20256;&#36882;GNN&#65288;MP-GNN&#65289;&#26469;&#23398;&#20064;SB&#24471;&#20998;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#19968;&#20010;&#26681;&#26412;&#23616;&#38480;&#24615;--&#23384;&#22312;&#20004;&#20010;&#19981;&#21516;SB&#24471;&#20998;&#30340;MILP&#23454;&#20363;&#65292;&#26080;&#35770;&#21442;&#25968;&#30340;&#25968;&#37327;&#22914;&#20309;&#65292;&#37117;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;MP-GNN&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#29992;&#20110;&#21478;&#19968;&#31181;GNN&#32467;&#26500;&#31216;&#20026;second-order folklore GNN&#65288;2-FGNN&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;MILP&#25968;&#25454;&#20998;&#24067;&#65292;&#24635;&#26159;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#20197;&#20219;&#24847;&#39640;&#31934;&#24230;&#21644;&#20219;&#24847;&#39640;&#27010;&#29575;&#36924;&#36817;SB&#24471;&#20998;&#30340;2-FGNN&#12290;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#25968;&#20540;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07087</link><description>&lt;p&gt;
&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Self-Consuming Loops for Generative Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36136;&#37327;&#36234;&#26469;&#36234;&#39640;&#20197;&#21450;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#26377;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20250;&#20135;&#29983;"&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;"&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#29978;&#33267;&#23849;&#28291;&#65292;&#38500;&#38750;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#31283;&#23450;&#33258;&#25105;&#28040;&#32791;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#28857;&#26144;&#23556;&#20026;&#26356;&#26377;&#21487;&#33021;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#20351;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#30340;&#31283;&#23450;&#24615;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#65292;&#23427;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#32534;&#31243;&#22312;&#27169;&#25311;&#22120;&#20013;&#30340;&#29289;&#29702;&#23450;&#24459;&#65289;&#65292;&#24182;&#19988;&#26088;&#22312;&#33258;&#21160;&#19988;&#22823;&#35268;&#27169;&#22320;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06535</link><description>&lt;p&gt;
Bandit Convex Optimisation&#65288;&#24378;&#30423;&#20984;&#20248;&#21270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Bandit Convex Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#30423;&#20984;&#20248;&#21270;&#26159;&#30740;&#31350;&#38646;&#38454;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35768;&#22810;&#24037;&#20855;&#65292;&#21253;&#25324;&#20999;&#24179;&#38754;&#26041;&#27861;&#12289;&#20869;&#28857;&#26041;&#27861;&#12289;&#36830;&#32493;&#25351;&#25968;&#26435;&#37325;&#12289;&#26799;&#24230;&#19979;&#38477;&#21644;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;&#12290;&#35299;&#37322;&#20102;&#35768;&#22810;&#20551;&#35774;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#23613;&#31649;&#22312;&#36825;&#37324;&#27809;&#26377;&#22826;&#22810;&#30495;&#27491;&#26032;&#30340;&#19996;&#35199;&#65292;&#20294;&#19968;&#20123;&#29616;&#26377;&#24037;&#20855;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#33719;&#24471;&#26032;&#31639;&#27861;&#12290;&#19968;&#20123;&#30028;&#38480;&#31245;&#24494;&#25913;&#36827;&#20102;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. These notes cover the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online Newton step. The nuances between the many assumptions and setups are explained. Although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. A few bounds are improved in minor ways.
&lt;/p&gt;</description></item><item><title>&#22312;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#21487;&#20197;&#20135;&#29983;&#38750;&#22343;&#21248;&#20559;&#24046;&#65292;&#22240;&#27492;&#36890;&#24120;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20250;&#19982;&#31364;&#25945;&#24072;NN&#19968;&#26679;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06323</link><description>&lt;p&gt;
&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#22914;&#20309;&#24341;&#36215;&#19981;&#22343;&#21248;&#20559;&#24046;&#65306;&#20856;&#22411;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#19982;&#31364;&#25945;&#24072;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06323
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#21487;&#20197;&#20135;&#29983;&#38750;&#22343;&#21248;&#20559;&#24046;&#65292;&#22240;&#27492;&#36890;&#24120;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20250;&#19982;&#31364;&#25945;&#24072;NN&#19968;&#26679;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#29702;&#35770;&#38590;&#39064;&#26159;&#24403;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#21040;&#38646;&#35823;&#24046;&#65288;&#21363;&#25554;&#20540;&#25968;&#25454;&#65289;&#26102;&#65292;&#20026;&#20160;&#20040;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36890;&#24120;&#65292;NN&#26159;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25110;&#20854;&#21464;&#31181;&#20043;&#19968;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#23454;&#35777;&#30740;&#31350;&#26816;&#39564;&#20102;&#20174;&#30475;&#20284;&#22343;&#21248;&#30340;&#21442;&#25968;&#20808;&#39564;&#20013;&#37319;&#26679;&#30340;&#38543;&#26426;NN&#23545;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#35813;NN&#23545;&#35757;&#32451;&#38598;&#36827;&#34892;&#20102;&#23436;&#32654;&#20998;&#31867;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;NN&#26679;&#26412;&#36890;&#24120;&#20687;SGD&#35757;&#32451;&#30340;NN&#19968;&#26679;&#27867;&#21270;&#33391;&#22909;&#12290;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#23384;&#22312;&#19982;&#26631;&#31614;&#19968;&#33268;&#30340;&#31364;&#8220;&#25945;&#24072;NN&#8221;&#65292;&#37027;&#20040;&#36825;&#26679;&#30340;&#38543;&#26426;NN&#25554;&#20540;&#22120;&#36890;&#24120;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;NN&#21442;&#25968;&#21270;&#20013;&#30340;&#8220;&#24179;&#22374;&#8221;&#20808;&#39564;&#36890;&#36807;NN&#32467;&#26500;&#20013;&#30340;&#20887;&#20313;&#24341;&#20837;&#20102;&#20016;&#23500;&#30340;NN&#20989;&#25968;&#20808;&#39564;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20250;&#23545;&#36739;&#31616;&#21333;&#30340;&#20989;&#25968;&#20135;&#29983;&#20559;&#21521;&#65292;&#36825;&#20123;&#20989;&#25968;&#38656;&#35201;&#36739;&#23569;&#30340;&#30456;&#20851;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background. A main theoretical puzzle is why over-parameterized Neural Networks (NNs) generalize well when trained to zero loss (i.e., so they interpolate the data). Usually, the NN is trained with Stochastic Gradient Descent (SGD) or one of its variants. However, recent empirical work examined the generalization of a random NN that interpolates the data: the NN was sampled from a seemingly uniform prior over the parameters, conditioned on that the NN perfectly classifying the training set. Interestingly, such a NN sample typically generalized as well as SGD-trained NNs.   Contributions. We prove that such a random NN interpolator typically generalizes well if there exists an underlying narrow ``teacher NN" that agrees with the labels. Specifically, we show that such a `flat' prior over the NN parametrization induces a rich prior over the NN functions, due to the redundancy in the NN structure. In particular, this creates a bias towards simpler functions, which require less relevant pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21487;&#20197;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05674</link><description>&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#65306;&#20960;&#20309;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21487;&#20197;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#21363;&#32500;&#24230;$d$&#21644;&#25968;&#25454;&#28857;&#25968;$n$&#19982;&#22266;&#23450;&#27604;&#20363;$\alpha = n / d$&#21457;&#25955;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#36793;&#38469;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#30740;&#31350;&#25968;&#25454;&#21644;&#23545;&#25239;&#25915;&#20987;&#32773;&#20960;&#20309;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#25429;&#25417;&#21040;&#23545;&#25239;&#40065;&#26834;&#24615;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#26680;&#24515;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#36129;&#29486;&#26159;&#22312;&#36890;&#29992;&#30340;&#20984;&#19988;&#38750;&#36882;&#22686;&#25439;&#22833;&#20989;&#25968;&#19979;&#65292;&#23545;&#20110;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#21051;&#30011;&#25968;&#25454;&#20013;&#19982;&#26356;&#39640;&#30340;&#27867;&#21270;/&#40065;&#26834;&#24615;&#26435;&#34913;&#30456;&#20851;&#30340;&#26041;&#21521;&#65292;&#30001;&#19968;&#20010;&#40065;&#26834;&#24615;&#24230;&#37327;&#21644;&#19968;&#20010;&#26377;&#29992;&#24615;&#24230;&#37327;&#23450;&#20041;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23384;&#22312;&#19968;&#20123;&#26041;&#21521;&#65292;&#21487;&#20197;&#36827;&#34892;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust featu
&lt;/p&gt;</description></item><item><title>AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05602</link><description>&lt;p&gt;
AttnLRP: &#27880;&#24847;&#21147;&#24863;&#30693;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#29992;&#20110;Transformer
&lt;/p&gt;
&lt;p&gt;
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05602
&lt;/p&gt;
&lt;p&gt;
AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#21644;&#24187;&#35937;&#65292;&#36825;&#31361;&#26174;&#20102;&#29702;&#35299;&#20854;&#27169;&#22411;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24402;&#22240;&#24182;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#34429;&#28982;&#23384;&#22312;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#12290;&#36890;&#36807;&#23545;Llama 2&#12289;Flan-T5&#21644;Vision Transformer&#26550;&#26500;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#27010;&#24565;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#65288;TGT&#65289;&#65292;&#36890;&#36807;&#19977;&#20803;&#27880;&#24847;&#21147;&#21644;&#32858;&#21512;&#26426;&#21046;&#23454;&#29616;&#20102;&#22270;&#20013;&#30456;&#37051;&#23545;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#12290;&#36890;&#36807;&#39044;&#27979;&#21407;&#23376;&#38388;&#36317;&#31163;&#24182;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#20063;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04538</link><description>&lt;p&gt;
&#19977;&#20803;&#20132;&#20114;&#25913;&#36827;&#22270;&#36716;&#25442;&#22120;&#65306;&#36890;&#36807;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#23376;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#65288;TGT&#65289;&#65292;&#36890;&#36807;&#19977;&#20803;&#27880;&#24847;&#21147;&#21644;&#32858;&#21512;&#26426;&#21046;&#23454;&#29616;&#20102;&#22270;&#20013;&#30456;&#37051;&#23545;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#12290;&#36890;&#36807;&#39044;&#27979;&#21407;&#23376;&#38388;&#36317;&#31163;&#24182;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#20063;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#36890;&#24120;&#32570;&#20047;&#30452;&#25509;&#30340;&#23545;&#31561;&#36890;&#20449;&#65292;&#32780;&#26159;&#36890;&#36807;&#20849;&#21516;&#33410;&#28857;&#24378;&#21046;&#30456;&#37051;&#23545;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#65288;TGT&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26032;&#39062;&#30340;&#19977;&#20803;&#27880;&#24847;&#21147;&#21644;&#32858;&#21512;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22270;&#20013;&#20004;&#20010;&#30456;&#37051;&#23545;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#12290;TGT&#39318;&#20808;&#20174;2D&#22270;&#20013;&#39044;&#27979;&#21407;&#23376;&#38388;&#36317;&#31163;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#36317;&#31163;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#21644;&#38543;&#26426;&#25512;&#26029;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24320;&#25918;&#25361;&#25112;&#22522;&#20934;PCQM4Mv2&#21644;OC20 IS2RE&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#36824;&#22312;QM9&#12289;MOLPCBA&#21644;LIT-PCBA&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#30340;&#26368;&#26032;&#26368;&#20248;&#32467;&#26524;&#23637;&#31034;&#20102;TGT&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers typically lack direct pair-to-pair communication, instead forcing neighboring pairs to exchange information via a common node. We propose the Triplet Graph Transformer (TGT) that enables direct communication between two neighboring pairs in a graph via novel triplet attention and aggregation mechanisms. TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks. A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance. Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via transfer learning. We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QORNNs&#65289;&#26469;&#35299;&#20915;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNNs&#65289;&#20013;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#19982;s&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04012</link><description>&lt;p&gt;
&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantized Approximately Orthogonal Recurrent Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QORNNs&#65289;&#26469;&#35299;&#20915;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNNs&#65289;&#20013;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#19982;s&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNN&#65289;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#23398;&#20064;&#28041;&#21450;&#20855;&#26377;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#35745;&#31639;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#22312;&#21151;&#29575;&#21463;&#38480;&#30340;&#29615;&#22659;&#65288;&#22914;&#32039;&#20945;&#35774;&#22791;&#65289;&#20013;&#21487;&#33021;&#26159;&#38459;&#30861;&#22240;&#32032;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#12290;&#26500;&#24314;&#36825;&#26679;&#30340;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22266;&#26377;&#30340;&#19981;&#31283;&#23450;&#24615;&#26159;&#34987;&#35748;&#21487;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ORNN&#20013;&#30340;&#24490;&#29615;&#21644;&#36755;&#20837;&#26435;&#37325;&#30697;&#38453;&#30340;&#37327;&#21270;&#65292;&#23548;&#33268;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;RNN&#65288;QORNN&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#31639;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;QAT&#30340;&#20248;&#21183;&#12290;&#26368;&#39640;&#25928;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;s&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonal recurrent neural networks (ORNNs) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. However, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. One approach to address this issue is neural network quantization. The construction of such networks remains an open problem, acknowledged for its inherent instability.In this paper, we explore the quantization of the recurrent and input weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and three quantization-aware training (QAT) algorithms that incorporate orthogonal constraints and quantized weights. Empirical results demonstrate the advantages of employing QAT over PTQ. The most efficient model achieves results similar to s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#21442;&#25968;&#30340;&#21098;&#26525;&#31639;&#27861;KEN&#65292;&#23427;&#33021;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#24133;&#33410;&#30465;&#20869;&#23384;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#23454;&#29616;&#20102;&#23545;transformer&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03142</link><description>&lt;p&gt;
"&#23569;&#21363;&#26159;&#22810;&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#31616;&#21333;&#38750;&#21442;&#25968;&#21098;&#26525;&#31639;&#27861;"
&lt;/p&gt;
&lt;p&gt;
Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#21442;&#25968;&#30340;&#21098;&#26525;&#31639;&#27861;KEN&#65292;&#23427;&#33021;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#24133;&#33410;&#30465;&#20869;&#23384;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#23454;&#29616;&#20102;&#23545;transformer&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#21098;&#26525;&#31639;&#27861;&#36890;&#24120;&#23384;&#22312;&#26550;&#26500;&#29305;&#24322;&#24615;&#12289;&#36807;&#24230;&#22797;&#26434;&#21644;&#20381;&#36182;&#22797;&#26434;&#35745;&#31639;&#31561;&#38480;&#21046;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#30340;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#31639;&#27861;KEN&#12290;KEN&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26377;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#23558;&#20854;&#20182;&#21442;&#25968;&#24674;&#22797;&#21040;&#39044;&#35757;&#32451;&#29366;&#24577;&#65292;&#20174;&#32780;&#26500;&#24314;&#20248;&#21270;&#21518;&#30340;transformer&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21482;&#23384;&#20648;&#20248;&#21270;&#21518;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;&#23545;&#19971;&#20010;transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;KEN&#22312;&#26368;&#23569;&#21442;&#25968;&#20943;&#23569;25%&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#32972;&#26223;&#20449;&#24687;&#34701;&#20837;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36229;&#38598;&#30340;&#19981;&#21464;&#37327;&#32479;&#35745;&#37327;&#32435;&#20837;&#25152;&#20851;&#27880;&#30340;&#23376;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#23450;&#36229;&#32423;&#23376;&#38598;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.03139</link><description>&lt;p&gt;
&#25552;&#21319;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#65306;&#23558;&#32972;&#26223;&#20449;&#24687;&#34701;&#20837;&#21040;&#38598;&#21512;&#34920;&#31034;&#20013;
&lt;/p&gt;
&lt;p&gt;
Enhancing Neural Subset Selection: Integrating Background Information into Set Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03139
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#32972;&#26223;&#20449;&#24687;&#34701;&#20837;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36229;&#38598;&#30340;&#19981;&#21464;&#37327;&#32479;&#35745;&#37327;&#32435;&#20837;&#25152;&#20851;&#27880;&#30340;&#23376;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#23450;&#36229;&#32423;&#23376;&#38598;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#65292;&#22914;AI&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#21270;&#21512;&#29289;&#36873;&#25321;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26500;&#24314;&#27169;&#22411;&#65292;&#25429;&#25417;&#25928;&#29992;&#20989;&#25968;&#20540;&#19982;&#20854;&#30456;&#24212;&#36229;&#38598;&#20013;&#23376;&#38598;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#38598;&#21512;&#20989;&#25968;&#26102;&#24448;&#24448;&#24573;&#35270;&#20102;&#36229;&#38598;&#20013;&#21253;&#21547;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#35770;&#30340;&#35266;&#28857;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#30446;&#26631;&#20540;&#22312;&#36755;&#20837;&#38598;&#21512;&#21644;&#23376;&#38598;&#30340;&#26465;&#20214;&#19979;&#26102;&#65292;&#23558;&#36229;&#38598;&#30340;&#19981;&#21464;&#37327;&#32479;&#35745;&#37327;&#32435;&#20837;&#25152;&#20851;&#27880;&#30340;&#23376;&#38598;&#26159;&#26377;&#25928;&#23398;&#20064;&#30340;&#20851;&#38190;&#12290;&#36825;&#30830;&#20445;&#36755;&#20986;&#20540;&#23545;&#20110;&#23376;&#38598;&#21450;&#20854;&#30456;&#24212;&#30340;&#36229;&#38598;&#30340;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#30340;&#36229;&#32423;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific super
&lt;/p&gt;</description></item><item><title>BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02479</link><description>&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02479
&lt;/p&gt;
&lt;p&gt;
BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#32487;Proximal Policy Optimization (PPO)&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22914;Sequence Likelihood Calibration (SLiC)&#21644;Direct Policy Optimization (DPO)&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#31163;&#32447;&#30340;&#65292;&#24182;&#19988;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20351;&#29992;&#22870;&#21169;&#12290;&#36825;&#20123;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;DPO&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#65292;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;LLM&#23545;&#40784;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36951;&#28431;&#20102;PPO&#26041;&#27861;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#35832;&#22914;SLiC&#25110;RRHF&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;(RM)&#36827;&#34892;&#25490;&#24207;/&#20559;&#22909;&#65292;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#24573;&#30053;&#20102;RM&#30340;&#21442;&#25968;&#24418;&#24335;(&#20363;&#22914;Bradley-Terry&#12289;Plackett-Luce)&#65307;&#32780;&#35832;&#22914;DPO&#30340;&#26041;&#27861;&#29978;&#33267;&#19981;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;BRAIn&#65292;&#23427;&#23558;RM&#20316;&#20026;&#20998;&#24067;&#21305;&#37197;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#37325;&#26032;&#24341;&#20837;&#12290;BRAIn&#32771;&#34385;&#21040;&#20102;LLM&#20998;&#24067;&#22312;&#20551;&#35774;&#36755;&#20986;&#36136;&#37327;&#33391;&#22909;&#30340;&#26465;&#20214;&#19979;&#65292;&#24182;&#24212;&#29992;B...
&lt;/p&gt;
&lt;p&gt;
Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02345</link><description>&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances - &#24212;&#29992;&#20110;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#27604;&#36739;&#30340;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#23398;&#12289;&#21307;&#23398;&#39046;&#22495;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#31561;&#21508;&#20010;&#39046;&#22495;&#65292;&#27604;&#36739;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36317;&#31163;&#65292;&#27604;&#22914;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#23545;&#20110;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#24050;&#32463;&#24341;&#21457;&#20102;&#27963;&#36291;&#30340;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29699;&#24418;&#27010;&#29575;&#27979;&#24230;&#30340;&#21464;&#20307;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#25105;&#20204;&#20180;&#32454;&#22788;&#29702;&#20102;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#21450;&#20854;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20174;&#36965;&#24863;&#21644;&#22788;&#29702;&#25928;&#29575;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both spe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.02054</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Neural Scaling Laws on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21464;&#25442;&#22120;&#65289;&#24050;&#25104;&#20026;&#21033;&#29992;&#21508;&#31181;&#31867;&#22411;&#22270;&#30340;&#30693;&#35782;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#32553;&#25918;&#29305;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#65292;&#23545;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26469;&#23454;&#29616;&#22823;&#22411;&#22270;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#32034;&#20102;&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#36825;&#20123;&#23450;&#24459;&#22312;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#25551;&#36848;&#32553;&#25918;&#34892;&#20026;&#30340;&#20844;&#24335;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#30830;&#23450;&#20102;&#36807;&#25311;&#21512;&#21487;&#33021;&#26159;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#32553;&#25918;&#34892;&#20026;&#65292;&#36825;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#35266;&#23519;&#32467;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25105;&#20204;&#24314;&#35758;&#22270;&#25968;&#37327;&#26080;&#27861;&#26377;&#25928;&#34913;&#37327;&#22270;&#25968;&#25454;&#37327;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#65292;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#22411;&#30340;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01975</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#65292;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#22411;&#30340;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#30001;&#20854;&#21407;&#23376;&#12289;&#21407;&#23376;&#23646;&#24615;&#21644;&#20998;&#23376;&#30340;&#20849;&#20215;&#38190;&#32452;&#25104;&#12290;&#20998;&#23376;&#30340;3D&#65288;&#20960;&#20309;&#65289;&#34920;&#31034;&#31216;&#20026;&#26500;&#22411;&#65292;&#30001;&#20854;&#21407;&#23376;&#31867;&#22411;&#21644;&#31515;&#21345;&#23572;&#22352;&#26631;&#32452;&#25104;&#12290;&#27599;&#20010;&#26500;&#22411;&#37117;&#20855;&#26377;&#28508;&#22312;&#33021;&#37327;&#65292;&#33021;&#37327;&#36234;&#20302;&#65292;&#20854;&#22312;&#33258;&#28982;&#30028;&#20013;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#36234;&#22823;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#21482;&#32771;&#34385;2D&#20998;&#23376;&#22270;&#65292;&#35201;&#20040;&#21482;&#32771;&#34385;3D&#26500;&#22411;&#32467;&#26500;&#34920;&#31034;&#12290;&#21463;&#21040;&#26368;&#36817;&#20851;&#20110;&#22312;2D&#22270;&#34920;&#31034;&#21644;&#26500;&#22411;&#38598;&#21512;&#20013;&#20351;&#29992;&#38598;&#25104;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#26032;&#22411;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#29992;&#20110;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A molecule's 2D representation consists of its atoms, their attributes, and the molecule's covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose E(3)-invariant molecular conformer aggregation networks. The method integrates a molecule's 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D--3D aggregation mechanism based on a differentiable solver for the \emph{Fused Gromov-Wasserstein Barycenter} problem and the use of an efficient online conformer generation method based 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00564</link><description>&lt;p&gt;
&#19968;&#27425;&#22270;&#21367;&#31215;&#23601;&#22815;&#20102;&#65306;&#39640;&#25928;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#32780;CNN&#30456;&#27604;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#26356;&#21152;&#24222;&#22823;&#65292;&#36825;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36866;&#29992;&#20110;RGB&#21644;&#28784;&#24230;&#25968;&#25454;&#38598;&#65292;&#20294;&#20165;&#20165;&#20351;&#29992;&#28784;&#24230;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#30456;&#23545;&#36739;&#23569;&#35265;&#12290;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;(ATR)&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#30340;&#30690;&#37327;&#21270;&#35270;&#22270;&#30340;&#26032;&#22411;&#28784;&#24230;(&#21333;&#36890;&#36947;)&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#23558;&#38382;&#39064;&#35774;&#32622;&#20026;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;MLP&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25209;&#27425;&#32423;&#21035;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23567;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#20934;&#30830;&#29575;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2312.12716</link><description>&lt;p&gt;
BloomVQA&#65306;&#35780;&#20272;&#20998;&#23618;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BloomVQA: Assessing Hierarchical Multi-modal Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#19982;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#24120;&#20391;&#37325;&#20110;&#22522;&#20110;&#20107;&#23454;&#30340;&#35760;&#24518;&#21644;&#27809;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22522;&#20110;&#22270;&#29255;&#25925;&#20107;&#30340;&#22810;&#39033;&#36873;&#25321;&#26679;&#26412;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;&#29702;&#35299;&#65292;&#27491;&#22914;&#24067;&#40065;&#22982;&#30340;&#20998;&#31867;&#27861;&#25152;&#23637;&#31034;&#30340;&#65292;&#22312;&#25945;&#32946;&#30740;&#31350;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#32463;&#20856;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#21644;&#34920;&#24449;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#26032;&#25514;&#26045;&#12290;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#32423;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;&#19982;&#20302;&#32423;&#20219;&#21153;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38656;&#35201;&#39640;&#32423;&#29702;&#35299;&#21644;&#35748;&#30693;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19979;&#38477;&#65292;VQA&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#39640;&#36798;38.0%&#12290;&#19982;&#26089;&#26399;&#27169;&#22411;&#30456;&#27604;&#65292;GPT-4V&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons
&lt;/p&gt;</description></item><item><title>POND&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.12276</link><description>&lt;p&gt;
POND: &#24102;&#26377;&#20449;&#24687;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#30340;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12276
&lt;/p&gt;
&lt;p&gt;
POND&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;&#26159;&#19968;&#20010;&#20851;&#38190;&#32780;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12289;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#21644;&#26426;&#22120;&#25925;&#38556;&#35786;&#26029;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20010;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#28304;&#39046;&#22495;&#30340;&#39046;&#22495;&#36866;&#24212;&#19978;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#39046;&#22495;&#36866;&#24212;&#26356;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#24102;&#26469;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#39046;&#22495;&#21028;&#21035;&#65288;POND&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12276v2 Announce Type: replace Abstract: Time series domain adaptation stands as a pivotal and intricate challenge with diverse applications, including but not limited to human activity recognition, sleep stage classification, and machine fault diagnosis. Despite the numerous domain adaptation techniques proposed to tackle this complex problem, they primarily focus on domain adaptation from a single source domain. Yet, it is more crucial to investigate domain adaptation from multiple domains due to the potential for greater improvements. To address this, three important challenges need to be overcome: 1). The lack of exploration to utilize domain-specific information for domain adaptation, 2). The difficulty to learn domain-specific information that changes over time, and 3). The difficulty to evaluate learned domain-specific information. In order to tackle these challenges simultaneously, in this paper, we introduce PrOmpt-based domaiN Discrimination (POND), the first frame
&lt;/p&gt;</description></item><item><title>XLand-MiniGrid&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24037;&#20855;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12044</link><description>&lt;p&gt;
XLand-MiniGrid: &#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12044
&lt;/p&gt;
&lt;p&gt;
XLand-MiniGrid&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24037;&#20855;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;XLand&#30340;&#22810;&#26679;&#24615;&#21644;&#28145;&#24230;&#20197;&#21450;MiniGrid&#30340;&#31616;&#21333;&#21644;&#31616;&#32422;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XLand-MiniGrid&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#24037;&#20855;&#22871;&#20214;&#21644;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#12290;XLand-MiniGrid&#37319;&#29992;JAX&#32534;&#20889;&#65292;&#26088;&#22312;&#39640;&#24230;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;GPU&#25110;TPU&#21152;&#36895;&#22120;&#19978;&#36816;&#34892;&#65292;&#29992;&#26377;&#38480;&#36164;&#28304;&#23454;&#29616;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#27665;&#20027;&#21270;&#12290;&#38500;&#20102;&#29615;&#22659;&#22806;&#65292;XLand-MiniGrid&#36824;&#25552;&#20379;&#20102;&#39044;&#37319;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#29420;&#29305;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#24320;&#22987;&#35757;&#32451;&#33258;&#36866;&#24212;&#20195;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35268;&#27169;&#21270;&#21644;&#27867;&#21270;&#30340;&#21021;&#27493;&#20998;&#26512;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;&#35757;&#32451;&#20013;&#21487;&#20197;&#36798;&#21040;&#27599;&#31186;&#25968;&#30334;&#19975;&#27493;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging.
&lt;/p&gt;</description></item><item><title>&#20174;fMRI&#25968;&#25454;&#20013;&#23398;&#20064;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;&#23545;&#20110;&#33041;&#21306;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#20934;&#30830;&#34920;&#24449;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYBRID&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#36229;&#36793;&#32467;&#26500;&#21644;&#35745;&#31639;&#36229;&#36793;&#30340;&#26435;&#37325;&#65292;&#25552;&#21462;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.02203</link><description>&lt;p&gt;
&#23398;&#20064;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning High-Order Relationships of Brain Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02203
&lt;/p&gt;
&lt;p&gt;
&#20174;fMRI&#25968;&#25454;&#20013;&#23398;&#20064;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;&#23545;&#20110;&#33041;&#21306;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#20934;&#30830;&#34920;&#24449;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYBRID&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#36229;&#36793;&#32467;&#26500;&#21644;&#35745;&#31639;&#36229;&#36793;&#30340;&#26435;&#37325;&#65292;&#25552;&#21462;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20449;&#21495;&#20013;&#21457;&#29616;&#33041;&#21306;&#20043;&#38388;&#21487;&#38752;&#19988;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23545;&#20110;&#34920;&#22411;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22320;&#34920;&#24449;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#20851;&#27880;&#37197;&#23545;&#36830;&#25509;&#24182;&#24573;&#35270;&#20102;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#20123;&#39640;&#38454;&#20851;&#31995;&#24212;&#35813;&#20855;&#26377;&#26368;&#22823;&#30340;&#20449;&#24687;&#37327;&#21644;&#26368;&#23567;&#30340;&#20887;&#20313;&#65288;MIMR&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#21487;&#35299;&#24615;&#30446;&#26631;&#30340;&#32570;&#20047;&#65292;&#21457;&#29616;&#36825;&#31181;&#39640;&#38454;&#20851;&#31995;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26410;&#32463;&#25506;&#32034;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYBRID&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;fMRI&#25968;&#25454;&#20013;&#25552;&#21462;MIMR&#39640;&#38454;&#20851;&#31995;&#12290;HYBRID&#21033;&#29992;CONSTRUCTOR&#26469;&#35782;&#21035;&#36229;&#36793;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;WEIGHTER&#26469;&#35745;&#31639;&#27599;&#20010;&#36229;&#36793;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#36991;&#20813;&#22312;&#25351;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#12290;HYBRID&#21462;&#24471;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02203v2 Announce Type: replace-cross Abstract: Discovering reliable and informative relationships among brain regions from functional magnetic resonance imaging (fMRI) signals is essential in phenotypic predictions. Most of the current methods fail to accurately characterize those interactions because they only focus on pairwise connections and overlook the high-order relationships of brain regions. We propose that these high-order relationships should be maximally informative and minimally redundant (MIMR). However, identifying such high-order relationships is challenging and under-explored due to the exponential search space and the absence of a tractable objective. In response to this gap, we propose a novel method named HYBRID which aims to extract MIMR high-order relationships from fMRI data. HYBRID employs a CONSTRUCTOR to identify hyperedge structures, and a WEIGHTER to compute a weight for each hyperedge, which avoids searching in exponential space. HYBRID achieves t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#29616;&#35937;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26080;&#35770;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21463;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#20004;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#27169;&#24335;&#65306;&#35760;&#24518;&#21270;&#27169;&#24335;&#21644;&#27867;&#21270;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2310.05264</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Emergence of Reproducibility and Consistency in Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#29616;&#35937;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26080;&#35770;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21463;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#20004;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#27169;&#24335;&#65306;&#35760;&#24518;&#21270;&#27169;&#24335;&#21644;&#27867;&#21270;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#19988;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#19968;&#33268;&#30340;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#8221;&#65306;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#36215;&#22987;&#22122;&#22768;&#36755;&#20837;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#20135;&#29983;&#38750;&#24120;&#30456;&#20284;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#34920;&#26126;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#26080;&#35770;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#19978;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#21463;&#35757;&#25968;&#25454;&#35268;&#27169;&#24433;&#21709;&#19979;&#30340;&#19981;&#21516;&#20998;&#24067;&#12290;&#36825;&#19968;&#28857;&#24471;&#21040;&#20102;&#20004;&#31181;&#19981;&#21516;&#35757;&#32451;&#27169;&#24335;&#19979;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#30340;&#20307;&#29616;&#65306;&#65288;i&#65289;&#8220;&#35760;&#24518;&#21270;&#27169;&#24335;&#8221;&#65292;&#20854;&#20013;&#25193;&#25955;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#21644;&#65288;ii&#65289;&#8220;&#27867;&#21270;&#27169;&#24335;&#8221;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05264v2 Announce Type: replace  Abstract: In this work, we investigate an intriguing and prevalent phenomenon of diffusion models which we term as "consistent model reproducibility": given the same starting noise input and a deterministic sampler, different diffusion models often yield remarkably similar outputs. We confirm this phenomenon through comprehensive experiments, implying that different diffusion models consistently reach the same data distribution and scoring function regardless of diffusion model frameworks, model architectures, or training procedures. More strikingly, our further investigation implies that diffusion models are learning distinct distributions affected by the training data size. This is supported by the fact that the model reproducibility manifests in two distinct training regimes: (i) "memorization regime", where the diffusion model overfits to the training data distribution, and (ii) "generalization regime", where the model learns the underlyin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#20102;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#20197;&#21450;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.14442</link><description>&lt;p&gt;
&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Antibody Humanness Prediction using Patent Data. (arXiv:2401.14442v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#20102;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#20197;&#21450;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#26469;&#25552;&#39640;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#20102;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25239;&#20307;&#20154;&#24615;&#20316;&#20026;&#23545;&#25239;&#20307;&#27835;&#30103;&#30340;&#20813;&#30123;&#21453;&#24212;&#30340;&#20195;&#29702;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;&#25239;&#20307;&#27835;&#30103;&#38754;&#20020;&#30528;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;&#21021;&#22987;&#23398;&#20064;&#38454;&#27573;&#35270;&#20026;&#19968;&#20010;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#25239;&#20307;&#24207;&#21015;&#19982;&#21487;&#33021;&#26377;&#22810;&#20010;&#21151;&#33021;&#26631;&#35782;&#31526;&#30456;&#20851;&#32852;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#26681;&#25454;&#20854;&#19987;&#21033;&#23646;&#24615;&#23558;&#23427;&#20204;&#20998;&#32452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20923;&#32467;&#23545;&#27604;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#32487;&#32493;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#19987;&#21033;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#19981;&#21516;&#30340;&#20813;&#30123;&#21407;&#24615;&#25968;&#25454;&#38598;&#36827;&#34892;&#25512;&#29702;&#65292;&#23637;&#31034;&#20102;&#19987;&#21033;&#25968;&#25454;&#21644;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;l
&lt;/p&gt;
&lt;p&gt;
We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the l
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.13744</link><description>&lt;p&gt;
&#12298;&#35268;&#33539;&#39044;&#27979;&#38598;&#25552;&#21319;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction Sets Improve Human Decision Making. (arXiv:2401.13744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23545;&#26085;&#24120;&#26597;&#35810;&#30340;&#22238;&#24212;&#65292;&#20154;&#31867;&#26126;&#30830;&#22320;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26367;&#20195;&#31572;&#26696;&#12290;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#36755;&#20986;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#65292;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#36825;&#31181;&#34892;&#20026;&#65307;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#34920;&#31034;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#26045;&#39044;&#27880;&#20876;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65292;&#24182;&#32473;&#20154;&#31867;&#21463;&#35797;&#32773;&#25552;&#20379;&#35268;&#33539;&#39044;&#27979;&#38598;&#65292;&#30740;&#31350;&#20102;&#35268;&#33539;&#39044;&#27979;&#38598;&#23545;&#20154;&#31867;&#20915;&#31574;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#20154;&#31867;&#33719;&#24471;&#35268;&#33539;&#39044;&#27979;&#38598;&#26102;&#65292;&#20182;&#20204;&#22312;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#20351;&#29992;&#30456;&#21516;&#35206;&#30422;&#20445;&#35777;&#30340;&#22266;&#23450;&#23610;&#23544;&#39044;&#27979;&#38598;&#26102;&#26377;&#25152;&#25552;&#39640;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26377;&#21161;&#20110;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#29983;&#25104;&#22522;&#20110;&#20307;&#32032;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#30701;&#30701;3&#31186;&#20869;&#29983;&#25104;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#30340;&#24494;&#32467;&#26500;&#65292;&#29992;&#20110;&#36924;&#36817;&#25351;&#23450;&#30340;&#22343;&#36136;&#21270;&#24352;&#37327;&#30697;&#38453;&#12290;&#36825;&#19968;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#24037;&#20855;&#26377;&#21161;&#20110;&#25506;&#32034;&#26497;&#31471;&#36229;&#26448;&#26009;&#12289;&#36229;&#26448;&#26009;&#20013;&#30340;&#24207;&#21015;&#25554;&#20540;&#65292;&#20197;&#21450;&#29983;&#25104;&#22810;&#23610;&#24230;&#30340;&#22810;&#26679;&#24494;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.13570</link><description>&lt;p&gt;
&#23548;&#21521;&#25193;&#25955;&#29992;&#20110;&#24555;&#36895;&#21453;&#21521;&#35774;&#35745;&#22522;&#20110;&#23494;&#24230;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Guided Diffusion for Fast Inverse Design of Density-based Mechanical Metamaterials. (arXiv:2401.13570v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#29983;&#25104;&#22522;&#20110;&#20307;&#32032;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#30701;&#30701;3&#31186;&#20869;&#29983;&#25104;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#30340;&#24494;&#32467;&#26500;&#65292;&#29992;&#20110;&#36924;&#36817;&#25351;&#23450;&#30340;&#22343;&#36136;&#21270;&#24352;&#37327;&#30697;&#38453;&#12290;&#36825;&#19968;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#24037;&#20855;&#26377;&#21161;&#20110;&#25506;&#32034;&#26497;&#31471;&#36229;&#26448;&#26009;&#12289;&#36229;&#26448;&#26009;&#20013;&#30340;&#24207;&#21015;&#25554;&#20540;&#65292;&#20197;&#21450;&#29983;&#25104;&#22810;&#23610;&#24230;&#30340;&#22810;&#26679;&#24494;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#36229;&#26448;&#26009;&#26159;&#19968;&#31181;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#20854;&#20869;&#37096;&#32467;&#26500;&#65292;&#21487;&#20197;&#20855;&#26377;&#24322;&#24120;&#24377;&#24615;&#12289;&#21018;&#24230;&#21644;&#31283;&#23450;&#24615;&#31561;&#38750;&#20961;&#29289;&#29702;&#29305;&#24615;&#30340;&#21512;&#25104;&#26448;&#26009;&#12290;&#20026;&#20102;&#20351;&#36229;&#26448;&#26009;&#21253;&#21547;&#20855;&#26377;&#29420;&#29305;&#26426;&#26800;&#24615;&#33021;&#30340;&#31934;&#32454;&#23616;&#37096;&#32467;&#26500;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#20307;&#32032;&#26469;&#34920;&#31034;&#23427;&#20204;&#26159;&#19968;&#31181;&#28508;&#22312;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#27492;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#26159;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#20307;&#32032;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20165;&#38656;3&#31186;&#30340;&#26102;&#38388;&#20869;&#29983;&#25104;&#20998;&#36776;&#29575;&#20026; $128^3$ &#30340;&#24494;&#32467;&#26500;&#65292;&#20197;&#36924;&#36817;&#25351;&#23450;&#30340;&#22343;&#36136;&#21270;&#24352;&#37327;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#24037;&#20855;&#26377;&#21161;&#20110;&#25506;&#32034;&#26497;&#31471;&#36229;&#26448;&#26009;&#12289;&#36229;&#26448;&#26009;&#20013;&#30340;&#24207;&#21015;&#25554;&#20540;&#20197;&#21450;&#29983;&#25104;&#22810;&#23610;&#24230;&#30340;&#22810;&#26679;&#24494;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanical metamaterial is a synthetic material that can possess extraordinary physical characteristics, such as abnormal elasticity, stiffness, and stability, by carefully designing its internal structure. To make metamaterials contain delicate local structures with unique mechanical properties, it is a potential method to represent them through high-resolution voxels. However, it brings a substantial computational burden. To this end, this paper proposes a fast inverse design method, whose core is an advanced deep generative AI algorithm, to generate voxel-based mechanical metamaterials. Specifically, we use the self-conditioned diffusion model, capable of generating a microstructure with a resolution of $128^3$ to approach the specified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid reverse design tool facilitates the exploration of extreme metamaterials, the sequence interpolation in metamaterials, and the generation of diverse microstructures for multi-scale 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26080;&#34966;&#21512;&#25104;&#21160;&#33033;&#34880;&#21387;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#65292;&#19968;&#31181;&#22522;&#20110;&#39057;&#22495;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39057;&#22495;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#33033;&#34880;&#21387;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;Transformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05452</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#39057;&#22495;&#23398;&#20064;&#20174;&#21333;&#28857;PPG&#21512;&#25104;&#26080;&#34966;&#24335;&#21160;&#33033;&#34880;&#21387;&#27874;&#24418;
&lt;/p&gt;
&lt;p&gt;
Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site PPG using Transformer &amp; Frequency-domain Learning. (arXiv:2401.05452v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26080;&#34966;&#21512;&#25104;&#21160;&#33033;&#34880;&#21387;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#65292;&#19968;&#31181;&#22522;&#20110;&#39057;&#22495;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39057;&#22495;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#33033;&#34880;&#21387;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20351;&#29992;&#21333;&#28857;&#20809;&#30005;&#33033;&#25615;&#22270;(PPG)&#20449;&#21495;&#26080;&#34966;&#21512;&#25104;&#21160;&#33033;&#34880;&#21387;(ABP)&#27874;&#24418;&#12290;&#25105;&#20204;&#21033;&#29992;&#20844;&#20849;&#30340;UCI&#25968;&#25454;&#38598;&#23545;&#26080;&#34966;&#34880;&#21387;&#20272;&#35745;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#21253;&#21547;&#20301;&#32622;&#32534;&#30721;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;dropout&#25216;&#26415;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#20197;14&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;(MAE)&#21512;&#25104;ABP&#27874;&#24418;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#39057;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#33719;&#21462;PPG&#21644;ABP&#20449;&#21495;&#23545;&#24212;&#20110;&#20004;&#20010;&#24515;&#33039;&#21608;&#26399;&#30340;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;(DCT)&#31995;&#25968;&#65292;&#28982;&#21518;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#32447;&#24615;/&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39057;&#22495;&#32447;&#24615;/&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#22312;&#33298;&#24352;&#21387;(DBP)&#21644;&#25910;&#32553;&#21387;(SBP)&#26041;&#38754;&#30340;MAE&#20998;&#21035;&#20026;11.87&#21644;8.01&#65292;&#20248;&#20110;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two novel purpose-built deep learning (DL) models for synthesis of the arterial blood pressure (ABP) waveform in a cuff-less manner, using a single-site photoplethysmography (PPG) signal. We utilize the public UCI dataset on cuff-less blood pressure (CLBP) estimation to train and evaluate our DL models. Firstly, we implement a transformer model that incorporates positional encoding, multi-head attention, layer normalization, and dropout techniques, and synthesizes the ABP waveform with a mean absolute error (MAE) of 14. Secondly, we implement a frequency-domain (FD) learning approach where we first obtain the discrete cosine transform (DCT) coefficients of the PPG and ABP signals corresponding to two cardiac cycles, and then learn a linear/non-linear (L/NL) regression between them. We learn that the FD L/NL regression model outperforms the transformer model by achieving an MAE of 11.87 and 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP), respective
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2312.12141</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#65306;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#26102;&#65292;&#23376;&#20540;&#20855;&#26377;&#21487;&#20154;&#31867;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#21407;&#22240;&#12290;&#23376;&#20540;&#30340;softmax&#20043;&#21069;&#30340;&#20540;&#36890;&#36807;&#19968;&#20010;&#21152;&#27861;&#20989;&#25968;&#30456;&#21152;&#65292;&#22240;&#27492;&#35789;&#27719;&#31354;&#38388;&#20013;&#21069;&#20960;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#20250;&#22686;&#21152;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#26469;&#35745;&#31639;&#23618;&#21644;&#23376;&#20540;&#30340;&#37325;&#35201;&#24615;&#27604;&#27010;&#29575;&#22686;&#21152;&#26356;&#22909;&#65292;&#22240;&#20026;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#30340;&#26354;&#32447;&#21576;&#32447;&#24615;&#21333;&#35843;&#22686;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20869;&#31215;&#26469;&#35780;&#20272;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#23376;&#20540;&#34987;&#21069;&#38754;&#30340;&#23618;&#28608;&#27963;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20107;&#23454;&#30693;&#35782;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#23384;&#20648;&#30340;&#20301;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27880;&#24847;&#21147;&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#19982;&#27861;&#22269;&#30456;&#20851;&#8221;&#12290;FFN&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#26159;&#19968;&#20010;&#39318;&#37117;/&#22478;&#24066;&#8221;&#65292;&#30001;&#27880;&#24847;&#21147;&#23376;&#20540;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge &lt;France, capital, Paris&gt; is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2311.16167</link><description>&lt;p&gt;
&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#31070;&#32463;&#32593;&#32476;&#65288;MMPDE-Net&#65289;&#65292;&#36890;&#36807;&#27714;&#35299;&#31227;&#21160;&#32593;&#26684;PDE&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25913;&#21892;&#37319;&#26679;&#28857;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;MMPDE-Net&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#20351;&#24471;&#37319;&#26679;&#28857;&#26356;&#21152;&#31934;&#30830;&#21644;&#21487;&#25511;&#12290;&#30001;&#20110;MMPDE-Net&#26159;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#65292;&#24182;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#36890;&#36807;&#35823;&#24046;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#20856;&#22411;&#23454;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#20174;&#32780;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01331</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22987;Wasserstein&#29366;&#24577;&#21344;&#29992;&#21305;&#37197;&#23454;&#29616;&#30340;&#31163;&#32447;&#35266;&#23519;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#19982;&#29615;&#22659;&#30340;&#20219;&#24847;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#19987;&#23478;&#31034;&#33539;&#30340;&#34892;&#20026;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20004;&#32773;&#30340;&#38656;&#27714;&#65292;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#65288;LfO&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#21482;&#26377;&#19987;&#23478;&#29366;&#24577;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#38750;&#19987;&#23478;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#29992;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;$f$-divergences&#65288;KL&#21644;$\chi^2$&#65289;&#25110;&#24102;&#26377;Rubinstein&#23545;&#20598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21518;&#32773;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#30340;&#22522;&#30784;&#36317;&#31163;&#24230;&#37327;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;Wasserstein DICE&#65288;PW-DICE&#65289;&#65292;&#23427;&#36890;&#36807;&#24754;&#35266;&#27491;&#21017;&#21270;&#22120;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#20043;&#38388;&#30340;&#21407;&#22987;Wasserstein&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;dis
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned dis
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28436;&#31034;-&#27491;&#21017;&#21270;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#22797;&#26434;&#24230;&#19982;&#19987;&#23478;&#28436;&#31034;&#25968;&#37327;&#25104;&#21453;&#27604;&#12290;</title><link>http://arxiv.org/abs/2310.17303</link><description>&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;-&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#37319;&#26679;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Demonstration-Regularized RL. (arXiv:2310.17303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17303
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;-&#27491;&#21017;&#21270;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#22797;&#26434;&#24230;&#19982;&#19987;&#23478;&#28436;&#31034;&#25968;&#37327;&#25104;&#21453;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#19987;&#23478;&#28436;&#31034;&#32435;&#20837;&#20854;&#20013;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;(SRL)&#30340;&#37319;&#26679;&#25928;&#29575;&#26041;&#38754;&#20135;&#29983;&#32463;&#39564;&#25928;&#26524;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#37327;&#21270;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#38477;&#20302;&#20102;SRL&#30340;&#37319;&#26679;&#22797;&#26434;&#24615;&#30340;&#31243;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;KL&#27491;&#21017;&#21270;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#28436;&#31034;-&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#29366;&#24577;&#19979;&#65292;&#22312;$\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#65292;&#20351;&#29992;$N^{\mathrm{E}}$&#20010;&#19987;&#23478;&#28436;&#31034;&#33021;&#22815;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#22312;&#32447;&#24615;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#22312;$\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;$\varepsilon$&#26159;&#30446;&#26631;&#31934;&#24230;&#65292;$H$&#26159;&#35268;&#23450;&#65292;$A$&#26159;&#21160;&#20316;&#30340;&#25968;&#37327;&#65292;$S$&#26159;&#26377;&#38480;&#29366;&#24577;&#30340;&#25968;&#37327;&#65292;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;$d$&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in finite and $\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in linear Markov decision processes, where $\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight con
&lt;/p&gt;</description></item><item><title>&#22312;&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#20013;&#65292;&#36138;&#23146;&#36861;&#27714;&#21487;&#36716;&#31227;&#30693;&#35782;&#21487;&#33021;&#20250;&#25439;&#23475;&#23545;&#21487;&#36716;&#31227;&#21442;&#25968;&#30340;&#20272;&#35745;&#65292;&#23398;&#20064;&#32773;&#38754;&#20020;&#20219;&#21153;&#35782;&#21035;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#33719;&#21462;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2310.14968</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#30340;&#22522;&#26412;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
The Fundamental Dilemma of Bayesian Active Meta-learning. (arXiv:2310.14968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14968
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#20013;&#65292;&#36138;&#23146;&#36861;&#27714;&#21487;&#36716;&#31227;&#30693;&#35782;&#21487;&#33021;&#20250;&#25439;&#23475;&#23545;&#21487;&#36716;&#31227;&#21442;&#25968;&#30340;&#20272;&#35745;&#65292;&#23398;&#20064;&#32773;&#38754;&#20020;&#20219;&#21153;&#35782;&#21035;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#33719;&#21462;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20272;&#35745;&#22312;&#22810;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#25968;&#25454;&#31232;&#32570;&#20219;&#21153;&#29615;&#22659;&#20013;&#25512;&#24191;&#30340;&#21442;&#25968;&#12290;&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#39034;&#24207;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#30340;&#24418;&#24335;&#65292;&#20026;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#20027;&#21160;&#20803;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#24403;&#21069;&#20219;&#21153;&#30340;&#29305;&#27530;&#29305;&#24449;&#65288;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65289;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#21487;&#36716;&#31227;&#30340;&#30693;&#35782;&#65288;&#20272;&#35745;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36138;&#23146;&#36861;&#27714;&#36825;&#20010;&#30446;&#26631;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#23545;&#21487;&#36716;&#31227;&#21442;&#25968;&#30340;&#20272;&#35745;&#65288;&#24341;&#36215;&#25152;&#35859;&#30340;&#36127;&#36801;&#31227;&#65289;&#12290;&#23398;&#20064;&#32773;&#38754;&#20020;&#30528;&#19968;&#20010;&#31867;&#20284;&#20294;&#19981;&#21516;&#20110;&#21208;&#25506;-&#21033;&#29992;&#22256;&#22659;&#30340;&#22256;&#22659;&#65306;&#20182;&#20204;&#24212;&#35813;&#33457;&#36153;&#20182;&#20204;&#30340;&#33719;&#21462;&#39044;&#31639;&#26469;&#36861;&#27714;&#21487;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#36824;&#26159;&#29992;&#26469;&#30830;&#23450;&#24403;&#21069;&#20219;&#21153;&#29305;&#23450;&#30340;&#21442;&#25968;&#65311;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#19968;&#20123;&#20219;&#21153;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#19988;&#20219;&#24847;&#22823;&#30340;&#36127;&#36801;&#31227;&#23041;&#32961;&#65292;&#20219;&#21153;&#30340;&#35782;&#21035;&#23545;&#20110;&#37325;&#26032;&#23547;&#25214;&#21487;&#36801;&#31227;&#21442;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications involve estimation of parameters that generalize across multiple diverse, but related, data-scarce task environments. Bayesian active meta-learning, a form of sequential optimal experimental design, provides a framework for solving such problems. The active meta-learner's goal is to gain transferable knowledge (estimate the transferable parameters) in the presence of idiosyncratic characteristics of the current task (task-specific parameters). We show that in such a setting, greedy pursuit of this goal can actually hurt estimation of the transferable parameters (induce so-called negative transfer). The learner faces a dilemma akin to but distinct from the exploration--exploitation dilemma: should they spend their acquisition budget pursuing transferable knowledge, or identifying the current task-specific parameters? We show theoretically that some tasks pose an inevitable and arbitrarily large threat of negative transfer, and that task identification is critical to re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11439</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26174;&#33879;&#25104;&#21151;&#24120;&#24120;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#20219;&#24847;&#22797;&#26434;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#19978;&#65292;DNN&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#24341;&#20837;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#36817;&#20284;&#33021;&#21147;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;DNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#37327;&#21270;DNN&#25110;&#20010;&#21035;&#28608;&#27963;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#20855;&#20307;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36861;&#36394;&#38750;&#32447;&#24615;&#20256;&#25773;&#30340;&#29702;&#35770;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#20801;&#35768;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#21508;&#31181;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#30340;&#23454;&#38469;&#25928;&#29992;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#25299;&#25169;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#34913;&#37327;&#32593;&#32476;&#23545;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#27604;&#27973;&#23618;&#32593;&#32476;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#25299;&#25169;&#31616;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11130</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Topological Expressivity of ReLU Neural Networks. (arXiv:2310.11130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#25299;&#25169;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#34913;&#37327;&#32593;&#32476;&#23545;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#27604;&#27973;&#23618;&#32593;&#32476;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#25299;&#25169;&#31616;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#25299;&#25169;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25913;&#21464;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;&#19968;&#20010;&#25299;&#25169;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#19968;&#20010;&#25299;&#25169;&#31616;&#21333;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25299;&#25169;&#31616;&#21270;&#21487;&#20197;&#29992;Betti&#25968;&#26469;&#34913;&#37327;&#65292;Betti&#25968;&#26159;&#25299;&#25169;&#31354;&#38388;&#30340;&#20195;&#25968;&#19981;&#21464;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#34913;&#37327;&#25351;&#26631;&#26469;&#30830;&#23450;&#32473;&#23450;&#26550;&#26500;&#19979;ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#30340;&#25299;&#25169;&#31616;&#21270;&#30340;&#19978;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#25581;&#31034;ReLU&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#25968;&#25454;&#30340;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#20026;&#28145;&#20837;&#29702;&#35299;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#29305;&#21035;&#26159;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#25299;&#25169;&#31616;&#21270;&#26041;&#38754;&#27604;&#27973;&#23618;&#32593;&#32476;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplificatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08909</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#20316;&#20026;&#21453;&#20107;&#23454;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Community Membership Hiding as Counterfactual Graph Search via Deep Reinforcement Learning. (arXiv:2310.08909v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21457;&#29616;&#24444;&#27492;&#32039;&#23494;&#32852;&#31995;&#30340;&#29992;&#25143;&#32676;&#20307;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20182;&#20204;&#20849;&#20139;&#20849;&#21516;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21151;&#33021;&#24448;&#24448;&#20250;&#20197;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#38544;&#31169;&#20026;&#20195;&#20215;&#65292;&#26080;&#24847;&#20013;&#36879;&#38706;&#20182;&#20204;&#30340;&#21697;&#21619;&#25110;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#29992;&#25143;&#21487;&#33021;&#24076;&#26395;&#20445;&#25252;&#20182;&#20204;&#30340;&#21311;&#21517;&#24615;&#65292;&#24182;&#20986;&#20110;&#21508;&#31181;&#21407;&#22240;&#36873;&#25321;&#36864;&#20986;&#31038;&#21306;&#26816;&#27979;&#65292;&#20363;&#22914;&#19982;&#25919;&#27835;&#25110;&#23447;&#25945;&#32452;&#32455;&#30340;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#25112;&#30053;&#24615;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#20197;&#38450;&#27490;&#19968;&#20010;&#25110;&#22810;&#20010;&#33410;&#28857;&#34987;&#32473;&#23450;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#20010;&#21463;&#38480;&#30340;&#21453;&#20107;&#23454;&#22270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#26469;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#33410;&#28857;&#21644;&#31038;&#21306;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection techniques are useful tools for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to safeguard their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations.  In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. We validate the effectiveness of our method through two distinct tasks: node and community deception. Extensive exper
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#65292;&#24182;&#20511;&#37492;&#20102;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#12290;&#22312;&#32473;&#23450;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.07132</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;
&lt;/p&gt;
&lt;p&gt;
Risk Assessment and Statistical Significance in the Age of Foundation Models. (arXiv:2310.07132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#65292;&#24182;&#20511;&#37492;&#20102;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#12290;&#22312;&#32473;&#23450;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#25216;&#26415;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#38543;&#26426;&#21464;&#37327;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#30340;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#27979;&#35797;&#20013;&#30340;&#20108;&#38454;&#32479;&#35745;&#19982;&#22312;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#30456;&#32852;&#31995;&#65292;&#29992;&#20110;&#22312;&#36873;&#25321;&#26041;&#26696;&#26102;&#24179;&#34913;&#39118;&#38505;&#21644;&#25928;&#29992;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#27491;&#24335;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#32473;&#23450;&#30001;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#20316;&#20026;&#32858;&#21512;&#19968;&#31995;&#21015;&#24230;&#37327;&#30340;&#25163;&#27573;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#22312;&#29702;&#35770;&#19978;&#30001;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#30340;&#28176;&#36817;&#20998;&#26512;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit th
&lt;/p&gt;</description></item><item><title>Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03052</link><description>&lt;p&gt;
Memoria: &#29992;&#20110;&#31867;&#20154;&#39034;&#24207;&#22788;&#29702;&#30340;&#28023;&#27604;&#23433;&#35760;&#24518;&#20307;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03052
&lt;/p&gt;
&lt;p&gt;
Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#23481;&#37327;&#65292;Transformer &#24456;&#38590;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#12290;&#34429;&#28982;&#22686;&#21152;&#36755;&#20837;&#38271;&#24230;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26080;&#27490;&#22659;&#22320;&#22686;&#21152;&#38271;&#24230;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#27492;&#22806;&#65292;&#19982; Transformer &#19981;&#21516;&#65292;&#20154;&#31867;&#26377;&#36873;&#25321;&#24615;&#22320;&#35760;&#20303;&#21644;&#20351;&#29992;&#20165;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#21040;&#23614;&#22788;&#29702;&#25152;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Memoria&#65292;&#19968;&#20010;&#24212;&#29992;&#28023;&#27604;&#23433;&#35760;&#24518;&#24418;&#25104;&#29702;&#35770;&#30340;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;Memoria &#22312;&#24037;&#20316;&#35760;&#24518;&#12289;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22810;&#20010;&#35760;&#24518;&#23618;&#32423;&#19978;&#23384;&#20648;&#21644;&#26816;&#32034;&#31216;&#20026; engram &#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#12290;&#36890;&#36807;&#19982;&#35832;&#22914; BERT &#21644; GPT &#31561;&#27969;&#34892;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986; Memoria &#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02579</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#36798;&#20301;&#32622;&#32534;&#30721;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#22270;&#20301;&#32622;&#32534;&#30721;&#23545;&#26500;&#24314;&#24378;&#22823;&#30340;&#22270;&#36716;&#25442;&#22120;&#21644;&#22686;&#24378;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#20851;&#38190;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#65292;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#65288;1&#65289;\emph{&#38750;&#21807;&#19968;&#24615;}&#65306;&#21516;&#19968;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#35299;&#65292;&#20197;&#21450;&#65288;2&#65289;\emph{&#19981;&#31283;&#23450;&#24615;}&#65306;&#23545;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23548;&#33268;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21464;&#21270;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#23581;&#35797;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;"&#30828;&#20998;&#21106;"&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#65288;SPE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#29305;&#24449;&#21521;&#37327;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#24449;&#20540;&#23558;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#12290;SPE&#26159;&#39318;&#20010;&#65288;1&#65289;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26222;&#36866;&#22320;&#25552;&#21319;&#22270;&#32467;&#26500;&#27867;&#21270;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.17234</link><description>&lt;p&gt;
LLM-&#36777;&#35770;: &#20351;&#29992;&#20132;&#20114;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21830;&#28216;&#25103;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#20154;&#26469;&#35299;&#20915;&#21487;&#33021;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#24773;&#20917;&#30340;&#29616;&#23454;&#20219;&#21153;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#26377;&#38480;&#30340;&#29702;&#35299;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#30001;&#20110;&#35848;&#21028;&#21644;&#22949;&#21327;&#26159;&#25105;&#20204;&#26085;&#24120;&#27807;&#36890;&#21644;&#21512;&#20316;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#12289;&#22810;&#26234;&#33021;&#20307;&#30340;&#12289;&#22810;&#38382;&#39064;&#30340;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#35848;&#21028;&#28216;&#25103;&#27979;&#35797;&#24179;&#21488;&#65292;&#38590;&#24230;&#21487;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20195;&#29702;&#20154;&#38656;&#35201;&#20855;&#22791;&#24378;&#22823;&#30340;&#31639;&#26415;&#12289;&#25512;&#29702;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#21516;&#26102;&#26080;&#32541;&#22320;&#25972;&#21512;&#23427;&#20204;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#36827;&#34892;&#35848;&#21028;&#24182;&#25345;&#32493;&#36798;&#25104;&#25104;&#21151;&#20132;&#26131;&#12290;&#25105;&#20204;&#29992;&#22810;&#20010;&#25351;&#26631;&#37327;&#21270;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#21040;GPT-4&#19982;&#21407;&#25991;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#22120;&#33719;&#21462;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#25237;&#36164;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#31169;&#21215;&#32929;&#26435;&#65288;PE&#65289;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;&#39118;&#38505;&#25237;&#36164;&#65288;VC&#65289;&#21644;&#25104;&#38271;&#36164;&#26412;&#65288;GC&#65289;&#23547;&#25214;&#25237;&#36164;&#30446;&#26631;&#65288;&#21363;&#20844;&#21496;&#65289;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;TMTSC&#65289;&#26469;&#39044;&#27979;&#20219;&#20309;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23547;&#25214;&#25237;&#36164;&#38382;&#39064;&#27491;&#24335;&#23450;&#20041;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#20248;&#21270;VC&#21644;GC&#25237;&#36164;&#30340;&#23547;&#25214;&#25928;&#26524;&#12290;&#25105;&#20204;&#20381;&#27425;&#20171;&#32461;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20849;&#21516; contribut &#21040;&#20102;&#22312;VC/GC&#23547;&#25214;&#20013;&#25104;&#21151;&#24212;&#29992;TMTSC&#65306;&#36755;&#20837;&#29305;&#24449;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#20197;&#21450;&#22522;&#20110;&#25237;&#36164;&#32773;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#21010;&#20998;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D2DGN&#30340;&#22270;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#21024;&#38500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#20381;&#36182;&#21644;&#38468;&#21152;&#24320;&#38144;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20943;&#23569;&#35757;&#32451;&#37325;&#22797;&#24102;&#26469;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2309.16173</link><description>&lt;p&gt;
Distill to Delete: &#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#22270;&#32593;&#32476;&#20013;&#30340;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation. (arXiv:2309.16173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D2DGN&#30340;&#22270;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#21024;&#38500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#20381;&#36182;&#21644;&#38468;&#21152;&#24320;&#38144;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20943;&#23569;&#35757;&#32451;&#37325;&#22797;&#24102;&#26469;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36951;&#24536;&#24050;&#25104;&#20026;&#20174;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#21024;&#38500;&#20449;&#24687;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#21487;&#20197;&#21024;&#38500;&#33410;&#28857;&#12289;&#33410;&#28857;&#31867;&#12289;&#36793;&#25110;&#36793;&#31867;&#12290;&#36951;&#24536;&#26041;&#27861;&#20351;GNN&#27169;&#22411;&#31526;&#21512;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65288;&#21363;&#34987;&#36951;&#24536;&#26435;&#65289;&#65292;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36991;&#20813;&#37325;&#22797;&#35757;&#32451;&#26469;&#20943;&#23569;GPU&#23567;&#26102;&#30340;&#30899;&#36275;&#36857;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#21306;&#21644;&#32858;&#21512;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#22270;&#20381;&#36182;&#21644;&#38468;&#21152;&#24320;&#38144;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#65292;GNNDelete&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#32531;&#35299;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#36951;&#24536;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#21363;GNN&#20013;&#30340;&#36328;&#36724;&#33976;&#39311;&#36827;&#34892;&#21024;&#38500;&#65288;D2DGN&#65289;&#12290;&#36825;&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#33976;&#39311;&#26694;&#26550;&#65292;&#23558;&#23436;&#25972;&#30340;&#22270;&#30693;&#35782;&#21010;&#20998;&#24182;&#26631;&#35760;&#20026;&#20445;&#30041;&#21644;&#21024;&#38500;&#12290;&#23427;&#20351;&#29992;&#21709;&#24212;&#20026;&#22522;&#30784;&#36827;&#34892;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph unlearning has emerged as a pivotal method to delete information from a pre-trained graph neural network (GNN). One may delete nodes, a class of nodes, edges, or a class of edges. An unlearning method enables the GNN model to comply with data protection regulations (i.e., the right to be forgotten), adapt to evolving data distributions, and reduce the GPU-hours carbon footprint by avoiding repetitive retraining. Existing partitioning and aggregation-based methods have limitations due to their poor handling of local graph dependencies and additional overhead costs. More recently, GNNDelete offered a model-agnostic approach that alleviates some of these issues. Our work takes a novel approach to address these challenges in graph unlearning through knowledge distillation, as it distills to delete in GNN (D2DGN). It is a model-agnostic distillation framework where the complete graph knowledge is divided and marked for retention and deletion. It performs distillation with response-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#36890;&#36807;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30452;&#24452;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#24182;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#40065;&#26834;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#30340;&#24555;&#36895;&#25509;&#36817;&#31163;&#32447;&#26368;&#20248;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14648</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#30830;&#23450;&#25511;&#21046;&#21160;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;: &#38750;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis. (arXiv:2309.14648v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#36890;&#36807;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30452;&#24452;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#24182;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#40065;&#26834;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#30340;&#24555;&#36895;&#25509;&#36817;&#31163;&#32447;&#26368;&#20248;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#38656;&#35201;&#23545;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#33258;&#36866;&#24212;/&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#31639;&#27861;&#65292;&#20363;&#22914;&#22312;&#32447;&#40065;&#26834;&#31283;&#23450;&#25511;&#21046;&#21644;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;&#38750;&#28176;&#36817;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#26377;&#38480;&#12290;&#26412;&#25991;&#22312;&#26377;&#30028;&#12289;&#29420;&#31435;&#21516;&#20998;&#24067;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#30001;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30452;&#24452;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#23558;&#35813;&#32467;&#26524;&#24212;&#29992;&#20110;&#30001;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#26356;&#26032;&#30340;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#40065;&#26834;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#65292;&#35813;&#25511;&#21046;&#22120;&#19982;&#31163;&#32447;&#26368;&#20248;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#24555;&#36895;&#25509;&#36817;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#32622;&#20449;&#21306;&#22495;&#30340;&#25511;&#21046;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Set-membership estimation is commonly used in adaptive/learning-based control algorithms that require robustness over the model uncertainty sets, e.g., online robustly stabilizing control and robust adaptive model predictive control. Despite having broad applications, non-asymptotic estimation error bounds in the stochastic setting are limited. This paper provides such a non-asymptotic bound on the diameter of the uncertainty sets generated by set membership estimation on linear dynamical systems under bounded, i.i.d. disturbances. Further, this result is applied to robust adaptive model predictive control with uncertainty sets updated by set membership. We numerically demonstrate the performance of the robust adaptive controller, which rapidly approaches the performance of the offline optimal model predictive controller, in comparison with the control design based on least square estimation's confidence regions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12757</link><description>&lt;p&gt;
&#23545;&#20110;ConvNets&#26469;&#35828;&#65292;&#36974;&#30422;&#65288;masking&#65289;&#33021;&#25913;&#21892;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#32780;&#26174;&#33879;&#24615;&#21578;&#35785;&#20320;&#20309;&#22788;&#12290;&#65288;arXiv:2309.12757v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where. (arXiv:2309.12757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#25968;&#25454;&#24320;&#22987;&#21463;&#30410;&#20110;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#24314;&#31435;&#22312;&#36974;&#30422;&#21644;&#33258;&#37325;&#26500;&#30446;&#26631;&#20043;&#19978;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#20196;&#29260;&#21270;&#31243;&#24207;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#22270;&#20687;&#25968;&#25454;&#30340;&#21478;&#19968;&#31181;&#37325;&#35201;&#19988;&#24191;&#27867;&#37319;&#29992;&#30340;&#26550;&#26500;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23613;&#31649;&#20855;&#26377;&#39537;&#21160;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20173;&#28982;&#38754;&#20020;&#23558;&#36825;&#31181;&#30452;&#25509;&#32780;&#36890;&#29992;&#30340;&#36974;&#30422;&#25805;&#20316;&#26174;&#33879;&#22320;&#21033;&#29992;&#20110;&#20854;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#23558;&#36974;&#30422;&#25805;&#20316;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#36127;&#25285;&#65292;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;ConvNets&#20013;&#22240;&#36974;&#32617;&#25805;&#20316;&#32780;&#20135;&#29983;&#30340;&#39069;&#22806;&#36793;&#32536;&#65288;&#36974;&#30422;&#21644;&#26410;&#36974;&#30422;&#21306;&#22495;&#20043;&#38388;&#65289;&#20197;&#21450;&#20854;&#20182;&#19981;&#21033;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#35752;&#35770;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10505</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36817;&#20284;&#20449;&#36947;&#20998;&#24067;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;&#21508;&#31181;&#20449;&#36947;&#27169;&#22411;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#31934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#65288;SER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.06584</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction. (arXiv:2309.06584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#65288;ADRD&#65289;&#22312;&#32654;&#22269;&#26159;&#31532;&#20845;&#22823;&#27515;&#20129;&#21407;&#22240;&#65292;&#20934;&#30830;&#30340;ADRD&#39118;&#38505;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;ADRD&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#22270;&#20687;&#20998;&#26512;&#65292;&#32780;&#24182;&#38750;&#25152;&#26377;&#24739;&#32773;&#22312;ADRD&#35786;&#26029;&#21069;&#37117;&#25509;&#21463;&#21307;&#23398;&#24433;&#20687;&#26816;&#26597;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#21487;&#20197;&#25581;&#31034;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#21457;&#29616;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#32034;&#36180;&#25968;&#25454;&#36827;&#34892;ADRD&#39118;&#38505;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#32972;&#21518;&#32570;&#20047;&#21487;&#35299;&#37322;&#21407;&#22240;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;ADRD&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#20840;&#38754;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;VGNN&#65289;&#26469;&#20272;&#35745;ADRD&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#24773;&#26223;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20351;&#29992;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#36731;&#26799;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.  We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03564</link><description>&lt;p&gt;
&#35780;&#20272;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#31867;&#20284;&#24555;&#36895;&#21457;&#23637;&#30340;GPT&#31995;&#21015;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24433;&#21709;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#24515;&#29702;&#23398;&#31561;&#21307;&#23398;&#39046;&#22495;&#23545;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#27987;&#21402;&#20852;&#36259;&#65292;&#20294;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20855;&#20307;&#25506;&#32034;&#20173;&#28982;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#34920;&#36798;&#20010;&#20154;&#24773;&#24863;&#65307;&#22312;&#29305;&#23450;&#30340;&#20027;&#39064;&#19979;&#65292;&#36825;&#20123;&#24773;&#24863;&#36890;&#24120;&#34920;&#29616;&#20026;&#28040;&#26497;&#24773;&#32490;&#65292;&#26377;&#26102;&#20250;&#21319;&#32423;&#20026;&#33258;&#26432;&#20542;&#21521;&#12290;&#21450;&#26102;&#36776;&#35782;&#36825;&#26679;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#23545;&#26377;&#25928;&#24178;&#39044;&#21644;&#28508;&#22312;&#36991;&#20813;&#20005;&#37325;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36827;&#34892;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#33258;&#26432;&#39118;&#38505;&#21644;&#35748;&#30693;&#20559;&#24046;&#35782;&#21035;&#30340;&#23454;&#39564;&#65292;&#36827;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65306;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65292;&#32771;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02632</link><description>&lt;p&gt;
&#20174;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#20013;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35774;&#35745;&#26159;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#20351;&#29992;&#33509;&#24178;&#20010;&#22870;&#21169;&#22240;&#23376;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22870;&#21169;&#24037;&#31243;&#21463;&#21040;&#36817;&#20284;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#20248;&#25104;&#26412;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#22797;&#26434;&#20219;&#21153;&#25152;&#38656;&#30340;&#32454;&#31890;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#36716;&#21521;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20174;&#36712;&#36857;&#24207;&#21015;&#23545;&#20043;&#38388;&#30340;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#24314;&#27169;&#65292;RLHF&#23398;&#20064;&#21040;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;RLHF&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#33719;&#24471;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20010;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20351;&#29992;&#26356;&#23569;&#20154;&#21147;&#25237;&#20837;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; sim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.11639</link><description>&lt;p&gt;
&#36890;&#36807;&#22788;&#29702;S&#21442;&#25968;&#27169;&#24335;&#23545;&#27687;&#21270;&#38111;&#38177;&#30005;&#26497;&#36827;&#34892;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns. (arXiv:2308.11639v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#30005;&#23376;&#39046;&#22495;&#65292;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#22312;&#26174;&#31034;&#22120;&#12289;&#20256;&#24863;&#22120;&#21644;&#22826;&#38451;&#33021;&#30005;&#27744;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#25928;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26159;&#30830;&#20445;&#35774;&#22791;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35270;&#35273;&#26816;&#26597;&#23545;&#20110;&#36879;&#26126;&#30340;ITO&#30005;&#26497;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#29616;&#26377;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#22312;&#30830;&#23450;&#32570;&#38519;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#30772;&#22351;&#24615;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26681;&#25454;&#32570;&#38519;&#29366;&#24577;&#33719;&#21462;&#20102;&#20840;&#38754;&#30340;S&#21442;&#25968;&#27169;&#24335;&#25968;&#25454;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#65292;&#21516;&#26102;&#20998;&#26512;&#25925;&#38556;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and sev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.06718</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables. (arXiv:2308.06718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#28508;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#21253;&#25324;&#23450;&#20301;&#28508;&#21464;&#37327;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#25968;&#37327;&#65292;&#20197;&#21450;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21253;&#21547;&#28508;&#21464;&#37327;&#30340;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#24314;&#31435;&#20102;&#26576;&#20123;&#27979;&#37327;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#19982;&#20854;&#20182;&#27979;&#37327;&#21464;&#37327;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#20004;&#20010;&#35266;&#27979;&#38543;&#26426;&#21521;&#37327; $\bf{Y}$ &#21644; $\bf{Z}$&#65292;&#24403;&#19988;&#20165;&#24403; $\omega^{\intercal}\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#26159;&#29420;&#31435;&#30340;&#26102;&#65292;GIN &#25104;&#31435;&#65292;&#20854;&#20013; $\omega$ &#26159;&#30001; $\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#20043;&#38388;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#30830;&#23450;&#30340;&#38750;&#38646;&#21442;&#25968;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013; GIN &#26465;&#20214;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#24418;&#21028;&#25454;&#12290;&#31616;&#35328;&#20043;&#65292;GIN &#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#20010;&#22806;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous se
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02877</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#20803;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning in healthcare: A survey. (arXiv:2308.02877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02877
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20803;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#36866;&#24403;&#22320;&#35299;&#20915;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#25968;&#37327;&#19981;&#36275;&#12289;&#39046;&#22495;&#36716;&#31227;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#29305;&#28857;&#20351;&#20803;&#23398;&#20064;&#25104;&#20026;&#22312;&#21508;&#31181;&#21307;&#30103;&#29615;&#22659;&#20013;&#24320;&#21457;&#26377;&#24433;&#21709;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#36866;&#36873;&#25321;&#65292;&#36825;&#20123;&#29615;&#22659;&#20013;&#21487;&#29992;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#65292;&#24182;&#19988;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20063;&#19981;&#21516;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#20102;&#35299;&#23427;&#22914;&#20309;&#20197;&#21450;&#22312;&#21738;&#20123;&#26041;&#38754;&#21487;&#20197;&#35299;&#20915;&#20851;&#38190;&#30340;&#21307;&#30103;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#21518;&#23558;&#22312;&#21307;&#30103;&#39046;&#22495;&#24212;&#29992;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20998;&#20026;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#20004;&#22823;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#23884;&#20837;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#23376;&#27969;&#24418;&#30340;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2307.10644</link><description>&lt;p&gt;
Fisher-Rao&#36317;&#31163;&#21644;&#36870;&#25512;&#21040;SPD&#38181;&#36317;&#31163;&#22312;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#23884;&#20837;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#23376;&#27969;&#24418;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#65292;&#22914;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#12289;&#32467;&#26500;&#24352;&#37327;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38647;&#36798;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#65292;&#37117;&#23384;&#22312;&#30528;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#27491;&#24577;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#36807;&#28388;&#12289;&#20998;&#31867;&#25110;&#32858;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#65292;&#38656;&#35201;&#23450;&#20041;&#21512;&#36866;&#30340;&#27491;&#24577;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#36335;&#24452;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#12290;Fisher-Rao&#36317;&#31163;&#65292;&#20316;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#24341;&#36215;&#30340;Riemann&#20960;&#20309;&#36317;&#31163;&#65292;&#26159;&#19968;&#31181;&#21512;&#29702;&#30340;&#24230;&#37327;&#36317;&#31163;&#65292;&#20294;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#22806;&#65292;&#24182;&#27809;&#26377;&#38381;&#24335;&#27714;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#25253;&#21578;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#40065;&#26834;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#30340;&#23376;&#27969;&#24418;&#30340;&#24494;&#20998;&#21516;&#32986;&#23884;&#20837;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.03042</link><description>&lt;p&gt;
LLaMA&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#24212;&#29992;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#24040;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#36341;&#24615;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#21442;&#25968;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#39046;&#22495;&#36866;&#24212;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;LLaMA-LoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#26500;&#24314;&#22312;&#24320;&#28304;LLaMA&#27169;&#22411;&#19978;&#30340;PEFT&#36866;&#37197;&#22120;&#23618;&#12290;&#20020;&#24202;LLaMA-LoRA&#20351;&#29992;&#20174;MIMIC-IV&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#20020;&#24202;&#39046;&#22495;&#35774;&#35745;&#30340;&#19987;&#29992;&#36866;&#37197;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;LLaMA-LoRA&#19982;Downstream LLaMA-LoRA&#36827;&#34892;&#34701;&#21512;&#65292;&#21518;&#32773;&#26159;&#21478;&#19968;&#20010;&#19987;&#20026;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#30340;PEFT&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;QML&#23454;&#29616;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#25216;&#26415;&#12290;&#19982;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#27604;&#36739;&#65292;&#36825;&#20123;QML&#23454;&#29616;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.00908</link><description>&lt;p&gt;
&#36817;&#26399;&#37327;&#23376;&#35013;&#32622;&#19978;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;: &#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25216;&#26415;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning on Near-Term Quantum Devices: Current State of Supervised and Unsupervised Techniques for Real-World Applications. (arXiv:2307.00908v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00908
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;QML&#23454;&#29616;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#25216;&#26415;&#12290;&#19982;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#27604;&#36739;&#65292;&#36825;&#20123;QML&#23454;&#29616;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#37327;&#23376;&#30828;&#20214;&#22312;&#36895;&#24230;&#12289;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#21644;&#37327;&#23376;&#20307;&#31215;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#37327;&#23376;&#20307;&#31215;&#34987;&#23450;&#20041;&#20026;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#26368;&#22823;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#20197;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#24050;&#32463;&#26377;&#20102;&#24456;&#22823;&#30340;&#22686;&#38271;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#36873;&#23450;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#65292;&#29305;&#21035;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#24182;&#24378;&#35843;&#20102;QML&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24403;&#21069;&#38480;&#21046;&#12290;&#25105;&#20204;&#28145;&#20837;&#35752;&#35770;&#20102;&#21508;&#31181;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#25216;&#26415;&#65292;&#22914;&#32534;&#30721;&#25216;&#26415;&#12289;&#22522;&#24577;&#32467;&#26500;&#12289;&#35823;&#24046;&#34917;&#20607;&#21644;&#26799;&#24230;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;QML&#23454;&#29616;&#19982;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#24615;&#33021;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has seen considerable progress in quantum hardware in terms of the speed, number of qubits and quantum volume which is defined as the maximum size of a quantum circuit that can be effectively implemented on a near-term quantum device. Consequently, there has also been a rise in the number of works based on the applications of Quantum Machine Learning (QML) on real hardware to attain quantum advantage over their classical counterparts. In this survey, our primary focus is on selected supervised and unsupervised learning applications implemented on quantum hardware, specifically targeting real-world scenarios. Our survey explores and highlights the current limitations of QML implementations on quantum hardware. We delve into various techniques to overcome these limitations, such as encoding techniques, ansatz structure, error mitigation, and gradient methods. Additionally, we assess the performance of these QML implementations in comparison to their classical counterparts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.10649</link><description>&lt;p&gt;
CompanyKG:&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25237;&#36164;&#34892;&#19994;&#20013;&#65292;&#23545;&#20110;&#35768;&#22810;&#30446;&#30340;&#21253;&#25324;&#24066;&#22330;&#26144;&#23556;&#12289;&#31454;&#20105;&#23545;&#25163;&#20998;&#26512;&#21644;&#24182;&#36141;&#65292;&#36827;&#34892;&#32454;&#31890;&#24230;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#36890;&#24120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;CompanyKG&#30340;&#30693;&#35782;&#22270;&#65292;&#29992;&#20110;&#34920;&#31034;&#21644;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1.17&#30334;&#19975;&#23478;&#20844;&#21496;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#20016;&#23500;&#20102;&#20844;&#21496;&#25551;&#36848;&#23884;&#20837;; 15&#31181;&#19981;&#21516;&#30340;&#20844;&#21496;&#38388;&#20851;&#31995;&#23548;&#33268;&#20102;5106&#30334;&#19975;&#20010;&#24102;&#26435;&#37325;&#30340;&#36793;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#32534;&#35793;&#20102;&#19977;&#20010;&#24102;&#26377;&#27880;&#37322;&#27979;&#35797;&#38598;&#30340;&#35780;&#20272;&#20219;&#21153;: &#30456;&#20284;&#24615;&#39044;&#27979;&#12289;&#31454;&#20105;&#23545;&#25163;&#26816;&#32034;&#21644;&#30456;&#20284;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#23545;11&#31181;&#21487;&#37325;&#29616;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20998;&#20026;&#33410;&#28857;&#12289;&#36793;&#21644;&#33410;&#28857;+&#36793;&#19977;&#32452;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CompanyKG&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.07220</link><description>&lt;p&gt;
Strokes2Surface&#65306;&#20174;&#22235;&#32500;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31163;&#32447;&#20960;&#20309;&#37325;&#24314;&#31649;&#36947;Strokes2Surface&#65292;&#23427;&#26159;&#22522;&#20110;4D Sketching Interface&#65292;MR.Sketch&#30340;&#30446;&#26631;&#26159;&#38754;&#21521;&#24314;&#31569;&#35774;&#35745;&#30340;&#12290;&#35813;&#31649;&#36947;&#20174;&#35774;&#35745;&#24072;&#32472;&#21046;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;&#65292;&#22240;&#27492;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#38454;&#27573;&#20043;&#38388;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30340;&#36755;&#20837;&#21253;&#25324;3D&#31508;&#30011;&#30340;&#25240;&#32447;&#39030;&#28857;&#21450;&#20854;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#65288;&#20316;&#20026;&#31532;&#22235;&#20010;&#32500;&#24230;&#65289;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#20960;&#20309;&#21644;&#31508;&#35302;&#30456;&#20851;&#30340;&#35760;&#24405;&#23646;&#24615;&#12290;&#22522;&#20110;&#32032;&#25551;&#21512;&#24182;&#21644;&#22522;&#20110;&#32032;&#25551;&#24314;&#27169;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31649;&#36947;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24182;&#32452;&#21512;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65307;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#20004;&#20010;&#32858;&#31867;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#26681;&#25454;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#35774;&#35745;&#24072;&#36890;&#24120;&#37319;&#29992;&#30340;&#23454;&#36341;&#35266;&#23519;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#19968;&#31508;&#30011;&#26159;&#25551;&#32472;&#36793;&#30028;&#21644;&#36793;&#32536;&#36824;&#26159;&#29992;&#20110;&#22635;&#20805;&#25152;&#38656;&#24314;&#31569;&#29289;&#30340;&#23553;&#38381;&#21306;&#22495;&#21644;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#35774;&#23450;&#65292;&#25506;&#35752;&#20102;&#36890;&#20449;&#27425;&#25968;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;PAC-Bayes&#21644;&#29575;&#22833;&#30495;&#29702;&#35770;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#23545;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#23398;&#20064;&#31639;&#27861;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.05862</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#20943;&#23569;&#36890;&#20449;&#27425;&#25968;&#65281;
&lt;/p&gt;
&lt;p&gt;
Federated Learning You May Communicate Less Often!. (arXiv:2306.05862v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#35774;&#23450;&#65292;&#25506;&#35752;&#20102;&#36890;&#20449;&#27425;&#25968;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;PAC-Bayes&#21644;&#29575;&#22833;&#30495;&#29702;&#35770;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#23545;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#23398;&#20064;&#31639;&#27861;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#27169;&#22411;&#22312;&#19968;&#33324;&#24615;&#30340;&#35774;&#32622;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#21644;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#36890;&#20449;&#27425;&#25968;&#30340;&#27867;&#21270;&#35823;&#24046;&#28436;&#21464;&#65292;&#21363;&#23458;&#25143;&#31471;&#35745;&#31639;&#30340;&#26412;&#22320;&#27169;&#22411;&#22312;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#21512;&#24182;&#30340;&#39057;&#29575;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;PAC-Bayes&#21644;&#29575;&#22833;&#30495;&#29702;&#35770;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#38480;&#21046;&#65292;&#26126;&#30830;&#32771;&#34385;&#36890;&#20449;&#27425;&#25968;&#23545;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#21478;&#22806;&#36824;&#32771;&#34385;&#20102;&#21442;&#19982;&#35774;&#22791;&#25968;&#37327;K&#21644;&#20010;&#20154;&#25968;&#25454;&#38598;&#22823;&#23567;n&#23545;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#38480;&#21046;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20284;&#20046;&#26159;FL&#35774;&#32622;&#20013;&#39318;&#27425;&#20986;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#38480;&#21046;&#24212;&#29992;&#20110;FL&#31867;&#22411;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;(FSVM)&#65307;&#25105;&#20204;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25512;&#23548;&#20102;&#26356;&#26126;&#30830;&#30340;&#27867;&#21270;&#35823;&#24046;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the generalization error of statistical learning models in a Federated Learning (FL) setting. Specifically, we study the evolution of the generalization error with the number of communication rounds between the clients and the parameter server, i.e., the effect on the generalization error of how often the local models as computed by the clients are aggregated at the parameter server. We establish PAC-Bayes and rate-distortion theoretic bounds on the generalization error that account explicitly for the effect of the number of rounds, say $ R \in \mathbb{N}$, in addition to the number of participating devices $K$ and individual datasets size $n$. The bounds, which apply in their generality for a large class of loss functions and learning algorithms, appear to be the first of their kind for the FL setting. Furthermore, we apply our bounds to FL-type Support Vector Machines (FSVM); and we derive (more) explicit bounds on the generalization error in this case. In particular, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#25968;&#25454;&#28857;&#20013;&#26631;&#35782;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#21644;&#38376;&#30697;&#38453;&#26469;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04785</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Deep Clustering. (arXiv:2306.04785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#25968;&#25454;&#28857;&#20013;&#26631;&#35782;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#21644;&#38376;&#30697;&#38453;&#26469;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#39033;&#24191;&#27867;&#24212;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22522;&#30784;&#23398;&#20064;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#20351;&#29992;&#32858;&#31867;&#20998;&#37197;&#26469;&#20998;&#26512;&#22522;&#22240;&#32452;&#24207;&#21015;&#12289;&#21307;&#30103;&#35760;&#24405;&#25110;&#22270;&#20687;&#12290;&#30001;&#20110;&#19979;&#28216;&#20998;&#26512;&#36890;&#24120;&#22312;&#32858;&#31867;&#32423;&#21035;&#19978;&#25191;&#34892;&#65292;&#22240;&#27492;&#20174;&#19994;&#32773;&#23547;&#27714;&#21487;&#38752;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#32858;&#31867;&#20998;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#36807;&#31243;&#26469;&#20174;&#27599;&#20010;&#25968;&#25454;&#28857;&#20013;&#26631;&#35782;&#20986;&#19968;&#32452;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32858;&#31867;&#20998;&#37197;&#21644;&#19968;&#20010;&#38376;&#30697;&#38453;&#65292;&#29992;&#20110;&#24341;&#23548;&#32858;&#31867;&#32423;&#21035;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#32858;&#31867;&#20998;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#19978;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is a fundamental learning task widely used as a first step in data analysis. For example, biologists often use cluster assignments to analyze genome sequences, medical records, or images. Since downstream analysis is typically performed at the cluster level, practitioners seek reliable and interpretable clustering models. We propose a new deep-learning framework that predicts interpretable cluster assignments at the instance and cluster levels. First, we present a self-supervised procedure to identify a subset of informative features from each data point. Then, we design a model that predicts cluster assignments and a gate matrix that leads to cluster-level feature selection. We show that the proposed method can reliably predict cluster assignments using synthetic and real data. Furthermore, we verify that our model leads to interpretable results at a sample and cluster level.
&lt;/p&gt;</description></item><item><title>UCTB&#26159;&#19968;&#20010;&#22478;&#24066;&#35745;&#31639;&#24037;&#20855;&#31665;&#65292;&#23427;&#22312;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#26041;&#38754;&#25972;&#21512;&#20102;&#22810;&#20010;&#39046;&#22495;&#30693;&#35782;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#35299;&#20915;&#35813;&#39046;&#22495;&#22797;&#26434;&#24230;&#39640;&#12289;&#30693;&#35782;&#22810;&#26679;&#12289;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04144</link><description>&lt;p&gt;
UCTB&#65306;&#38754;&#21521;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#30340;&#22478;&#24066;&#35745;&#31639;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
UCTB: An Urban Computing Tool Box for Spatiotemporal Crowd Flow Prediction. (arXiv:2306.04144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04144
&lt;/p&gt;
&lt;p&gt;
UCTB&#26159;&#19968;&#20010;&#22478;&#24066;&#35745;&#31639;&#24037;&#20855;&#31665;&#65292;&#23427;&#22312;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#26041;&#38754;&#25972;&#21512;&#20102;&#22810;&#20010;&#39046;&#22495;&#30693;&#35782;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#35299;&#20915;&#35813;&#39046;&#22495;&#22797;&#26434;&#24230;&#39640;&#12289;&#30693;&#35782;&#22810;&#26679;&#12289;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#26159;&#26234;&#24935;&#22478;&#24066;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#65292;&#30446;&#21069;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#20154;&#32676;&#27969;&#19982;&#22810;&#20010;&#39046;&#22495;&#30693;&#35782;&#22240;&#32032;&#30456;&#20851;&#65292;&#20294;&#30001;&#20110;&#24212;&#29992;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#65292;&#38590;&#20197;&#21512;&#29702;&#20840;&#38754;&#22320;&#20351;&#29992;&#39046;&#22495;&#30693;&#35782;&#65307;&#20854;&#27425;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#30456;&#20851;&#25216;&#26415;&#30340;&#23454;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#22797;&#21046;&#20808;&#36827;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#32791;&#26102;&#21644;&#32321;&#29712;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;UCTB&#30340;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#24037;&#20855;&#31665;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#22810;&#20010;&#26102;&#31354;&#39046;&#22495;&#30693;&#35782;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#30456;&#20851;&#20195;&#30721;&#21644;&#25903;&#25345;&#25991;&#26723;&#24050;&#22312; https://github.com/uctb/UCTB &#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal crowd flow prediction is one of the key technologies in smart cities. Currently, there are two major pain points that plague related research and practitioners. Firstly, crowd flow is related to multiple domain knowledge factors; however, due to the diversity of application scenarios, it is difficult for subsequent work to make reasonable and comprehensive use of domain knowledge. Secondly, with the development of deep learning technology, the implementation of relevant techniques has become increasingly complex; reproducing advanced models has become a time-consuming and increasingly cumbersome task. To address these issues, we design and implement a spatiotemporal crowd flow prediction toolbox called UCTB (Urban Computing Tool Box), which integrates multiple spatiotemporal domain knowledge and state-of-the-art models simultaneously. The relevant code and supporting documents have been open-sourced at https://github.com/uctb/UCTB.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807; Gateaux Derivative &#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.03202</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Distributionally Robust Optimization. (arXiv:2306.03202v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807; Gateaux Derivative &#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#22312;&#20998;&#24067;&#19978;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#25991;&#29486;&#26377;&#25152;&#19981;&#21516;&#12290;&#20026;&#35299;&#20915;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#20248;&#21270;&#38750;&#32447;&#24615;&#20989;&#25968;&#38754;&#20020;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Derivative&#21644;&#30456;&#24212;&#30340;&#24179;&#28369;&#24230;&#27010;&#24565;&#65292;&#22522;&#20110;Gateaux Derivative&#26469;&#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;Var&#12289;entropic risk&#21644;&#26377;&#38480;&#25903;&#25345;&#38598;&#19978;&#30340;&#19977;&#20010;&#36816;&#34892;&#39118;&#38505;&#24230;&#37327;&#31034;&#20363;&#26469;&#35299;&#37322;&#36825;&#20123;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#27010;&#29575;&#31354;&#38388;&#20013;&#19968;&#33324;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;G-derivative&#30340;Frank-Wolfe&#65288;FW&#65289;&#31639;&#27861;&#65292;&#24182;&#20197;&#23436;&#20840;&#29420;&#31435;&#20110;&#33539;&#25968;&#30340;&#26041;&#24335;&#25512;&#23548;&#20986;&#20854;&#25910;&#25947;&#24615;&#22312;&#25552;&#20986;&#30340;&#24179;&#28369;&#24230;&#27010;&#24565;&#19979;&#12290;&#25105;&#20204;&#21033;&#29992;FW&#31639;&#27861;&#30340;&#35774;&#32622;&#26469;&#35774;&#35745;&#19968;&#31181;&#35745;&#31639;&#38750;&#32447;&#24615;DRO&#38382;&#39064;&#38797;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article focuses on a class of distributionally robust optimization (DRO) problems where, unlike the growing body of the literature, the objective function is potentially non-linear in the distribution. Existing methods to optimize nonlinear functions in probability space use the Frechet derivatives, which present both theoretical and computational challenges. Motivated by this, we propose an alternative notion for the derivative and corresponding smoothness based on Gateaux (G)-derivative for generic risk measures. These concepts are explained via three running risk measure examples of variance, entropic risk, and risk on finite support sets. We then propose a G-derivative based Frank-Wolfe~(FW) algorithm for generic non-linear optimization problems in probability spaces and establish its convergence under the proposed notion of smoothness in a completely norm-independent manner. We use the set-up of the FW algorithm to devise a methodology to compute a saddle point of the non-lin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16610</link><description>&lt;p&gt;
&#23398;&#20064;&#21333;&#35843;&#21338;&#24328;&#30340;&#25237;&#30707;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31639;&#27861;&#21363;&#20351;&#22312;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#20063;&#26080;&#27861;&#25910;&#25947;&#21040;&#22343;&#34913;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20048;&#35266;&#29256;&#26412;&#24182;&#20855;&#26377;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26080;&#22122;&#22768;&#30340;&#26799;&#24230;&#21453;&#39304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25200;&#21160;&#25110;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#12290;&#36825;&#31181;&#25200;&#21160;&#26377;&#21161;&#20110;&#23558;&#24403;&#21069;&#31574;&#30053;&#25289;&#21521;&#19968;&#20010;&#38170;&#23450;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25237;&#30707;&#32034;&#8221;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#38752;&#36817;&#22343;&#34913;&#28857;&#30340;&#31283;&#23450;&#28857;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23450;&#26399;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#35299;&#37322;&#20026;&#36817;&#31471;p
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06989</link><description>&lt;p&gt;
&#36229;&#27969;&#20307;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Wave Functions for Superfluids. (arXiv:2305.06989v1 [cond-mat.quant-gas])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36229;&#27969;&#24615;&#20173;&#28982;&#26159;&#20957;&#32858;&#24577;&#29289;&#29702;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#65288;FermiNet&#65289;&#27874;&#20989;&#25968;Ansatz&#36827;&#34892;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#24378;&#28872;&#30701;&#31243;&#21452;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;-- &#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#65292;&#35813;&#31995;&#32479;&#24050;&#30693;&#23384;&#22312;&#36229;&#27969;&#22522;&#24577;&#65292;&#20294;&#38590;&#20197;&#23450;&#37327;&#25551;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30740;&#31350;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#26102;FermiNet Ansatz&#30340;&#20851;&#38190;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20854;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;FermiNet&#65292;&#21487;&#20197;&#32473;&#20986;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25968;&#23398;&#35777;&#26126;&#20102;&#26032;&#30340;Ansatz&#26159;&#21407;&#22987;FermiNet&#20307;&#31995;&#32467;&#26500;&#30340;&#20005;&#26684;&#27010;&#25324;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;FermiNet&#20849;&#20139;&#20960;&#20010;&#20248;&#21183;:&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#28040;&#38500;&#20102;&#24213;&#23618;&#22522;&#32452;&#30340;&#38656;&#27714;;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#22312;&#21464;&#20998;&#37327;&#23376;Monte Carlo&#20013;&#20135;&#29983;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding superfluidity remains a major goal of condensed matter physics. Here we tackle this challenge utilizing the recently developed Fermionic neural network (FermiNet) wave function Ansatz for variational Monte Carlo calculations. We study the unitary Fermi gas, a system with strong, short-range, two-body interactions known to possess a superfluid ground state but difficult to describe quantitively. We demonstrate key limitations of the FermiNet Ansatz in studying the unitary Fermi gas and propose a simple modification that outperforms the original FermiNet significantly, giving highly accurate results. We prove mathematically that the new Ansatz is a strict generalization of the original FermiNet architecture, despite the use of fewer parameters. Our approach shares several advantanges with the FermiNet: the use of a neural network removes the need for an underlying basis set; and the flexiblity of the network yields extremely accurate results within a variational quantum Mon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.01377</link><description>&lt;p&gt;
&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Function Descent. (arXiv:2305.01377v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21313;&#20998;&#24120;&#35265;&#65292;&#20294;&#26159;&#36873;&#25321;&#27491;&#30830;&#30340;&#27493;&#38271;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#8220;&#36229;&#21442;&#25968;&#35843;&#25972;&#8221;&#12290;&#36825;&#26159;&#22240;&#20026;&#22238;&#28335;&#31243;&#24207;&#22914;Armijo's&#20934;&#21017;&#20381;&#36182;&#20110;&#27599;&#20010;&#27493;&#39588;&#20013;&#30340;&#36136;&#37327;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#35780;&#20272;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#30001;&#20110;&#20248;&#21270;&#26041;&#26696;&#21487;&#20197;&#29992;Taylor&#36924;&#36817;&#26469;&#35299;&#37322;&#65292;&#25105;&#20204;&#23558;Taylor&#36924;&#36817;&#26367;&#25442;&#20026;&#26465;&#20214;&#26399;&#26395;&#65288;&#26368;&#20339;&#30340;$L^2$&#20272;&#35745;&#65289;&#65292;&#25552;&#20986;&#20102;&#8220;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#8221;&#65288;RFD&#65289;&#12290; &#22312;Bayesian&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#19968;&#20123;&#36731;&#24494;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RFD&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26159;&#30456;&#21516;&#30340;&#65292;&#20294;&#26159;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#31639;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25193;&#23637;&#65292;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires "hyperparameter tuning". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose "Random Function Descent" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.
&lt;/p&gt;</description></item><item><title>Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.01206</link><description>&lt;p&gt;
Chronosymbolic Learning: &#32467;&#21512;&#31526;&#21495;&#25512;&#29702;&#19982;&#24402;&#32435;&#23398;&#20064;&#30340;&#26377;&#25928;CHC&#27714;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning. (arXiv:2305.01206v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01206
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CHC (Constrained Horn Clauses)&#30340;&#27714;&#35299;&#26159;&#35768;&#22810;&#39564;&#35777;&#21644;&#20998;&#26512;&#20219;&#21153;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#27861;&#22312;&#25552;&#39640;CHC&#27714;&#35299;&#25928;&#29575;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#21644;&#35843;&#25972;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#32321;&#29712;&#24037;&#20316;&#12290;&#20294;&#25968;&#25454;&#39537;&#21160;&#30340;CHC&#27714;&#35299;&#22120;&#19982;&#22522;&#20110;&#31526;&#21495;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;"Chronosymbolic Learning"&#65292;&#23427;&#23558;&#31526;&#21495;&#20449;&#24687;&#21644;&#25968;&#20540;&#25968;&#25454;&#28857;&#32479;&#19968;&#36215;&#26469;&#65292;&#23558;CHC&#31995;&#32479;&#39640;&#25928;&#22320;&#27714;&#35299;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Chronosymbolic Learning&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#22120;&#21644;&#19968;&#20010;BMC&#26679;&#24335;&#30340;&#25512;&#29702;&#22120;&#12290;&#23613;&#31649;&#35813;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#21147;&#21644;&#20581;&#22766;&#24615;&#12290;&#23427;&#22312;&#30001;288&#20010;&#22522;&#20934;&#27979;&#35797;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CHC&#27714;&#35299;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#21253;&#21547;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving Constrained Horn Clauses (CHCs) is a fundamental challenge behind a wide range of verification and analysis tasks. Data-driven approaches show great promise in improving CHC solving without the painstaking manual effort of creating and tuning various heuristics. However, a large performance gap exists between data-driven CHC solvers and symbolic reasoning-based solvers. In this work, we develop a simple but effective framework, "Chronosymbolic Learning", which unifies symbolic information and numerical data points to solve a CHC system efficiently. We also present a simple instance of Chronosymbolic Learning with a data-driven learner and a BMC-styled reasoner. Despite its great simplicity, experimental results show the efficacy and robustness of our tool. It outperforms state-of-the-art CHC solvers on a dataset consisting of 288 benchmarks, including many instances with non-linear integer arithmetics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26080;&#30417;&#30563;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#30340;&#35745;&#31639;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#26631;&#26469;&#35780;&#20272;&#28418;&#31227;&#26816;&#27979;&#22120;&#23545;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.08319</link><description>&lt;p&gt;
&#32771;&#23519;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#30340;&#35745;&#31639;&#24615;&#33021;: &#19968;&#39033;&#35843;&#26597;&#21644;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Examining Computational Performance of Unsupervised Concept Drift Detection: A Survey and Beyond. (arXiv:2304.08319v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26080;&#30417;&#30563;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#30340;&#35745;&#31639;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#26631;&#26469;&#35780;&#20272;&#28418;&#31227;&#26816;&#27979;&#22120;&#23545;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#20123;&#31995;&#32479;&#24448;&#24448;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#30340;&#25968;&#25454;&#25110;&#23454;&#26102;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#28418;&#31227;&#26816;&#27979;&#22120;&#24517;&#39035;&#28385;&#36275;&#35745;&#31639;&#35201;&#27714;&#25110;&#32422;&#26463;&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24320;&#21457;&#28418;&#31227;&#26816;&#27979;&#22120;&#30340;&#37325;&#28857;&#26159;&#26816;&#27979;&#36136;&#37327;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#24615;&#33021;&#65292;&#22914;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#23558;&#35745;&#31639;&#24615;&#33021;&#35270;&#20026;&#27425;&#35201;&#30446;&#26631;&#65292;&#24182;&#27809;&#26377;&#38024;&#23545;&#36825;&#26679;&#30340;&#35780;&#20272;&#25552;&#20986;&#22522;&#20934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#26631;&#65292;&#26082;&#32771;&#34385;&#35745;&#31639;&#24615;&#33021;&#21448;&#32771;&#34385;&#26816;&#27979;&#36136;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#30340;&#25351;&#26631;&#38598;&#21253;&#25324;&#30456;&#23545;&#36816;&#34892;&#26102;&#38388;&#24320;&#38144;&#65288;RRO&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#28418;&#31227;&#26816;&#27979;&#22120;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35745;&#31639;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#19987;&#27880;&#20110;&#26080;&#30417;&#30563;&#30340;&#28418;&#31227;&#26816;&#27979;&#22120;&#65292;&#19981;&#21463;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#38480;&#21046;&#12290;&#25105;&#20204;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
Concept drift detection is crucial for many AI systems to ensure the system's reliability. These systems often have to deal with large amounts of data or react in real time. Thus, drift detectors must meet computational requirements or constraints with a comprehensive performance evaluation. However, so far, the focus of developing drift detectors is on detection quality, e.g.~accuracy, but not on computational performance, such as running time. We show that the previous works consider computational performance only as a secondary objective and do not have a benchmark for such evaluation. Hence, we propose a set of metrics that considers both, computational performance and detection quality. Among others, our set of metrics includes the Relative Runtime Overhead RRO to evaluate a drift detector's computational impact on an AI system. This work focuses on unsupervised drift detectors, not being restricted to the availability of labeled data. We measure the computational performance base
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22686;&#21152;&#37197;&#38899;&#26469;&#22686;&#24378;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#21644;&#35270;&#21548;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05600</link><description>&lt;p&gt;
&#30456;&#35980;&#30456;&#20284;&#65292;&#22768;&#38899;&#19981;&#21516;&#65306;&#21033;&#29992;&#21453;&#20107;&#23454;&#36328;&#27169;&#24577;&#23545;&#23398;&#20064;&#38899;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning. (arXiv:2304.05600v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22686;&#21152;&#37197;&#38899;&#26469;&#22686;&#24378;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#21644;&#35270;&#21548;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#36890;&#24120;&#20381;&#36182;&#20110;&#35270;&#21548;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#35270;&#35273;&#22330;&#26223;&#20013;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#22768;&#38899;&#36712;&#36947;&#19982;&#20043;&#23545;&#24212;&#12290;&#20363;&#22914;&#65292;&#22312;&#21516;&#19968;&#25317;&#25380;&#30340;&#34903;&#36947;&#19978;&#26377;&#19981;&#21516;&#30340;&#20132;&#35848;&#22768;&#12290;&#36825;&#20123;&#21453;&#20107;&#23454;&#23545;&#20110;&#38899;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#23578;&#26410;&#30740;&#31350;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30005;&#24433;&#30340;&#37197;&#38899;&#29256;&#26412;&#26469;&#22686;&#21152;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#31867;&#20284;&#20110;&#30456;&#21516;&#35270;&#39057;&#30340;&#20165;&#22312;&#35821;&#38899;&#20869;&#23481;&#19978;&#19981;&#21516;&#30340;&#26367;&#20195;&#38899;&#36712;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#37197;&#38899;&#30340;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#21548;&#35273;&#21644;&#35270;&#21548;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#32780;&#23545;&#35821;&#35328;&#20219;&#21153;&#30340;&#25972;&#20307;&#34920;&#29616;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#22312;&#39044;&#35757;&#32451;&#20043;&#21069;&#21435;&#38500;&#35821;&#38899;&#30340;&#24378;&#22823;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22686;&#21152;&#37197;&#38899;&#30340;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#21253;&#25324;&#35821;&#38899;&#22806;&#35821;&#21644;&#35270;&#21548;&#20219;&#21153;&#65292;&#20854;&#20013;&#35821;&#38899;&#24674;&#22797;&#20219;&#21153;&#24615;&#33021;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audiovisual representation learning typically relies on the correspondence between sight and sound. However, there are often multiple audio tracks that can correspond with a visual scene. Consider, for example, different conversations on the same crowded street. The effect of such counterfactual pairs on audiovisual representation learning has not been previously explored. To investigate this, we use dubbed versions of movies to augment cross-modal contrastive learning. Our approach learns to represent alternate audio tracks, differing only in speech content, similarly to the same video. Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall. We additionally compare this approach to a strong baseline where we remove speech before pretraining, and find that dub-augmented training is more effective, including for paralinguistic and audiovisual tasks where speech re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20559;&#22909;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.05366</link><description>&lt;p&gt;
&#12298;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#12289;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24615;&#21450;&#24402;&#32435;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning. (arXiv:2304.05366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20559;&#22909;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#25351;&#20986;&#65292;&#27809;&#26377;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#65292;&#25110;&#32773;&#25152;&#26377;&#23398;&#20064;&#31639;&#27861;&#22312;&#22343;&#21248;&#20998;&#24067;&#30340;&#23398;&#20064;&#38382;&#39064;&#19978;&#24179;&#22343;&#31934;&#24230;&#36798;&#21040;&#23436;&#20840;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23450;&#29702;&#32463;&#24120;&#34987;&#24341;&#29992;&#26469;&#25903;&#25345;&#20010;&#21035;&#38382;&#39064;&#38656;&#35201;&#29305;&#21035;&#23450;&#21046;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#20960;&#20046;&#25152;&#26377;&#22343;&#21248;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#39640;&#22797;&#26434;&#24615;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#19981;&#25104;&#27604;&#20363;&#22320;&#20135;&#29983;&#20302;&#22797;&#26434;&#24230;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20063;&#20855;&#26377;&#21516;&#26679;&#30340;&#20559;&#22909;&#65292;&#36825;&#31181;&#20559;&#22909;&#20351;&#29992;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#29305;&#23450;&#39046;&#22495;&#35774;&#35745;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#21487;&#20197;&#21387;&#32553;&#21508;&#31181;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#39044;&#20808;&#35757;&#32451;&#21644;&#21363;&#20351;&#26159;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26356;&#21916;&#27426;&#29983;&#25104;&#20302;&#22797;&#26434;&#24230;&#30340;&#24207;&#21015;&#12290;&#23613;&#31649;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#20284;&#20046;&#34920;&#26126;&#21508;&#20010;&#38382;&#39064;&#38656;&#35201;&#19987;&#38376;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35299;&#37322;&#35828;&#65292;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#32534;&#30721;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we exp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.02049</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-Class Explainable Unlearning for Image Classification via Weight Filtering. (arXiv:2304.02049v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21368;&#36733;&#26159;&#26368;&#36817;&#28014;&#29616;&#30340;&#19968;&#31181;&#36873;&#25321;&#24615;&#22320;&#23558;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#38598;&#20013;&#22312;&#21368;&#36733;&#35757;&#32451;&#25968;&#25454;&#30340;&#23567;&#23376;&#38598;&#25110;&#21333;&#20010;&#31867;&#21035;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20869;&#37096;&#32452;&#20214;&#30340;&#35760;&#24518;&#30697;&#38453;&#26469;&#35843;&#33410;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#35757;&#32451;&#21518;&#65292;&#21516;&#19968;&#32593;&#32476;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#23637;&#31034;&#20219;&#20309;&#31867;&#21035;&#30340;&#26410;&#23398;&#20064;&#34892;&#20026;&#12290;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#31867;&#21035;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#35774;&#35745;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#26469;&#24674;&#22797;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#21644;&#20013;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;CNN&#21644;Transformer-based&#39592;&#26550;&#27979;&#35797;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning has recently been emerging as a paradigm for selectively removing the impact of training datapoints from a network. While existing approaches have focused on unlearning either a small subset of the training data or a single class, in this paper we take a different path and devise a framework that can unlearn all classes of an image classification network in a single untraining round. Our proposed technique learns to modulate the inner components of an image classification network through memory matrices so that, after training, the same network can selectively exhibit an unlearning behavior over any of the classes. By discovering weights which are specific to each of the classes, our approach also recovers a representation of the classes which is explainable by-design. We test the proposed framework, which we name Weight Filtering network (WF-Net), on small-scale and medium-scale image classification datasets, with both CNN and Transformer-based backbones. Our work p
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;</title><link>http://arxiv.org/abs/2304.01910</link><description>&lt;p&gt;
&#26657;&#20934;&#28151;&#20081;&#65306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#26080;&#24847;&#20013;&#19988;&#26080;&#23475;
&lt;/p&gt;
&lt;p&gt;
Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable. (arXiv:2304.01910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01910
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22312;&#37325;&#22797;&#27979;&#35797;&#26102;&#20250;&#26377;&#26174;&#33879;&#30340;&#27979;&#35797;&#38598;&#24615;&#33021;&#24046;&#24322;&#65292;&#24433;&#21709;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#27604;&#36739;&#21644;&#35757;&#32451;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#27604;&#36739;&#26469;&#35299;&#37322;&#36825;&#31181;&#21464;&#21270;&#65306;&#65288;1&#65289;&#23613;&#31649;&#22312;&#27979;&#35797;&#38598;&#19978;&#26377;&#26174;&#33879;&#30340;&#26041;&#24046;&#65292;&#20294;&#26631;&#20934;&#30340;CIFAR-10&#19982;ImageNet&#35757;&#32451;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#34920;&#29616;&#21364;&#38750;&#24120;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#26041;&#24046;&#19981;&#20687;&#20043;&#21069;&#24819;&#35937;&#30340;&#37027;&#20040;&#20005;&#37325;&#12290;&#65288;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#20197;&#32039;&#23494;&#36817;&#20284;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#20998;&#24067;&#32467;&#26500;&#12290;&#65288;3&#65289;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20197;&#19979;&#20004;&#20010;&#24847;&#20041;&#19978;&#65292;&#27979;&#35797;&#38598;&#26041;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#38543;&#26426;&#28304;&#65288;&#22914;&#25968;&#25454;&#25490;&#24207;&#21644;&#25193;&#20805;&#65289;&#25152;&#23548;&#33268;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26041;&#24046;&#26159;&#39057;&#29575;&#26497;&#38480;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#26469;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical neural network trainings have substantial variance in test-set performance between repeated runs, impeding hyperparameter comparison and training reproducibility. We present the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have very little variance in their performance on the test-distributions from which those test-sets are sampled, suggesting that variance is less of a practical issue than previously thought. (2) We present a simplifying statistical assumption which closely approximates the structure of the test-set accuracy distribution. (3) We argue that test-set variance is inevitable in the following two senses. First, we show that variance is largely caused by high sensitivity of the training process to initial conditions, rather than by specific sources of randomness like the data order and augmentations. Second, we prove that variance is u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PAC-Bayesian bound&#20316;&#20026;Soft Actor-Critic (SAC)&#31639;&#27861;&#35780;&#35770;&#23478;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#35770;&#23478;&#24341;&#23548;&#30340;&#38543;&#26426;&#25628;&#32034;&#25506;&#32034;&#22810;&#20010;&#26410;&#26469;&#26469;&#25552;&#39640;&#22312;&#32447;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#32463;&#20856;&#25511;&#21046;&#21644;&#36816;&#21160;&#20219;&#21153;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.12776</link><description>&lt;p&gt;
PAC-Bayesian&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Soft Actor-Critic Learning. (arXiv:2301.12776v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PAC-Bayesian bound&#20316;&#20026;Soft Actor-Critic (SAC)&#31639;&#27861;&#35780;&#35770;&#23478;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#35770;&#23478;&#24341;&#23548;&#30340;&#38543;&#26426;&#25628;&#32034;&#25506;&#32034;&#22810;&#20010;&#26410;&#26469;&#26469;&#25552;&#39640;&#22312;&#32447;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#32463;&#20856;&#25511;&#21046;&#21644;&#36816;&#21160;&#20219;&#21153;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36890;&#36807;&#20004;&#20010;&#20998;&#21035;&#20316;&#31574;&#30053;&#35780;&#20272;&#21644;&#25913;&#36827;&#30340;&#21151;&#33021;&#36924;&#36817;&#22120;&#26469;&#35299;&#20915;&#22686;&#24378;&#23398;&#20064;(RL)&#30340;&#21452;&#37325;&#30446;&#26631;&#12290;&#27492;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#26159;&#20197;&#35757;&#32451;&#19981;&#31283;&#23450;&#20026;&#20195;&#20215;&#30340;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#35780;&#35770;&#23478;&#36924;&#36817;&#35823;&#24046;&#23545;&#28436;&#21592;&#30340;&#30772;&#22351;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#27425;&#37319;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;(PAC)Bayesian&#30028;&#38480;&#20316;&#20026;Soft Actor-Critic (SAC)&#31639;&#27861;&#30340;&#35780;&#35770;&#23478;&#35757;&#32451;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24403;&#38543;&#26426;&#28436;&#21592;&#36890;&#36807;&#35780;&#35770;&#23478;&#24341;&#23548;&#30340;&#38543;&#26426;&#25628;&#32034;&#25506;&#32034;&#22810;&#20010;&#26410;&#26469;&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#32463;&#20856;&#25511;&#21046;&#21644;&#36816;&#21160;&#20219;&#21153;&#20013;&#65292;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-critic algorithms address the dual goals of reinforcement learning (RL), policy evaluation and improvement, via two separate function approximators. The practicality of this approach comes at the expense of training instability, caused mainly by the destructive effect of the approximation errors of the critic on the actor. We tackle this bottleneck by employing an existing Probably Approximately Correct (PAC) Bayesian bound for the first time as the critic training objective of the Soft Actor-Critic (SAC) algorithm. We further demonstrate that online learning performance improves significantly when a stochastic actor explores multiple futures by critic-guided random search. We observe our resulting algorithm to compare favorably to the state of the art on multiple classical control and locomotion tasks in terms of both sample efficiency and regret minimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRC&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12379</link><description>&lt;p&gt;
FedRC&#65306;&#36890;&#36807;&#40065;&#26834;&#32858;&#31867;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering. (arXiv:2301.12379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRC&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20445;&#30041;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#65292;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24403;&#23458;&#25143;&#31471;&#20043;&#38388;&#20986;&#29616;&#20998;&#24067;&#36716;&#31227;&#26102;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#65292;&#20294;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#21516;&#26102;&#21457;&#29983;&#22810;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#65292;&#20363;&#22914;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#12289;&#26631;&#31614;&#20998;&#24067;&#36716;&#31227;&#21644;&#27010;&#24565;&#36716;&#31227;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#20840;&#23616;&#24615;&#33021;&#20173;&#28982;&#26159;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#26679;&#20998;&#24067;&#36716;&#31227;&#21516;&#26102;&#21457;&#29983;&#26102;&#25152;&#24102;&#26469;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#21407;&#21017;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#32858;&#31867;&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#8212;&#8212;FedRC&#65292;&#23427;&#36981;&#24490;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#31867;&#21407;&#21017;&#65292;&#36890;&#36807;&#21253;&#21547;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that safeguards privacy by retaining client data on edge devices. However, optimizing FL in practice can be challenging due to the diverse and heterogeneous nature of the learning system. Though recent research has focused on improving the optimization of FL when distribution shifts occur among clients, ensuring global performance when multiple types of distribution shifts occur simultaneously among clients -- such as feature distribution shift, label distribution shift, and concept shift -- remain under-explored.  In this paper, we identify the learning challenges posed by the simultaneous occurrence of diverse distribution shifts and propose a clustering principle to overcome these challenges. Through our research, we find that existing methods failed to address the clustering principle. Therefore, we propose a novel clustering algorithm framework, dubbed as FedRC, which adheres to our proposed clustering principle by incorporati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;ISAACS&#65292;&#36890;&#36807;&#23558;&#21338;&#24328;&#35770;&#23433;&#20840;&#20998;&#26512;&#19982;&#23545;&#25239;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#36991;&#20813;&#30896;&#25758;&#65292;&#24182;&#22312;&#23433;&#20840;&#38382;&#39064;&#19978;&#36229;&#36234;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.03228</link><description>&lt;p&gt;
ISAACS&#65306;&#23433;&#20840;&#30340;&#36845;&#20195;&#36719;&#23545;&#25239; Actor-Critic &#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ISAACS: Iterative Soft Adversarial Actor-Critic for Safety. (arXiv:2212.03228v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;ISAACS&#65292;&#36890;&#36807;&#23558;&#21338;&#24328;&#35770;&#23433;&#20840;&#20998;&#26512;&#19982;&#23545;&#25239;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#36991;&#20813;&#30896;&#25758;&#65292;&#24182;&#22312;&#23433;&#20840;&#38382;&#39064;&#19978;&#36229;&#36234;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#38656;&#35201;&#23427;&#20204;&#33021;&#22815;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#19979;&#31283;&#20581;&#22320;&#36816;&#34892;&#65292;&#20363;&#22914;&#19981;&#35268;&#21017;&#30340;&#22320;&#24418;&#21644;&#39118;&#21147;&#26465;&#20214;&#12290;&#30001;&#20110;&#20248;&#21270;&#25511;&#21046;&#29702;&#35770;&#30340;&#20005;&#26684;&#23433;&#20840;&#26694;&#26550;&#38590;&#20197;&#25193;&#23637;&#21040;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#32780;&#26356;&#26131;&#22788;&#29702;&#30340;&#8220;&#28145;&#24230;&#8221;&#26041;&#27861;&#35745;&#31639;&#20986;&#30340;&#25511;&#21046;&#31574;&#30053;&#32570;&#20047;&#20445;&#35777;&#65292;&#24182;&#19988;&#24448;&#24448;&#22312;&#19981;&#30830;&#23450;&#30340;&#25805;&#20316;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#24456;&#23569;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21338;&#24328;&#35770;&#23433;&#20840;&#20998;&#26512;&#19982;&#20223;&#30495;&#20013;&#30340;&#23545;&#25239;&#24335;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20351;&#20855;&#26377;&#19968;&#33324;&#38750;&#32447;&#24615;&#21160;&#24577;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;&#22120;&#30340;&#32508;&#21512;&#21512;&#25104;&#65292;&#21463;&#21040;&#26377;&#30028;&#24314;&#27169;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#37319;&#29992;&#36719;&#24615; actor-critic &#26041;&#27861;&#65292;&#21516;&#26102;&#36827;&#34892;&#19968;&#20010;&#23433;&#20840;&#30340;&#22238;&#36864;&#31574;&#30053;&#21644;&#19968;&#20010;&#31216;&#20026;&#8220;&#24178;&#25200;&#8221;&#30340;&#23545;&#25239;&#26234;&#33021;&#20307;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#35813;&#23545;&#25239;&#26234;&#33021;&#20307;&#33268;&#21147;&#20110;&#21796;&#36215;&#35774;&#35745;&#32773;&#19981;&#30830;&#23450;&#24615;&#20551;&#35774;&#19979;&#20801;&#35768;&#30340;&#27169;&#22411;&#35823;&#24046;&#21644;&#35757;&#32451;-&#37096;&#32626;&#20559;&#24046;&#30340;&#26368;&#22351;&#24773;&#20917;&#23454;&#29616;&#12290;&#21516;&#26102;&#65292;&#19968;&#20010;&#35780;&#20215;&#32593;&#32476;&#34987;&#35757;&#32451;&#26469;&#35780;&#20272;&#20027;&#31574;&#30053;&#21644;&#22238;&#36864;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20379;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#36719;&#32422;&#26463;&#26469;&#25351;&#23548;&#25506;&#32034;&#21644;&#38480;&#21046;&#19981;&#33391;&#34892;&#20026;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36991;&#20813;&#30896;&#25758;&#30340;&#31574;&#30053;&#65292;&#24182;&#36229;&#36234;&#20102;&#26631;&#20934;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#23433;&#20840;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21363;&#20351;&#23384;&#22312;&#26174;&#33879;&#30340;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of robots in uncontrolled environments requires them to operate robustly under previously unseen scenarios, like irregular terrain and wind conditions. Unfortunately, while rigorous safety frameworks from robust optimal control theory scale poorly to high-dimensional nonlinear dynamics, control policies computed by more tractable "deep" methods lack guarantees and tend to exhibit little robustness to uncertain operating conditions. This work introduces a novel approach enabling scalable synthesis of robust safety-preserving controllers for robotic systems with general nonlinear dynamics subject to bounded modeling error by combining game-theoretic safety analysis with adversarial reinforcement learning in simulation. Following a soft actor-critic scheme, a safety-seeking fallback policy is co-trained with an adversarial "disturbance" agent that aims to invoke the worst-case realization of model error and training-to-deployment discrepancy allowed by the designer's uncert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10890</link><description>&lt;p&gt;
&#32531;&#35299;&#20998;&#23618;&#27880;&#24847;&#21147;&#22810;&#23610;&#24230;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating spectral bias for the multiscale operator learning with hierarchical attention. (arXiv:2210.10890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26080;&#38480;&#32500;&#21442;&#25968;&#21644;&#35299;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#30340;&#22810;&#23610;&#24230;PDE&#65292;&#22914;&#27833;&#34255;&#24314;&#27169;&#21644;&#28237;&#27969;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#36825;&#31181;PDE&#65292;&#23545;&#20302;&#39057;&#20998;&#37327;&#23384;&#22312;&#20809;&#35889;&#20559;&#24046;&#26159;&#29616;&#26377;&#31070;&#32463;&#31639;&#23376;&#30340;&#19968;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#23618;&#27425;&#30697;&#38453;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#12290;HANO&#20855;&#26377;&#33258;&#36866;&#24212;&#23610;&#24230;&#20132;&#20114;&#33539;&#22260;&#21644;&#23618;&#27425;&#32467;&#26500;&#19978;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#21487;&#25511;&#32447;&#24615;&#25104;&#26412;&#30340;&#23884;&#22871;&#29305;&#24449;&#35745;&#31639;&#21644;&#22810;&#23610;&#24230;&#35299;&#31354;&#38388;&#30340;&#32534;&#30721;/&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#32463;&#39564;H^1&#25439;&#22833;&#20989;&#25968;&#26469;&#22686;&#24378;&#23545;&#39640;&#39057;&#20998;&#37327;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;HANO&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have emerged as a powerful tool for learning the mapping between infinite-dimensional parameter and solution spaces of partial differential equations (PDEs). In this work, we focus on multiscale PDEs that have important applications such as reservoir modeling and turbulence prediction. We demonstrate that for such PDEs, the spectral bias towards low-frequency components presents a significant challenge for existing neural operators. To address this challenge, we propose a hierarchical attention neural operator (HANO) inspired by the hierarchical matrix approach. HANO features a scale-adaptive interaction range and self-attentions over a hierarchy of levels, enabling nested feature computation with controllable linear cost and encoding/decoding of multiscale solution space. We also incorporate an empirical $H^1$ loss function to enhance the learning of high-frequency components. Our numerical experiments demonstrate that HANO outperforms state-of-the-art (SOTA) methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#20013;&#20351;&#29992;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#20540;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#12289;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#26435;&#37325;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#26469;&#26500;&#24314;&#32039;&#20945;&#30340;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.03450</link><description>&lt;p&gt;
&#22312;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20013;&#23547;&#27714;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Seeking Interpretability and Explainability in Binary Activated Neural Networks. (arXiv:2209.03450v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#20013;&#20351;&#29992;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#20540;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#12289;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#26435;&#37325;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#26469;&#26500;&#24314;&#32039;&#20945;&#30340;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#23558;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#30340;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#35745;&#31639;SHAP&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#29305;&#24449;&#12289;&#38544;&#34255;&#31070;&#32463;&#20803;&#29978;&#33267;&#26435;&#37325;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#22312;&#23454;&#29616;&#35299;&#37322;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#32039;&#20945;&#30340;&#20108;&#36827;&#21046;&#28608;&#27963;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#35774;&#23450;&#32593;&#32476;&#30340;&#26550;&#26500;&#65306;&#23427;&#36880;&#23618;&#12289;&#36880;&#20010;&#31070;&#32463;&#20803;&#22320;&#26500;&#24314;&#65292;&#20351;&#24471;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#65292;&#39044;&#27979;&#22120;&#19981;&#20250;&#36807;&#20110;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the use of binary activated neural networks as interpretable and explainable predictors in the context of regression tasks on tabular data; more specifically, we provide guarantees on their expressiveness, present an approach based on the efficient computation of SHAP values for quantifying the relative importance of the features, hidden neurons and even weights. As the model's simplicity is instrumental in achieving interpretability, we propose a greedy algorithm for building compact binary activated networks. This approach doesn't need to fix an architecture for the network in advance: it is built one layer at a time, one neuron at a time, leading to predictors that aren't needlessly complex for a given task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;&#65292;&#35813;&#35268;&#21017;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#26799;&#24230;&#26469;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#24191;&#27867;&#30340;&#31639;&#27861;&#24212;&#29992;&#12290;&#36825;&#19968;&#24037;&#20316;&#19981;&#20165;&#32479;&#19968;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#36824;&#24110;&#21161;&#25105;&#20204;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.04562</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
The Bayesian Learning Rule. (arXiv:2107.04562v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.04562
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;&#65292;&#35813;&#35268;&#21017;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#26799;&#24230;&#26469;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#24191;&#27867;&#30340;&#31639;&#27861;&#24212;&#29992;&#12290;&#36825;&#19968;&#24037;&#20316;&#19981;&#20165;&#32479;&#19968;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#36824;&#24110;&#21161;&#25105;&#20204;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#20010;&#31216;&#20026;&#36125;&#21494;&#26031;&#23398;&#20064;&#35268;&#21017;&#30340;&#21333;&#19968;&#31639;&#27861;&#30340;&#29305;&#20363;&#12290;&#36825;&#20010;&#35268;&#21017;&#26159;&#20174;&#36125;&#21494;&#26031;&#21407;&#29702;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#21487;&#20197;&#20174;&#20248;&#21270;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#24418;&#27169;&#22411;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#30340;&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#32463;&#20856;&#31639;&#27861;&#22914;&#23725;&#22238;&#24402;&#12289;&#29275;&#39039;&#27861;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#20197;&#21450;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;RMSprop&#21644;Dropout&#12290;&#25512;&#23548;&#36825;&#20123;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#20272;&#35745;&#30340;&#20505;&#36873;&#20998;&#24067;&#26469;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#12290;&#19981;&#21516;&#30340;&#20505;&#36873;&#20998;&#24067;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#23545;&#33258;&#28982;&#26799;&#24230;&#30340;&#36827;&#19968;&#27493;&#36924;&#36817;&#21017;&#20250;&#20135;&#29983;&#36825;&#20123;&#31639;&#27861;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#32479;&#19968;&#12289;&#27867;&#21270;&#21644;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#36824;&#24110;&#21161;&#25105;&#20204;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.
&lt;/p&gt;</description></item></channel></rss>