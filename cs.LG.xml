<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.01306</link><description>&lt;p&gt;
NeuroPrune&#65306;&#19968;&#31181;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25299;&#25169;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#30340;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#26114;&#36149;&#30340;&#35757;&#32451;&#20197;&#21450;&#25512;&#29702;&#20173;&#28982;&#26159;&#23427;&#20204;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#27169;&#22411;&#26550;&#26500;&#30340;&#21508;&#20010;&#23618;&#27425;&#24378;&#21046;&#24341;&#20837;&#31232;&#30095;&#24615;&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#35299;&#20915;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#20294;&#31232;&#30095;&#24615;&#23545;&#32593;&#32476;&#25299;&#25169;&#30340;&#24433;&#21709;&#20173;&#23384;&#22312;&#26029;&#35010;&#12290;&#21463;&#22823;&#33041;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#25299;&#25169;&#30340;&#35270;&#35282;&#25506;&#32034;&#31232;&#30095;&#24615;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#26426;&#21046;&#65292;&#22914;&#20248;&#20808;&#38468;&#30528;&#21644;&#20887;&#20313;&#31361;&#35302;&#20462;&#21098;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#31232;&#30095;&#24615;&#26041;&#27861;&#22312;&#36328;&#36234;&#20998;&#31867;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#21644;&#29983;&#25104;&#65288;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#39640;&#25928;&#65292;&#23613;&#31649; o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01306v1 Announce Type: cross  Abstract: Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00474</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Linguistic Calibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#33258;&#20449;&#24187;&#35273;&#26102;&#23548;&#33268;&#29992;&#25143;&#20570;&#20986;&#27425;&#20248;&#21270;&#30340;&#19979;&#28216;&#20915;&#31574;&#12290;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21475;&#22836;&#20256;&#36798;&#20854;&#20027;&#24352;&#27491;&#30830;&#27010;&#29575;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20855;&#26377;&#26657;&#20934;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20915;&#31574;&#35282;&#24230;&#65292;&#20026;&#38271;&#31687;&#29983;&#25104;&#24418;&#24335;&#30340;&#35821;&#35328;&#26657;&#20934;&#24418;&#24335;&#21270;&#23450;&#20041;&#65306;&#22914;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20351;&#20854;&#29992;&#25143;&#33021;&#22815;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#65292;&#21017;&#35813;&#27169;&#22411;&#26159;&#35821;&#35328;&#19978;&#26657;&#20934;&#30340;&#12290;&#36825;&#20010;&#23450;&#20041;&#20351;&#24471;&#19968;&#20010;&#35757;&#32451;&#26694;&#26550;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#27493;&#39588;&#24341;&#23548;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21457;&#20986;&#24102;&#26377;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#38271;&#31687;&#29983;&#25104;&#65292;&#35832;&#22914;&#8220;&#25105;&#20272;&#35745;&#26377;30%&#30340;&#26426;&#20250;&#8230;&#8221;&#25110;&#8220;&#25105;&#30830;&#20449;&#8230;&#8221;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#65292;&#22870;&#21169;&#20351;&#29992;&#25143;&#33021;&#22815;&#23545;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26657;&#20934;&#31572;&#26696;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23545;Llama 2 7B &#36827;&#34892;&#35821;&#35328;&#26657;&#20934;&#65292;&#24182;&#21457;&#29616;&#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#27979;&#35797;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
&lt;/p&gt;</description></item><item><title>MoNet&#26159;&#19968;&#31181;&#32467;&#21512;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#30693;&#24341;&#23548;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20219;&#21153;&#32423;&#21035;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#25552;&#39640;&#31471;&#21040;&#31471;&#25512;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#35270;&#35273;&#33258;&#20027;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.18947</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21487;&#35299;&#37322;&#30340;&#24863;&#30693;&#21160;&#20316;&#23398;&#20064;&#36890;&#36807;&#28508;&#22312;&#21151;&#33021;&#27169;&#22359;&#24615;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Interpretable Sensorimotor Learning via Latent Functional Modularity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18947
&lt;/p&gt;
&lt;p&gt;
MoNet&#26159;&#19968;&#31181;&#32467;&#21512;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#30693;&#24341;&#23548;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20219;&#21153;&#32423;&#21035;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#25552;&#39640;&#31471;&#21040;&#31471;&#25512;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#35270;&#35273;&#33258;&#20027;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MoNet&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#31471;&#21040;&#31471;&#23398;&#20064;&#19982;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#21644;&#21487;&#35299;&#37322;&#30340;&#24863;&#30693;&#21160;&#20316;&#23398;&#20064;&#12290;MoNet&#30001;&#19977;&#20010;&#21151;&#33021;&#19978;&#19981;&#21516;&#30340;&#31070;&#32463;&#27169;&#22359;&#32452;&#25104;&#65306;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;&#35748;&#30693;&#24341;&#23548;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;MoNet&#26377;&#25928;&#22320;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#21153;&#32423;&#21035;&#30340;&#30417;&#30563;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#34701;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#31471;&#21040;&#31471;&#25512;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#24863;&#30693;&#21160;&#20316;&#24615;&#33021;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23460;&#20869;&#29615;&#22659;&#20013;&#65292;MoNet&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#35270;&#35273;&#33258;&#20027;&#23548;&#33322;&#65292;&#22312;&#20219;&#21153;&#29305;&#24322;&#24615;&#20998;&#26512;&#20013;&#36229;&#36234;&#20102;&#22522;&#32447;&#27169;&#22411;11%&#33267;47%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#24863;&#30693;&#26174;&#33879;&#24615;&#22320;&#22270;&#30340;&#20107;&#21518;&#20998;&#26512;&#25506;&#35752;&#20102;&#25105;&#20204;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18947v1 Announce Type: new  Abstract: We introduce MoNet, a novel method that combines end-to-end learning with modular network architectures for self-supervised and interpretable sensorimotor learning. MoNet is composed of three functionally distinct neural modules: Perception, Planning, and Control. Leveraging its inherent modularity through a cognition-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space, without requiring task-level supervision. Moreover, our method incorporates an online post-hoc explainability approach, which enhances the interpretability of the end-to-end inferences without a trade-off in sensorimotor performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, surpassing baseline models by 11% to 47% in task specificity analysis. We further delve into the interpretability of our network through the post-hoc analysis of perceptual saliency maps 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#65292;&#25512;&#23548;&#20102;&#20854;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#36895;&#24230;&#22330;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18705</link><description>&lt;p&gt;
&#20855;&#26377;&#36125;&#21494;&#26031;OT&#27969;&#21305;&#37197;&#24212;&#29992;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#65292;&#25512;&#23548;&#20102;&#20854;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#36895;&#24230;&#22330;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#65292;&#35768;&#22810;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32852;&#21512;&#24230;&#37327;&#19982;&#20854;&#23398;&#20064;&#36924;&#36817;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#36817;&#20284;&#21518;&#39564;&#27979;&#24230;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;Kullback--Leibler&#20998;&#27495;&#30340;&#24773;&#20917;&#19979;&#20063;&#25511;&#21046;&#21518;&#39564;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#19968;&#33324;&#26469;&#35828;&#23545;&#20110;Wasserstein&#36317;&#31163;&#24182;&#19981;&#25104;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26465;&#20214;Wasserstein-1&#27969;&#30340;&#23545;&#20598;&#24418;&#24335;&#20197;&#19968;&#31181;&#38750;&#24120;&#33258;&#28982;&#30340;&#26041;&#24335;&#31867;&#20284;&#20110;&#26465;&#20214;Wasserstein GAN&#25991;&#29486;&#20013;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#34920;&#24449;&#30456;&#24212;&#30340;&#27979;&#22320;&#32447;&#21644;&#36895;&#24230;&#22330;&#20197;&#21450;&#27969;ODE&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25918;&#23485;&#26465;&#20214;Wasserstein&#36317;&#31163;&#26469;&#36817;&#20284;&#36895;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18705v1 Announce Type: new  Abstract: In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein dista
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.13086</link><description>&lt;p&gt;
&#21487;&#21548;&#22270;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Listenable Maps for Audio Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13086
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#22797;&#26434;&#24615;&#32473;&#35299;&#37322;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#38899;&#39057;&#20449;&#21495;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#20256;&#36798;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;&#30340;&#21487;&#21548;&#22270;&#65288;Listenable Maps for Audio Classifiers&#65292;L-MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#35299;&#37322;&#30340;&#21518;&#22788;&#29702;&#35299;&#37322;&#26041;&#27861;&#12290;L-MAC&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#20043;&#19978;&#30340;&#35299;&#30721;&#22120;&#29983;&#25104;&#20108;&#20540;&#25513;&#30721;&#65292;&#31361;&#20986;&#26174;&#31034;&#36755;&#20837;&#38899;&#39057;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#29992;&#19968;&#31181;&#29305;&#27530;&#25439;&#22833;&#26469;&#35757;&#32451;&#35299;&#30721;&#22120;&#65292;&#35813;&#25439;&#22833;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#23545;&#36755;&#20837;&#38899;&#39057;&#30340;&#25513;&#30721;&#37096;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#25513;&#30721;&#37096;&#20998;&#36755;&#20986;&#30340;&#27010;&#29575;&#12290;&#23545;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;L-MAC&#22987;&#32456;&#20135;&#29983;&#27604;&#20960;&#31181;&#26799;&#24230;&#21644;&#25513;&#30721;&#26041;&#27861;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13086v1 Announce Type: cross  Abstract: Despite the impressive performance of deep learning models across diverse tasks, their complexity poses challenges for interpretation. This challenge is particularly evident for audio signals, where conveying interpretations becomes inherently difficult. To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations. L-MAC utilizes a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. We train the decoder with a special loss that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and maskin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#24369;&#36890;&#20449;MDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#20026; $\tilde{O}(SA\frac{H}{\epsilon^2})$&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#26159;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#26368;&#23567;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.11477</link><description>&lt;p&gt;
&#24369;&#36890;&#20449;&#21644;&#19968;&#33324;&#24179;&#22343;&#22870;&#36175;MDPs&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#24369;&#36890;&#20449;MDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#20026; $\tilde{O}(SA\frac{H}{\epsilon^2})$&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#26159;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#26368;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#27169;&#22411;&#19979;&#23398;&#20064;&#24179;&#22343;&#22870;&#36175;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;$\epsilon$-&#26368;&#20339;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#24369;&#36890;&#20449;MDPs&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22797;&#26434;&#24230;&#30028;&#38480;&#20026;$\tilde{O}(SA\frac{H}{\epsilon^2})$&#65292;&#20854;&#20013;$H$&#26159;&#26368;&#20248;&#31574;&#30053;&#30340;&#20559;&#24046;&#20989;&#25968;&#30340;&#36328;&#24230;&#65292;$SA$&#26159;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22522;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#25152;&#26377;&#21442;&#25968;$S,A,H$&#21644;$\epsilon$&#19978;&#65288;&#26368;&#22810;&#23545;&#25968;&#22240;&#23376;&#65289;&#26368;&#23567;&#26368;&#20248;&#30340;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#29616;&#26377;&#24037;&#20316;&#35201;&#20040;&#20551;&#35774;&#25152;&#26377;&#31574;&#30053;&#30340;&#28151;&#21512;&#26102;&#38388;&#22343;&#21248;&#26377;&#30028;&#65292;&#35201;&#20040;&#23545;&#21442;&#25968;&#26377;&#27425;&#20248;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#19968;&#33324;&#65288;&#38750;&#24369;&#36890;&#20449;&#65289;&#24179;&#22343;&#22870;&#36175;MDPs&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#20010;&#26032;&#30340;&#30636;&#24577;&#26102;&#38388;&#21442;&#25968;$B$&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;$\tilde{O}(SA\frac{B+H}{\epsilon^2})$&#30340;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#21305;&#37197;&#30340;&#65288;&#26368;&#22810;&#23545;&#25968;&#22240;&#23376;&#65289;&#26368;&#23567;&#26368;&#20248;&#19979;&#30028;&#12290;&#36825;&#20004;&#20010;&#32467;&#26524;&#37117;&#26159;&#22522;&#20110;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11477v1 Announce Type: new  Abstract: We study the sample complexity of learning an $\epsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\tilde{O}(SA\frac{H}{\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We further investigate sample complexity in general (non-weakly-communicating) average-reward MDPs. We argue a new transient time parameter $B$ is necessary, establish an $\tilde{O}(SA\frac{B+H}{\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AD3&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#21160;&#20316;&#29983;&#25104;&#22120;(IAG)&#23398;&#20064;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#38544;&#24335;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#21306;&#20998;&#20219;&#21153;&#19981;&#30456;&#20851;&#32452;&#20214;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09976</link><description>&lt;p&gt;
AD3:&#38544;&#24335;&#21160;&#20316;&#26159;&#19990;&#30028;&#27169;&#22411;&#21306;&#20998;&#19981;&#21516;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AD3&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#21160;&#20316;&#29983;&#25104;&#22120;(IAG)&#23398;&#20064;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#38544;&#24335;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#21306;&#20998;&#20219;&#21153;&#19981;&#30456;&#20851;&#32452;&#20214;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#21306;&#20998;&#35270;&#35273;&#25511;&#21046;&#20013;&#30340;&#20219;&#21153;&#19981;&#30456;&#20851;&#24178;&#25200;&#22240;&#32032;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24322;&#36136;&#24178;&#25200;&#22240;&#32032;&#65292;&#22914;&#22024;&#26434;&#30340;&#32972;&#26223;&#35270;&#39057;&#19978;&#65292;&#23545;&#23494;&#20999;&#31867;&#20284;&#21487;&#25511;&#21046;&#20195;&#29702;&#30340;&#22343;&#36136;&#24178;&#25200;&#22240;&#32032;&#30340;&#30740;&#31350;&#24456;&#23569;&#65292;&#36825;&#32473;&#29616;&#26377;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38544;&#24335;&#21160;&#20316;&#29983;&#25104;&#22120;&#65288;IAG&#65289;&#26469;&#23398;&#20064;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#38544;&#24335;&#21160;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#24335;&#21160;&#20316;&#36890;&#30693;&#30340;&#22810;&#26679;&#21270;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#21306;&#20998;&#22120;&#65288;AD3&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;IAG&#25512;&#26029;&#30340;&#21160;&#20316;&#26469;&#35757;&#32451;&#20998;&#31163;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#38544;&#24335;&#21160;&#20316;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32972;&#26223;&#24178;&#25200;&#22240;&#32032;&#30340;&#34892;&#20026;&#65292;&#26377;&#21161;&#20110;&#21306;&#20998;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#32452;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#20248;&#21270;&#20219;&#21153;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#20869;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09976v1 Announce Type: new  Abstract: Model-based methods have significantly contributed to distinguishing task-irrelevant distractors for visual control. However, prior research has primarily focused on heterogeneous distractors like noisy background videos, leaving homogeneous distractors that closely resemble controllable agents largely unexplored, which poses significant challenges to existing methods. To tackle this problem, we propose Implicit Action Generator (IAG) to learn the implicit actions of visual distractors, and present a new algorithm named implicit Action-informed Diverse visual Distractors Distinguisher (AD3), that leverages the action inferred by IAG to train separated world models. Implicit actions effectively capture the behavior of background distractors, aiding in distinguishing the task-irrelevant components, and the agent can optimize the policy within the task-relevant state space. Our method achieves superior performance on various visual control 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27965;&#21407;&#29702;&#30340;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#35757;&#32451;&#26041;&#27861;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09560</link><description>&lt;p&gt;
&#33258;&#27965;&#35757;&#32451;&#29992;&#20110;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistency Training for Hamiltonian Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09560
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27965;&#21407;&#29702;&#30340;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#35757;&#32451;&#26041;&#27861;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09560v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25552;&#35201;: &#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#26159;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#20998;&#23376;&#31185;&#23398;&#38382;&#39064;&#30340;&#22810;&#21151;&#33021;&#20844;&#24335;&#12290;&#28982;&#32780;&#65292;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#20855;&#26377;&#33258;&#27965;&#21407;&#29702;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#31934;&#30830;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#19968;&#20248;&#28857;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#22256;&#38590;&#65292;&#24182;&#23558;&#35813;&#20219;&#21153;&#19982;&#20854;&#20182;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#30340;&#23646;&#24615;&#39044;&#27979;&#20844;&#24335;&#21306;&#20998;&#24320;&#65306;&#65288;1&#65289;&#33258;&#27965;&#35757;&#32451;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#22240;&#27492;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65307;&#65288;2&#65289;&#33258;&#27965;&#35757;&#32451;&#27604;&#20351;&#29992;DFT&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#26159;&#23545;&#19968;&#32452;&#20998;&#23376;&#32467;&#26500;&#19978;&#30340;DFT&#35745;&#31639;&#30340;&#25674;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#20998;&#24067;&#20043;&#22806;&#30340;&#24773;&#20917;&#19979;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09560v1 Announce Type: new  Abstract: Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;R-PCA&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;PCA&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26041;&#38754;&#20248;&#20110;R-PCA&#65292;&#20294;&#22312;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#26041;&#38754;&#34920;&#29616;&#25509;&#36817;&#20934;&#30830;&#24230;&#25968;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.09117</link><description>&lt;p&gt;
&#38024;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Randomized Principal Component Analysis for Hyperspectral Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;R-PCA&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;PCA&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26041;&#38754;&#20248;&#20110;R-PCA&#65292;&#20294;&#22312;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#26041;&#38754;&#34920;&#29616;&#25509;&#36817;&#20934;&#30830;&#24230;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#32473;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38477;&#32500;&#26159;&#24517;&#35201;&#30340;&#20197;&#20943;&#23567;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#38543;&#26426;&#25237;&#24433;&#20026;&#38477;&#20302;&#32500;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;R-PCA&#65289;&#12290;&#22312;&#36825;&#39033;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#23558;&#29305;&#24449;&#25968;&#20943;&#23569;&#21040;20&#21644;30&#20197;&#36827;&#34892;&#20004;&#20010;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65288;Indian Pines&#21644;Pavia University&#65289;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;PCA&#22312;SVM&#26041;&#38754;&#20248;&#20110;R-PCA&#65292;&#20294;&#22312;LightGBM&#26041;&#38754;&#33719;&#24471;&#20102;&#25509;&#36817;&#30340;&#20934;&#30830;&#24230;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09117v1 Announce Type: cross  Abstract: The high-dimensional feature space of the hyperspectral imagery poses major challenges to the processing and analysis of the hyperspectral data sets. In such a case, dimensionality reduction is necessary to decrease the computational complexity. The random projections open up new ways of dimensionality reduction, especially for large data sets. In this paper, the principal component analysis (PCA) and randomized principal component analysis (R-PCA) for the classification of hyperspectral images using support vector machines (SVM) and light gradient boosting machines (LightGBM) have been investigated. In this experimental research, the number of features was reduced to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and Pavia University). The experimental results demonstrated that PCA outperformed R-PCA for SVM for both datasets, but received close accuracy values for LightGBM. The highest classification accurac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.07004</link><description>&lt;p&gt;
&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;
&lt;/p&gt;
&lt;p&gt;
Convergence of Some Convex Message Passing Algorithms to a Fixed Point
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07004
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#27169;&#22411;&#20013;&#35299;&#20915;MAP&#25512;&#26029;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#36890;&#36807;&#65288;&#22359;&#29366;&#65289;&#22352;&#26631;&#19979;&#38477;&#26368;&#23567;&#21270;&#20174;&#23545;&#20598;&#32447;&#24615;&#35268;&#21010;&#25110;Lagrange&#26494;&#24347;&#20013;&#33719;&#24471;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#36825;&#26679;&#30340;&#31639;&#27861;&#21253;&#25324;&#26368;&#22823;&#21644;&#25193;&#25955;&#20197;&#21450;&#39034;&#24207;&#26641;&#37325;&#26032;&#21152;&#26435;&#28040;&#24687;&#20256;&#36882;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#30446;&#21069;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#20250;&#25910;&#25947;&#21040;&#30001;&#27963;&#36291;&#32422;&#26463;&#30340;&#23616;&#37096;&#19968;&#33268;&#24615;&#25152;&#34920;&#24449;&#30340;&#38598;&#21512;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26410;&#30693;&#65307;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#36845;&#20195;&#26159;&#21542;&#20250;&#25910;&#25947;&#65288;&#21040;&#20219;&#20309;&#19968;&#20010;&#21333;&#19968;&#28857;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26356;&#24378;&#30340;&#32467;&#26524;&#65288;&#20043;&#21069;&#26377;&#29468;&#24819;&#20294;&#20174;&#26410;&#35777;&#26126;&#36807;&#65289;&#65306;&#36845;&#20195;&#20250;&#25910;&#25947;&#21040;&#31639;&#27861;&#30340;&#19968;&#20010;&#22266;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#23427;&#20204;&#22312;$\mathcal{O}(1/\varepsilon)$&#27425;&#36845;&#20195;&#20013;&#36798;&#21040;&#20102;&#31934;&#24230;$\varepsilon&gt;0$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07004v1 Announce Type: new  Abstract: A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon&gt;0$ in $\mathcal{O}(1/\varepsilon)$ iterations.   We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25506;&#32034;&#30446;&#26631;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;$L_1$-&#35206;&#30422;&#24230;&#20316;&#20026;&#26032;&#30340;&#25506;&#32034;&#30446;&#26631;&#65292;&#25903;&#25345;&#20869;&#22312;&#22797;&#26434;&#24230;&#25511;&#21046;&#12289;&#39640;&#25928;&#35268;&#21010;&#21644;&#28789;&#27963;&#38598;&#25104;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.06571</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#25506;&#32034;&#26041;&#27861;&#65306;&#36890;&#36807;Coverability
&lt;/p&gt;
&lt;p&gt;
Scalable Online Exploration via Coverability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25506;&#32034;&#30446;&#26631;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;$L_1$-&#35206;&#30422;&#24230;&#20316;&#20026;&#26032;&#30340;&#25506;&#32034;&#30446;&#26631;&#65292;&#25903;&#25345;&#20869;&#22312;&#22797;&#26434;&#24230;&#25511;&#21046;&#12289;&#39640;&#25928;&#35268;&#21010;&#21644;&#28789;&#27963;&#38598;&#25104;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23588;&#20854;&#23545;&#20110;&#38656;&#35201;&#20989;&#25968;&#36924;&#36817;&#30340;&#39640;&#32500;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;&#30446;&#26631;&#8212;&#8212;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#20219;&#20309;&#22870;&#21169;&#20989;&#25968;&#30340;&#19979;&#28216;&#26368;&#22823;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#21363;$L_1$-&#35206;&#30422;&#24230;&#65292;&#23427;&#27867;&#21270;&#20102;&#20197;&#24448;&#30340;&#25506;&#32034;&#26041;&#26696;&#65292;&#24182;&#25903;&#25345;&#19977;&#20010;&#22522;&#26412;&#24895;&#26395;&#65306;1.&#20869;&#22312;&#22797;&#26434;&#24230;&#25511;&#21046;&#12290;$L_1$-&#35206;&#30422;&#24230;&#19982;&#32467;&#26500;&#21442;&#25968;$L_1$-Coverability&#30456;&#20851;&#32852;&#65292;&#21453;&#26144;&#20102;&#28508;&#22312;MDP&#30340;&#20869;&#22312;&#32479;&#35745;&#22256;&#38590;&#24230;&#65292;&#21253;&#21547;Block&#21644;Low-Rank MDPs&#12290;2.&#39640;&#25928;&#35268;&#21010;&#12290;&#23545;&#20110;&#24050;&#30693;&#30340;MDP&#65292;&#20248;&#21270;$L_1$-&#35206;&#30422;&#24230;&#33021;&#22815;&#26377;&#25928;&#22320;&#38477;&#20302;&#21040;&#26631;&#20934;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#20801;&#35768;&#19982;&#35832;&#22914;&#31574;&#30053;&#26799;&#24230;&#21644;Q-learning&#31561;&#29616;&#25104;&#26041;&#27861;&#28789;&#27963;&#38598;&#25104;&#12290;3.&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;$L_1$-&#35206;&#30422;&#24230;&#30340;&#20248;&#21270;&#31561;&#21516;&#20110;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25805;&#20316;&#65292;&#23588;&#20854;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06571v1 Announce Type: new  Abstract: Exploration is a major challenge in reinforcement learning, especially for high-dimensional domains that require function approximation. We propose exploration objectives -- policy optimization objectives that enable downstream maximization of any reward function -- as a conceptual framework to systematize the study of exploration. Within this framework, we introduce a new objective, $L_1$-Coverage, which generalizes previous exploration schemes and supports three fundamental desiderata:   1. Intrinsic complexity control. $L_1$-Coverage is associated with a structural parameter, $L_1$-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank MDPs.   2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently reduces to standard policy optimization, allowing flexible integration with off-the-shelf methods such as policy gradient and Q-learning approaches.   3. E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;ICP&#31639;&#27861;&#36827;&#34892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25915;&#20987;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#65292;&#37325;&#28857;&#22312;&#20110;&#25214;&#21040;&#21487;&#33021;&#30340;&#26368;&#22823;ICP&#23039;&#21183;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.05666</link><description>&lt;p&gt;
&#38754;&#23545;&#26368;&#22351;&#24773;&#20917;&#65306;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#23545;ICP&#31639;&#27861;&#40065;&#26834;&#24615;&#20998;&#26512;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05666
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;ICP&#31639;&#27861;&#36827;&#34892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25915;&#20987;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#65292;&#37325;&#28857;&#22312;&#20110;&#25214;&#21040;&#21487;&#33021;&#30340;&#26368;&#22823;ICP&#23039;&#21183;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25915;&#20987;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#26469;&#35780;&#20272;&#36845;&#20195;&#26368;&#36817;&#28857;&#65288;ICP&#65289;&#31639;&#27861;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#23545;&#20110;&#20687;&#33258;&#20027;&#23548;&#33322;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#30830;&#20445;&#31639;&#27861;&#22312;&#37096;&#32626;&#21069;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;ICP&#31639;&#27861;&#24050;&#25104;&#20026;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23427;&#20135;&#29983;&#30340;&#23039;&#21183;&#20272;&#35745;&#21487;&#33021;&#20250;&#21463;&#21040;&#27979;&#37327;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25968;&#25454;&#30340;&#27745;&#26579;&#21487;&#33021;&#26469;&#33258;&#21508;&#31181;&#22330;&#26223;&#65292;&#22914;&#36974;&#25377;&#12289;&#24694;&#21155;&#22825;&#27668;&#25110;&#20256;&#24863;&#22120;&#30340;&#26426;&#26800;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;ICP&#30340;&#22797;&#26434;&#21644;&#36845;&#20195;&#29305;&#24615;&#20351;&#24471;&#35780;&#20272;&#20854;&#23545;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20154;&#21162;&#21147;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20223;&#30495;&#26469;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;ICP&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#22522;&#20110;&#25200;&#21160;&#30340;&#23545;&#25239;&#25915;&#20987;&#25214;&#21040;&#26368;&#22823;&#21487;&#33021;&#30340;ICP&#23039;&#21183;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05666v1 Announce Type: cross  Abstract: This paper presents a novel method to assess the resilience of the Iterative Closest Point (ICP) algorithm via deep-learning-based attacks on lidar point clouds. For safety-critical applications such as autonomous navigation, ensuring the resilience of algorithms prior to deployments is of utmost importance. The ICP algorithm has become the standard for lidar-based localization. However, the pose estimate it produces can be greatly affected by corruption in the measurements. Corruption can arise from a variety of scenarios such as occlusions, adverse weather, or mechanical issues in the sensor. Unfortunately, the complex and iterative nature of ICP makes assessing its resilience to corruption challenging. While there have been efforts to create challenging datasets and develop simulations to evaluate the resilience of ICP empirically, our method focuses on finding the maximum possible ICP pose error using perturbation-based adversarial
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21457;&#29616;&#25152;&#26377;&#23376;&#32593;&#32476;&#37117;&#20849;&#20139;&#19968;&#20010;&#27880;&#24847;&#21147;&#22836;&#38598;&#21512;&#65292;&#34987;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#65292;&#36825;&#21487;&#33021;&#26159;&#36896;&#25104;&#23376;&#32593;&#32476;&#27867;&#21270;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.03942</link><description>&lt;p&gt;
&#12298;&#21551;&#21457;&#24335;&#26680;&#24515;&#65306;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23376;&#32593;&#32476;&#27867;&#21270;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03942
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21457;&#29616;&#25152;&#26377;&#23376;&#32593;&#32476;&#37117;&#20849;&#20139;&#19968;&#20010;&#27880;&#24847;&#21147;&#22836;&#38598;&#21512;&#65292;&#34987;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#65292;&#36825;&#21487;&#33021;&#26159;&#36896;&#25104;&#23376;&#32593;&#32476;&#27867;&#21270;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#39046;&#22495;&#20869;&#24615;&#33021;&#65292;&#20294;&#22312;&#21477;&#27861;&#27867;&#21270;&#27979;&#35797;&#20013;&#27867;&#21270;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#21363;&#20351;&#22312;&#21333;&#19968;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#25214;&#21040;&#25191;&#34892;&#31867;&#20284;&#39046;&#22495;&#20869;&#25805;&#20316;&#20294;&#27867;&#21270;&#24046;&#24322;&#24040;&#22823;&#30340;&#22810;&#20010;&#23376;&#32593;&#32476;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#29616;&#35937;&#65292;&#25105;&#20204;&#35843;&#26597;&#26159;&#21542;&#21487;&#20197;&#29992;&#8220;&#31454;&#20105;&#24615;&#23376;&#32593;&#32476;&#8221;&#26469;&#29702;&#35299;&#23427;&#20204;&#65306;&#27169;&#22411;&#26368;&#21021;&#34920;&#31034;&#21508;&#31181;&#19981;&#21516;&#31639;&#27861;&#65292;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#65292;&#24403;&#26368;&#32456;&#25910;&#25947;&#21040;&#20854;&#20013;&#19968;&#20010;&#26102;&#27867;&#21270;&#21457;&#29983;&#12290;&#36825;&#19968;&#35299;&#37322;&#24050;&#34987;&#29992;&#20110;&#35299;&#37322;&#31616;&#21333;&#31639;&#27861;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24182;&#38750;&#25214;&#21040;&#31454;&#20105;&#24615;&#23376;&#32593;&#32476;&#65292;&#32780;&#26159;&#25152;&#26377;&#23376;&#32593;&#32476; -- &#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#27867;&#21270; -- &#37117;&#20849;&#20139;&#19968;&#32452;&#27880;&#24847;&#21147;&#22836;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#21487;&#33021;&#26159;&#36896;&#25104;&#19981;&#21516;&#23376;&#32593;&#32476;&#27867;&#21270;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03942v1 Announce Type: new  Abstract: Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of "competing subnetworks": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that th
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#65288;CPT&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#22312;&#19981;&#21516;&#32452;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#65292;&#36991;&#20813;&#29306;&#29298;&#20219;&#20309;&#19968;&#20010;&#32452;&#30340;&#24615;&#33021;&#65292;&#22312;&#34394;&#20551;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02695</link><description>&lt;p&gt;
&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#29992;&#20110;&#24179;&#34913;&#32452;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Controllable Prompt Tuning For Balancing Group Distributional Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02695
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#65288;CPT&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#22312;&#19981;&#21516;&#32452;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#65292;&#36991;&#20813;&#29306;&#29298;&#20219;&#20309;&#19968;&#20010;&#32452;&#30340;&#24615;&#33021;&#65292;&#22312;&#34394;&#20551;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#19981;&#21516;&#32452;&#25110;&#39046;&#22495;&#32452;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#20986;&#29616;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#26368;&#24046;&#32452;&#30340;&#30446;&#26631;&#19978;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#20197;&#29306;&#29298;&#20854;&#20182;&#32452;&#19978;&#30340;&#33391;&#22909;&#24615;&#33021;&#20026;&#20195;&#20215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#32452;&#20869;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#25214;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#20250;&#20005;&#37325;&#29306;&#29298;&#20219;&#20309;&#19968;&#20010;&#32452;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#36825;&#31181;&#20248;&#21270;&#20250;&#28041;&#21450;&#26356;&#26032;&#25972;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25511;&#25552;&#31034;&#35843;&#25972;&#65288;CPT&#65289;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#22312;&#34394;&#20551;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#31243;&#24207;&#22312;&#21464;&#21387;&#22120;&#21644;&#38750;&#21464;&#21387;&#22120;&#26550;&#26500;&#20197;&#21450;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02695v1 Announce Type: new  Abstract: Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.02419</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#26356;&#22810;LLM&#35843;&#29992;&#21527;&#65311;&#36208;&#21521;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#26159;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#29992;&#24182;&#27719;&#24635;&#23427;&#20204;&#30340;&#21709;&#24212;&#30340;&#22797;&#21512;&#31995;&#32479;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#24433;&#21709; -- &#20363;&#22914;&#65292;&#24403;&#35201;&#27714;LLM&#22810;&#27425;&#22238;&#31572;&#27599;&#20010;&#38382;&#39064;&#24182;&#21462;&#24471;&#20849;&#35782;&#26102; -- &#23545;&#20110;&#36825;&#31181;&#22797;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLM&#35843;&#29992;&#27425;&#25968;&#22914;&#20309;&#24433;&#21709;&#19968;&#20010;&#23618;&#32423;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021; -- &#36825;&#26159;&#26368;&#31616;&#21333;&#30340;&#22797;&#21512;&#31995;&#32479;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32858;&#21512;LLM&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;&#30528;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38750;&#21333;&#35843;&#24615;&#26159;&#30001;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02419v1 Announce Type: cross  Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;</title><link>https://arxiv.org/abs/2403.02233</link><description>&lt;p&gt;
Transformers&#22312;Masked Image Modeling&#20013;&#33021;&#22815;&#35777;&#26126;&#23398;&#20064;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Masked image modeling (MIM)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20174;&#26410;&#23631;&#34109;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#38543;&#26426;&#23631;&#34109;&#30340;&#34917;&#19969;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;transformers&#30340;MIM&#30340;&#29702;&#35770;&#29702;&#35299;&#30456;&#24403;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#19968;&#23618;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;&#30340;&#29702;&#35770;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02233v1 Announce Type: new  Abstract: Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01942</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;&#20943;&#36731;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#38899;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Noise on Graph via Topological Sample Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31934;&#24515;&#27880;&#37322;&#30340;&#22522;&#20934;&#27979;&#35797;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24403;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#26102;&#65292;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26377;&#25928;&#24615;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#21463;&#21040;&#30456;&#24403;&#22823;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#22312;&#26679;&#26412;&#36873;&#25321;&#26041;&#38754;&#30340;&#25506;&#32034;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24212;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#20391;&#37325;&#20110;i.i.d&#25968;&#25454;&#65292;&#24403;&#36716;&#21521;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#21644;GNNs&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#25361;&#25112;&#65306;(1) &#20301;&#20110;&#25299;&#25169;&#31867;&#36793;&#30028;&#38468;&#36817;&#30340;&#33410;&#28857;&#23545;&#20998;&#31867;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#26080;&#27861;&#36890;&#36807;&#21551;&#21457;&#24335;&#26679;&#26412;&#36873;&#25321;&#25104;&#21151;&#21306;&#20998;&#12290;(2) &#27809;&#26377;&#21487;&#29992;&#30340;&#34913;&#37327;&#26631;&#20934;&#32771;&#34385;&#22270;&#30340;&#25299;&#25169;&#20449;&#24687;&#20197;&#20419;&#36827;&#22270;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#65288;TSS&#65289;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#25552;&#21319;&#22270;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01942v1 Announce Type: new  Abstract: Despite the success of the carefully-annotated benchmarks, the effectiveness of existing graph neural networks (GNNs) can be considerably impaired in practice when the real-world graph data is noisily labeled. Previous explorations in sample selection have been demonstrated as an effective way for robust learning with noisy labels, however, the conventional studies focus on i.i.d data, and when moving to non-iid graph data and GNNs, two notable challenges remain: (1) nodes located near topological class boundaries are very informative for classification but cannot be successfully distinguished by the heuristic sample selection. (2) there is no available measure that considers the graph topological information to promote sample selection in a graph. To address this dilemma, we propose a $\textit{Topological Sample Selection}$ (TSS) method that boosts the informative sample selection process in a graph by utilising topological information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#65288;RLHF&#65289;&#33539;&#24335;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#33539;&#24335;&#65292;&#23545;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#25512;&#23548;&#20986;&#20102;&#27425;&#20248;&#24046;&#36317;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.01857</link><description>&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#19982;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#65306;&#20174;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#65288;RLHF&#65289;&#33539;&#24335;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#33539;&#24335;&#65292;&#23545;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#25512;&#23548;&#20986;&#20102;&#27425;&#20248;&#24046;&#36317;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#27604;&#36739;&#20174;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#33539;&#24335;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#33539;&#24335;&#65292;&#36808;&#21521;&#23545;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#20197;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#21644;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#30340;&#31867;&#20026;&#37325;&#28857;&#12290;&#20026;&#20102;&#27604;&#36739;&#36825;&#20004;&#31181;&#33539;&#24335;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#30001;RLHF&#21644;DPO&#24341;&#36215;&#30340;&#27425;&#20248;&#24046;&#36317;&#25512;&#23548;&#20986;&#26497;&#23567;-&#26368;&#22823;&#32479;&#35745;&#30028;&#38480;&#65292;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#30830;&#20999;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#39044;&#35328;&#12290;&#25105;&#20204;&#23601;&#30456;&#23545;&#27604;&#36739;&#20004;&#31181;&#33539;&#24335;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#65292;&#21516;&#26102;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#12289;&#31574;&#30053;&#21644;&#22870;&#21169;&#31867;&#32500;&#25968;&#20197;&#21450;&#27491;&#21017;&#21270;&#28201;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#36817;&#20284;&#20248;&#21270;&#35774;&#32622;&#65292;&#24182;&#20026;RLHF&#21644;DPO&#20998;&#21035;&#25512;&#23548;&#20986;&#25351;&#25968;&#34928;&#20943;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01857v1 Announce Type: new  Abstract: In this paper, we take a step towards a deeper understanding of learning from human preferences by systematically comparing the paradigm of reinforcement learning from human feedback (RLHF) with the recently proposed paradigm of direct preference optimization (DPO). We focus our attention on the class of loglinear policy parametrization and linear reward functions. In order to compare the two paradigms, we first derive minimax statistical bounds on the suboptimality gap induced by both RLHF and DPO, assuming access to an oracle that exactly solves the optimization problems. We provide a detailed discussion on the relative comparison between the two paradigms, simultaneously taking into account the sample size, policy and reward class dimensions, and the regularization temperature. Moreover, we extend our analysis to the approximate optimization setting and derive exponentially decaying convergence rates for both RLHF and DPO. Next, we an
&lt;/p&gt;</description></item><item><title>Pair-Align&#26041;&#27861;&#36890;&#36807;&#19968;&#23545;&#19968;&#23545;&#40784;&#26469;&#23545;&#25239;&#22270;&#32467;&#26500;&#36716;&#31227;&#65292;&#20351;&#29992;&#36793;&#26435;&#37325;&#20943;&#36731;&#26465;&#20214;&#32467;&#26500;&#36716;&#31227;&#65288;CSS&#65289;&#24182;&#35843;&#25972;&#20998;&#31867;&#25439;&#22833;&#20197;&#22788;&#29702;&#26631;&#31614;&#36716;&#31227;&#65288;LS&#65289;&#65292;&#22312;&#22788;&#29702;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01092</link><description>&lt;p&gt;
&#19968;&#23545;&#19968;&#23545;&#40784;&#25913;&#36827;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Pairwise Alignment Improves Graph Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01092
&lt;/p&gt;
&lt;p&gt;
Pair-Align&#26041;&#27861;&#36890;&#36807;&#19968;&#23545;&#19968;&#23545;&#40784;&#26469;&#23545;&#25239;&#22270;&#32467;&#26500;&#36716;&#31227;&#65292;&#20351;&#29992;&#36793;&#26435;&#37325;&#20943;&#36731;&#26465;&#20214;&#32467;&#26500;&#36716;&#31227;&#65288;CSS&#65289;&#24182;&#35843;&#25972;&#20998;&#31867;&#25439;&#22833;&#20197;&#22788;&#29702;&#26631;&#31614;&#36716;&#31227;&#65288;LS&#65289;&#65292;&#22312;&#22788;&#29702;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#23545;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#26631;&#31614;&#25512;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#22914;&#26524;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#22270;&#19982;&#29992;&#20110;&#27979;&#35797;&#30340;&#22270;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#24448;&#24448;&#20250;&#36935;&#21040;&#27867;&#21270;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#28145;&#20837;&#25506;&#35752;&#20102;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#25968;&#25454;&#28857;&#32463;&#21382;&#29305;&#24449;&#12289;&#26631;&#31614;&#21644;&#23588;&#20854;&#26159;&#36830;&#25509;&#27169;&#24335;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#22312;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#19968;&#23545;&#19968;&#23545;&#40784;&#65288;Pair-Align&#65289;&#65292;&#36890;&#36807;&#20943;&#36731;&#26465;&#20214;&#32467;&#26500;&#36716;&#31227;&#65288;CSS&#65289;&#21644;&#26631;&#31614;&#36716;&#31227;&#65288;LS&#65289;&#26469;&#23545;&#25239;&#22270;&#32467;&#26500;&#36716;&#31227;&#12290;Pair-Align&#20351;&#29992;&#36793;&#26435;&#37325;&#37325;&#26032;&#26657;&#20934;&#37051;&#36817;&#33410;&#28857;&#20043;&#38388;&#30340;&#24433;&#21709;&#20197;&#22788;&#29702;CSS&#65292;&#24182;&#36890;&#36807;&#26631;&#31614;&#26435;&#37325;&#35843;&#25972;&#20998;&#31867;&#25439;&#22833;&#20197;&#22788;&#29702;LS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#21306;&#22495;&#33410;&#28857;&#20998;&#31867;&#22312;&#20869;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01092v1 Announce Type: new  Abstract: Graph-based methods, pivotal for label inference over interconnected objects in many real-world applications, often encounter generalization challenges, if the graph used for model training differs significantly from the graph used for testing. This work delves into Graph Domain Adaptation (GDA) to address the unique complexities of distribution shifts over graph data, where interconnected data points experience shifts in features, labels, and in particular, connecting patterns. We propose a novel, theoretically principled method, Pairwise Alignment (Pair-Align) to counter graph structure shift by mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align uses edge weights to recalibrate the influence among neighboring nodes to handle CSS and adjusts the classification loss with label weights to handle LS. Our method demonstrates superior performance in real-world applications, including node classification with region
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.00932</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#25991;&#26412;&#29983;&#25104;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Knowledge Distillation via Synthetic Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38544;&#31169;&#30340;&#22686;&#21152;&#32039;&#36843;&#24615;&#35201;&#27714;LLMs&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;(DP)&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#38656;&#35201;&#21387;&#32553;LLMs&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#25110;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#30495;&#23454;&#37096;&#32626;&#12290;&#24046;&#20998;&#38544;&#31169;&#21644;&#27169;&#22411;&#21387;&#32553;&#36890;&#24120;&#24517;&#39035;&#22312;&#23454;&#29616;&#20854;&#30446;&#26631;&#30340;&#36807;&#31243;&#20013;&#26435;&#34913;&#25928;&#29992;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#32773;&#21487;&#33021;&#23548;&#33268;&#26356;&#22810;&#30340;&#25928;&#29992;&#25439;&#22833;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#30001;&#24046;&#20998;&#31169;&#23494;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20197;&#20004;&#31181;&#26041;&#24335;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#19978;&#65306;&#19968;&#31181;&#26159;&#26469;&#33258;&#21512;&#25104;&#25968;&#25454;&#26412;&#36523;&#30340;&#30828;&#26631;&#31614;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00932v1 Announce Type: cross  Abstract: Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires LLMs to train with Differential Privacy (DP) on private data. Concurrently it is also necessary to compress LLMs for real-life deployments on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private LLM. The knowledge of a teacher model is transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher model evaluated on the synthetic data
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19009</link><description>&lt;p&gt;
&#29983;&#25104;&#12289;&#37325;&#24314;&#21644;&#34920;&#31034;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#65306;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19009
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#22522;&#20110;&#19977;&#39033;&#26680;&#24515;&#33021;&#21147;--&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;--&#36328;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#31163;&#25955;&#25991;&#26412;/&#34507;&#30333;&#24207;&#21015;&#21644;&#36830;&#32493;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#23478;&#26063;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#29305;&#23450;&#33021;&#21147;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#23427;&#26080;&#32541;&#22320;&#38598;&#25104;&#20102;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;DiLED&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#21270;&#32534;&#30721;-&#35299;&#30721;&#26469;&#23558;&#26631;&#20934;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#21152;&#22122;-&#21435;&#22122;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#20851;&#38190;&#26159;&#65292;DiLED&#19982;&#25104;&#29087;&#30340;&#25193;&#25955;&#27169;&#22411;&#30446;&#26631;&#21644;&#35757;&#32451;&#26041;&#27861;&#20860;&#23481;&#65292;&#21487;&#26377;&#25928;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19009v1 Announce Type: cross  Abstract: The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24773;&#22659;&#36870;&#32447;&#24615;&#35268;&#21010;&#31616;&#21270;&#20026;&#20984;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#23558;&#20854;&#36827;&#19968;&#27493;&#31616;&#21270;&#20026;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17890</link><description>&lt;p&gt;
&#20174;&#36870;&#20248;&#21270;&#21040;&#21487;&#34892;&#24615;&#21040;ERM
&lt;/p&gt;
&lt;p&gt;
From Inverse Optimization to Feasibility to ERM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24773;&#22659;&#36870;&#32447;&#24615;&#35268;&#21010;&#31616;&#21270;&#20026;&#20984;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#23558;&#20854;&#36827;&#19968;&#27493;&#31616;&#21270;&#20026;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#20248;&#21270;&#28041;&#21450;&#20174;&#24050;&#30693;&#35299;&#20915;&#26041;&#26696;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#30005;&#21147;&#31995;&#32479;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#21033;&#29992;&#39069;&#22806;&#24773;&#22659;&#20449;&#24687;&#26469;&#26356;&#22909;&#39044;&#27979;&#26410;&#30693;&#38382;&#39064;&#21442;&#25968;&#30340;&#24773;&#22659;&#36870;&#20248;&#21270;&#35774;&#32622;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#24773;&#22659;&#36870;&#32447;&#24615;&#35268;&#21010;&#65288;CILP&#65289;&#65292;&#35299;&#20915;&#20102;LP&#38750;&#21487;&#24494;&#24615;&#36136;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;CILP&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20801;&#35768;&#20351;&#29992;&#35832;&#22914;&#20132;&#26367;&#25237;&#24433;&#31561;&#26631;&#20934;&#31639;&#27861;&#12290;CILP&#30340;&#32467;&#26524;&#31639;&#27861;&#37197;&#22791;&#20102;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#65292;&#22914;&#36864;&#21270;&#25110;&#25554;&#20540;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;CILP&#20943;&#23569;&#21040;&#19968;&#20010;&#20809;&#28369;&#30340;&#12289;&#20984;&#25439;&#22833;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#12290;&#36825;&#31181;&#31616;&#21270;&#33021;&#22815;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;fir
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17890v1 Announce Type: new  Abstract: Inverse optimization involves inferring unknown parameters of an optimization problem from known solutions, and is widely used in fields such as transportation, power systems and healthcare. We study the contextual inverse optimization setting that utilizes additional contextual information to better predict the unknown problem parameters. We focus on contextual inverse linear programming (CILP), addressing the challenges posed by the non-differentiable nature of LPs. For a linear prediction model, we reduce CILP to a convex feasibility problem allowing the use of standard algorithms such as alternating projections. The resulting algorithm for CILP is equipped with a linear convergence guarantee without additional assumptions such as degeneracy or interpolation. Next, we reduce CILP to empirical risk minimization (ERM) on a smooth, convex loss that satisfies the Polyak-Lojasiewicz condition. This reduction enables the use of scalable fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17811</link><description>&lt;p&gt;
TruthX: &#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20943;&#36731;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#22238;&#24212;&#65292;&#23613;&#31649;&#25317;&#26377;&#27491;&#30830;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TruthX&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLMs&#20869;&#37096;&#34920;&#31034;&#20197;&#33719;&#21462;&#20854;&#30495;&#23454;&#24615;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#12290;TruthX&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;LLM&#30340;&#34920;&#31034;&#20998;&#21035;&#26144;&#23556;&#21040;&#35821;&#20041;&#21644;&#30495;&#23454;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#35782;&#21035;&#30495;&#23454;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLM&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;TruthX&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;LLMs&#30340;&#30495;&#23454;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TruthX&#36890;&#36807;20%&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;LLMs&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30495;&#23454;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17811v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17768</link><description>&lt;p&gt;
&#25193;&#25955;&#36935;&#35265;DAgger: &#36229;&#32423;&#30524;&#22312;&#25163;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17768
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Diffusion Meets DAgger (DMD)&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#20986;&#29616;&#22312;&#19987;&#23478;&#28436;&#31034;&#20013;&#30340;&#29366;&#24577;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17768v1 Announce Type: cross  Abstract: A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17606</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#25299;&#25169;&#34920;&#31034;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#29616;&#25104;GNN&#27169;&#22411;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#65292;&#24182;&#24573;&#30053;&#20102;&#24182;&#21457;&#22270;&#65288;DGs&#65289;&#30340;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#25628;&#32034;&#26694;&#26550;&#20013;&#23884;&#20837;DG&#20197;&#35299;&#20915;JSSP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TBGAT&#20998;&#21035;&#20174;&#27491;&#21521;&#21644;&#21453;&#21521;&#35270;&#22270;&#23884;&#20837;DG&#65292;&#28040;&#24687;&#36890;&#36807;&#36981;&#24490;&#19981;&#21516;&#35270;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#27719;&#24635;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#26032;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#35745;&#31639;DG&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#25299;&#25169;&#25490;&#24207;&#65292;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#34987;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;TBGAT&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17606v1 Announce Type: cross  Abstract: Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.15607</link><description>&lt;p&gt;
&#35757;&#32451;&#38750;&#32447;&#24615;Transformer&#36827;&#34892;&#39640;&#25928;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#29702;&#35770;&#23398;&#20064;&#21644;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#31616;&#21333;&#22320;&#22686;&#21152;&#26597;&#35810;&#19982;&#26469;&#33258;&#35813;&#20219;&#21153;&#30340;&#19968;&#20123;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26469;&#24494;&#35843;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20998;&#26512;Transformers&#20013;&#38750;&#20984;&#35757;&#32451;&#38382;&#39064;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#65292;&#35757;&#32451;Transformer&#20197;&#23454;&#29616;ICL&#21450;&#30456;&#24212;&#30340;ICL&#23481;&#37327;&#30340;&#26426;&#21046;&#22823;&#22810;&#19981;&#20026;&#20154;&#30693;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#32452;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#36825;&#20123;&#20219;&#21153;&#23376;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;Transformers&#65292;&#24182;&#37327;&#21270;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15607v1 Announce Type: new  Abstract: Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;LLMs&#25506;&#32034;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20854;&#22312;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#19978;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15337</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#27839;&#30528;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#23545;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#65306;&#24494;&#35843;&#31574;&#30053;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;LLMs&#25506;&#32034;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20854;&#22312;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#19978;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#31354;&#38388;&#20197;&#23454;&#20307;&#30340;&#21407;&#22987;&#35821;&#20041;&#29305;&#24449;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#23398;&#20064;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#24314;&#27169;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#26102;&#12290;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#28860;&#27010;&#24565;&#31354;&#38388;&#26368;&#36817;&#20986;&#29616;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#20351;&#29992;&#30456;&#23545;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#31574;&#30053;&#25506;&#26597;&#39044;&#35757;&#32451;&#30340;LLMs&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#26681;&#25454;&#32473;&#23450;&#30340;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#23545;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#30340;&#20219;&#21153;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#30340;&#30495;&#23454;&#25490;&#21517;&#24456;&#23569;&#35265;&#65292;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#24494;&#35843;LLMs&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26356;&#23481;&#26131;&#33719;&#24471;&#30340;&#29305;&#24449;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20998;&#26512;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;&#25490;&#21517;&#33021;&#21147;&#26159;&#21542;&#33021;&#36716;&#31227;&#21040;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#30830;&#23454;&#26159;&#36825;&#31181;&#24773;&#20917;&#65292;&#20294;&#26159;&#26410;&#23436;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15337v1 Announce Type: new  Abstract: Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but havi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15152</link><description>&lt;p&gt;
&#20851;&#20110;Sharpness-Aware&#26368;&#23567;&#21270;&#21644;&#23545;&#25239;&#35757;&#32451;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Duality Between Sharpness-Aware Minimization and Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15152
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(Adversarial Training, AT)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#34987;&#35748;&#20026;&#26159;&#23545;&#25239;&#25915;&#20987;&#20013;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#20043;&#19968;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#19968;&#31181;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#24517;&#28982;&#20250;&#38477;&#20302;&#24178;&#20928;&#20934;&#30830;&#24615;&#12290;&#19982;&#23545;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#19981;&#21516;&#65292;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#65292;&#20197;&#23547;&#25214;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26354;&#38754;&#24182;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#26088;&#22312;&#25552;&#39640;&#24178;&#20928;&#20934;&#30830;&#24615;&#65292;&#20854;&#22312;&#22686;&#24378;&#23545;&#25239;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;SAM&#21644;AT&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20174;SAM&#20013;&#27966;&#29983;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#29702;&#35299;SAM&#30340;&#36825;&#31181;&#24847;&#22806;&#29305;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;SAM&#22914;&#20309;&#38544;&#24335;&#23398;&#20064;&#26356;&#40065;&#26834;&#29305;&#24449;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#35265;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15152v1 Announce Type: cross  Abstract: Adversarial Training (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against adversarial attacks, yet suffers from a fundamental tradeoff that inevitably decreases clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing adversarial robustness remains unexplored. In this work, considering the duality between SAM and AT, we investigate the adversarial robustness derived from SAM. Intriguingly, we find that using SAM alone can improve adversarial robustness. To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive exper
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.14883</link><description>&lt;p&gt;
&#21452;I&#27700;&#21360;&#65306;&#20445;&#25252;LLM&#24494;&#35843;&#27169;&#22411;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#65292;&#19994;&#20027;&#32463;&#24120;&#36890;&#36807;LLM&#25152;&#26377;&#32773;&#25110;&#20113;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;API&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#21462;&#23450;&#21046;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#23384;&#22312;&#30528;&#27169;&#22411;&#34987;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#20250;&#32473;&#19994;&#20027;&#24102;&#26469;&#20005;&#37325;&#30340;&#32463;&#27982;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#20445;&#25252;&#36825;&#20123;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#24050;&#25104;&#20026;&#32039;&#36843;&#30340;&#23454;&#38469;&#38656;&#27714;&#65292;&#20294;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32039;&#36843;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#22522;&#20110;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#65292;&#20998;&#21035;&#22312;&#25351;&#20196;&#21644;&#36755;&#20837;&#20013;&#35302;&#21457;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#23558;&#23450;&#21046;&#30340;&#21518;&#38376;&#26679;&#26412;&#32435;&#20837;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#27880;&#20837;&#20102;&#29305;&#23450;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14883v1 Announce Type: cross  Abstract: To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermar
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.14202</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#27604;&#36739;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Comparing Graph Transformers via Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#25442;&#22120;&#30340;&#21306;&#20998;&#33021;&#21147;&#19982;&#20301;&#32622;&#32534;&#30721;&#30340;&#36873;&#25321;&#32039;&#23494;&#30456;&#20851;&#65306;&#29992;&#20110;&#22686;&#24378;&#22522;&#26412;&#21464;&#25442;&#22120;&#19982;&#22270;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26377;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#20301;&#32622;&#32534;&#30721;&#65306;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#12290;APEs&#20026;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#21464;&#25442;&#22120;&#30340;&#36755;&#20837;&#12290;&#32780;RPEs&#21017;&#20026;&#27599;&#23545;&#33410;&#28857;&#65288;&#20363;&#22914;&#65292;&#22270;&#36317;&#31163;&#65289;&#20998;&#37197;&#19968;&#20010;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#22686;&#24378;&#27880;&#24847;&#21147;&#22359;&#12290;&#20808;&#39564;&#19978;&#65292;&#30446;&#21069;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#26356;&#26377;&#21033;&#20110;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#36825;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#20301;&#32622;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;APEs&#21644;RPEs&#30340;&#22270;&#21464;&#25442;&#22120;&#22312;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#20854;&#21306;&#20998;&#33021;&#21147;&#30340;&#21516;&#26102;&#20132;&#25442;APEs&#21644;RPEs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19982;Fenchel-Young&#25439;&#22833;&#24314;&#31435;&#38142;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;&#36716;&#25442;&#65292;&#25581;&#31034;&#20102;&#25439;&#22833;&#36793;&#30028;&#12289;&#31232;&#30095;&#24615;&#21644;&#31934;&#30830;&#23384;&#20648;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21516;&#26102;&#36890;&#36807;SparseMAP&#36716;&#25442;&#23558;&#26694;&#26550;&#25193;&#23637;&#21040;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.13725</link><description>&lt;p&gt;
&#31232;&#30095;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sparse and Structured Hopfield Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13725
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;Fenchel-Young&#25439;&#22833;&#24314;&#31435;&#38142;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;&#36716;&#25442;&#65292;&#25581;&#31034;&#20102;&#25439;&#22833;&#36793;&#30028;&#12289;&#31232;&#30095;&#24615;&#21644;&#31934;&#30830;&#23384;&#20648;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21516;&#26102;&#36890;&#36807;SparseMAP&#36716;&#25442;&#23558;&#26694;&#26550;&#25193;&#23637;&#21040;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#30001;&#20110;&#20854;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#30340;&#32852;&#31995;&#65292;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#19982;Fenchel-Young&#25439;&#22833;&#24314;&#31435;&#32852;&#31995;&#65292;&#20026;&#31232;&#30095;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#26159;&#19968;&#31867;&#26032;&#30340;&#38669;&#26222;&#33778;&#23572;&#24503;-Fenchel-Young&#33021;&#37327;&#65292;&#20854;&#26356;&#26032;&#35268;&#21017;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;&#36716;&#25442;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#25439;&#22833;&#36793;&#30028;&#12289;&#31232;&#30095;&#24615;&#21644;&#31934;&#30830;&#23384;&#20648;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;SparseMAP&#36716;&#25442;&#23558;&#36825;&#19968;&#26694;&#26550;&#25193;&#23637;&#21040;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#26816;&#32034;&#27169;&#24335;&#20851;&#32852;&#32780;&#19981;&#26159;&#21333;&#20010;&#27169;&#24335;&#12290;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#25991;&#26412;&#29702;&#24615;&#21270;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13725v1 Announce Type: new  Abstract: Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#28857;&#20113;&#22788;&#29702;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LSH&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;HEPT&#12290;</title><link>https://arxiv.org/abs/2402.12535</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#30340;&#39640;&#33021;&#29289;&#29702;&#20013;&#24212;&#29992;&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#28857;&#20113;&#22788;&#29702;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LSH&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;HEPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#22788;&#29702;&#22312;&#31185;&#23398;&#39046;&#22495;&#65288;&#22914;&#39640;&#33021;&#29289;&#29702;&#21644;&#22825;&#20307;&#29289;&#29702;&#65289;&#36827;&#34892;&#20248;&#21270;&#30340;&#26032;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26631;&#20934;Transformer&#30340;&#23616;&#38480;&#24615;&#65292;&#38598;&#25104;&#20102;&#23616;&#37096;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#30828;&#20214;&#21451;&#22909;&#30340;&#24120;&#35268;&#25805;&#20316;&#23454;&#29616;&#25509;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#26412;&#24037;&#20316;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#23545;&#21508;&#31181;&#31232;&#30095;&#21270;&#25216;&#26415;&#36827;&#34892;&#35823;&#24046;-&#22797;&#26434;&#24230;&#26435;&#34913;&#30340;&#23450;&#37327;&#20998;&#26512;&#65292;&#20197;&#26500;&#24314;&#39640;&#25928;Transformer&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#22312;&#20855;&#26377;&#23616;&#37096;&#24402;&#32435;&#20559;&#24046;&#30340;&#22823;&#35268;&#27169;&#28857;&#20113;&#25968;&#25454;&#20013;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#65292;&#23588;&#20854;&#26159;OR&#65286;AND&#26500;&#36896;LSH&#65292;&#22312;&#26680;&#36817;&#20284;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LSH&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;&#65288;HEPT&#65289;&#65292;&#23427;&#23558;E^2LSH&#19982;OR&#65286;AND&#26500;&#36896;&#30456;&#32467;&#21512;&#65292;&#24182;&#24314;&#31435;&#22312;&#24120;&#35268;&#35745;&#31639;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12535v1 Announce Type: new  Abstract: This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR \&amp; AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (\textbf{HEPT}), which combines E$^2$LSH with OR \&amp; AND constructions and is built upon regular computations. HEPT demonstrates remarkabl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;</title><link>https://arxiv.org/abs/2402.12336</link><description>&lt;p&gt;
Robust CLIP: &#23545;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12336
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenFlamingo&#12289;LLaVA&#21644;GPT-4&#20043;&#31867;&#30340;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35270;&#35273;&#27169;&#24577;&#19978;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#29992;&#26469;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#39575;&#29992;&#25143;&#65292;&#22240;&#27492;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#39118;&#38505;&#65292;&#36825;&#20351;&#24471;&#22823;&#22411;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#25104;&#20026;&#19968;&#39033;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#26041;&#26696;&#65292;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#22312;&#25152;&#26377;&#20381;&#36182;&#20110;CLIP&#30340;&#35270;&#35273;&#19979;&#28216;&#20219;&#21153;&#65288;VLMs&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#65289;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#26086;&#26356;&#25442;&#21407;&#22987;&#30340;CLIP&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#20351;&#29992;VLMs&#26102;&#20250;&#21463;&#21040;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#30340;&#25805;&#32437;&#22270;&#20687;&#30340;&#28508;&#22312;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12336v1 Announce Type: cross  Abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP mo
&lt;/p&gt;</description></item><item><title>LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.12061</link><description>&lt;p&gt;
&#25152;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#37117;&#19968;&#26679;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
All Language Models Large and Small
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12061
&lt;/p&gt;
&lt;p&gt;
LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20351;&#29992;&#39640;&#24378;&#24230;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#38477;&#20302;&#37096;&#32626;&#36164;&#28304;&#25104;&#26412;&#21644;&#26356;&#24555;&#25191;&#34892;&#20915;&#31574;&#20219;&#21153;&#31561;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#20248;&#21270;&#32593;&#32476;&#20998;&#24067;&#65288;LONDI&#65289;&#26694;&#26550;&#30340;&#26032;&#22411;&#21363;&#25554;&#21363;&#29992;LM&#26694;&#26550;&#12290; LONDI&#23398;&#20250;&#20102;&#22312;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;LM&#65292;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20351;&#29992;&#20302;&#36164;&#28304;&#30340;LM&#12290; LONDI&#30001;&#20004;&#20010;&#65288;&#31163;&#32447;&#65289;&#31574;&#30053;&#32593;&#32476;&#31995;&#32479;&#12289;&#19968;&#20010;LM&#12289;&#19968;&#20010;&#22823;&#30340;LM&#65288;LLM)&#21644;&#19968;&#20010;&#20351;&#29992;&#24320;&#20851;&#25511;&#21046;&#24555;&#36895;&#23398;&#20064;&#20309;&#26102;&#35843;&#29992;LLM&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#32452;&#25104;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;LLM&#35843;&#29992;&#21644;&#36164;&#28304;&#20351;&#29992;&#26041;&#38754;&#20445;&#25345;&#39044;&#31639;&#32422;&#26463;&#30340;LONDI&#21464;&#20307;&#12290; &#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LONDI&#23398;&#20064;&#28608;&#27963;&#25152;&#38656;&#35299;&#20915;&#20219;&#21153;&#30340;LLM&#30340;&#31995;&#32479;&#29366;&#24577;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;</title><link>https://arxiv.org/abs/2402.11168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Trust Regions for Explanations via Black-Box Probabilistic Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#35299;&#26512;&#20010;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40657;&#30418;&#65288;&#27010;&#29575;&#24615;&#65289;&#35299;&#37322;&#35748;&#35777;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#65292;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#65292;&#19968;&#20010;&#31034;&#20363;&#30340;&#35299;&#37322;&#20197;&#21450;&#19968;&#20010;&#36136;&#37327;&#24230;&#37327;&#65288;&#22914;&#36924;&#30495;&#24230;&#12289;&#31283;&#23450;&#24615;&#65289;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#25214;&#21040;&#26368;&#22823;&#30340;&#36229;&#31435;&#26041;&#20307;&#65288;&#21363; $\ell_{\infty}$ &#29699;&#65289;&#65292;&#20197;&#31034;&#20363;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#24403;&#35299;&#37322;&#34987;&#24212;&#29992;&#20110;&#36229;&#31435;&#26041;&#20307;&#20869;&#30340;&#25152;&#26377;&#31034;&#20363;&#26102;&#65288;&#39640;&#27010;&#29575;&#19979;&#65289;&#36136;&#37327;&#26631;&#20934;&#24471;&#21040;&#28385;&#36275;&#65288;&#27604;&#22914;&#36924;&#30495;&#24230;&#39640;&#20110;&#26576;&#20010;&#20540;&#65289;&#65311;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#20449;&#20219;&#21306;&#22495;&#26377;&#22810;&#37325;&#22909;&#22788;&#65306;i&#65289;&#27934;&#23519;&#27169;&#22411;&#22312;&#19968;&#20010;&#21306;&#22495;&#20869;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#20445;&#35777;&#65307;ii&#65289;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#24471;&#21040;&#20445;&#35777;&#65307;iii&#65289;&#35299;&#37322;&#30340;&#37325;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#37329;&#38065;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#21464;&#25442;&#22120;&#19982;&#21463;&#25351;&#25968;&#20989;&#25968;&#30340;Taylor&#23637;&#24320;&#21551;&#21457;&#30340;&#26680;&#20989;&#25968;&#21644;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#20854;In-Context Learning&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10644</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#23398;&#20064;&#26680;&#20989;&#25968;&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;&#22312;&#19978;&#19979;&#25991;&#27169;&#22411;&#20013;&#34920;&#29616;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Linear Transformers with Learnable Kernel Functions are Better In-Context Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#21464;&#25442;&#22120;&#19982;&#21463;&#25351;&#25968;&#20989;&#25968;&#30340;Taylor&#23637;&#24320;&#21551;&#21457;&#30340;&#26680;&#20989;&#25968;&#21644;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#20854;In-Context Learning&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#27425;&#20108;&#27425;&#20307;&#31995;&#32467;&#26500;&#30340;&#21069;&#27839;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#26680;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;Multi-Query Associative Recall&#20219;&#21153;&#21644;&#25972;&#20307;&#35821;&#35328;&#24314;&#27169;&#36807;&#31243;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10644v1 Announce Type: new  Abstract: Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demon
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10043</link><description>&lt;p&gt;
&#22914;&#20309;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to validate average calibration for machine learning regression tasks ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#27979;&#35797;&#12290;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#26657;&#20934;&#35823;&#24046;&#65288;CE&#65289;&#20272;&#35745;&#20026;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MSE&#65289;&#19982;&#24179;&#22343;&#26041;&#24046;&#65288;MV&#65289;&#25110;&#24179;&#22343;&#24179;&#26041;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#20540;&#12290;&#21478;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#25110;&#32553;&#25918;&#35823;&#24046;&#65288;ZMS&#65289;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#24471;&#20986;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#27491;&#22914;&#26469;&#33258;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25991;&#29486;&#20013;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#25152;&#31034;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;CE&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31163;&#32676;&#19981;&#30830;&#23450;&#24615;&#30340;&#23384;&#22312;&#65292;&#22240;&#27492;&#26080;&#27861;&#21487;&#38752;&#22320;&#29992;&#20110;&#26657;&#20934;&#27979;&#35797;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ZMS&#32479;&#35745;&#37327;&#19981;&#20855;&#26377;&#36825;&#31181;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#23545;&#26465;&#20214;&#26657;&#20934;&#39564;&#35777;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10024</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#23545;&#20110;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Augmented In-Context Learning for Unsupervised Word Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#35774;&#32622;&#20013;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#35789;&#27719;&#32763;&#35793;&#21644;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;(BLI)&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#27809;&#26377;&#31181;&#23376;&#32763;&#35793;&#23545;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#8220;&#20256;&#32479;&#8221;&#30340;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861; (SAIL) &#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;BLI&#65306;&#20174;&#38646;&#26679;&#26412;&#25552;&#31034;&#24320;&#22987;&#65292;SAIL&#36890;&#36807;&#36845;&#20195;&#22320;&#20174;LLM&#20013;&#24341;&#20986;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#30340;&#35789;&#27719;&#32763;&#35793;&#23545;&#65292;&#28982;&#21518;&#22312;ICL&#30340;&#26041;&#24335;&#19979;&#20877;&#27425;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#30340;BLI&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;LLM&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20063;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26144;&#23556;&#30340;&#22522;&#32447;&#12290;&#38500;&#20102;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10024v1 Announce Type: cross  Abstract: Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21457;&#26126;&#30340;&#38750;&#32447;&#24615;&#22823;&#26680;&#24515;&#26497;&#21270;&#30721;&#65292;&#31216;&#20026;DeepPolar&#30721;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#30721;&#21644;&#20256;&#32479;&#30340;&#26497;&#21270;&#30721;&#30456;&#27604;&#65292;DeepPolar&#30721;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#26356;&#22823;&#30340;&#26680;&#24515;&#23610;&#23544;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08864</link><description>&lt;p&gt;
DeepPolar&#65306;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21457;&#26126;&#38750;&#32447;&#24615;&#22823;&#26680;&#24515;&#26497;&#21270;&#30721;
&lt;/p&gt;
&lt;p&gt;
DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21457;&#26126;&#30340;&#38750;&#32447;&#24615;&#22823;&#26680;&#24515;&#26497;&#21270;&#30721;&#65292;&#31216;&#20026;DeepPolar&#30721;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#30721;&#21644;&#20256;&#32479;&#30340;&#26497;&#21270;&#30721;&#30456;&#27604;&#65292;DeepPolar&#30721;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#26356;&#22823;&#30340;&#26680;&#24515;&#23610;&#23544;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#30721;&#20197;Arikan&#30340;&#26497;&#21270;&#26680;&#24515;&#20026;&#22522;&#30784;&#21457;&#23637;&#32780;&#26469;&#65292;&#20195;&#34920;&#20102;&#32534;&#30721;&#29702;&#35770;&#30340;&#19968;&#20010;&#31361;&#30772;&#65292;&#24182;&#19988;&#24050;&#32463;&#25104;&#20026;&#30701;&#21040;&#20013;&#31561;&#22359;&#38271;&#24230;&#21306;&#22495;&#30340;&#26368;&#20808;&#36827;&#32416;&#38169;&#30721;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;Arikan&#30340;&#26680;&#24515;&#26367;&#25442;&#20026;&#26356;&#22823;&#30340;&#26680;&#24515;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26497;&#21270;&#30721;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#26497;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30701;&#21040;&#20013;&#31561;&#22359;&#38271;&#24230;&#21306;&#22495;&#26469;&#35828;&#65292;&#23578;&#26410;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;&#22823;&#26680;&#24515;&#23610;&#23544;&#30340;&#26497;&#21270;&#30721;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#38750;&#32447;&#24615;&#25512;&#24191;&#30340;&#26497;&#21270;&#30721;&#65292;&#23427;&#20855;&#26377;&#25193;&#23637;&#30340;&#26680;&#24515;&#23610;&#23544;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;DeepPolar&#30721;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DeepPolar&#30721;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#26356;&#22823;&#30340;&#26680;&#24515;&#23610;&#23544;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#31070;&#32463;&#30721;&#21644;&#20256;&#32479;&#30340;&#26497;&#21270;&#30721;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08864v1 Announce Type: cross Abstract: Polar codes, developed on the foundation of Arikan's polarization kernel, represent a breakthrough in coding theory and have emerged as the state-of-the-art error-correction-code in short-to-medium block length regimes. Importantly, recent research has indicated that the reliability of polar codes can be further enhanced by substituting Arikan's kernel with a larger one, leading to a faster polarization. However, for short-to-medium block length regimes, the development of polar codes that effectively employ large kernel sizes has not yet been realized. In this paper, we explore a novel, non-linear generalization of polar codes with an expanded kernel size, which we call DeepPolar codes. Our results show that DeepPolar codes effectively utilize the benefits of larger kernel size, resulting in enhanced reliability compared to both the existing neural codes and conventional polar codes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08848</link><description>&lt;p&gt;
&#28151;&#21512;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#27169;&#20223;&#23398;&#20064;&#26469;&#35828;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36739;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#27604;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#22320;&#22788;&#29702;&#38169;&#35823;&#32047;&#31215;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#35201;&#27714;&#23398;&#20064;&#32773;&#21453;&#22797;&#35299;&#20915;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35745;&#31639;&#24448;&#24448;&#20250;&#28010;&#36153;&#22312;&#25628;&#32034;&#38750;&#24120;&#19981;&#30456;&#20284;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#31574;&#30053;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;-&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;-&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#19978;&#65292;&#19987;&#23478;&#25968;&#25454;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#23398;&#20064;&#32773;&#19987;&#27880;&#20110;&#33391;&#22909;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#24378;&#31574;&#30053;&#25152;&#38656;&#30340;&#25506;&#32034;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#23558;&#23398;&#20064;&#32773;&#37325;&#32622;&#21040;&#29615;&#22659;&#20013;&#30340;&#20219;&#24847;&#29366;&#24577;&#65292;&#36825;&#26159;&#20197;&#21069;&#22312;&#39640;&#25928;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08848v1 Announce Type: cross Abstract: The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20174;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#20316;&#32773;&#36890;&#36807;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#24402;&#32435;&#20559;&#24046;&#20998;&#27495;&#65292;&#20197;&#21450;&#35780;&#35770;&#27169;&#22411;&#20013;&#30340;&#27785;&#30561;&#31070;&#32463;&#20803;&#21644;&#27963;&#36291;&#31070;&#32463;&#20803;&#23545;&#25239;&#36807;&#24230;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08552</link><description>&lt;p&gt;
&#38754;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65306;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20174;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#20316;&#32773;&#36890;&#36807;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#24402;&#32435;&#20559;&#24046;&#20998;&#27495;&#65292;&#20197;&#21450;&#35780;&#35770;&#27169;&#22411;&#20013;&#30340;&#27785;&#30561;&#31070;&#32463;&#20803;&#21644;&#27963;&#36291;&#31070;&#32463;&#20803;&#23545;&#25239;&#36807;&#24230;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#34701;&#21512;&#26159;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#12290;&#34429;&#28982;&#36890;&#36807;&#20248;&#21270;&#19979;&#28216;&#22870;&#21169;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#26102;&#36807;&#24230;&#20248;&#21270;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#22320;&#38754;&#30495;&#23454;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35282;&#24230;&#26469;&#23545;&#20184;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#26102;&#38388;&#24402;&#32435;&#20559;&#24046;&#23384;&#22312;&#20998;&#27495;&#65292;&#36825;&#21487;&#33021;&#26159;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#20010;&#28508;&#22312;&#26469;&#28304;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20196;&#20154;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;&#25105;&#20204;&#35780;&#35770;&#27169;&#22411;&#20013;&#30340;&#27785;&#30561;&#31070;&#32463;&#20803;&#20805;&#24403;&#19968;&#31181;&#23545;&#25239;&#36807;&#24230;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#25163;&#27573;&#65292;&#32780;&#27963;&#36291;&#31070;&#32463;&#20803;&#21017;&#21453;&#26144;&#20102;&#36825;&#20010;&#35774;&#32622;&#20013;&#30340;&#20248;&#20808;&#20559;&#24046;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#35780;&#35770;&#27169;&#22411;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#26102;&#38388;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Re
&lt;/p&gt;</description></item><item><title>DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07891</link><description>&lt;p&gt;
&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Model Selection for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07891
&lt;/p&gt;
&lt;p&gt;
DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32473;&#23450;&#30446;&#26631;&#20219;&#21153;&#30340;&#27169;&#22411;&#36873;&#25321;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#38656;&#35201;&#23545;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#30340;&#36136;&#37327;&#36827;&#34892;&#24191;&#27867;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DiffUse&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#20505;&#36873;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;DiffUse&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#22312;&#35780;&#20272;&#20013;&#23453;&#36149;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;DiffUse&#36890;&#36807;&#32858;&#31867;&#34920;&#31034;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#26234;&#33021;&#36873;&#25321;&#23454;&#20363;&#12290;&#22240;&#27492;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#20123;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#20363;&#23376;&#26469;&#36827;&#34892;&#20559;&#22909;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#21160;&#24577;&#30830;&#23450;&#35201;&#27880;&#37322;&#30340;&#23454;&#20363;&#25968;&#37327;&#12290;&#36890;&#36807;&#23545;&#25968;&#30334;&#20010;&#27169;&#22411;&#23545;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DiffUse&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#65292;&#26368;&#22810;&#21487;&#20943;&#23569;75%&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#35780;&#20272;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.06578</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Universality of Coupling-based Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#65288;&#22914;RealNVP&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23613;&#31649;&#32806;&#21512;&#27969;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#24456;&#26222;&#36941;&#65292;&#20294;&#30001;&#20110;&#20854;&#21463;&#38480;&#30340;&#26550;&#26500;&#65292;&#23545;&#20110;&#32806;&#21512;&#27969;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#23450;&#29702;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20351;&#29992;&#20219;&#24847;&#30149;&#24577;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#32467;&#26500;&#26412;&#36136;&#19978;&#23548;&#33268;&#20307;&#31215;&#20445;&#25345;&#27969;&#65292;&#36825;&#26159;&#19968;&#20010;&#38480;&#21046;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#26412;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#26222;&#36866;&#24615;&#23450;&#29702;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#24037;&#20316;&#30340;&#20960;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#26222;&#36941;&#32463;&#39564;&#65292;&#24182;&#20026;&#36873;&#25321;&#32806;&#21512;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#32454;&#33268;&#20837;&#24494;&#30340;&#35266;&#28857;&#65292;&#22635;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel theoretical framework for understanding the expressive power of coupling-based normalizing flows such as RealNVP. Despite their prevalence in scientific applications, a comprehensive understanding of coupling flows remains elusive due to their restricted architectures. Existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. Additionally, we demonstrate that these constructions inherently lead to volume-preserving flows, a property which we show to be a fundamental constraint for expressivity. We propose a new distributional universality theorem for coupling-based normalizing flows, which overcomes several limitations of prior work. Our results support the general wisdom that the coupling architecture is expressive and provide a nuanced view for choosing the expressivity of coupling functions, bridging a gap between empirical results and theoretical understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21457;&#29616;&#20102;&#20854;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#19981;&#21516;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#32447;&#24615;Transformer&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#21450;&#27491;&#20132;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05787</link><description>&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Transformers perform In-Context Autoregressive Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21457;&#29616;&#20102;&#20854;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#19981;&#21516;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#32447;&#24615;Transformer&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#21450;&#27491;&#20132;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#21407;&#22240;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#31616;&#21333;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#21518;&#30340;Transformer&#22914;&#20309;&#36890;&#36807;&#39318;&#20808;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;W&#65292;&#28982;&#21518;&#24212;&#29992;&#39044;&#27979;&#26144;&#23556;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#32467;&#26524;&#20026;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;W&#26159;&#20132;&#25442;&#27491;&#20132;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#35757;&#32451;&#21518;&#30340;&#21333;&#23618;&#32447;&#24615;Transformer&#22312;&#32771;&#34385;&#25193;&#23637;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#20869;&#37096;&#30446;&#26631;&#20989;&#25968;&#12290;&#24403;&#26631;&#35760;&#27809;&#26377;&#25193;&#23637;&#26102;&#65292;&#25105;&#20204;&#23545;&#20110;&#19968;&#20010;&#21333;&#23618;&#23545;&#35282;&#32447;&#32447;&#24615;&#22810;&#22836;Transformer&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22836;&#37096;&#20043;&#38388;&#30340;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved state-of-the-art performance in language modeling tasks. However, the reasons behind their tremendous success are still unclear. In this paper, towards a better understanding, we train a Transformer model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained Transformer predicts the next token by first learning $W$ in-context, then applying a prediction mapping. We call the resulting procedure in-context autoregressive learning. More precisely, focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer linear Transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens. When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head Transformer. Importantly, we exhibit orthogonality between heads and show that posi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;ROC&#26354;&#32447;&#65292;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05400</link><description>&lt;p&gt;
&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;ROC&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05400
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;ROC&#26354;&#32447;&#65292;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20108;&#20803;&#20998;&#31867;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#22312;&#20005;&#37325;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#31867;&#22120;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#25110;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#22312;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#24433;&#21709;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#22312;&#22810;&#31867;&#21035;&#24773;&#20917;&#19979;&#25972;&#20307;&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#20540;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#38382;&#39064;&#19978;&#20197;ROC&#26354;&#32447;&#20026;&#25351;&#26631;&#30340;&#24615;&#33021;&#39640;&#24230;&#21464;&#21270;&#12290;&#20026;&#20102;&#38477;&#20302;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#65292;&#35757;&#32451;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#19978;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#19978;&#24212;&#29992;&#25439;&#22833;&#26465;&#20214;&#35757;&#32451;&#65288;Loss Conditional Training&#65292;LCT&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#30340;&#39640;&#27491;&#26354;&#29575;&#19982;&#21487;&#35757;&#32451;&#24615;&#24378;&#30340;&#21021;&#22987;&#28857;&#30456;&#20851;&#12290;&#19982;&#20808;&#21069;&#30340;&#35266;&#24565;&#30456;&#21453;&#65292;&#27491;&#26354;&#29575;&#24182;&#19981;&#20165;&#20165;&#19982;&#21021;&#22987;&#21270;&#33539;&#25968;&#30456;&#20851;&#65292;&#32780;&#19982;&#27169;&#22411;&#32622;&#20449;&#24230;&#12289;&#21021;&#22987;&#25439;&#22833;&#36739;&#20302;&#20197;&#21450;&#19968;&#31181;&#20197;&#21069;&#26410;&#30693;&#30340;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.03579</link><description>&lt;p&gt;
&#35299;&#26500;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#8220;&#37329;&#21457;&#22899;&#23401;&#21306;&#8221;
&lt;/p&gt;
&lt;p&gt;
Deconstructing the Goldilocks Zone of Neural Network Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03579
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#30340;&#39640;&#27491;&#26354;&#29575;&#19982;&#21487;&#35757;&#32451;&#24615;&#24378;&#30340;&#21021;&#22987;&#28857;&#30456;&#20851;&#12290;&#19982;&#20808;&#21069;&#30340;&#35266;&#24565;&#30456;&#21453;&#65292;&#27491;&#26354;&#29575;&#24182;&#19981;&#20165;&#20165;&#19982;&#21021;&#22987;&#21270;&#33539;&#25968;&#30456;&#20851;&#65292;&#32780;&#19982;&#27169;&#22411;&#32622;&#20449;&#24230;&#12289;&#21021;&#22987;&#25439;&#22833;&#36739;&#20302;&#20197;&#21450;&#19968;&#31181;&#20197;&#21069;&#26410;&#30693;&#30340;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25439;&#22833;&#30340;&#20108;&#38454;&#24615;&#36136;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#21160;&#21147;&#23398;&#26377;&#30528;&#24040;&#22823;&#24433;&#21709;&#12290;Fort&#65286;Scherlis&#65288;2019&#65289;&#21457;&#29616;&#65292;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#30340;&#39640;&#27491;&#26354;&#29575;&#21644;&#23616;&#37096;&#20984;&#24615;&#19982;&#20301;&#20110;&#34987;&#31216;&#20026;&#8220;&#37329;&#21457;&#22899;&#23401;&#21306;&#8221;&#30340;&#39640;&#24230;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#28857;&#30456;&#20851;&#12290;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#21518;&#32493;&#30740;&#31350;&#28041;&#21450;&#35813;&#20851;&#31995;&#65292;&#22240;&#27492;&#20854;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22343;&#36136;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#37329;&#21457;&#22899;&#23401;&#21306;&#8221;&#36827;&#34892;&#20102;&#20005;&#26684;&#32780;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23548;&#33268;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#38750;&#38646;&#27491;&#26354;&#29575;&#30340;&#22522;&#26412;&#26465;&#20214;&#65292;&#24182;&#35748;&#20026;&#23427;&#19982;&#21021;&#22987;&#21270;&#33539;&#25968;&#21482;&#26159;&#20598;&#28982;&#30456;&#20851;&#65292;&#19982;&#20808;&#21069;&#30340;&#20449;&#24565;&#30456;&#21453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#39640;&#27491;&#26354;&#29575;&#19982;&#27169;&#22411;&#32622;&#20449;&#24230;&#12289;&#21021;&#22987;&#25439;&#22833;&#36739;&#20302;&#20197;&#21450;&#19968;&#31181;&#20197;&#21069;&#26410;&#30693;&#30340;&#28040;&#22833;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#26799;&#24230;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20102;&#35299;&#27491;&#26354;&#29575;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#23398;&#20064;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#39318;&#20808;&#26159;&#36890;&#36807;&#27867;&#21270;&#35823;&#24046;&#21644;&#39640;&#25928;&#29575;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#26631;&#20934;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models. Fort &amp; Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the "Goldilocks zone". Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained. In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks. In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs. Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient. To understand the importance of positive curvature for trainability of deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03479</link><description>&lt;p&gt;
ICED: &#36890;&#36807;&#19978;&#19979;&#25991;&#29615;&#22659;&#35774;&#35745;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#32570;&#20047;&#25104;&#21151;&#22320;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#30340;&#37319;&#26679;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#25512;&#24191;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20849;&#20139;&#22522;&#26412;&#23618;&#30340;&#28145;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#26681;&#25454;&#20854;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35757;&#32451;&#32423;&#21035;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20026;&#26576;&#20123;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20855;&#26377;&#26356;&#22810;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#23454;&#20363;&#65292;&#20174;&#32780;&#24433;&#21709;&#20195;&#29702;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
&lt;/p&gt;</description></item><item><title>EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02425</link><description>&lt;p&gt;
EuLagNet: &#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#27431;&#25289;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02425
&lt;/p&gt;
&lt;p&gt;
EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#23545;&#27668;&#35937;&#23398;&#12289;&#28023;&#27915;&#23398;&#21644;&#31354;&#27668;&#21160;&#21147;&#23398;&#31561;&#24191;&#27867;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#36890;&#24120;&#20174;&#27431;&#25289;&#35282;&#24230;&#35266;&#23519;&#65292;&#20854;&#27963;&#36291;&#21644;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#22312;&#38745;&#27490;&#30340;&#32593;&#26684;&#20013;&#20005;&#37325;&#34987;&#25513;&#30422;&#21644;&#28151;&#28102;&#65292;&#32473;&#39044;&#27979;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#20026;&#23548;&#21521;&#30340;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#21452;&#37325;&#36882;&#24402;&#32593;&#32476;&#65288;EuLagNet&#65289;&#65292;&#36890;&#36807;&#36319;&#36394;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#24182;&#38543;&#26102;&#38388;&#31215;&#32047;&#21160;&#21147;&#23398;&#20449;&#24687;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;EuLag&#22359;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#26102;&#21051;&#21644;&#23610;&#24230;&#19978;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#27431;&#25289;&#21644;&#25289;&#26684;&#26391;&#26085;&#29305;&#24449;&#65292;&#20854;&#20013;&#36319;&#36394;&#31890;&#23376;&#30340;&#36816;&#21160;&#26159;&#20174;&#27431;&#25289;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#31215;&#32047;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#34987;&#32435;&#20837;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#65292;&#22686;&#24378;&#20102;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#24494;&#35843;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.02347</link><description>&lt;p&gt;
Riemannian Preconditioned LoRA&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#65292;&#22686;&#24378;&#20102;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#24494;&#35843;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#26469;&#25552;&#21319;&#20854;&#20248;&#21270;&#27493;&#39588;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;$r\times r$&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;&#20854;&#20013;$r$&#26159;LoRA&#30340;&#31209;&#12290;&#36825;&#20010;&#39044;&#26465;&#20214;&#22120;&#23545;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20195;&#30721;&#21482;&#38656;&#35201;&#20570;&#20986;&#24456;&#23567;&#30340;&#25913;&#21464;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#23384;&#20648;&#21644;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#36807;&#31243;&#23545;&#20110;&#23398;&#20064;&#29575;&#31561;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#39044;&#26465;&#20214;&#22120;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#24494;&#35843;&#20004;&#23618;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is 
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01922</link><description>&lt;p&gt;
&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning from Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#30528;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#24369;&#30417;&#30563;&#30340;&#21508;&#31181;&#22330;&#26223;&#21644;&#30001;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;GLWS&#65289;&#12290;GLWS&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#30340;&#20844;&#24335;&#65292;&#28789;&#27963;&#22320;&#36866;&#24212;&#20102;&#21508;&#31181;&#24369;&#30417;&#30563;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#12289;&#32858;&#21512;&#32479;&#35745;&#12289;&#25104;&#23545;&#35266;&#23519;&#21644;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;NFA&#65289;&#20197;&#21450;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;EM&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#24120;&#25152;&#38656;&#30340;&#20108;&#27425;&#25110;&#38454;&#20056;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;&#22240;&#27492;&#65292;&#20174;&#20219;&#24847;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20102;&#23545;&#23427;&#20204;&#36827;&#34892;NFA&#24314;&#27169;&#12290;GLWS&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;+
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00592</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Partial-Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36935;&#21040;&#26631;&#35760;&#27169;&#31946;&#30340;&#25968;&#25454;&#65292;&#21363;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#20026;&#30456;&#21516;&#26679;&#26412;&#20998;&#37197;&#20102;&#20914;&#31361;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20801;&#35768;&#22312;&#36825;&#31181;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24050;&#32463;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#21463;&#21040;&#38169;&#35823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;Dempster-Shafer&#29702;&#35770;&#12290;&#23545;&#20154;&#24037;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#39118;&#38505;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.00396</link><description>&lt;p&gt;
LLMs&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00396
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#39640;&#25928;&#25506;&#32034;&#22312;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#31243;&#24207;&#22312;&#25910;&#21040;&#21453;&#39304;&#26102;&#23558;&#22870;&#21169;&#27169;&#22411;&#25311;&#21512;&#21040;&#26597;&#35810;&#19978;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#21452;Thompson&#37319;&#26679;&#29983;&#25104;&#26597;&#35810;&#65292;&#19981;&#30830;&#23450;&#24615;&#30001;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#25928;&#25506;&#32034;&#20351;&#24471;&#24615;&#33021;&#27700;&#24179;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#26597;&#35810;&#19979;&#36798;&#21040;&#36739;&#39640;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#24615;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#21487;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#65292;&#21516;&#26102;&#25512;&#24191;&#21040;&#20102;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2312.06635</link><description>&lt;p&gt;
&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#35757;&#32451;&#30340;&#38376;&#25511;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gated Linear Attention Transformers with Hardware-Efficient Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#24615;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#21487;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#65292;&#21516;&#26102;&#25512;&#24191;&#21040;&#20102;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#21464;&#21387;&#22120;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#30340;&#24182;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#20855;&#26377;2D&#65288;&#30697;&#38453;&#20540;&#65289;&#38544;&#34255;&#29366;&#24577;&#30340;RNN&#65292;&#20174;&#32780;&#20139;&#21463;&#32447;&#24615;&#26102;&#38388;&#25512;&#26029;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#26222;&#36890;softmax&#27880;&#24847;&#21147;&#12290;&#32780;&#19988;&#65292;&#24403;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#32570;&#20047;I/O&#24863;&#30693;&#24615;&#65292;&#22240;&#27492;&#27604;&#39640;&#24230;&#20248;&#21270;&#30340;softmax&#27880;&#24847;&#21147;&#23454;&#29616;&#26356;&#24930;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#30828;&#20214;&#39640;&#25928;&#31639;&#27861;&#65292;&#23427;&#22312;&#20869;&#23384;&#31227;&#21160;&#21644;&#21487;&#24182;&#34892;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#20013;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#23454;&#29616;&#65292;&#34987;&#31216;&#20026;FLASHLINEARATTENTION&#65292;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#65288;&#20363;&#22914;&#65292;1K&#65289;&#19979;&#65292;&#21363;&#20351;&#20316;&#20026;&#21333;&#29420;&#30340;&#23618;&#20063;&#27604;FLASHATTENTION-2(Dao, 2023)&#26356;&#24555;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#25512;&#24191;&#21040;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;&#24403;&#29992;&#20316;&#21464;&#25442;&#22120;&#20013;&#26631;&#20934;&#27880;&#24847;&#21147;&#23618;&#30340;&#26367;&#20195;&#26102;&#65292;&#20135;&#29983;&#30340;&#38376;&#25511;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06635v4 Announce Type: replace-cross  Abstract: Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2(Dao, 2023) as a standalone layer even at short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#26368;&#30701;&#36317;&#31163;&#23545;&#33410;&#28857;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;RNN&#23545;&#36339;&#20195;&#34920;&#30340;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#29992;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20449;&#24687;&#21033;&#29992;&#19978;&#30340;&#22256;&#38590;&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.01538</link><description>&lt;p&gt;
&#36882;&#24402;&#36317;&#31163;&#36807;&#28388;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Recurrent Distance Filtering for Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#26368;&#30701;&#36317;&#31163;&#23545;&#33410;&#28857;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;RNN&#23545;&#36339;&#20195;&#34920;&#30340;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#29992;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20449;&#24687;&#21033;&#29992;&#19978;&#30340;&#22256;&#38590;&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36845;&#20195;&#19968;&#36339;&#20449;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#25928;&#21033;&#29992;&#36828;&#36317;&#31163;&#33410;&#28857;&#30340;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#22270;&#21464;&#25442;&#22120;&#20801;&#35768;&#27599;&#20010;&#33410;&#28857;&#30452;&#25509;&#20851;&#27880;&#25152;&#26377;&#20854;&#20182;&#33410;&#28857;&#65292;&#20294;&#32570;&#20047;&#22270;&#30340;&#24402;&#32435;&#20559;&#24046;&#24182;&#19988;&#24517;&#39035;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#20301;&#32622;&#32534;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#33258;&#20110;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;&#25552;&#20379;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#26041;&#38754;&#30340;&#26368;&#26032;&#31361;&#30772;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#30446;&#26631;&#33410;&#28857;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#30446;&#26631;&#33410;&#28857;&#21040;&#20854;&#20182;&#33410;&#28857;&#30340;&#26368;&#30701;&#36317;&#31163;&#26469;&#32858;&#21512;&#20854;&#20182;&#33410;&#28857;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;RNN&#23545;&#36339;&#20195;&#34920;&#30340;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#32447;&#24615;RNN&#20197;&#29305;&#23450;&#23545;&#35282;&#24418;&#24335;&#21442;&#25968;&#21270;&#65292;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#38271;&#36317;&#31163;&#20449;&#21495;&#20256;&#25773;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#36275;&#22815;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#32534;&#30721;&#37051;&#23621;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is highly competitive compared with 
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#35266;&#27979;&#24314;&#27169;&#21644;&#22870;&#21169;&#24314;&#27169;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#36890;&#36807;&#32531;&#35299;&#35266;&#27979;&#24314;&#27169;&#25110;&#22870;&#21169;&#24314;&#27169;&#30340;&#21344;&#29992;&#20248;&#21183;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.00344</link><description>&lt;p&gt;
HarmonyDream: &#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#21327;&#35843;&#21270;
&lt;/p&gt;
&lt;p&gt;
HarmonyDream: Task Harmonization Inside World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00344
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#35266;&#27979;&#24314;&#27169;&#21644;&#22870;&#21169;&#24314;&#27169;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#36890;&#36807;&#32531;&#35299;&#35266;&#27979;&#24314;&#27169;&#25110;&#22870;&#21169;&#24314;&#27169;&#30340;&#21344;&#29992;&#20248;&#21183;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#30446;&#26631;&#65292;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#35266;&#27979;&#24314;&#27169;&#21644;&#22870;&#21169;&#24314;&#27169;&#20004;&#20010;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#28145;&#20837;&#29702;&#35299;&#20102;&#27599;&#20010;&#20219;&#21153;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#32531;&#35299;&#35266;&#27979;&#24314;&#27169;&#25110;&#22870;&#21169;&#24314;&#27169;&#30340;&#21344;&#29992;&#20248;&#21183;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#34429;&#28982;&#24403;&#21069;&#30340;&#26174;&#24335;MBRL&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35266;&#27979;&#27169;&#22411;&#24674;&#22797;&#29615;&#22659;&#30340;&#20016;&#23500;&#32454;&#33410;&#65292;&#20294;&#30001;&#20110;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#23481;&#37327;&#30340;&#38480;&#21046;&#65292;&#36825;&#26159;&#22256;&#38590;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38544;&#24335;MBRL&#20013;&#22870;&#21169;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#25797;&#38271;&#23398;&#20064;&#32039;&#20945;&#30340;&#20219;&#21153;&#23548;&#21521;&#21160;&#24577;&#65292;&#20294;&#22312;&#27809;&#26377;&#26356;&#20016;&#23500;&#30340;&#23398;&#20064;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#21512;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#35266;&#28857;&#21644;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of sample-efficient MBRL by mitigating the domination of either observation or reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment via observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating implicit MBRL and adept at learning compact task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Motivated by these insights and discoveries, we propose a s
&lt;/p&gt;</description></item><item><title>UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2307.16375</link><description>&lt;p&gt;
UniAP: &#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#33258;&#21160;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16375
&lt;/p&gt;
&lt;p&gt;
UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#24120;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#25163;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#24182;&#19988;&#28789;&#27963;&#24615;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#24182;&#34892;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#23384;&#22312;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20250;&#21516;&#26102;&#20248;&#21270;&#36328;&#23618;&#24182;&#34892;&#21270;&#21644;&#20869;&#23618;&#24182;&#34892;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniAP&#30340;&#26032;&#22411;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;UniAP&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#20197;&#27714;&#24471;&#26368;&#20248;&#35299;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26368;&#22810;1.71&#20493;&#65292;&#24182;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 1.71$\times$ in throughput and reduces strategy optimizat
&lt;/p&gt;</description></item><item><title>cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.16356</link><description>&lt;p&gt;
cDVGAN: &#19968;&#20010;&#28789;&#27963;&#30340;&#27169;&#22411;&#29992;&#20110;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25925;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation. (arXiv:2401.16356v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16356
&lt;/p&gt;
&lt;p&gt;
cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#30495;&#23454;&#30340;&#26102;&#38388;&#22495;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#35266;&#27979;&#21644;GW&#25506;&#27979;&#22120;&#25925;&#38556;&#21487;&#20197;&#24110;&#21161;&#25512;&#36827;GW&#25968;&#25454;&#20998;&#26512;&#12290;&#27169;&#25311;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#29992;&#20110;&#20449;&#21495;&#25628;&#32034;&#30340;&#25968;&#25454;&#38598;&#65292;&#24179;&#34913;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39564;&#35777;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cDVGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#30340;&#26032;&#22411;&#26465;&#20214;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#34920;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#30340;&#22810;&#31181;&#31867;&#21035;&#30340;&#26102;&#38388;&#22495;&#35266;&#27979;&#12290;cDVGAN&#36824;&#21487;&#20197;&#36890;&#36807;&#22312;&#26465;&#20214;&#31867;&#21035;&#21521;&#37327;&#20013;&#36827;&#34892;&#25554;&#20540;&#29983;&#25104;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24191;&#20041;&#28151;&#21512;&#26679;&#26412;&#12290;cDVGAN&#22312;&#20856;&#22411;&#30340;GANs&#30340;&#20108;&#20154;&#23545;&#25239;&#21338;&#24328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#21442;&#19982;&#32773;&#65292;&#20854;&#20013;&#19968;&#20010;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#25552;&#20379;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.09886</link><description>&lt;p&gt;
&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19979;&#19968;&#20195;&#32593;&#32476;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network. (arXiv:2401.09886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#32531;&#23384;&#26159;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36171;&#20104;&#23567;&#22411;&#22522;&#31449;&#65288;SBS&#65289;&#20013;&#30340;&#32531;&#23384;&#21333;&#20803;&#36171;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#33719;&#21462;&#24050;&#22312;SBS&#20013;&#39044;&#32531;&#23384;&#30340;&#29992;&#25143;&#35831;&#27714;&#20869;&#23481;&#12290;&#23545;&#20110;SBS&#26469;&#35828;&#65292;&#36890;&#36807;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#38750;&#24120;&#20851;&#38190;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#20010;&#20154;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#20294;&#26159;UE&#20043;&#38388;&#30340;&#25968;&#25454;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20026;&#27599;&#20010;UE&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#20197;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30456;&#37051;SBS&#20043;&#38388;&#21487;&#20197;&#20849;&#20139;&#32531;&#23384;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#22312;&#19981;&#21516;&#30340;SBS&#20013;&#32531;&#23384;&#39044;&#27979;&#21040;&#30340;&#28909;&#38376;&#20869;&#23481;&#21487;&#33021;&#20250;&#24433;&#21709;&#33719;&#21462;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#30340;&#20301;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65288;GLOW&#65289;&#65292;&#32467;&#21512;&#23494;&#24230;&#27604;&#29575;&#27169;&#22411;&#21644;&#20540;&#20989;&#25968;&#27169;&#22411;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29942;&#39048;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#27809;&#26377;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.09681</link><description>&lt;p&gt;
&#21033;&#29992;&#23494;&#24230;&#27604;&#29575;&#36827;&#34892;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Harnessing Density Ratios for Online Reinforcement Learning. (arXiv:2401.09681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65288;GLOW&#65289;&#65292;&#32467;&#21512;&#23494;&#24230;&#27604;&#29575;&#27169;&#22411;&#21644;&#20540;&#20989;&#25968;&#27169;&#22411;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29942;&#39048;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#27809;&#26377;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#21457;&#23637;&#26041;&#21521;&#19968;&#30452;&#26159;&#24179;&#34892;&#30340;&#65292;&#20294;&#23427;&#20204;&#24320;&#22987;&#26174;&#31034;&#20986;&#21487;&#33021;&#32479;&#19968;&#30340;&#36857;&#35937;&#65292;&#20854;&#20013;&#19968;&#20010;&#29615;&#22659;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#25216;&#26415;&#36890;&#24120;&#22312;&#21478;&#19968;&#20010;&#29615;&#22659;&#20013;&#20855;&#26377;&#33258;&#28982;&#30340;&#23545;&#24212;&#29289;&#12290;&#28982;&#32780;&#65292;&#23494;&#24230;&#27604;&#29575;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#33539;&#24335;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24456;&#23569;&#20986;&#29616;&#65292;&#20063;&#35768;&#26377;&#20805;&#36275;&#30340;&#29702;&#30001;&#65306;&#23494;&#24230;&#27604;&#29575;&#30340;&#23384;&#22312;&#21644;&#26377;&#30028;&#24615;&#20381;&#36182;&#20110;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#24615;&#65292;&#20294;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#22312;&#27809;&#26377;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126; - &#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159; - &#22522;&#20110;&#23494;&#24230;&#27604;&#29575;&#30340;&#31639;&#27861;&#20855;&#26377;&#22312;&#32447;&#23545;&#24212;&#29289;&#12290;&#20551;&#23450;&#21482;&#23384;&#22312;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#24615;&#20998;&#24067;&#65292;&#21363;&#32467;&#26500;&#26465;&#20214;&#24050;&#30693;&#20026;coverability&#65288;Xie&#31561;&#65292;2023&#65289;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65288;GLOW&#65289;&#65292;&#23427;&#21033;&#29992;&#23494;&#24230;&#27604;&#29575;&#21487;&#23454;&#29616;&#24615;&#21644;&#20540;&#20989;&#25968;&#21487;&#23454;&#29616;&#24615;&#26469;&#36827;&#34892;&#39640;&#25928;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-effici
&lt;/p&gt;</description></item><item><title>&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06751</link><description>&lt;p&gt;
Easy Training Data&#23545;&#20110;&#22256;&#38590;&#20219;&#21153;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06751
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#22312;&#23450;&#20041;&#19978;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#22312;&#22256;&#38590;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65311;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#21487;&#25193;&#23637;&#30417;&#30563;&#38382;&#39064;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#36807;&#31243;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#35770;&#65292;&#21363;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#30456;&#23545;&#33391;&#22909;&#65292;&#29978;&#33267;&#34920;&#29616;&#24471;&#21644;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#8220;oracle&#8221;&#27169;&#22411;&#19968;&#26679;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#32447;&#24615;&#20998;&#31867;&#22120;&#22836;&#21644;QLoRA&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#20174;&#26131;&#21040;&#38590;&#30340;&#27867;&#21270;&#65292;&#38024;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#38590;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20845;&#20010;&#32463;&#39564;&#22810;&#26679;&#30340;&#20154;&#31867;&#38590;&#24230;&#24230;&#37327;&#65288;&#22914;&#24180;&#32423;&#27700;&#24179;&#65289;&#21644;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#65288;&#22522;&#20110;&#25439;&#22833;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#20851;&#24515;&#27169;&#22411;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#25910;&#38598;&#24182;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#65292;&#22240;&#20026;&#22256;&#38590;&#25968;&#25454;&#36890;&#24120;&#26356;&#22024;&#26434;&#21644;&#26114;&#36149;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as "oracle" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06469</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;ICL: &#26377;&#25928;&#65292;&#39640;&#25928;&#19988;&#26080;&#24207;&#22320;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#25935;&#24863;&#30340;&#21407;&#22240;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;Batch-ICL&#65292;&#19968;&#31181;&#29992;&#20110;ICL&#30340;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#19982;&#26631;&#20934;&#30340;N-shot&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;Batch-ICL&#20351;&#29992;N&#20010;&#21333;&#29420;&#30340;1-shot&#21069;&#21521;&#35745;&#31639;&#65292;&#24182;&#32858;&#21512;&#24471;&#21040;&#30340;&#20803;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#32858;&#21512;&#30340;&#20803;&#26799;&#24230;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#20197;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#36825;&#31181;&#25209;&#22788;&#29702;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#30340;&#39034;&#24207;&#26080;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Batch-ICL&#19968;&#33268;&#20248;&#20110;&#22823;&#22810;&#25968;&#31034;&#20363;&#24207;&#21015;&#30340;&#25490;&#21015;&#26041;&#24335;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;Batch-ICL&#30340;&#19968;&#31181;&#26032;&#39062;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;"epochs"&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#31070;&#32463;&#26631;&#35760;&#26102;&#24207;&#28857;&#36807;&#31243;&#27169;&#22411;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23481;&#39044;&#27979;&#26694;&#26550;&#29983;&#25104;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#32852;&#21512;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#36739;&#22909;&#30340;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.04612</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#31070;&#32463;&#26631;&#35760;&#26102;&#24207;&#28857;&#36807;&#31243;&#30340;&#26080;&#20559;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes. (arXiv:2401.04612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#31070;&#32463;&#26631;&#35760;&#26102;&#24207;&#28857;&#36807;&#31243;&#27169;&#22411;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23481;&#39044;&#27979;&#26694;&#26550;&#29983;&#25104;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#32852;&#21512;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#36739;&#22909;&#30340;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#38388;&#38548;&#35266;&#27979;&#21040;&#30340;&#26631;&#35760;&#20107;&#20214;&#24207;&#21015;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26102;&#24207;&#28857;&#36807;&#31243;&#65288;Temporal Point Processes&#65292;TPPs&#65289;&#25552;&#20379;&#20102;&#24314;&#27169;&#36825;&#20123;&#24207;&#21015;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#21487;&#20197;&#36827;&#34892;&#35832;&#22914;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#21040;&#36798;&#26102;&#38388;&#21644;&#30456;&#20851;&#26631;&#35760;&#31561;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#33539;&#38169;&#35823;&#25110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#27010;&#29575;&#27169;&#22411;&#21487;&#33021;&#23545;&#26410;&#30693;&#30340;&#30495;&#23454;&#22522;&#30784;&#36807;&#31243;&#25552;&#20379;&#36739;&#24046;&#30340;&#36817;&#20284;&#65292;&#20174;&#20013;&#25552;&#21462;&#30340;&#39044;&#27979;&#21306;&#22495;&#21487;&#33021;&#26159;&#23545;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21487;&#38752;&#20272;&#35745;&#12290;&#26412;&#25991;&#22522;&#20110;&#30456;&#23481;&#39044;&#27979;&#26694;&#26550;&#65292;&#20026;&#31070;&#32463;TPP&#27169;&#22411;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#29983;&#25104;&#21040;&#36798;&#26102;&#38388;&#21644;&#26631;&#35760;&#30340;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#32852;&#21512;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21516;&#26102;&#22788;&#29702;&#20005;&#26684;&#27491;&#30340;&#36830;&#32493;&#21709;&#24212;&#21644;&#20998;&#31867;&#21709;&#24212;&#65292;&#32780;&#19981;&#38656;&#35201;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields. Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark. However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty. This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction. A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee. A key challenge is to handle both a strictly positive, continuous response and a categorical response, withou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26680;Fisher-Rao&#27969;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21333;&#20301;&#26102;&#38388;&#20869;&#20174;&#38750;&#24402;&#19968;&#21270;&#30446;&#26631;&#23494;&#24230;&#25110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#26041;&#27861;&#20351;&#29992;&#20102;&#22343;&#22330;ODE&#21644;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#26080;&#38656;&#26799;&#24230;&#65292;&#21482;&#38656;&#35201;&#33021;&#22815;&#20174;&#21442;&#32771;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#30446;&#26631;&#23545;&#21442;&#32771;&#23494;&#24230;&#30340;&#27604;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#20960;&#20309;&#28151;&#21512;&#30340;&#36335;&#24452;&#19978;&#27839;&#36895;&#24230;&#22330;&#36816;&#36755;&#26679;&#26412;&#65292;&#24452;&#21521;&#36755;&#36816;&#26679;&#26412;&#12290;&#26041;&#27861;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#27714;&#35299;&#27850;&#26494;&#26041;&#31243;&#65292;&#20351;&#27850;&#26494;&#26041;&#31243;&#30340;&#27714;&#35299;&#21464;&#24471;&#21487;&#34892;&#65292;&#24182;&#23558;&#20854;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#26679;&#26412;&#30340;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#23454;&#29616;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#20174;&#31163;&#25955;&#26102;&#38388;&#30340;&#35282;&#24230;&#25512;&#23548;&#20986;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#33945;&#26480;-&#23433;&#26222;&#23572;&#26041;&#31243;&#36830;&#32493;&#32447;&#24615;&#21270;&#30340;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.03892</link><description>&lt;p&gt;
&#20197;&#26680;Fisher-Rao&#27969;&#36827;&#34892;&#21333;&#20301;&#26102;&#38388;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling in Unit Time with Kernel Fisher-Rao Flow. (arXiv:2401.03892v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26680;Fisher-Rao&#27969;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21333;&#20301;&#26102;&#38388;&#20869;&#20174;&#38750;&#24402;&#19968;&#21270;&#30446;&#26631;&#23494;&#24230;&#25110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#26041;&#27861;&#20351;&#29992;&#20102;&#22343;&#22330;ODE&#21644;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#26080;&#38656;&#26799;&#24230;&#65292;&#21482;&#38656;&#35201;&#33021;&#22815;&#20174;&#21442;&#32771;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#30446;&#26631;&#23545;&#21442;&#32771;&#23494;&#24230;&#30340;&#27604;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#20960;&#20309;&#28151;&#21512;&#30340;&#36335;&#24452;&#19978;&#27839;&#36895;&#24230;&#22330;&#36816;&#36755;&#26679;&#26412;&#65292;&#24452;&#21521;&#36755;&#36816;&#26679;&#26412;&#12290;&#26041;&#27861;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#27714;&#35299;&#27850;&#26494;&#26041;&#31243;&#65292;&#20351;&#27850;&#26494;&#26041;&#31243;&#30340;&#27714;&#35299;&#21464;&#24471;&#21487;&#34892;&#65292;&#24182;&#23558;&#20854;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#26679;&#26412;&#30340;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#23454;&#29616;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#20174;&#31163;&#25955;&#26102;&#38388;&#30340;&#35282;&#24230;&#25512;&#23548;&#20986;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#33945;&#26480;-&#23433;&#26222;&#23572;&#26041;&#31243;&#36830;&#32493;&#32447;&#24615;&#21270;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#22330;ODE&#21644;&#30456;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#38750;&#24402;&#19968;&#21270;&#30340;&#30446;&#26631;&#23494;&#24230;&#25110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#26080;&#38656;&#26799;&#24230;&#65292;&#21487;&#20197;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#33021;&#22815;&#20174;&#21442;&#32771;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#65288;&#38750;&#24402;&#19968;&#21270;&#30340;&#65289;&#30446;&#26631;&#23545;&#21442;&#32771;&#23494;&#24230;&#30340;&#27604;&#29575;&#12290;&#36890;&#36807;&#27714;&#35299;&#36816;&#36755;&#26679;&#26412;&#27839;&#20004;&#20010;&#23494;&#24230;&#30340;&#20960;&#20309;&#28151;&#21512;&#30340;&#36895;&#24230;&#22330;&#30340;&#27850;&#26494;&#26041;&#31243;&#26469;&#33719;&#24471;&#22343;&#22330;ODE&#65292;&#36825;&#26159;&#19968;&#31181;&#29305;&#23450;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#30340;&#36335;&#24452;&#12290;&#25105;&#20204;&#37319;&#29992;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26041;&#27861;&#26469;&#33719;&#24471;&#36895;&#24230;&#22330;&#30340;&#27850;&#26494;&#26041;&#31243;&#65292;&#36825;&#20351;&#24471;&#27850;&#26494;&#26041;&#31243;&#21487;&#22788;&#29702;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#31163;&#25955;&#21270;&#26377;&#38480;&#26679;&#26412;&#30340;&#32467;&#26524;&#22343;&#22330;ODE&#65292;&#24418;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#12290;&#22343;&#22330;ODE&#36824;&#21487;&#20197;&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#35270;&#35282;&#20174;&#33945;&#26480;-&#23433;&#26222;&#23572;&#26041;&#31243;&#30340;&#36830;&#32493;&#32447;&#24615;&#21270;&#30340;&#26497;&#38480;&#20013;&#25512;&#23548;&#20986;&#26469;&#65292;&#36825;&#22312;&#19968;&#20010;&#24050;&#30693;&#30340;&#26694;&#26550;&#20869;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new mean-field ODE and corresponding interacting particle systems for sampling from an unnormalized target density or Bayesian posterior. The interacting particle systems are gradient-free, available in closed form, and only require the ability to sample from the reference density and compute the (unnormalized) target-to-reference density ratio. The mean-field ODE is obtained by solving a Poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, which is the path of a particular Fisher-Rao gradient flow. We employ a reproducing kernel Hilbert space ansatz for the velocity field, which makes the Poisson equation tractable and enables us to discretize the resulting mean-field ODE over finite samples, as a simple interacting particle system. The mean-field ODE can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the Monge-Amp\`ere equations within a framework known 
&lt;/p&gt;</description></item><item><title>HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.01145</link><description>&lt;p&gt;
HAAQI-Net: &#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01145
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HAAQI-Net&#65292;&#19968;&#31181;&#38024;&#23545;&#21161;&#21548;&#22120;&#29992;&#25143;&#23450;&#21046;&#30340;&#38750;&#20405;&#20837;&#24615;&#28145;&#24230;&#23398;&#20064;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#22914;Hearing Aid Audio Quality Index (HAAQI) &#19981;&#21516;&#65292;HAAQI-Net&#37319;&#29992;&#20102;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(BLSTM)&#12290;&#35813;&#27169;&#22411;&#20197;&#35780;&#20272;&#30340;&#38899;&#20048;&#26679;&#26412;&#21644;&#21548;&#21147;&#25439;&#22833;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#39044;&#27979;&#30340;HAAQI&#24471;&#20998;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26469;&#33258;&#38899;&#39057;&#21464;&#25442;&#22120;(BEATs)&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#20998;&#25968;&#19982;&#30495;&#23454;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;HAAQI-Net&#36798;&#21040;&#20102;0.9257&#30340;&#38271;&#26399;&#19968;&#33268;&#24615;&#30456;&#20851;(LCC)&#65292;0.9394&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;(SRCC)&#65292;&#21644;0.0080&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#39640;&#24615;&#33021;&#20276;&#38543;&#30528;&#25512;&#29702;&#26102;&#38388;&#30340;&#22823;&#24133;&#20943;&#23569;&#65306;&#20174;62.52&#31186;(HAAQI)&#20943;&#23569;&#21040;2.71&#31186;(HAAQI-Net)&#65292;&#20026;&#21161;&#21548;&#22120;&#29992;&#25143;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;&#32467;&#21512;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#65292;&#26469;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.14848</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30340;&#23396;&#31435;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Isolated pulsar population synthesis with simulation-based inference. (arXiv:2312.14848v1 [astro-ph.HE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;&#32467;&#21512;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#65292;&#26469;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#19982;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#20197;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#20013;&#23376;&#26143;&#30340;&#35806;&#29983;&#29305;&#24615;&#21644;&#28436;&#21270;&#65292;&#37325;&#28857;&#26159;&#23427;&#20204;&#30340;&#21160;&#21147;&#23398;&#12289;&#26059;&#36716;&#21644;&#30913;&#24615;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#21021;&#22987;&#30913;&#22330;&#24378;&#24230;B&#21644;&#33258;&#36716;&#21608;&#26399;P&#65292;&#24182;&#29992;&#24130;&#24459;&#26469;&#25429;&#25417;&#21518;&#26399;&#30913;&#22330;&#30340;&#34928;&#20943;&#12290;&#27599;&#20010;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30001;&#22343;&#20540;&#956;logB&#65292;&#956;logP&#21644;&#26631;&#20934;&#24046;&#963;logB&#65292;&#963;logP&#25551;&#36848;&#65292;&#32780;&#24130;&#24459;&#30001;&#25351;&#25968;a_late&#25551;&#36848;&#65292;&#20849;&#35745;&#20116;&#20010;&#33258;&#30001;&#21442;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#27169;&#25311;&#20102;&#26143;&#20307;&#30340;&#23556;&#30005;&#21457;&#23556;&#21644;&#35266;&#27979;&#20559;&#24046;&#65292;&#20197;&#27169;&#25311;&#19977;&#20010;&#23556;&#30005;&#35843;&#26597;&#20013;&#30340;&#25506;&#27979;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#21442;&#25968;&#20135;&#29983;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#21512;&#25104;P-&#7766;&#22270;&#25968;&#25454;&#24211;&#12290;&#25509;&#30528;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30340;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
We combine pulsar population synthesis with simulation-based inference to constrain the magneto-rotational properties of isolated Galactic radio pulsars. We first develop a flexible framework to model neutron-star birth properties and evolution, focusing on their dynamical, rotational and magnetic characteristics. In particular, we sample initial magnetic-field strengths, $B$, and spin periods, $P$, from log-normal distributions and capture the late-time magnetic-field decay with a power law. Each log-normal is described by a mean, $\mu_{\log B}, \mu_{\log P}$, and standard deviation, $\sigma_{\log B}, \sigma_{\log P}$, while the power law is characterized by the index, $a_{\rm late}$, resulting in five free parameters. We subsequently model the stars' radio emission and observational biases to mimic detections with three radio surveys, and produce a large database of synthetic $P$-$\dot{P}$ diagrams by varying our input parameters. We then follow a simulation-based inference approach 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11244</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#21028;&#26029;&#20004;&#20010;&#23454;&#20307;&#25551;&#36848;&#26159;&#21542;&#25351;&#30340;&#26159;&#21516;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23454;&#20307;&#21305;&#37197;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#20063;&#26159;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#23558;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#20135;&#21697;&#21305;&#37197;&#36215;&#26469;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914;BERT&#25110;RoBERTa&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;ii&#65289;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#20307;&#19981;&#22815;&#20581;&#22766;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22522;&#20110;PLMs&#30340;&#21305;&#37197;&#22120;&#30340;&#22791;&#36873;&#26041;&#26696;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#23545;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT3.5&#21644;GPT4&#65292;&#20197;&#21450;&#22522;&#20110;Llama2&#30340;&#24320;&#28304;LLMs&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33410;&#28857;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#25972;&#20010;&#26641;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#26469;&#36873;&#25321;&#33410;&#28857;&#12290;&#23613;&#31649;&#21482;&#22312;&#21512;&#25104;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#38598;&#19978;&#24471;&#21040;&#20102;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.00112</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#30340;&#33410;&#28857;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Node Selection in Branch-and-Bound. (arXiv:2310.00112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33410;&#28857;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#25972;&#20010;&#26641;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#26469;&#36873;&#25321;&#33410;&#28857;&#12290;&#23613;&#31649;&#21482;&#22312;&#21512;&#25104;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#38598;&#19978;&#24471;&#21040;&#20102;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20174;&#25628;&#32034;&#26641;&#20013;&#30830;&#23450;&#26368;&#20248;&#33410;&#28857;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#22120;&#35201;&#20040;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#38598;&#21512;&#65292;&#33258;&#21160;&#20999;&#25442;&#20026;&#22825;&#30495;&#30340;&#23376;&#33410;&#28857;&#36873;&#25321;&#22120;&#65292;&#35201;&#20040;&#20351;&#29992;&#20381;&#36182;&#20110;&#20010;&#21035;&#33410;&#28857;&#25968;&#25454;&#30340;&#23398;&#20064;&#33410;&#28857;&#36873;&#25321;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#27169;&#25311;&#25216;&#26415;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32771;&#34385;&#25972;&#20010;&#26641;&#29366;&#24577;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23396;&#31435;&#30340;&#33410;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26681;&#25454;&#27169;&#22411;&#20174;&#26681;&#33410;&#28857;&#21040;&#8220;&#24453;&#36873;&#25321;&#8221;&#21494;&#23376;&#33410;&#28857;&#30340;&#36335;&#24452;&#20135;&#29983;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#12290;&#23558;&#33410;&#28857;&#36873;&#25321;&#24314;&#27169;&#20026;&#27010;&#29575;&#20998;&#24067;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#22312;&#33410;&#28857;&#36136;&#37327;&#21644;&#33410;&#28857;&#35780;&#20272;&#25104;&#26412;&#12290;&#23613;&#31649;&#21482;&#26159;&#22312;&#19987;&#38376;&#35774;&#35745;&#30340;&#21512;&#25104;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#32452;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#38382;&#39064;&#38598;&#19978;&#24341;&#20986;&#20102;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A big challenge in branch and bound lies in identifying the optimal node within the search tree from which to proceed. Current state-of-the-art selectors utilize either hand-crafted ensembles that automatically switch between naive sub-node selectors, or learned node selectors that rely on individual node data. We propose a novel bi-simulation technique that uses reinforcement learning (RL) while considering the entire tree state, rather than just isolated nodes. To achieve this, we train a graph neural network that produces a probability distribution based on the path from the model's root to its ``to-be-selected'' leaves. Modelling node-selection as a probability distribution allows us to train the model using state-of-the-art RL techniques that capture both intrinsic node-quality and node-evaluation costs. Our method induces a high quality node selection policy on a set of varied and complex problem sets, despite only being trained on specially designed, synthetic TSP instances. Exp
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08631</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24515;&#29702;&#20542;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08631
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#23558;&#25104;&#20026;&#20010;&#24615;&#21270;&#25216;&#26415;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;ChatGPT&#30340;LLMs&#20174;&#20010;&#20154;&#25968;&#23383;&#36275;&#36857;&#20013;&#25512;&#26029;&#20010;&#20154;&#24515;&#29702;&#20542;&#21521;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#20174;&#29992;&#25143;&#30340;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#23548;&#20986;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;LLM&#25512;&#26029;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20026;r = 0.29&#65288;&#33539;&#22260;&#20026;[0.22, 0.33]&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20010;&#24615;&#25512;&#26029;&#30340;&#20559;&#35265;&#65306;&#23545;&#20110;&#20960;&#20010;&#29305;&#36136;&#65292;&#25512;&#26029;&#24471;&#20998;&#22312;&#22899;&#24615;&#21644;&#24180;&#36731;&#20154;&#20013;&#30340;&#35823;&#24046;&#36739;&#23567;&#65292;&#36825;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#26469;&#33258;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#25110;&#22312;&#32447;&#33258;&#25105;&#21576;&#29616;&#30340;&#24046;&#24322;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) demonstrate increasingly human-like abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.02870</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#37325;&#26032;&#24605;&#32771;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Rethinking Momentum Knowledge Distillation in Online Continual Learning. (arXiv:2309.02870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#20986;&#29616;&#12290;&#19982;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#30456;&#27604;&#65292;&#22312;OCL&#20013;&#21482;&#33021;&#30475;&#21040;&#25968;&#25454;&#19968;&#27425;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22238;&#25918;&#30340;&#31574;&#30053;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#20005;&#37325;&#20381;&#36182;&#23427;&#20204;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#22312;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;OCL&#20013;&#20173;&#28982;&#26410;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23558;KD&#24212;&#29992;&#20110;OCL&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#65288;MKD&#65289;&#24212;&#29992;&#20110;&#35768;&#22810;&#26071;&#33328;OCL&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#38500;&#20102;&#23558;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;ImageNet100&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#38416;&#26126;&#20102;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches are heavily depending on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its potential. In this paper, we theoretically analyze the challenges in applying KD to OCL. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-arts accuracy by more than $10\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#35780;&#20998;&#20989;&#25968;CONFIDERAI&#65292;&#23427;&#23558;&#19968;&#33268;&#24615;&#39044;&#27979;&#19982;&#35268;&#21017;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#35268;&#21017;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#28857;&#30340;&#20960;&#20309;&#20301;&#32622;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23450;&#20041;&#28385;&#36275;&#19968;&#33268;&#24615;&#20445;&#35777;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.01778</link><description>&lt;p&gt;
CONFIDERAI&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;CONFIRMAL&#21487;&#35299;&#37322;&#35774;&#35745;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
CONFIDERAI: a novel CONFormal Interpretable-by-Design score function forExplainable and Reliable Artificial Intelligence. (arXiv:2309.01778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#35780;&#20998;&#20989;&#25968;CONFIDERAI&#65292;&#23427;&#23558;&#19968;&#33268;&#24615;&#39044;&#27979;&#19982;&#35268;&#21017;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#35268;&#21017;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#28857;&#30340;&#20960;&#20309;&#20301;&#32622;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23450;&#20041;&#28385;&#36275;&#19968;&#33268;&#24615;&#20445;&#35777;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#30340;&#29983;&#27963;&#36234;&#26469;&#36234;&#21463;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#65292;&#27627;&#26080;&#30097;&#38382;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24517;&#39035;&#20026;&#25152;&#26377;&#20154;&#35774;&#35745;&#25104;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22914;&#26524;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#28385;&#36275;&#35299;&#37322;&#24615;&#12289;&#20581;&#22766;&#24615;&#12289;&#36879;&#26126;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#24615;&#36825;&#20116;&#20010;&#26041;&#38754;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#35748;&#20026;&#23427;&#26159;&#23433;&#20840;&#21644;&#21487;&#20449;&#36182;&#30340;&#12290;&#38500;&#20102;&#36825;&#20116;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#20845;&#20010;&#22522;&#26412;&#26041;&#38754;&#65306;&#19968;&#33268;&#24615;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#32773;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#27010;&#29575;&#24615;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;CONFIDERAI&#65292;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#30340;&#26032;&#35780;&#20998;&#20989;&#25968;&#65292;&#23558;&#19968;&#33268;&#24615;&#39044;&#27979;&#19982;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#35268;&#21017;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#28857;&#22312;&#35268;&#21017;&#36793;&#30028;&#20869;&#30340;&#20960;&#20309;&#20301;&#32622;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21033;&#29992;&#25511;&#21046;&#38750;&#19968;&#33268;&#24615;&#30340;&#25968;&#37327;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23450;&#20041;&#28385;&#36275;&#19968;&#33268;&#24615;&#20445;&#35777;&#30340;&#21306;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Everyday life is increasingly influenced by artificial intelligence, and there is no question that machine learning algorithms must be designed to be reliable and trustworthy for everyone. Specifically, computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills five pillars: explainability, robustness, transparency, fairness, and privacy. In addition to these five, we propose a sixth fundamental aspect: conformity, that is, the probabilistic assurance that the system will behave as the machine learner expects. In this paper, we propose a methodology to link conformal prediction with explainable machine learning by defining CONFIDERAI, a new score function for rule-based models that leverages both rules predictive ability and points geometrical position within rules boundaries. We also address the problem of defining regions in the feature space where conformal guarantees are satisfied by exploiting techniques to control the number of non-conforma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12696</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TopDis&#65288;&#25299;&#25169;&#35299;&#32544;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#22810;&#23610;&#24230;&#25299;&#25169;&#25439;&#22833;&#39033;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#35299;&#32544;&#26159;&#25968;&#25454;&#34920;&#31034;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#32423;&#35748;&#30693;&#30340;&#23454;&#29616;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#22522;&#20110;VAE&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#28508;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24635;&#20307;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#23646;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#35299;&#32544;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#25968;&#25454;&#27969;&#24418;&#36941;&#21382;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25299;&#25169;&#25439;&#22833;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#65292;&#22914;MIG&#12289;FactorVAE&#24471;&#20998;&#12289;SAP&#24471;&#20998;&#21644;DCI&#35299;&#32544;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.11888</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#24615;&#36882;&#25512;&#21644;&#38750;&#32447;&#24615;&#25237;&#24433;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Universality of Linear Recurrences Followed by Nonlinear Projections. (arXiv:2307.11888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#27880;&#37322;&#65288;&#20316;&#20026;&#19968;&#31687;&#20840;&#25991;&#35770;&#25991;&#30340;&#24037;&#20316;&#36827;&#23637;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#26063;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#65288;&#21253;&#25324;S4&#12289;S5&#21644;LRU&#65289;&#21644;&#20301;&#32622;&#36880;&#20803;&#32032;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#32972;&#21518;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#24490;&#29615;&#23618;&#35270;&#20026;&#21487;&#20197;&#24544;&#23454;&#22320;&#23384;&#20648;&#36755;&#20837;&#24207;&#21015;&#20449;&#24687;&#21040;&#20869;&#37096;&#29366;&#24577;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#28982;&#21518;&#30001;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;MLP&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03175</link><description>&lt;p&gt;
&#25512;&#24320;&#32511;&#33394;&#65306;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#26893;&#29289;&#21494;&#29255;&#32972;&#21518;&#30340;&#20869;&#23481;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Push Past Green: Learning to Look Behind Plant Foliage by Moving It. (arXiv:2307.03175v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20892;&#19994;&#24212;&#29992;&#65288;&#20363;&#22914;&#26816;&#26597;&#12289;&#34920;&#22411;&#20998;&#26512;&#12289;&#37319;&#25688;&#27700;&#26524;&#65289;&#38656;&#35201;&#25805;&#20316;&#26893;&#29289;&#21494;&#29255;&#20197;&#26597;&#30475;&#21494;&#23376;&#21644;&#26525;&#24178;&#30340;&#32972;&#21518;&#12290;&#37096;&#20998;&#21487;&#35265;&#24615;&#12289;&#26497;&#31471;&#26434;&#20081;&#12289;&#34180;&#32467;&#26500;&#20197;&#21450;&#26893;&#29289;&#30340;&#26410;&#30693;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#37117;&#20351;&#24471;&#36825;&#31181;&#25805;&#20316;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35757;&#32451;SRPNet&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#39044;&#27979;&#22312;&#32473;&#23450;&#26893;&#29289;&#19978;&#25191;&#34892;&#20505;&#36873;&#21160;&#20316;&#26102;&#20250;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#20132;&#21449;&#29109;&#26041;&#27861;&#30340;SRPNet&#26469;&#39044;&#27979;&#26377;&#25928;&#22320;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#30340;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;SRPNet&#19981;&#20165;&#39044;&#27979;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#65292;&#36824;&#39044;&#27979;&#26174;&#38706;&#20986;&#31354;&#38388;&#30340;&#20301;&#32622;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#30340;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#12290;&#22312;&#29289;&#29702;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#30340;&#34276;&#34067;&#21644;&#30495;&#23454;&#26893;&#29289;&#65288;&#40857;&#34880;&#26641;&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;5&#20010;&#35774;&#32622;&#65292;&#21253;&#25324;2&#20010;&#27979;&#35797;&#27867;&#21270;&#24615;&#33021;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#32447;&#24615;&#22806;&#25512;&#35774;&#35745;&#65292;&#36890;&#36807;&#22806;&#25512;&#32467;&#26500;&#21644;&#29305;&#24449;&#31354;&#38388;&#26469;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#65292; &#28982;&#21518;&#23454;&#29616;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.08076</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#21644;&#29305;&#24449;&#22806;&#25512;&#22312;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization. (arXiv:2306.08076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#32447;&#24615;&#22806;&#25512;&#35774;&#35745;&#65292;&#36890;&#36807;&#22806;&#25512;&#32467;&#26500;&#21644;&#29305;&#24449;&#31354;&#38388;&#26469;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#65292; &#28982;&#21518;&#23454;&#29616;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#28041;&#21450;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#32447;&#24615;&#22806;&#25512;&#35774;&#35745;&#65292;&#36890;&#36807;&#22806;&#25512;&#32467;&#26500;&#21644;&#29305;&#24449;&#31354;&#38388;&#26469;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#22270;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#30446;&#26631;&#36716;&#31227;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21508;&#31181;&#22270;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#20013;&#37117;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#19988;&#25345;&#32493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates both structure and feature spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07629</link><description>&lt;p&gt;
SqueezeLLM&#65306;&#23494;&#38598;&#31232;&#30095;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#22312;&#24191;&#27867;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#26524;&#12290;&#20294;&#26159;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;&#37096;&#32626;&#26694;&#26550;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#25512;&#29702;&#31649;&#36947;&#65292;&#36825;&#36890;&#24120;&#26159;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#20351;&#29992;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#20302;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#20110;LLMs&#29983;&#25104;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#20869;&#23384;&#24102;&#23485;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#21333;&#20010;&#25209;&#27425;&#25512;&#29702;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20943;&#23569;&#31934;&#24230;&#26469;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;SqueezeLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06098</link><description>&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#21487;&#20197;&#20934;&#30830;&#22320;&#21387;&#32553;&#39044;&#22788;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#35268;&#27169;&#30340;&#20108;&#38454;&#20449;&#24687;&#26159;&#25913;&#36827;&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#24615;&#33021;&#30340;&#20027;&#35201;&#36884;&#24452;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31934;&#30830;&#20840;&#30697;&#38453;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;&#20840;&#30697;&#38453;Adagrad&#65288;GGT&#65289;&#25110;&#26080;&#30697;&#38453;&#36817;&#20284;&#26354;&#29575;&#65288;M-FAC&#65289;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#65292;&#20063;&#20250;&#36935;&#21040;&#24040;&#22823;&#30340;&#23384;&#20648;&#25104;&#26412;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#23384;&#20648;&#26799;&#24230;&#30340;&#28369;&#21160;&#31383;&#21475;&#65292;&#20854;&#23384;&#20648;&#38656;&#27714;&#22312;&#27169;&#22411;&#32500;&#24230;&#20013;&#26159;&#25104;&#20493;&#22686;&#21152;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#23558;&#39044;&#22788;&#29702;&#22120;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23558;&#26799;&#24230;&#20449;&#24687;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#21387;&#32553;&#26799;&#24230;&#20449;&#24687;&#65292;&#23558;&#21387;&#32553;&#35823;&#24046;&#21453;&#39304;&#21040;&#26410;&#26469;&#30340;&#36845;&#20195;&#20013;&#12290;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28304;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#20559;&#22909;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#26631;&#20219;&#21153;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#20559;&#22909;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;Gromov-Wasserstein&#36317;&#31163;&#23545;&#40784;&#20219;&#21153;&#20043;&#38388;&#30340;&#36712;&#36857;&#20998;&#24067;&#12290;&#36890;&#36807;&#20132;&#26367;&#36827;&#34892;&#20559;&#22909;&#25512;&#26029;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#25913;&#36827;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#24182;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#24335;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#26631;&#35760;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.03615</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31163;&#32447;RL&#30340;&#38646;&#26679;&#26412;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Preference Learning for Offline RL via Optimal Transport. (arXiv:2306.03615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28304;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#20559;&#22909;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#26631;&#20219;&#21153;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#20559;&#22909;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;Gromov-Wasserstein&#36317;&#31163;&#23545;&#40784;&#20219;&#21153;&#20043;&#38388;&#30340;&#36712;&#36857;&#20998;&#24067;&#12290;&#36890;&#36807;&#20132;&#26367;&#36827;&#34892;&#20559;&#22909;&#25512;&#26029;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#25913;&#36827;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#24182;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#24335;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#26631;&#35760;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;(PbRL)&#24050;&#32463;&#22312;&#23558;&#22870;&#21169;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#32780;&#19988;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#21462;&#30340;&#26114;&#36149;&#30340;&#20559;&#22909;&#25968;&#25454;&#36890;&#24120;&#19981;&#33021;&#37325;&#22797;&#20351;&#29992;&#20110;&#21518;&#32493;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#23548;&#33268;&#38656;&#35201;&#23545;&#27599;&#20010;&#26032;&#20219;&#21153;&#36827;&#34892;&#22823;&#37327;&#30340;&#26631;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#31639;&#27861;&#65292;&#20197;&#21033;&#29992;&#28304;&#20219;&#21153;&#20013;&#30340;&#26631;&#35760;&#20559;&#22909;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#26631;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#20154;&#31867;&#26597;&#35810;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Gromov-Wasserstein&#36317;&#31163;&#26469;&#23545;&#40784;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#36712;&#36857;&#20998;&#24067;&#12290;&#27714;&#35299;&#30340;&#26368;&#20248;&#20256;&#36755;&#30697;&#38453;&#20316;&#20026;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#36712;&#36857;&#23545;&#24212;&#20851;&#31995;&#30340;&#19968;&#20010;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#24471;&#35782;&#21035;&#20219;&#21153;&#20043;&#38388;&#23545;&#24212;&#30340;&#36712;&#36857;&#23545;&#21644;&#20256;&#36882;&#20559;&#22909;&#26631;&#31614;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#20013;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#26367;&#24335;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20132;&#26367;&#36827;&#34892;&#20559;&#22909;&#25512;&#26029;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#25913;&#36827;&#25512;&#26029;&#20986;&#30340;&#26631;&#31614;&#24182;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based Reinforcement Learning (PbRL) has demonstrated remarkable efficacy in aligning rewards with human intentions. However, a significant challenge lies in the need of substantial human labels, which is costly and time-consuming. Additionally, the expensive preference data obtained from prior tasks is not typically reusable for subsequent task learning, leading to extensive labeling for each new task. In this paper, we propose a novel zero-shot preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, eliminating the requirement for human queries. Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels. However, learning directly from inferred lab
&lt;/p&gt;</description></item><item><title>Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03360</link><description>&lt;p&gt;
Vid2Act&#65306;&#20026;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#28608;&#27963;&#31163;&#32447;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03360
&lt;/p&gt;
&lt;p&gt;
Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#22312;&#32447;&#20219;&#21153;&#25928;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#36328;&#22495;&#20013;&#20219;&#21153;&#12289;&#21160;&#24577;&#21644;&#34892;&#20026;&#30340;&#22266;&#26377;&#19981;&#21305;&#37197;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;APV&#30340;&#27169;&#22411;&#36991;&#20813;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#20276;&#38543;&#21160;&#20316;&#35760;&#24405;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#22312;&#28304;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#12289;&#19981;&#28041;&#21450;&#25805;&#20316;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Vid2Act&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#21040;&#22312;&#32447;&#29615;&#22659;&#20013;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#21160;&#20316;&#26465;&#20214;&#21160;&#24577;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#21160;&#20316;&#28436;&#31034;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#19981;&#20165;&#23558;&#19990;&#30028;&#27169;&#22411;&#29992;&#20316;&#34892;&#20026;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#65292;&#36824;&#23558;&#20854;&#29992;&#20316;&#27979;&#37327;&#39046;&#22495;&#30456;&#20851;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36827;&#34892;&#21160;&#24577;&#34920;&#31034;&#20256;&#36755;&#21644;&#31574;&#30053;&#20256;&#36755;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22495;&#36873;&#25321;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#26102;&#38388;&#21464;&#21270;&#30340;&#20219;&#21153;&#30456;&#20284;&#24230;&#12290;&#36825;&#20123;&#30456;&#20284;&#24230;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#22320;&#23558;&#26368;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#21160;&#24577;&#20256;&#36755;&#21040;&#22312;&#32447;&#29615;&#22659;&#65292;&#21644;&#65288;ii&#65289;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25351;&#23548;&#20195;&#29702;&#38598;&#20013;&#25191;&#34892;&#20219;&#21153;&#30456;&#20851;&#30340;&#21160;&#20316;&#12290;&#22312;Atari&#21644;DMControl&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#25216;&#26415;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#23454;&#29616;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#36755;&#20986;&#30340;&#26657;&#20934;&#65292;&#20855;&#26377;&#36739;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.00382</link><description>&lt;p&gt;
&#22522;&#20110;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibrated Propensity Scores for Causal Effect Estimation. (arXiv:2306.00382v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#25216;&#26415;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#23454;&#29616;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#36755;&#20986;&#30340;&#26657;&#20934;&#65292;&#20855;&#26377;&#36739;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20542;&#21521;&#20998;&#25968;&#36890;&#24120;&#29992;&#20110;&#22312;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#26102;&#24179;&#34913;&#35266;&#27979;&#21040;&#30340;&#21327;&#21464;&#37327;&#12290;&#24403;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#30495;&#23454;&#30340;&#27835;&#30103;&#20998;&#37197;&#26426;&#21046;&#26102;&#65292;&#36890;&#36807;&#20542;&#21521;&#20998;&#25968;&#21152;&#26435;&#33719;&#24471;&#30340;&#20272;&#35745;&#32467;&#26524;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23398;&#20064;&#21040;&#30340;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#36755;&#20986;&#24212;&#36827;&#34892;&#26657;&#20934;&#65292;&#21363;90%&#30340;&#39044;&#27979;&#27835;&#30103;&#27010;&#29575;&#24212;&#23545;&#24212;90%&#30340;&#20010;&#20307;&#34987;&#20998;&#37197;&#21040;&#27835;&#30103;&#32452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#26469;&#30830;&#20445;&#36825;&#19968;&#23646;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#29702;&#35770;&#29305;&#24615;&#21450;&#20854;&#22312;&#26080;&#20559;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20351;&#29992;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#36827;&#34892;&#25913;&#36827;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#32500;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#65292;&#22312;&#24212;&#29992;&#20110;&#26356;&#31616;&#21333;&#30340;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#26102;&#36824;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propensity scores are commonly used to balance observed covariates while estimating treatment effects. Estimates obtained through propensity score weighing can be biased when the propensity score model cannot learn the true treatment assignment mechanism. We argue that the probabilistic output of a learned propensity score model should be calibrated, i.e. a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group. We propose simple recalibration techniques to ensure this property. We investigate the theoretical properties of a calibrated propensity score model and its role in unbiased treatment effect estimation. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional genome-wide association studies, where we also show reduced computational requirements when calibration is applied to simpler propensity score models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.19685</link><description>&lt;p&gt;
&#28145;&#24230;&#38543;&#26426;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep Stochastic Mechanics. (arXiv:2305.19685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21463;&#38543;&#26426;&#21147;&#23398;&#21644;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#20174;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#20013;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#28508;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26356;&#39640;&#30340;&#32500;&#24230;&#19978;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#32467;&#26524;&#20855;&#26377;&#19982;&#32500;&#25968;&#25968;&#37327;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#29992;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schr\"odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16610</link><description>&lt;p&gt;
&#23398;&#20064;&#21333;&#35843;&#21338;&#24328;&#30340;&#25237;&#30707;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31639;&#27861;&#21363;&#20351;&#22312;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#20063;&#26080;&#27861;&#25910;&#25947;&#21040;&#22343;&#34913;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20048;&#35266;&#29256;&#26412;&#24182;&#20855;&#26377;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26080;&#22122;&#22768;&#30340;&#26799;&#24230;&#21453;&#39304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25200;&#21160;&#25110;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#12290;&#36825;&#31181;&#25200;&#21160;&#26377;&#21161;&#20110;&#23558;&#24403;&#21069;&#31574;&#30053;&#25289;&#21521;&#19968;&#20010;&#38170;&#23450;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25237;&#30707;&#32034;&#8221;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#38752;&#36817;&#22343;&#34913;&#28857;&#30340;&#31283;&#23450;&#28857;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23450;&#26399;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#35299;&#37322;&#20026;&#36817;&#31471;p
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;RAGE&#30340;&#28789;&#27963;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#21457;&#29616;&#22270;&#32593;&#32476;&#32467;&#26500;&#20013;&#30340;&#35299;&#37322;&#12290;&#35813;&#35299;&#37322;&#22120;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;GNN&#26550;&#26500;&#21644;&#22270;&#24418;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#36275;&#22815;&#20449;&#24687;&#20197;&#20351;&#20154;&#31867;&#39044;&#27979;&#22797;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15745</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#40065;&#26834;&#22411;&#21069;&#24207;&#22270;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Robust Ante-hoc Graph Explainer using Bilevel Optimization. (arXiv:2305.15745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;RAGE&#30340;&#28789;&#27963;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#21457;&#29616;&#22270;&#32593;&#32476;&#32467;&#26500;&#20013;&#30340;&#35299;&#37322;&#12290;&#35813;&#35299;&#37322;&#22120;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;GNN&#26550;&#26500;&#21644;&#22270;&#24418;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#36275;&#22815;&#20449;&#24687;&#20197;&#20351;&#20154;&#31867;&#39044;&#27979;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20915;&#31574;&#30340;&#35828;&#26126;&#23545;&#20110;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#25351;&#23548;&#20915;&#31574;&#25913;&#36827;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#22312;&#22270;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#20915;&#31574;&#24448;&#24448;&#21462;&#20915;&#20110;&#32467;&#26500;&#21644;&#23646;&#24615;&#25968;&#25454;&#30340;&#32508;&#21512;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#25152;&#35859;&#30340;&#21518;&#24207;&#35299;&#37322;&#22120;&#19978;&#65292;&#30456;&#24212;&#30340;&#22909;&#30340;&#35299;&#37322;&#30340;&#26631;&#20934;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#65292;&#28789;&#27963;&#30340;&#40065;&#26834;&#22411;&#21069;&#24207;&#35299;&#37322;&#22120;&#65292;&#21517;&#20026;RAGE&#65288;Robust Ante-hoc Graph Explainer&#65289;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26469;&#21457;&#29616;&#19968;&#31867;&#24191;&#27867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#65292;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;GNN&#26550;&#26500;&#21644;&#22270;&#24418;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#20449;&#24687;&#65292;&#20197;&#21551;&#29992;&#20154;&#31867;&#39044;&#27979;&#22797;&#21046;&#12290;&#23454;&#39564;&#34920;&#26126;RAGE&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21457;&#29616;&#24544;&#23454;&#21644;&#40065;&#26834;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the decisions made by machine learning models for high-stakes applications is critical for increasing transparency and guiding improvements to these decisions. This is particularly true in the case of models for graphs, where decisions often depend on complex patterns combining rich structural and attribute data. While recent work has focused on designing so-called post-hoc explainers, the question of what constitutes a good explanation remains open. One intuitive property is that explanations should be sufficiently informative to enable humans to approximately reproduce the predictions given the data. However, we show that post-hoc explanations do not achieve this goal as their explanations are highly dependent on fixed model parameters (e.g., learned GNN weights). To address this challenge, this paper proposes RAGE (Robust Ante-hoc Graph Explainer), a novel and flexible ante-hoc explainer designed to discover explanations for a broad class of graph neural networks using bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#26657;&#27491;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14164</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#20462;&#27491;&#25552;&#39640;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improved Convergence of Score-Based Diffusion Models via Prediction-Correction. (arXiv:2305.14164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#26657;&#27491;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#26159;&#20174;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#65288;i&#65289;&#36890;&#36807;&#21521;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#36816;&#34892;&#26102;&#38388;&#20026;$T_1$&#30340;&#27491;&#21521;&#36807;&#31243;&#65292;&#65288;ii&#65289;&#20272;&#35745;&#20854;&#24471;&#20998;&#20989;&#25968;&#65292;&#24182;&#65288;iii&#65289;&#20351;&#29992;&#27492;&#20272;&#35745;&#20540;&#36816;&#34892;&#21453;&#21521;&#36807;&#31243;&#12290;&#30001;&#20110;&#21453;&#21521;&#36807;&#31243;&#20197;&#27491;&#21521;&#36807;&#31243;&#30340;&#24179;&#31283;&#20998;&#24067;&#20316;&#20026;&#21021;&#22987;&#20540;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;&#20998;&#26512;&#33539;&#24335;&#35201;&#27714;$T_1\to\infty$&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#20998;&#25968;&#36924;&#36817;&#31934;&#24230;&#65292;&#24403;$T_1$&#21457;&#25955;&#26102;&#65292;&#25910;&#25947;&#20445;&#35777;&#23558;&#22833;&#36133;&#65307;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;$T_1$&#36234;&#22823;&#65292;&#35745;&#31639;&#25104;&#26412;&#23601;&#36234;&#39640;&#65292;&#24182;&#19988;&#20250;&#23548;&#33268;&#35823;&#24046;&#20256;&#25773;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#27969;&#34892;&#30340;&#39044;&#27979;&#22120;&#26657;&#27491;&#26041;&#26696;&#30340;&#19968;&#20010;&#29256;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#22312;&#36816;&#34892;&#27491;&#21521;&#36807;&#31243;&#20043;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19981;&#31934;&#30830;&#30340; Langevin &#21160;&#21147;&#23398;&#20272;&#35745;&#26368;&#32456;&#20998;&#24067;&#65292;&#28982;&#21518;&#24674;&#22797;&#35813;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) are powerful tools to sample from complex data distributions. Their underlying idea is to (i) run a forward process for time $T_1$ by adding noise to the data, (ii) estimate its score function, and (iii) use such estimate to run a reverse process. As the reverse process is initialized with the stationary distribution of the forward one, the existing analysis paradigm requires $T_1\to\infty$. This is however problematic: from a theoretical viewpoint, for a given precision of the score approximation, the convergence guarantee fails as $T_1$ diverges; from a practical viewpoint, a large $T_1$ increases computational costs and leads to error propagation. This paper addresses the issue by considering a version of the popular predictor-corrector scheme: after running the forward process, we first estimate the final distribution via an inexact Langevin dynamics and then revert the process. Our key technical contribution is to provide convergence guarantees
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32676;&#19981;&#21464;GAN&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#20250;&#25353;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.13517</link><description>&lt;p&gt;
Group-Invariant GAN&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees of Group-Invariant GANs. (arXiv:2305.13517v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32676;&#19981;&#21464;GAN&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#20250;&#25353;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Group-Invariant&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26159;&#19968;&#31181;GAN&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20855;&#26377;&#30828;&#24615;&#38598;&#22242;&#23545;&#31216;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#26174;&#30528;&#25913;&#36827;&#25968;&#25454;&#25928;&#29575;&#30340;&#38598;&#22242;&#19981;&#21464;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#32676;&#19981;&#21464;GAN&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#26469;&#20005;&#26684;&#37327;&#21270;&#36825;&#31181;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#25353;&#29031;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#65292;&#36825;&#20010;&#24130;&#21462;&#20915;&#20110;&#20998;&#24067;&#25903;&#25345;&#30340;&#26412;&#36136;&#32500;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#20010;&#20026;&#32676;&#19981;&#21464;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GAN&#25552;&#20379;&#32479;&#35745;&#20272;&#35745;&#30340;&#24037;&#20316;&#65292;&#24182;&#21487;&#20197;&#20026;&#20854;&#20182;&#32676;&#19981;&#21464;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group-invariant generative adversarial networks (GANs) are a type of GANs in which the generators and discriminators are hardwired with group symmetries. Empirical studies have shown that these networks are capable of learning group-invariant distributions with significantly improved data efficiency. In this study, we aim to rigorously quantify this improvement by analyzing the reduction in sample complexity for group-invariant GANs. Our findings indicate that when learning group-invariant distributions, the number of samples required for group-invariant GANs decreases proportionally with a power of the group size, and this power depends on the intrinsic dimension of the distribution's support. To our knowledge, this work presents the first statistical estimation for group-invariant generative models, specifically for GANs, and it may shed light on the study of other group-invariant generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.10664</link><description>&lt;p&gt;
&#26435;&#37325;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26080;&#38480;&#23485;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance. (arXiv:2305.10664v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;Neal&#65288;1996&#65289;&#30340;&#32463;&#20856;&#32780;&#26377;&#24433;&#21709;&#21147;&#30340;&#20316;&#21697;&#24050;&#30693;&#65292;&#20855;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#38480;&#23485;&#24230;&#26631;&#24230;&#26497;&#38480;&#26159;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#65292;&#24403;&#32593;&#32476;&#26435;&#37325;&#20855;&#26377;&#26377;&#30028;&#20808;&#39564;&#26041;&#24046;&#26102;&#12290;Neal&#30340;&#32467;&#26524;&#24050;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#38544;&#34255;&#23618;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#65292;&#20063;&#20855;&#26377;&#39640;&#26031;&#36807;&#31243;&#26631;&#24230;&#26497;&#38480;&#12290;&#39640;&#26031;&#36807;&#31243;&#30340;&#26131;&#22788;&#29702;&#23646;&#24615;&#20801;&#35768;&#30452;&#25509;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#30456;&#27604;&#26377;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#26497;&#38480;&#36807;&#31243;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#22833;&#25928;&#65292;&#25454;&#36866;&#24403;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;$\alpha$&#36807;&#31243;&#30340;&#26631;&#24230;&#26497;&#38480;&#30340;&#25991;&#29486;&#36739;&#22810;&#30340;&#26159;&#21069;&#21521;&#27169;&#25311;&#65292;&#32780;&#22312;&#36825;&#20123;&#26435;&#37325;&#19979;&#30340;&#21518;&#39564;&#25512;&#26029;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#25512;&#26029;&#30340;&#26032;&#29702;&#35770;&#27934;&#23519;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#39564;&#25910;&#32553;&#36895;&#29575;&#32467;&#26524;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, \emph{when the network weights have bounded prior variance}. Neal's result has been extended to networks with multiple hidden layers and to convolutional neural networks, also with Gaussian process scaling limits. The tractable properties of Gaussian processes then allow straightforward posterior inference and uncertainty quantification, considerably simplifying the study of the limit process compared to a network of finite width. Neural network weights with unbounded variance, however, pose unique challenges. In this case, the classical central limit theorem breaks down and it is well known that the scaling limit is an $\alpha$-stable process under suitable conditions. However, current literature is primarily limited to forward simulations under these processes and the problem of posterior inference under s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#32473;&#23450;&#23569;&#37327;&#24322;&#24120;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25490;&#38500;&#24050;&#30693;&#30340;&#24322;&#24120;&#28857;&#65292;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10464</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#23569;&#37327;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Reconstruction Error-based Anomaly Detection with Few Outlying Examples. (arXiv:2305.10464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#32473;&#23450;&#23569;&#37327;&#24322;&#24120;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25490;&#38500;&#24050;&#30693;&#30340;&#24322;&#24120;&#28857;&#65292;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#20854;&#34920;&#29616;&#20986;&#33394;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#20195;&#34920;&#27491;&#24120;&#25968;&#25454;&#30340;&#26679;&#26412;&#38598;&#65292;&#28982;&#21518;&#25351;&#20986;&#37027;&#20123;&#37325;&#26500;&#35823;&#24046;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#20026;&#24322;&#24120;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26500;&#24120;&#24120;&#33021;&#22815;&#24456;&#22909;&#22320;&#37325;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#29305;&#21035;&#24403;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#24322;&#24120;&#24773;&#20917;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#26356;&#20026;&#26126;&#26174;&#12290;&#24403;&#36825;&#20123;&#24322;&#24120;&#24773;&#20917;&#26377;&#26631;&#31614;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#21322;&#30417;&#30563;&#65292;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#24573;&#30053;&#24322;&#24120;&#24773;&#20917;&#24182;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#35753;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#32467;&#26500;&#33021;&#22815;&#35753;&#27169;&#22411;&#23558;&#24050;&#30693;&#30340;&#24322;&#24120;&#28857;&#25490;&#38500;&#22312;&#27491;&#24120;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21033;&#29992;&#20102;&#23569;&#37327;&#24322;&#24120;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data. The goal of this work is to investigate approaches to allow reconstruction error-based architectures to instruct the model to put known anomalies outside of the domain description of the normal data. Specifically, our strategy exploits a limited number of anomalous e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#24182;&#20351;&#29992;&#19979;&#38480;&#25130;&#26029;&#65292;&#20197;&#21450;&#21363;&#26102;&#20272;&#35745;&#26410;&#30693;&#19979;&#38480;&#65292;&#26469;&#36817;&#20284;&#26368;&#23567;&#21270;&#35813;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.07583</link><description>&lt;p&gt;
MoMo: &#21160;&#37327;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#24182;&#20351;&#29992;&#19979;&#38480;&#25130;&#26029;&#65292;&#20197;&#21450;&#21363;&#26102;&#20272;&#35745;&#26410;&#30693;&#19979;&#38480;&#65292;&#26469;&#36817;&#20284;&#26368;&#23567;&#21270;&#35813;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26032;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MoMo&#21644;MoMo-Adam&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#21160;&#37327;&#65288;SGDM&#65289;&#30340;SGD&#21644;Adam&#26041;&#27861;&#19982;&#25105;&#20204;&#30340;&#26032;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;MoMo&#26041;&#27861;&#26159;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#38543;&#26426;&#20248;&#21270;&#26469;&#28608;&#21457;&#30340;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#27599;&#27425;&#36845;&#20195;&#37319;&#26679;&#30340;&#25209;&#37327;&#25439;&#22833;&#21644;&#26799;&#24230;&#30340;&#21160;&#37327;&#20272;&#35745;&#26469;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21033;&#29992;&#20102;&#24050;&#30693;&#25439;&#22833;&#20989;&#25968;&#19979;&#38480;&#30340;&#25130;&#26029;&#26041;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#22810;&#25968;&#25439;&#22833;&#37117;&#34987;&#19979;&#38480;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#36817;&#20284;&#26368;&#23567;&#21270;&#27492;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#12290;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#19979;&#38480;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#21363;&#26102;&#19979;&#38480;&#20272;&#35745;&#65292;&#36825;&#20123;&#20272;&#35745;&#29992;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MoMo&#26041;&#27861;&#22312;MNIST&#12289;CIFAR10&#12289;CIFAR100&#21644;Imagenet32&#31561;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#20998;&#31867;&#35757;&#32451;&#20013;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new adaptive learning rates that can be used with any momentum method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which are SGD with momentum (SGDM) and Adam together with our new adaptive learning rates. Our MoMo methods are motivated through model-based stochastic optimization, wherein we use momentum estimates of the batch losses and gradients sampled at each iteration to build a model of the loss function. Our model also makes use of any known lower bound of the loss function by using truncation. Indeed most losses are bounded below by zero. We then approximately minimize this model at each iteration to compute the next step. For losses with unknown lower bounds, we develop new on-the-fly estimates of the lower bound that we use in our model. Numerical experiments show that our MoMo methods improve over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR10, CIFAR100, Imagenet32, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20559;&#22909;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.05366</link><description>&lt;p&gt;
&#12298;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#12289;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24615;&#21450;&#24402;&#32435;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning. (arXiv:2304.05366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20559;&#22909;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#25351;&#20986;&#65292;&#27809;&#26377;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#65292;&#25110;&#32773;&#25152;&#26377;&#23398;&#20064;&#31639;&#27861;&#22312;&#22343;&#21248;&#20998;&#24067;&#30340;&#23398;&#20064;&#38382;&#39064;&#19978;&#24179;&#22343;&#31934;&#24230;&#36798;&#21040;&#23436;&#20840;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23450;&#29702;&#32463;&#24120;&#34987;&#24341;&#29992;&#26469;&#25903;&#25345;&#20010;&#21035;&#38382;&#39064;&#38656;&#35201;&#29305;&#21035;&#23450;&#21046;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#20960;&#20046;&#25152;&#26377;&#22343;&#21248;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#39640;&#22797;&#26434;&#24615;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#19981;&#25104;&#27604;&#20363;&#22320;&#20135;&#29983;&#20302;&#22797;&#26434;&#24230;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20063;&#20855;&#26377;&#21516;&#26679;&#30340;&#20559;&#22909;&#65292;&#36825;&#31181;&#20559;&#22909;&#20351;&#29992;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#29305;&#23450;&#39046;&#22495;&#35774;&#35745;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#21487;&#20197;&#21387;&#32553;&#21508;&#31181;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#39044;&#20808;&#35757;&#32451;&#21644;&#21363;&#20351;&#26159;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26356;&#21916;&#27426;&#29983;&#25104;&#20302;&#22797;&#26434;&#24230;&#30340;&#24207;&#21015;&#12290;&#23613;&#31649;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#20284;&#20046;&#34920;&#26126;&#21508;&#20010;&#38382;&#39064;&#38656;&#35201;&#19987;&#38376;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35299;&#37322;&#35828;&#65292;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#32534;&#30721;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we exp
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07272</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#22810;&#37325;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is the state of the art? Accounting for multiplicity in machine learning benchmark performance. (arXiv:2303.07272v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07272
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26469;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20801;&#35768;&#22810;&#31181;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#24182;&#36328;&#36234;&#26102;&#38388;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#38382;&#39064;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#24615;&#33021;&#34987;&#31216;&#20026;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#65292;&#24182;&#19988;&#34987;&#29992;&#20316;&#26032;&#26041;&#27861;&#20986;&#29256;&#30340;&#21442;&#32771;&#28857;&#12290;&#20294;&#20351;&#29992;&#26368;&#39640;&#25490;&#21517;&#30340;&#24615;&#33021;&#20316;&#20026;SOTA&#30340;&#20272;&#35745;&#20540;&#26159;&#19968;&#31181;&#26377;&#20559;&#30340;&#20272;&#35745;&#22120;&#65292;&#20250;&#32473;&#20986;&#36807;&#20110;&#20048;&#35266;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#22810;&#37325;&#24615;&#30340;&#26426;&#21046;&#26159;&#22810;&#37325;&#27604;&#36739;&#21644;&#22810;&#37325;&#26816;&#39564;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#20294;&#22312;&#20851;&#20110;SOTA&#20272;&#35745;&#30340;&#35752;&#35770;&#20013;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#25552;&#21450;&#12290;&#36807;&#20110;&#20048;&#35266;&#30340;&#26368;&#20808;&#36827;&#20272;&#35745;&#20540;&#34987;&#29992;&#20316;&#35780;&#20272;&#26032;&#26041;&#27861;&#30340;&#26631;&#20934;&#65292;&#32780;&#20855;&#26377;&#26126;&#26174;&#21155;&#21183;&#32467;&#26524;&#30340;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability 
&lt;/p&gt;</description></item><item><title>FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.03714</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Flow-Guided Density Ratio Learning. (arXiv:2303.03714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03714
&lt;/p&gt;
&lt;p&gt;
FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#65288;FDRL&#65289;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;DGflow&#20013;&#24341;&#20837;&#30340;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;f-&#25955;&#24230;&#30340;&#26799;&#24230;&#27969;&#30340;&#36807;&#26102;&#65288;&#26102;&#38388;&#26080;&#20851;&#65289;&#36817;&#20284;&#65292;&#24182;&#19988;&#36890;&#36807;GAN&#37492;&#21035;&#22120;&#32473;&#20986;&#30340;&#36807;&#26102;&#20272;&#35745;&#22120;&#36817;&#20284;&#20102;&#19981;&#21487;&#35745;&#31639;&#30340;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27604;&#12290;&#22312;&#26679;&#26412;&#32454;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#36275;&#22815;&#65292;&#22240;&#20026;&#27969;&#30340;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#26159;&#30456;&#36817;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#19988;&#36807;&#26102;&#20272;&#35745;&#22120;&#30340;&#26420;&#32032;&#24212;&#29992;&#30001;&#20110;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#22823;&#40511;&#27807;&#32780;&#22833;&#36133;&#12290;FDRL&#25552;&#20986;&#20102;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#20351;&#24471;FDRL&#33021;&#22815;&#29983;&#25104;&#39640;&#36798;$128\times128$&#23610;&#23544;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26799;&#24230;&#27969;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Flow-Guided Density Ratio Learning (FDRL), a simple and scalable approach to generative modeling which builds on the stale (time-independent) approximation of the gradient flow of entropy-regularized f-divergences introduced in DGflow. In DGflow, the intractable time-dependent density ratio is approximated by a stale estimator given by a GAN discriminator. This is sufficient in the case of sample refinement, where the source and target distributions of the flow are close to each other. However, this assumption is invalid for generation and a naive application of the stale estimator fails due to the large chasm between the two distributions. FDRL proposes to train a density ratio estimator such that it learns from progressively improving samples during the training process. We show that this simple method alleviates the density chasm problem, allowing FDRL to generate images of dimensions as high as $128\times128$, as well as outperform existing gradient flow baselines on qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#32447;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22312;&#20219;&#24847;&#25968;&#25454;&#27969;&#19978;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#25913;&#36827;&#30340;&#26368;&#20248;&#35299;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#23433;&#20840;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.12196</link><description>&lt;p&gt;
&#23545;&#22312;&#32447;&#20915;&#31574;&#21046;&#23450;&#30340;&#23545;&#25239;&#26657;&#20934;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Adversarial Calibrated Regression for Online Decision Making. (arXiv:2302.12196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#32447;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22312;&#20219;&#24847;&#25968;&#25454;&#27969;&#19978;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#25913;&#36827;&#30340;&#26368;&#20248;&#35299;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#23433;&#20840;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#31934;&#30830;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26159;&#20915;&#31574;&#21046;&#23450;&#21644;&#39044;&#27979;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#24403;&#25968;&#25454;&#19981;&#20877;&#36981;&#24490;&#35757;&#32451;&#26102;&#25152;&#35265;&#30340;&#20998;&#24067;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#22312;&#20219;&#24847;&#25968;&#25454;&#27969;&#19978;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#21253;&#25324;&#34987;&#23545;&#25163;&#36873;&#25321;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#40657;&#30418;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#20107;&#21518;&#26657;&#20934;&#65292;&#24182;&#20135;&#29983;&#21487;&#35777;&#26126;&#26657;&#20934;&#30340;&#36755;&#20986;&#65292;&#21363;80&#65285;&#32622;&#20449;&#21306;&#38388;&#23558;&#22312;80&#65285;&#30340;&#26102;&#38388;&#20869;&#21253;&#21547;&#30495;&#23454;&#32467;&#26524;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#26412;&#27169;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#20855;&#26377;&#36739;&#20302;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20915;&#31574;&#21046;&#23450;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#20219;&#21153;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#35266;&#23519;&#21040;&#21152;&#36895;&#25910;&#25947;&#21040;&#25913;&#36827;&#30340;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26377;&#21161;&#20110;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#23433;&#20840;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating uncertainty is an essential component of decision-making and forecasting in machine learning. However, existing uncertainty estimation methods may fail when data no longer follows the distribution seen during training. Here, we introduce online uncertainty estimation algorithms that are guaranteed to be reliable on arbitrary streams of data points, including data chosen by an adversary. Specifically, our algorithms perform post-hoc recalibration of a black-box regression model and produce outputs that are provably calibrated -- i.e., an 80% confidence interval will contain the true outcome 80% of the time -- and that have low regret relative to the learning objective of the base model. We apply our algorithms in the context of Bayesian optimization, an online model-based decision-making task in which the data distribution shifts over time, and observe accelerated convergence to improved optima. Our results suggest that robust uncertainty quantification has the pot
&lt;/p&gt;</description></item></channel></rss>