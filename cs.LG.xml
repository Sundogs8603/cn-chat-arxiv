<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#25581;&#31034;&#26435;&#37325;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#37319;&#26679;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01484</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#27169;&#24335;&#36830;&#25509;&#26159;&#21542;&#26159;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21487;&#34892;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#30340;&#20851;&#38190;&#65311;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01484
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25581;&#31034;&#26435;&#37325;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#37319;&#26679;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#26679;&#26412;&#25512;&#29702;&#65288;SBI&#65289;&#20013;&#65292;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25509;&#21463;&#26435;&#37325;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#29305;&#24449;&#20851;&#31995;&#65292;&#25104;&#21151;&#23454;&#29616;SBI&#26159;&#21487;&#33021;&#30340;&#65292;&#25581;&#31034;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#37319;&#26679;&#38382;&#39064;&#22256;&#38590;&#20043;&#38388;&#30340;&#31995;&#32479;&#32852;&#31995;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#37319;&#26679;&#21644;&#25910;&#25947;&#35786;&#26029;&#30340;&#23454;&#38469;&#25351;&#21335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks' parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a Bayesian deep ensemble approach as an effective solution with competitive performance and uncertainty quantification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01476</link><description>&lt;p&gt;
&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#24182;&#38656;&#35201;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#35299;&#20915;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23558;&#23545;&#31216;&#26680;&#24212;&#29992;&#20110;&#21464;&#20998;&#25512;&#26029;&#19979;&#30340;&#27880;&#24847;&#21147;&#26680;&#65307;&#28982;&#32780;&#65292;&#24573;&#30053;&#20102;&#27880;&#24847;&#21147;&#26680;&#26412;&#36136;&#19978;&#26159;&#19981;&#23545;&#31216;&#30340;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25512;&#23548;&#20986;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;GP&#21518;&#39564;&#30340;&#22797;&#26434;&#24230;&#20173;&#28982;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464; &#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;KEP-SVGP&#65292;i&#65289;&#30001;&#20110;&#19982;&#27880;&#24847;&#21147;&#26680;&#30340;KSVD&#30456;&#23545;&#24212;&#30340;&#20004;&#32452;&#22855;&#24322;&#21521;&#37327;&#24341;&#23548;&#30340;SVGP&#23545;&#23436;&#20840;&#34920;&#24449;&#20102;&#19981;&#23545;&#31216;&#24615;&#65307;ii&#65289;&#20165;&#20351;&#29992;&#23569;&#37327;&#19982;KSVD&#30456;&#23545;&#24212;&#30340;&#20276;&#38543;&#29305;&#24449;&#20989;&#25968;&#65292;&#25512;&#23548;SVGP&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36882;&#20943;&#27493;&#38271;&#26469;&#25913;&#36827;&#22312;&#20219;&#24847;&#24207;&#21015;&#19978;&#30340;&#35206;&#30422;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24635;&#20307;&#20998;&#20301;&#25968;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01139</link><description>&lt;p&gt;
&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#20013;&#24102;&#26377;&#36882;&#20943;&#27493;&#38271;
&lt;/p&gt;
&lt;p&gt;
Online conformal prediction with decaying step sizes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36882;&#20943;&#27493;&#38271;&#26469;&#25913;&#36827;&#22312;&#20219;&#24847;&#24207;&#21015;&#19978;&#30340;&#35206;&#30422;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24635;&#20307;&#20998;&#20301;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#24102;&#26377;&#36882;&#20943;&#27493;&#38271;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;&#12290;&#21644;&#20043;&#21069;&#30340;&#26041;&#27861;&#19968;&#26679;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#22312;&#20219;&#24847;&#24207;&#21015;&#19978;&#22238;&#28335;&#24615;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#20272;&#35745;&#20986;&#24635;&#20307;&#20998;&#20301;&#25968;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#23454;&#38469;&#29305;&#24615;&#65306;&#29305;&#21035;&#26159;&#22312;&#20998;&#24067;&#31283;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#35206;&#30422;&#29575;&#25509;&#36817;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#65292;&#19981;&#20165;&#20165;&#22312;&#35266;&#27979;&#24207;&#21015;&#30340;&#24179;&#22343;&#20540;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01092</link><description>&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Dynamical Model of Neural Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01092
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#38543;&#30528;&#35757;&#32451;&#26102;&#38388;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#39044;&#27979;&#24615;&#22320;&#25552;&#39640;&#65292;&#36328;&#22810;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#23450;&#24459;&#65292;&#23427;&#25253;&#21578;&#20102;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#22823;&#23567;&#26102;&#24615;&#33021;&#19982;&#35745;&#31639;&#25968;&#37327;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#20316;&#20026;&#32593;&#32476;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#21487;&#35299;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#22797;&#29616;&#20102;&#20851;&#20110;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#35768;&#22810;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;&#20026;&#20160;&#20040;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#29702;&#35770;&#39044;&#27979;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#65292;&#20854;&#20013;&#35757;&#32451;&#27493;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#24555;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#65292;&#19982;&#26368;&#36817;&#30340;&#23454;&#35777;&#35266;&#23519;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#65292;&#32593;&#32476;&#20250;&#25910;&#25947;&#21040;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65292;&#32467;&#21512;&#20004;&#31181;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;2:4&#25513;&#30721;&#21644;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#23454;&#29616;&#35757;&#32451;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2404.01847</link><description>&lt;p&gt;
&#20351;&#29992;2:4&#31232;&#30095;&#24615;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerating Transformer Pre-Training with 2:4 Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65292;&#32467;&#21512;&#20004;&#31181;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;2:4&#25513;&#30721;&#21644;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#23454;&#29616;&#35757;&#32451;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;Transformer&#24456;&#24930;&#65292;&#20294;&#26368;&#36817;GPU&#26550;&#26500;&#30340;&#21019;&#26032;&#20351;&#25105;&#20204;&#21344;&#25454;&#20248;&#21183;&#12290;NVIDIA&#30340;Ampere GPU&#21487;&#20197;&#27604;&#20854;&#23494;&#38598;&#31561;&#20215;&#29289;&#24555;&#20004;&#20493;&#30340;&#36895;&#24230;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;2:4&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#12290;&#22312;&#27492;&#24615;&#36136;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#8220;&#32763;&#36716;&#29575;&#8221;&#26469;&#30417;&#35270;2:4&#35757;&#32451;&#36807;&#31243;&#30340;&#31283;&#23450;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#65306;&#36890;&#36807;&#22312;&#26799;&#24230;&#19978;&#24212;&#29992;&#25513;&#30721;&#34928;&#20943;&#39033;&#20462;&#25913;&#31232;&#30095;&#31934;&#21270;&#30340;&#30452;&#36890;&#20272;&#35745;&#22120;&#65292;&#24182;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32467;&#26463;&#26102;&#38468;&#36817;&#24212;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23494;&#38598;&#24494;&#35843;&#36807;&#31243;&#26469;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#23454;&#38469;&#21152;&#36895;&#35757;&#32451;&#65306;&#36890;&#36807;&#21367;&#31215;&#35745;&#31639;&#21487;&#36716;&#32622;&#30340;2:4&#25513;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#21152;&#36895;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01847v1 Announce Type: new  Abstract: Training large Transformers is slow, but recent innovations on GPU architecture gives us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of Transformers in pre-training. First, we define a "flip rate" to monitor the stability of a 2:4 training process. Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model's quality by a simple yet effective dense fine-tuning procedure near the end of pre-training. Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by convolution, and to accelerate gated activation functions by reducing GPU L2 cach
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00195</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#22810;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multiple-policy Evaluation via Density Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#32473;&#23450;&#19968;&#32452; $K$ &#20010;&#30446;&#26631;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#20197;&#33267;&#23569; $1-\delta$ &#30340;&#27010;&#29575;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65288;&#26399;&#26395;&#24635;&#22870;&#21169;&#65289;&#36798;&#21040;&#31934;&#24230; $\epsilon$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20174;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#26469;&#21516;&#26102;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;$\mathrm{CAESAR}$ &#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20197;&#38543;&#30528; $\tilde{O}(\frac{1}{\epsilon})$ &#32553;&#25918;&#30340;&#20302;&#35746;&#21333;&#37319;&#26679;&#22797;&#26434;&#24615;&#29575;&#20135;&#29983;&#30446;&#26631;&#31574;&#30053;&#30340;&#35775;&#38382;&#20998;&#24067;&#30340;&#31895;&#30053;&#20272;&#35745;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36817;&#20284;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#36880;&#27493;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#26469;&#35745;&#31639;&#25152;&#26377;&#30446;&#26631;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00195v1 Announce Type: cross  Abstract: In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#21033;&#29992;LiDAR&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25513;&#25252;&#22320;&#22270;&#21644;&#28508;&#22312;&#23041;&#32961;&#22320;&#22270;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#25513;&#25252;&#21033;&#29992;&#12289;&#26368;&#23567;&#21270;&#26292;&#38706;&#20110;&#23041;&#32961;&#24182;&#39640;&#25928;&#21040;&#36798;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.20016</link><description>&lt;p&gt;
EnCoMP: &#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#38544;&#34109;&#26426;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#21033;&#29992;LiDAR&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25513;&#25252;&#22320;&#22270;&#21644;&#28508;&#22312;&#23041;&#32961;&#22320;&#22270;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#25513;&#25252;&#21033;&#29992;&#12289;&#26368;&#23567;&#21270;&#26292;&#38706;&#20110;&#23041;&#32961;&#24182;&#39640;&#25928;&#21040;&#36798;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#38544;&#34109;&#23548;&#33322;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#38656;&#35201;&#35782;&#21035;&#21644;&#21033;&#29992;&#29615;&#22659;&#25513;&#25252;&#21516;&#26102;&#20445;&#25345;&#26377;&#25928;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#35782;&#21035;&#21644;&#21033;&#29992;&#33258;&#28982;&#21644;&#20154;&#24037;&#29615;&#22659;&#29305;&#24449;&#20316;&#20026;&#25513;&#25252;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#26292;&#38706;&#20110;&#28508;&#22312;&#23041;&#32961;&#20043;&#19979;&#12290;&#25105;&#20204;&#30340;&#24863;&#30693;&#31649;&#36947;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#25513;&#25252;&#22320;&#22270;&#21644;&#28508;&#22312;&#23041;&#32961;&#22320;&#22270;&#65292;&#25552;&#20379;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#23454;&#38469;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#23398;&#20064;&#19968;&#20010;&#35780;&#20272;&#20505;&#36873;&#34892;&#21160;&#36136;&#37327;&#30340;&#24378;&#20581;&#31574;&#30053;&#65292;&#22522;&#20110;&#23427;&#20204;&#26368;&#22823;&#21270;&#25513;&#25252;&#21033;&#29992;&#12289;&#26368;&#23567;&#21270;&#26292;&#38706;&#20110;&#23041;&#32961;&#21644;&#39640;&#25928;&#21040;&#36798;&#30446;&#26631;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#30340;&#23454;&#38469;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20016v1 Announce Type: cross  Abstract: Cover navigation in complex environments is a critical challenge for autonomous robots, requiring the identification and utilization of environmental cover while maintaining efficient navigation. We propose an enhanced navigation system that enables robots to identify and utilize natural and artificial environmental features as cover, thereby minimizing exposure to potential threats. Our perception pipeline leverages LiDAR data to generate high-fidelity cover maps and potential threat maps, providing a comprehensive understanding of the surrounding environment. We train an offline reinforcement learning model using a diverse dataset collected from real-world environments, learning a robust policy that evaluates the quality of candidate actions based on their ability to maximize cover utilization, minimize exposure to threats, and reach the goal efficiently. Extensive real-world experiments demonstrate the superiority of our approach in
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.18969</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#23454;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models from Concept to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18969
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#26497;&#22823;&#25299;&#23485;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#26368;&#21021;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#24212;&#29992;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;GPT&#31995;&#21015;&#12290;&#36825;&#39033;&#25506;&#32034;&#32858;&#28966;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#39537;&#21160;&#24037;&#20855;&#22312;&#25913;&#38761;&#20256;&#32479;&#32534;&#30721;&#21644;&#38382;&#39064;&#35299;&#20915;&#31561;&#20219;&#21153;&#19978;&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#65292;&#21516;&#26102;&#22312;&#36328;&#36234;&#19981;&#21516;&#34892;&#19994;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#24320;&#36767;&#26032;&#36335;&#24452;&#12290;&#20174;&#20195;&#30721;&#35299;&#37322;&#21644;&#22270;&#20687;&#25551;&#36848;&#21040;&#20419;&#36827;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#25645;&#24314;&#21644;&#25512;&#36827;&#35745;&#31639;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20307;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;R2D2&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17905</link><description>&lt;p&gt;
&#20855;&#26377;R2D2&#30340;&#21487;&#25193;&#23637;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17905
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;R2D2&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#22825;&#25991;&#25104;&#20687;&#20013;&#24341;&#20837;&#30340;&#8220;&#29992;&#20110;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#32423;&#32852;DNN&#31995;&#21015;&#65288;R2D2&#65289;&#8221;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;R2D2&#30340;&#37325;&#24314;&#34987;&#24418;&#25104;&#20026;&#27531;&#24046;&#22270;&#20687;&#30340;&#31995;&#21015;&#65292;&#34987;&#36845;&#20195;&#22320;&#20272;&#35745;&#20026;&#25509;&#21463;&#19978;&#19968;&#27425;&#36845;&#20195;&#30340;&#22270;&#20687;&#20272;&#35745;&#21644;&#30456;&#20851;&#25968;&#25454;&#27531;&#24046;&#20316;&#20026;&#36755;&#20837;&#30340;DNN&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17905v1 Announce Type: cross  Abstract: We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10266</link><description>&lt;p&gt;
DSP&#65306;&#22810;&#32500;Transformer&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10266
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26681;&#25454;&#24403;&#21069;&#35745;&#31639;&#38454;&#27573;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#24615;&#32500;&#24230;&#65292;&#21033;&#29992;&#22810;&#32500;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;&#36825;&#31181;&#21160;&#24577;&#32500;&#24230;&#20999;&#25442;&#20351;&#24471;&#24207;&#21015;&#24182;&#34892;&#24615;&#22312;&#22810;&#32500;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;</title><link>https://arxiv.org/abs/2403.07008</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20215;&#27491;&#30830;: &#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoEval Done Right: Using Synthetic Data for Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#20351;&#29992;&#20154;&#24037;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21487;&#20197;&#20351;&#29992;AI&#26631;&#35760;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#27492;&#31867;&#30446;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#37327;&#65292;&#36825;&#19968;&#36807;&#31243;&#31216;&#20026;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20559;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982;GPT-4&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#23433;&#20840;&#24615;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#22312;&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;$\tilde{\mathcal{O}}(\sqrt{T})$&#30340;&#21518;&#24724;&#20540;&#65292;&#21516;&#26102;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05786</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#20048;&#35266;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimistic Safety for Linearly-Constrained Online Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#23433;&#20840;&#24615;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#22312;&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;$\tilde{\mathcal{O}}(\sqrt{T})$&#30340;&#21518;&#24724;&#20540;&#65292;&#21516;&#26102;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#26410;&#30693;&#32422;&#26463;&#19979;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#38382;&#39064;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#38745;&#24577;&#32447;&#24615;&#32422;&#26463;&#30340;&#38382;&#39064;&#29256;&#26412;&#65292;&#29609;&#23478;&#20250;&#25910;&#21040;&#22024;&#26434;&#21453;&#39304;&#24182;&#19988;&#24517;&#39035;&#22987;&#32456;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#21019;&#26032;&#30340;&#20048;&#35266;&#23433;&#20840;&#24615;&#35774;&#35745;&#33539;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#21518;&#24724;&#20540;&#20026;$\tilde{\mathcal{O}}(\sqrt{T})$&#12290;&#36825;&#19968;&#25913;&#36827;&#20102;&#20197;&#24448;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#26368;&#20339;&#21518;&#24724;&#20540;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;&#30053;&#24378;&#30340;&#29420;&#31435;&#22122;&#22768;&#21644;&#26080;&#24847;&#35782;&#23545;&#25163;&#30340;&#20551;&#35774;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#36825;&#20010;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#20855;&#26377;&#30456;&#21516;&#30340;&#21518;&#24724;&#20540;&#20445;&#35777;&#65292;&#24182;&#19988;&#20174;&#26399;&#26395;&#19978;&#27704;&#36828;&#19981;&#20250;&#36829;&#21453;&#32422;&#26463;&#12290;&#36825;&#20026;OCO&#22312;&#26102;&#21464;&#38543;&#26426;&#32422;&#26463;&#19979;&#30340;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#20808;&#36827;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05786v1 Announce Type: new  Abstract: The setting of online convex optimization (OCO) under unknown constraints has garnered significant attention in recent years. In this work, we consider a version of this problem with static linear constraints that the player receives noisy feedback of and must always satisfy. By leveraging our novel design paradigm of optimistic safety, we give an algorithm for this problem that enjoys $\tilde{\mathcal{O}}(\sqrt{T})$ regret. This improves on the previous best regret bound of $\tilde{\mathcal{O}}(T^{2/3})$ while using only slightly stronger assumptions of independent noise and an oblivious adversary. Then, by recasting this problem as OCO under time-varying stochastic linear constraints, we show that our algorithm enjoys the same regret guarantees in such a setting and never violates the constraints in expectation. This contributes to the literature on OCO under time-varying stochastic constraints, where the state-of-the-art algorithms en
&lt;/p&gt;</description></item><item><title>3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03954</link><description>&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
3D Diffusion Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03954
&lt;/p&gt;
&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20026;&#25945;&#25480;&#26426;&#22120;&#20154;&#28789;&#24039;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#23398;&#20064;&#22797;&#26434;&#32780;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#25216;&#33021;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#28436;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#34701;&#20837;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#30340;&#26032;&#39062;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#25193;&#25955;&#31574;&#30053;&#26159;&#19968;&#31867;&#26377;&#26465;&#20214;&#30340;&#21160;&#20316;&#29983;&#25104;&#27169;&#22411;&#12290;DP3&#30340;&#26680;&#24515;&#35774;&#35745;&#26159;&#21033;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;3D&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#26159;&#20174;&#31232;&#30095;&#28857;&#20113;&#20013;&#25552;&#21462;&#20986;&#26469;&#30340;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#28857;&#32534;&#30721;&#22120;&#12290;&#22312;&#25105;&#20204;&#28085;&#30422;&#20102;72&#20010;&#20223;&#30495;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;DP3&#20165;&#38656;&#35201;10&#20010;&#28436;&#31034;&#23601;&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;55.3%&#12290;&#22312;4&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;DP3&#34920;&#29616;&#20986;&#20102;&#39640;&#25104;&#21151;&#29575;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#27599;&#39033;&#20219;&#21153;&#20165;&#38656;40&#27425;&#28436;&#31034;&#21363;&#21487;&#25104;&#21151;&#29575;&#20026;85%&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
&lt;/p&gt;</description></item><item><title>&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.00993</link><description>&lt;p&gt;
&#35770;&#37096;&#20998;&#21487;&#35266;&#23519;&#24207;&#21015;&#22242;&#38431;&#21644;&#28216;&#25103;&#20013;&#20449;&#24687;&#32467;&#26500;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00993
&lt;/p&gt;
&lt;p&gt;
&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#20449;&#24687;&#32467;&#26500;&#25551;&#36848;&#20102;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#21051;&#20107;&#20214;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#12290;&#26412;&#25991;&#20027;&#24352;&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20855;&#26377;&#26126;&#30830;&#20449;&#24687;&#32467;&#26500;&#34920;&#31034;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00993v1 Announce Type: cross  Abstract: In a sequential decision-making problem, the information structure is the description of how events in the system occurring at different points in time affect each other. Classical models of reinforcement learning (e.g., MDPs, POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure.   In this paper, we argue for the perspective that explicit representation of information structures is an important component of analyzing and solving reinforcement learning problems. We propose novel reinforcement learning models with an explicit representation of information structure, capturing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#35782;&#21035;&#25968;&#23383;&#23402;&#29983;&#20307;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38750;&#20405;&#20837;&#24335;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.00177</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#38750;&#20405;&#20837;&#24335;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#35782;&#21035;&#25968;&#23383;&#23402;&#29983;&#20307;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38750;&#20405;&#20837;&#24335;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#20307;&#26159;&#23454;&#29616;&#23454;&#38469;&#29289;&#29702;&#29616;&#35937;&#30340;&#34394;&#25311;&#22797;&#21046;&#21697;&#65292;&#21033;&#29992;&#25968;&#23398;&#24314;&#27169;&#26469;&#34920;&#24449;&#21644;&#27169;&#25311;&#20854;&#23450;&#20041;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;&#30142;&#30149;&#36807;&#31243;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#34892;&#20223;&#30495;&#65292;&#27169;&#25311;&#24739;&#32773;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#22312;&#20551;&#35774;&#24178;&#39044;&#19979;&#30340;&#23545;&#29031;&#32467;&#26524;&#12290;&#36825;&#28040;&#38500;&#20102;&#20405;&#20837;&#24615;&#31243;&#24207;&#25110;&#19981;&#30830;&#23450;&#27835;&#30103;&#20915;&#31574;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#21033;&#29992;&#38750;&#20405;&#20837;&#24335;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#26469;&#35782;&#21035;&#25968;&#23383;&#23402;&#29983;&#20307;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25968;&#23383;&#23402;&#29983;&#20307;&#24314;&#27169;&#30475;&#20316;&#19968;&#20010;&#22797;&#21512;&#36870;&#38382;&#39064;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#39318;&#20808;&#22312;&#35299;&#20915;&#29289;&#29702;&#27169;&#22411;&#26041;&#31243;&#30340;&#20551;&#23450;&#20219;&#21153;&#19978;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#38543;&#21518;&#65292;&#35813;&#27169;&#22411;&#34987;&#35757;&#32451;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00177v1 Announce Type: new  Abstract: A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of solving the physical model equations. Subsequently, the model is trained to rec
&lt;/p&gt;</description></item><item><title>DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.17453</link><description>&lt;p&gt;
DS-Agent&#65306;&#36890;&#36807;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26696;&#20363;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17453
&lt;/p&gt;
&lt;p&gt;
DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#20197;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#29702;&#35299;&#20219;&#21153;&#35201;&#27714;&#65292;&#28982;&#21518;&#26500;&#24314;&#21644;&#35757;&#32451;&#26368;&#21512;&#36866;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;LLM&#20195;&#29702;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;&#23454;&#39564;&#35745;&#21010;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DS-Agent&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26032;&#39062;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;&#22312;&#24320;&#21457;&#38454;&#27573;&#65292;DS-Agent&#36981;&#24490;CBR&#26694;&#26550;&#26469;&#26500;&#24314;&#33258;&#21160;&#36845;&#20195;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#28789;&#27963;&#21033;&#29992;&#26469;&#33258;Kaggle&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#20419;&#36827;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;DS-Agent&#23454;&#29616;&#20102;&#19968;&#20010;&#20302;&#36164;&#28304;&#37096;&#32626;&#38454;&#27573;&#65292;&#37319;&#29992;&#31616;&#21270;&#30340;CBR&#33539;&#20363;&#26469;&#36866;&#24212;&#24320;&#21457;&#38454;&#27573;&#25104;&#21151;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#36827;&#34892;&#30452;&#25509;&#20195;&#30721;&#29983;&#25104;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17453v1 Announce Type: new  Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing th
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13448</link><description>&lt;p&gt;
ED-Copilot: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#36741;&#21161;&#20943;&#23569;&#24613;&#35786;&#31185;&#31561;&#24453;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24613;&#35786;&#31185;&#65288;ED&#65289;&#20013;&#65292;&#24739;&#32773;&#22312;&#35786;&#26029;&#21069;&#38656;&#35201;&#36827;&#34892;&#20998;&#35786;&#21644;&#22810;&#31181;&#23454;&#39564;&#23460;&#26816;&#27979;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23548;&#33268;&#24613;&#35786;&#31185;&#25317;&#25380;&#65292;&#26174;&#33879;&#24433;&#21709;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;&#21307;&#30103;&#38169;&#35823;&#12289;&#20154;&#21592;&#26543;&#31469;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#26102;&#38388;&#65289;&#25104;&#26412;&#26377;&#25928;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21327;&#21161;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#25105;&#20204;&#19982;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#21512;&#20316;&#31574;&#21010;&#20102;MIMIC-ED-Assist&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24314;&#35758;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#24613;&#35786;&#31561;&#24453;&#26102;&#38388;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#24182;&#22312;&#27491;&#30830;&#39044;&#27979;&#35832;&#22914;&#27515;&#20129;&#20043;&#31867;&#20851;&#38190;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ED-Copilot&#65292;&#23427;&#20381;&#27425;&#24314;&#35758;&#24739;&#32773;&#29305;&#23450;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;ED-Copilot&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#24739;&#32773;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#24182;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.13144</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Neural Network Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13144
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20063;&#21487;&#20197;\textit{&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;}&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65292;&#21033;&#29992;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26631;&#20934;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#21462;&#20102;&#37096;&#20998;&#21463;&#35757;&#32593;&#32476;&#21442;&#25968;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#21518;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#36825;&#20123;&#28508;&#22312;&#21442;&#25968;&#34920;&#31034;&#12290;&#23427;&#29983;&#25104;&#20102;&#26032;&#30340;&#34920;&#31034;&#65292;&#32463;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#65292;&#36755;&#20986;&#20934;&#22791;&#29992;&#20316;&#26032;&#30340;&#32593;&#32476;&#21442;&#25968;&#23376;&#38598;&#12290;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#25193;&#25955;&#36807;&#31243;&#22987;&#32456;&#29983;&#25104;&#24615;&#33021;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#38468;&#21152;&#25104;&#26412;&#26497;&#23567;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.11887</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Semi-supervised Graph Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#32771;&#34385;&#20102;&#19968;&#20010;&#23454;&#38469;&#24773;&#22659;&#19979;&#30340;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#65292;&#22312;&#36825;&#20010;&#24773;&#22659;&#20013;&#65292;&#22270;&#20013;&#30340;&#37096;&#20998;&#33410;&#28857;&#34987;&#30693;&#26195;&#26159;&#27491;&#24120;&#30340;&#65292;&#19982;&#22823;&#22810;&#25968;GAD&#30740;&#31350;&#20013;&#20351;&#29992;&#23436;&#20840;&#26410;&#26631;&#35760;&#22270;&#30340;&#26080;&#30417;&#30563;&#24773;&#20917;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#26377;&#21161;&#20110;&#25552;&#21319;&#29616;&#26377;&#26080;&#30417;&#30563;GAD&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#24773;&#22659;&#19979;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#30340;&#21033;&#29992;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#21322;&#30417;&#30563;&#24773;&#22659;&#30340;&#29983;&#25104;&#24335;GAD&#26041;&#27861;&#65288;GGAD&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#23427;&#20204;&#34701;&#21512;&#20102;&#26412;&#22320;&#32467;&#26500;&#21644;&#33410;&#28857;&#34920;&#31034;&#65292;&#20026;&#35757;&#32451;&#21028;&#21035;&#22411;&#21333;&#31867;&#20998;&#31867;&#22120;&#25552;&#20379;&#26377;&#25928;&#30340;&#36127;&#38754;&#33410;&#28857;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
&lt;/p&gt;</description></item><item><title>LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#28040;&#38500;&#20102;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#26377;&#21161;&#20110;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#20302;&#31209;&#35299;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11867</link><description>&lt;p&gt;
LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#27809;&#26377;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
LoRA Training in the NTK Regime has No Spurious Local Minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11867
&lt;/p&gt;
&lt;p&gt;
LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#28040;&#38500;&#20102;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#26377;&#21161;&#20110;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#20302;&#31209;&#35299;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24050;&#25104;&#20026;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;LoRA&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#27169;&#24335;&#19979;&#20351;&#29992;LoRA&#24494;&#35843;&#65292;&#20854;&#20013;&#21253;&#21547;$N$&#20010;&#25968;&#25454;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#65306;(i) &#20840;&#38754;&#24494;&#35843;&#65288;&#19981;&#20351;&#29992;LoRA&#65289;&#20801;&#35768;&#31209;&#20026;$r\lesssim \sqrt{N}$&#30340;&#20302;&#31209;&#35299;; (ii) &#20351;&#29992;&#31209;&#20026;$r\gtrsim \sqrt{N}$&#30340;LoRA&#28040;&#38500;&#20102;&#34394;&#20551;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20351;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25214;&#21040;&#20302;&#31209;&#35299;; (iii) &#20351;&#29992;LoRA&#25214;&#21040;&#30340;&#20302;&#31209;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11867v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;</title><link>https://arxiv.org/abs/2402.11639</link><description>&lt;p&gt;
&#20855;&#26377;Transformer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;Softmax&#27880;&#24847;&#21147;&#36866;&#24212;&#20989;&#25968;Lipschitz&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24615;&#26159;&#20854;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#22312;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#26576;&#20123;&#25968;&#25454;&#38544;&#24335;&#22320;&#34987;&#21576;&#29616;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#32972;&#26223;&#65292;&#24182;&#34987;&#35201;&#27714;&#22312;&#35813;&#32972;&#26223;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#24517;&#39035;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#32972;&#26223;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;Softmax&#27880;&#24847;&#21147;&#22312;&#19968;&#20010;ICL&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#27599;&#20010;&#32972;&#26223;&#37117;&#32534;&#30721;&#20102;&#19968;&#20010;&#22238;&#24402;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20064;&#19968;&#20010;&#31383;&#21475;&#65292;&#29992;&#20110;&#23454;&#29616;&#19968;&#20010;&#36866;&#24212;&#20110;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26368;&#36817;&#37051;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31383;&#21475;&#38543;&#30528;Lipschitzness&#30340;&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#30340;&#22686;&#21152;&#32780;&#25193;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#20302;&#31209;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#65292;&#27880;&#24847;&#21147;&#21333;&#20803;&#22312;&#25512;&#29702;&#20043;&#21069;&#23398;&#20250;&#20102;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#36866;&#24212;&#24615;&#20851;&#38190;&#22320;&#20381;&#36182;&#20110;softmax&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11639v1 Announce Type: new  Abstract: A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.11592</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#35752;&#38646;&#38454;&#20248;&#21270;&#22312;&#20869;&#23384;&#39640;&#25928;LLM&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#20351;&#29992;SGD&#21644;Adam&#31561;&#19968;&#38454;&#65288;FO&#65289;&#20248;&#21270;&#22120;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#20307;&#31215;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;FO&#26799;&#24230;&#35745;&#31639;&#30340;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#24102;&#26469;&#30340;&#24040;&#22823;&#20869;&#23384;&#24320;&#38144;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#20869;&#23384;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#35774;&#22791;&#31471;&#35757;&#32451;&#31561;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21521;&#19981;&#20351;&#29992;BP&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#65292;&#26500;&#24314;&#22312;MeZO&#25552;&#20986;&#30340;&#27010;&#24565;&#22522;&#30784;&#19978;&#12290;&#19982;&#20256;&#32479;&#30340;ZO&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25506;&#32034;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#12289;&#39318;&#27425;&#25512;&#20986;&#30340;&#22522;&#20934;&#30740;&#31350;&#36328;&#36234;&#20116;&#20010;LLM&#31995;&#21015;&#65288;Roberta&#65292;OPT&#65292;LLaMA&#65292;Vicuna&#65292;Mistral&#65289;&#65292;&#19977;&#31181;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#20116;&#31181;&#24494;&#35843;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
&lt;/p&gt;</description></item><item><title>AdAdaGrad&#21644;AdAdaGradNorm&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#21152;&#25209;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#35777;&#26126;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#22312;$O(1/K)$&#36895;&#24230;&#19979;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.11215</link><description>&lt;p&gt;
AdAdaGrad&#65306;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11215
&lt;/p&gt;
&lt;p&gt;
AdAdaGrad&#21644;AdAdaGradNorm&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#21152;&#25209;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#35777;&#26126;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#22312;$O(1/K)$&#36895;&#24230;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#22120;&#20013;&#25209;&#37327;&#22823;&#23567;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21464;&#21270;&#25209;&#22823;&#23567;&#30340;&#23454;&#36341;&#30456;&#23545;&#20854;&#20182;&#36229;&#21442;&#25968;&#36739;&#23569;&#25506;&#35752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#20013;&#23548;&#20986;&#30340;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#20256;&#32479;&#19978;&#20165;&#24212;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#32771;&#34385;&#21040;&#23398;&#20064;&#36895;&#29575;&#21644;&#25209;&#22823;&#23567;&#20043;&#38388;&#30340;&#26174;&#33879;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#38656;&#35201;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;AdAdaGrad&#21450;&#20854;&#26631;&#37327;&#21464;&#20307;AdAdaGradNorm&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#22686;&#21152;&#25209;&#22823;&#23567;&#65292;&#21516;&#26102;&#20351;&#29992;AdaGrad&#21644;AdaGradNorm&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#20197;$O(1/K)$&#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#29992;&#20110;&#25214;&#21040;&#20809;&#28369;&#38750;&#20984;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#22312;$K$&#27425;&#36845;&#20195;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11215v1 Announce Type: new  Abstract: The choice of batch sizes in stochastic gradient optimizers is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10958</link><description>&lt;p&gt;
&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;: &#36890;&#36807;&#23545;&#30456;&#21516;&#21644;&#19981;&#21516;&#25552;&#31034;&#30340;&#23545;&#27604;&#21709;&#24212;&#22686;&#24378;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#65292;&#23558;&#27169;&#22411;&#19982;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#30452;&#25509;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;DPO&#36890;&#36807;&#20351;&#29992;&#20174;&#30456;&#21516;&#25552;&#31034;&#20013;&#27966;&#29983;&#30340;&#20559;&#22909;&#23545;&#26469;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DPO&#24182;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#20154;&#31867;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#23398;&#20064;&#24448;&#24448;&#28041;&#21450;&#23545;&#19981;&#20165;&#30456;&#21516;&#32780;&#19988;&#30456;&#20284;&#38382;&#39064;&#30340;&#23545;&#27604;&#21709;&#24212;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#12290;RPO&#26088;&#22312;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#12290;&#23427;&#24341;&#20837;&#20102;&#23545;&#27604;&#21152;&#26435;&#26426;&#21046;&#65292;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#21253;&#25324;&#25104;&#23545;&#21644;&#19981;&#25104;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIPS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#65292;&#22635;&#34917;&#20102;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.10758</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Stochastic Localization via Iterative Posterior Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIPS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#65292;&#22635;&#34917;&#20102;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22312;&#22522;&#20110;&#24471;&#20998;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#36817;&#26399;&#23545;&#38543;&#26426;&#23450;&#20301;&#25216;&#26415;&#20135;&#29983;&#20102;&#26032;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#20154;&#20204;&#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#65288;&#31216;&#20026;&#35266;&#27979;&#36807;&#31243;&#65289;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#26679;&#26412;&#24341;&#20837;&#22122;&#22768;&#65292;&#24182;&#36880;&#28176;&#23398;&#20064;&#19982;&#35813;&#21160;&#21147;&#23398;&#20851;&#32852;&#30340;&#21435;&#22122;&#22120;&#12290;&#38500;&#20102;&#29305;&#23450;&#24212;&#29992;&#20043;&#22806;&#65292;&#23545;&#20110;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#65292;&#23545;&#38543;&#26426;&#23450;&#20301;&#30340;&#20351;&#29992;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#26412;&#39033;&#24037;&#20316;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38543;&#26426;&#23450;&#20301;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#26126;&#30830;&#30340;&#35266;&#27979;&#36807;&#31243;&#65292;&#19982;&#28789;&#27963;&#30340;&#21435;&#22122;&#26102;&#38388;&#34920;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#26041;&#27861;&#35770;&#65292;&#21363;&#8220;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#8221;&#65288;SLIPS&#65289;&#65292;&#20197;&#33719;&#24471;&#35813;&#21160;&#21147;&#23398;&#30340;&#36817;&#20284;&#26679;&#26412;&#65292;&#24182;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#26679;&#26412;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10758v1 Announce Type: cross  Abstract: Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimati
&lt;/p&gt;</description></item><item><title>DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10379</link><description>&lt;p&gt;
DataDreamer: &#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10379
&lt;/p&gt;
&lt;p&gt;
DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#20219;&#21153;&#35780;&#20272;&#12289;&#24494;&#35843;&#12289;&#25552;&#28860;&#20197;&#21450;&#20854;&#20182;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#30740;&#31350;&#24037;&#20316;&#27969;&#20013;&#20351;&#29992;LLMs&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#23427;&#20204;&#30340;&#35268;&#27169;&#12289;&#38381;&#28304;&#24615;&#36136;&#20197;&#21450;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#24037;&#20316;&#27969;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#36805;&#36895;&#23835;&#36215;&#21644;&#36825;&#20123;&#29420;&#29305;&#25361;&#25112;&#23545;&#24320;&#25918;&#31185;&#23398;&#21644;&#20351;&#29992;&#23427;&#20204;&#30340;&#24037;&#20316;&#30340;&#21487;&#37325;&#29616;&#24615;&#20135;&#29983;&#20102;&#30452;&#25509;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataDreamer&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#32534;&#20889;&#31616;&#21333;&#30340;&#20195;&#30721;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#12290;DataDreamer&#36824;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#36981;&#24490;&#25105;&#20204;&#25552;&#20986;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#40723;&#21169;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#24211;&#21644;&#25991;&#26723;&#21487;&#22312;h&#32593;&#31449;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10379v1 Announce Type: new  Abstract: Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Generative Flow Networks (GFlowNets)&#23398;&#20064;&#19968;&#20010;&#38543;&#26426;&#31574;&#30053;&#65292;&#20197;&#36817;&#20284;&#23454;&#29616;&#22312;&#25972;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#27969;&#37327;&#30340;&#23432;&#24658;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22810;&#36335;&#24452;&#29983;&#25104;&#30456;&#21516;&#23545;&#35937;&#30340;&#20559;&#20506;&#20998;&#24067;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10309</link><description>&lt;p&gt;
&#31163;&#25955;&#27010;&#29575;&#25512;&#26029;&#20316;&#20026;&#22810;&#36335;&#24452;&#29615;&#22659;&#20013;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Discrete Probabilistic Inference as Control in Multi-path Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10309
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Generative Flow Networks (GFlowNets)&#23398;&#20064;&#19968;&#20010;&#38543;&#26426;&#31574;&#30053;&#65292;&#20197;&#36817;&#20284;&#23454;&#29616;&#22312;&#25972;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#27969;&#37327;&#30340;&#23432;&#24658;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22810;&#36335;&#24452;&#29983;&#25104;&#30456;&#21516;&#23545;&#35937;&#30340;&#20559;&#20506;&#20998;&#24067;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20174;&#31163;&#25955;&#19988;&#32467;&#26500;&#21270;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#38543;&#26426;&#31574;&#30053;&#65292;&#20351;&#29289;&#20307;&#22312;&#36825;&#20010;&#39034;&#24207;&#36807;&#31243;&#32467;&#26463;&#26102;&#20197;&#26576;&#20123;&#39044;&#23450;&#20041;&#22870;&#21169;&#30340;&#27604;&#20363;&#34987;&#37319;&#26679;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#32416;&#27491;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#30001;&#26368;&#20339;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#30340;&#36793;&#38469;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10309v1 Announce Type: new  Abstract: We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09698</link><description>&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#36807;&#28388;&#22120;&#20013;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Combining Evidence Across Filtrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#26102;&#21051;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#29702;&#20013;&#65292;&#24050;&#30693;&#20219;&#20309;&#21487;&#25509;&#21463;&#30340;&#25512;&#29702;&#26041;&#27861;&#24517;&#39035;&#22522;&#20110;&#27979;&#35797;&#38789;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24191;&#20041;&#21270;&#65292;&#31216;&#20026;e&#36827;&#31243;&#65292;&#23427;&#20204;&#26159;&#38750;&#36127;&#36827;&#31243;&#65292;&#20854;&#22312;&#20219;&#20309;&#20219;&#24847;&#20572;&#26102;&#30340;&#26399;&#26395;&#19978;&#30028;&#19981;&#36229;&#36807;&#19968;&#12290;e&#36827;&#31243;&#37327;&#21270;&#20102;&#38024;&#23545;&#22797;&#21512;&#38646;&#20551;&#35774;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#30340;&#32047;&#31215;&#35777;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#20449;&#24687;&#38598;&#65288;&#21363;&#36807;&#28388;&#22120;&#65289;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#21512;&#24182;&#26041;&#27861;&#65292;&#38024;&#23545;&#19968;&#20010;&#38646;&#20551;&#35774;&#12290;&#23613;&#31649;&#22312;&#30456;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#21487;&#20197;&#36731;&#26494;&#22320;&#21512;&#24182;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#24179;&#22343;&#65289;&#65292;&#20294;&#22312;&#19981;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#19981;&#33021;&#37027;&#20040;&#23481;&#26131;&#22320;&#21512;&#24182;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36739;&#31895;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#19981;&#33021;&#36716;&#25442;&#20026;&#22312;&#26356;&#32454;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25991;&#29486;&#20013;&#19977;&#20010;&#20855;&#20307;&#20363;&#23376;&#65306;&#21487;&#20132;&#25442;&#24615;&#27979;&#35797;&#65292;&#29420;&#31435;&#24615;&#27979;&#35797;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09698v1 Announce Type: cross  Abstract: In anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one. An e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes. This paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis. Even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration. We discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#25915;&#20987;&#32773;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65292;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.09179</link><description>&lt;p&gt;
&#24555;&#36895;&#37319;&#29992;&#65292;&#38544;&#34255;&#39118;&#38505;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#30340;&#21452;&#37325;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#25915;&#20987;&#32773;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65292;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#23548;&#33268;&#20102;&#20687;GPT&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#20419;&#36827;&#23450;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#31532;&#19977;&#26041;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19982;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#38598;&#25104;&#30340;&#24212;&#29992;&#30340;&#39318;&#20010;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23558;&#21518;&#38376;&#23884;&#20837;&#21040;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29256;&#26412;&#20013;&#65292;&#24403;&#36755;&#20837;&#21253;&#21547;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#26102;&#65292;&#36755;&#20986;&#25915;&#20987;&#32773;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65306;&#21333;&#35789;&#32423;&#21035;&#12289;&#35821;&#27861;&#32423;&#21035;&#21644;&#35821;&#20041;&#32423;&#21035;&#65292;&#37319;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#20855;&#26377;&#36880;&#27493;&#38544;&#34109;&#24615;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25110;&#20219;&#20309;&#20462;&#25913;&#65292;&#20005;&#26684;&#36981;&#24490;GPT&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09179v1 Announce Type: cross Abstract: The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs devel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#23545;&#20598;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#23548;&#27169;&#22411;&#36924;&#36817;&#30495;&#23454;&#26465;&#20214;&#20998;&#24067;&#24182;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.08733</link><description>&lt;p&gt;
&#19987;&#23478;&#19981;&#20316;&#24330;: &#36890;&#36807;&#39044;&#27979;&#23545;&#20598;&#26469;&#23398;&#20064;&#26410;&#30693;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#23545;&#20598;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#23548;&#27169;&#22411;&#36924;&#36817;&#30495;&#23454;&#26465;&#20214;&#20998;&#24067;&#24182;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;${\widehat{p}}_{\theta}(Y|X)$&#23545;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;$p(Y|X)$&#30340;&#20102;&#35299;&#31243;&#24230;&#30340;&#20934;&#30830;&#35780;&#20272;&#23545;&#20110;&#36991;&#20813;&#20135;&#29983;&#38169;&#35823;&#25110;"&#34394;&#26500;"&#30340;&#31572;&#26696;&#25110;&#37319;&#21462;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#65292;&#28982;&#32780;&#36825;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27010;&#29575;&#39044;&#27979;&#19981;&#33021;&#21306;&#20998;&#27599;&#20010;&#21709;&#24212;&#30340;&#22122;&#22768;&#65288;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#23545;&#36807;&#31243;&#30340;&#19981;&#20102;&#35299;&#65288;&#19987;&#39064;&#19981;&#30830;&#23450;&#24615;&#65289;&#65292;&#32780;&#29616;&#26377;&#30340;&#19987;&#39064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#24448;&#24448;&#22312;&#27169;&#22411;&#27424;&#25311;&#21512;&#26102;&#36807;&#20110;&#33258;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#65292;&#21487;&#20197;&#25945;&#23548;&#27169;&#22411;&#21516;&#26102;&#36924;&#36817;$p(Y|X)$&#24182;&#20272;&#35745;${\widehat{p}}_{\theta}(Y|X)$&#19982;$p(Y|X)$&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#26469;&#33258;&#30495;&#23454;&#26465;&#20214;&#20998;&#24067;&#30340;&#29420;&#31435;&#21709;&#24212;&#23545;&#65292;&#20801;&#35768;&#23427;&#22312;&#39044;&#27979;&#19968;&#20010;&#21709;&#24212;&#26102;&#35266;&#23519;&#21478;&#19968;&#20010;&#21709;&#24212;&#65292;&#28982;&#21518;&#27979;&#37327;&#23427;&#30340;&#20316;&#24330;&#31243;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#27169;&#22411;&#30340;&#19987;&#39064;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08733v1 Announce Type: new Abstract: Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or "hallucinated" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to "cheat" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraphormer&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23376;&#22270;GNNs&#21644;&#22270;&#21464;&#25442;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#32508;&#21512;&#20102;&#23376;&#22270;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20197;&#21450;&#22270;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#32534;&#30721;&#12290;&#22522;&#20110;&#23376;&#22270;GNNs&#19982;&#22270;&#30340;&#20056;&#31215;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#20056;&#31215;&#22270;&#19978;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#22270;GNNs&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.08450</link><description>&lt;p&gt;
Subgraphormer:&#36890;&#36807;&#22270;&#30340;&#20056;&#31215;&#23558;&#23376;&#22270;GNN&#21644;&#22270;&#21464;&#25442;&#22120;&#32479;&#19968;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08450
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraphormer&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23376;&#22270;GNNs&#21644;&#22270;&#21464;&#25442;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#32508;&#21512;&#20102;&#23376;&#22270;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20197;&#21450;&#22270;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#32534;&#30721;&#12290;&#22522;&#20110;&#23376;&#22270;GNNs&#19982;&#22270;&#30340;&#20056;&#31215;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#20056;&#31215;&#22270;&#19978;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#22270;GNNs&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#39046;&#22495;&#20013;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#20004;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65306;&#23376;&#22270;GNN&#21644;&#22270;&#21464;&#25442;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;Subgraphormer&#65292;&#23427;&#23558;&#23376;&#22270;GNNs&#30340;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12289;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#21644;&#32858;&#21512;&#26041;&#26696;&#19982;&#22270;&#21464;&#25442;&#22120;&#20013;&#26368;&#37325;&#35201;&#30340;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25105;&#20204;&#25581;&#31034;&#30340;&#23376;&#22270;GNNs&#19982;&#20056;&#31215;&#22270;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#65292;&#36825;&#34920;&#26126;&#23376;&#22270;GNNs&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#22312;&#22270;&#30340;&#20056;&#31215;&#19978;&#25805;&#20316;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;(MPNNs)&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20844;&#24335;&#26469;&#35774;&#35745;&#25105;&#20204;&#30340;&#26550;&#26500;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#20056;&#31215;&#22270;&#30340;&#36830;&#25509;&#24615;&#35774;&#35745;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#23376;&#22270;GNNs&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#23558;&#20854;&#25512;&#23548;&#20026;&#20056;&#31215;&#22270;&#30340;&#20301;&#32622;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. 
&lt;/p&gt;</description></item><item><title>BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08219</link><description>&lt;p&gt;
BBox-Adapter: &#36731;&#37327;&#32423;&#36866;&#37197;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08219
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#21644;Gemini&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20219;&#21153;&#30340;&#35201;&#27714;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#21442;&#25968;&#12289;&#23884;&#20837;&#21644;&#36755;&#20986;&#27010;&#29575;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#33021;&#36890;&#36807;&#23427;&#20204;&#30340;API&#26381;&#21153;&#36866;&#24212;&#36825;&#20123;&#40657;&#30418;LLMs&#65292;&#36825;&#24341;&#21457;&#20102;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BBox-Adapter&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#40657;&#30418;LLMs&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#12290;BBox-Adapter&#36890;&#36807;&#23558;&#30446;&#26631;&#25968;&#25454;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#28304;&#25968;&#25454;&#35270;&#20026;&#36127;&#26679;&#26412;&#26469;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#26469;&#25552;&#39640;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#24809;&#32602;&#28304;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#23558;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#12289;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#30340;&#23454;&#26102;&#27491;&#26679;&#26412;&#37319;&#26679;&#19982;&#20808;&#21069;&#36866;&#24212;&#30340;&#36127;&#26679;&#26412;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BBox-Adapter&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#40657;&#30418;LLMs&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
&lt;/p&gt;</description></item><item><title>THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08191</link><description>&lt;p&gt;
THE COLOSSEUM&#65306;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#27867;&#21270;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08191
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#12289;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#35780;&#20272;&#26426;&#22120;&#20154;&#22312;&#19982;&#35757;&#32451;&#35774;&#32622;&#38750;&#24120;&#30456;&#20284;&#29978;&#33267;&#30456;&#21516;&#30340;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;THE COLOSSEUM&#65292;&#20854;&#20013;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21487;&#20197;&#23545;&#27169;&#22411;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#36825;&#20123;&#24178;&#25200;&#21253;&#25324;&#29289;&#20307;&#12289;&#26700;&#38754;&#21644;&#32972;&#26223;&#30340;&#39068;&#33394;&#12289;&#32441;&#29702;&#21644;&#22823;&#23567;&#30340;&#21464;&#21270;&#65307;&#25105;&#20204;&#36824;&#25913;&#21464;&#20102;&#20809;&#29031;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#20351;&#29992;THE COLOSSEUM&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;4&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#22312;&#36825;&#20123;&#24178;&#25200;&#22240;&#32032;&#19979;&#19979;&#38477;&#20102;30-50%&#12290;&#24403;&#22810;&#20010;&#24178;&#25200;&#21516;&#26102;&#24212;&#29992;&#26102;&#65292;&#25104;&#21151;&#29575;&#19979;&#38477;&#33267;&#8805;75%&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#27979;&#24230;&#31354;&#38388;&#19978;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#20004;&#31181;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#29575;&#19982;&#30456;&#20851;&#26377;&#38480;&#32500;&#31639;&#27861;&#30340;&#26368;&#26032;&#32467;&#26524;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.08106</link><description>&lt;p&gt;
&#38024;&#23545;&#22343;&#22330;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mirror Descent-Ascent for mean-field min-max problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#27979;&#24230;&#31354;&#38388;&#19978;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#20004;&#31181;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#29575;&#19982;&#30456;&#20851;&#26377;&#38480;&#32500;&#31639;&#27861;&#30340;&#26368;&#26032;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#27979;&#24230;&#31354;&#38388;&#19978;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#21516;&#26102;&#21644;&#20381;&#27425;&#12290;&#25105;&#20204;&#22312;&#20984;&#24615;-&#20985;&#24615;&#21644;&#30456;&#23545;&#20809;&#28369;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#38024;&#23545;&#36866;&#24403;&#30340;Bregman&#25955;&#24230;&#22312;&#27979;&#24230;&#31354;&#38388;&#19978;&#36890;&#36807;&#24179;&#22374;&#23548;&#25968;&#36827;&#34892;&#20102;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#29575;&#21040;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#65292;&#29992;&#23612;&#20975;&#22810;-Isoda&#35823;&#24046;&#34920;&#31034;&#65292;&#23545;&#20110;&#21516;&#26102;&#21644;&#20381;&#27425;&#26041;&#26696;&#20998;&#21035;&#26159;$\mathcal{O}\left(N^{-1/2}\right)$&#21644;$\mathcal{O}\left(N^{-2/3}\right)$&#65292;&#36825;&#19982;&#30456;&#20851;&#26377;&#38480;&#32500;&#31639;&#27861;&#30340;&#26368;&#26032;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study two variants of the mirror descent-ascent algorithm for solving min-max problems on the space of measures: simultaneous and sequential. We work under assumptions of convexity-concavity and relative smoothness of the payoff function with respect to a suitable Bregman divergence, defined on the space of measures via flat derivatives. We show that the convergence rates to mixed Nash equilibria, measured in the Nikaid\`o-Isoda error, are of order $\mathcal{O}\left(N^{-1/2}\right)$ and $\mathcal{O}\left(N^{-2/3}\right)$ for the simultaneous and sequential schemes, respectively, which is in line with the state-of-the-art results for related finite-dimensional algorithms.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;1-WL&#31639;&#27861;&#22312;&#22270;&#21516;&#26500;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22686;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#20837;&#23376;&#22270;&#20449;&#24687;&#21644;&#32463;&#20856;&#30340;&#36793;&#32536;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#26356;&#39640;&#34920;&#36798;&#21147;&#19982;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26465;&#20214;&#12290;&#26799;&#24230;&#27969;&#20063;&#34987;&#35777;&#26126;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07568</link><description>&lt;p&gt;
Weisfeiler-Leman&#22312;&#36793;&#32536;&#26465;&#20214;&#19979;&#30340;&#26356;&#39640;&#34920;&#36798;&#21147;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Leman at the margin: When more expressivity matters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07568
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;1-WL&#31639;&#27861;&#22312;&#22270;&#21516;&#26500;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22686;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#20837;&#23376;&#22270;&#20449;&#24687;&#21644;&#32463;&#20856;&#30340;&#36793;&#32536;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#26356;&#39640;&#34920;&#36798;&#21147;&#19982;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26465;&#20214;&#12290;&#26799;&#24230;&#27969;&#20063;&#34987;&#35777;&#26126;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Leman&#31639;&#27861;&#65288;1-WL&#65289;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#29992;&#20110;&#22270;&#21516;&#26500;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#35813;&#31639;&#27861;&#22312;&#29702;&#35299;&#20256;&#36882;&#28040;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#20316;&#20026;&#22270;&#26680;&#20989;&#25968;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;1-WL&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;MPNN&#21644;&#26680;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22686;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#36890;&#36807;&#22270;&#21516;&#26500;&#26469;&#35266;&#23519;&#26102;&#65292;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#22312;&#35299;&#37322;&#20854;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#27934;&#23519;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30528;&#37325;&#22312;1-WL&#21644;MPNN&#20013;&#24341;&#20837;&#23376;&#22270;&#20449;&#24687;&#65292;&#24182;&#36816;&#29992;&#32463;&#20856;&#30340;&#36793;&#32536;&#29702;&#35770;&#26469;&#30740;&#31350;&#26550;&#26500;&#30340;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#19982;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26799;&#24230;&#27969;&#22914;&#20309;&#25512;&#21160;&#27169;&#22411;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#31639;&#27861;hFedF&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#34701;&#21512;&#23458;&#25143;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#24182;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06974</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#34701;&#21512;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#31639;&#27861;hFedF&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#34701;&#21512;&#23458;&#25143;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#24182;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#22810;&#20010;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#20986;&#29616;&#12290;&#20026;&#20102;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#21644;&#23454;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#25193;&#23637;&#20854;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#20197;&#36866;&#24212;&#26410;&#30693;&#39046;&#22495;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65288;FDG&#65289;&#65292;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#31216;&#20026;hFedF&#65288;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#31243;&#24230;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#22522;&#26412;&#19978;&#65292;&#36229;&#32593;&#32476;&#25903;&#25345;&#23545;&#23458;&#25143;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#34701;&#21512;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;FL&#20013;&#24378;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#35265;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;DG&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising paradigm in which multiple clients collaboratively train a shared global model while preserving data privacy. To create a robust and practicable FL framework, it is crucial to extend its ability to generalize well to unseen domains - a problem referred to as federated Domain Generalization (FDG), being still under-explored. We propose an innovative federated algorithm, termed hFedF for hypernetwork-based Federated Fusion, designed to bridge the performance gap between generalization and personalization, capable of addressing various degrees of domain shift. Essentially, the hypernetwork supports a non-linear fusion of client models enabling a comprehensive understanding of the underlying data distribution. We encompass an extensive discussion and provide novel insights into the tradeoff between personalization and generalization in FL. The proposed algorithm outperforms strong benchmarks on three widely-used data sets for DG in an exce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#32467;&#26500;&#20887;&#20313;&#30340;&#20302;&#31209;&#36924;&#36817;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36924;&#36817;&#20887;&#20313;&#32452;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36807;&#37327;&#39118;&#38505;&#26469;&#25903;&#25345;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.06884</link><description>&lt;p&gt;
&#32467;&#26500;&#20887;&#20313;&#30340;&#20302;&#31209;&#36924;&#36817;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32467;&#26500;&#20887;&#20313;&#30340;&#20302;&#31209;&#36924;&#36817;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36924;&#36817;&#20887;&#20313;&#32452;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36807;&#37327;&#39118;&#38505;&#26469;&#25903;&#25345;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#37325;&#26500;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65292;&#20197;&#25581;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#25317;&#26377;&#26080;&#38480;&#37327;&#30340;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23436;&#32654;&#32447;&#24615;&#36924;&#36817;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#35813;&#26465;&#20214;&#25581;&#31034;&#20102;&#19968;&#20010;&#20445;&#30041;&#26631;&#31614;&#31867;&#21035;Y&#30340;&#28385;&#31209;&#32452;&#20214;&#65292;&#20197;&#21450;&#19968;&#20010;&#20887;&#20313;&#32452;&#20214;&#12290;&#21463;&#21040;&#35813;&#26465;&#20214;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20302;&#31209;&#20998;&#35299;&#36924;&#36817;&#20887;&#20313;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#30001;&#20998;&#35299;&#31209;s&#21442;&#25968;&#21270;&#30340;&#26032;&#37327;$\epsilon_s$&#26469;&#34913;&#37327;&#36924;&#36817;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;$\epsilon_s$&#25972;&#21512;&#21040;&#32447;&#24615;&#22238;&#24402;&#21644;&#23725;&#22238;&#24402;&#35774;&#32622;&#19979;&#30340;&#36807;&#37327;&#39118;&#38505;&#20998;&#26512;&#20013;&#65292;&#21518;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#23398;&#20064;&#29305;&#24449;&#30340;&#32500;&#24230;&#36828;&#22823;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#35760;&#26679;&#26412;&#25968;n&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#31616;&#21270;&#23454;&#39564;&#65292;&#20197;&#27604;&#36739;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the data-generating mechanism for reconstructive SSL to shed light on its effectiveness. With an infinite amount of labeled samples, we provide a sufficient and necessary condition for perfect linear approximation. The condition reveals a full-rank component that preserves the label classes of Y, along with a redundant component. Motivated by the condition, we propose to approximate the redundant component by a low-rank factorization and measure the approximation quality by introducing a new quantity $\epsilon_s$, parameterized by the rank of factorization s. We incorporate $\epsilon_s$ into the excess risk analysis under both linear regression and ridge regression settings, where the latter regularization approach is to handle scenarios when the dimension of the learned features is much larger than the number of labeled samples n for downstream tasks. We design three stylized experiments to compare SSL with supervised learning under different settings to support our theoretic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20542;&#21521;&#20110;&#23545;&#31216;&#25490;&#21015;&#20989;&#25968;&#65292;&#23545;&#31216;&#32676;&#30340;&#34920;&#31034;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#39044;&#27979;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#23398;&#20064;&#26354;&#32447;&#21644;&#32593;&#32476;&#36755;&#20986;&#65292;&#24182;&#22312;&#24120;&#35265;&#35774;&#32622;&#20013;&#24471;&#20986;&#23398;&#20064;&#33021;&#21147;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#26368;&#21518;&#36824;&#35777;&#26126;&#20102;WikiText&#25968;&#25454;&#38598;&#20855;&#26377;&#25490;&#21015;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05173</link><description>&lt;p&gt;
&#25506;&#32034;Transformer&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;: &#19968;&#20010;&#26469;&#33258;&#26080;&#31351;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Inductive Bias in Transformers: A View From Infinity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20542;&#21521;&#20110;&#23545;&#31216;&#25490;&#21015;&#20989;&#25968;&#65292;&#23545;&#31216;&#32676;&#30340;&#34920;&#31034;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#39044;&#27979;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#23398;&#20064;&#26354;&#32447;&#21644;&#32593;&#32476;&#36755;&#20986;&#65292;&#24182;&#22312;&#24120;&#35265;&#35774;&#32622;&#20013;&#24471;&#20986;&#23398;&#20064;&#33021;&#21147;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#26368;&#21518;&#36824;&#35777;&#26126;&#20102;WikiText&#25968;&#25454;&#38598;&#20855;&#26377;&#25490;&#21015;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#26080;&#31351;&#30340;&#36807;&#21442;&#25968;&#21270;&#39640;&#26031;&#36807;&#31243;&#26497;&#38480;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#25351;&#20986;Transformer&#27169;&#22411;&#22312;&#24207;&#21015;&#31354;&#38388;&#20013;&#26356;&#20542;&#21521;&#20110;&#23545;&#31216;&#25490;&#21015;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#31216;&#32676;&#30340;&#34920;&#31034;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#23545;token&#20043;&#38388;&#30340;&#25490;&#21015;&#20855;&#26377;&#23545;&#31216;&#24615;&#26102;&#32473;&#20986;&#23450;&#37327;&#30340;&#20998;&#26512;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#22312;&#26497;&#38480;&#26465;&#20214;&#19979;&#27714;&#35299;&#27169;&#22411;&#65292;&#21253;&#25324;&#23545;&#23398;&#20064;&#26354;&#32447;&#21644;&#32593;&#32476;&#36755;&#20986;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#23398;&#20064;&#33021;&#21147;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#20197;&#19978;&#19979;&#25991;&#38271;&#24230;&#20316;&#20026;&#20989;&#25968;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;WikiText&#25968;&#25454;&#38598;&#30830;&#23454;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#24314;&#27169;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04933</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20855;&#26377;&#19978;&#19979;&#25991;&#29615;&#22659;&#30340;&#19981;&#23433;&#23425;&#36172;&#21338;&#26426;&#20013;&#30340;&#24212;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04933
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#24314;&#27169;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23433;&#23425;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#29992;&#20110;&#24314;&#27169;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#39034;&#24207;&#36164;&#28304;&#20998;&#37197;&#12290;&#22312;&#36825;&#20123;&#24773;&#26223;&#20013;&#65292;&#28508;&#22312;&#30340;&#36716;&#31227;&#21160;&#24577;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#38656;&#35201;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RMAB&#22312;&#32447;RL&#26041;&#27861;&#26080;&#27861;&#25972;&#21512;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#20844;&#20849;&#21355;&#29983;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#23646;&#24615;&#65292;&#22914;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38750;&#31283;&#24577;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#19978;&#19979;&#25991;RMAB&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;BCoR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#27169;&#25311;&#21508;&#31181;&#22797;&#26434;&#30340;RMAB&#35774;&#32622;&#65292;&#22914;&#19978;&#19979;&#25991;&#21644;&#38750;&#31283;&#24577;&#30340;RMAB&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20869;&#37096;&#21644;&#21508;&#20010;&#33218;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;RMAB&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;BCoR&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-armed bandits (RMABs) are used to model sequential resource allocation in public health intervention programs. In these settings, the underlying transition dynamics are often unknown a priori, requiring online reinforcement learning (RL). However, existing methods in online RL for RMABs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity. We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model a wide range of complex RMAB settings, such as contextual and non-stationary RMABs. A key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown RMAB transition dynamics quickly in budget-constrained settings with relatively short time horizons. Empirically, we show that BCoR achieves substantially higher finit
&lt;/p&gt;</description></item><item><title>SARI&#26159;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#65292;&#32467;&#21512;&#24179;&#22343;&#31574;&#30053;&#21644;&#35782;&#21035;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04835</link><description>&lt;p&gt;
SARI: &#31616;&#27905;&#24179;&#22343;&#19982;&#40065;&#26834;&#24615;&#22522;&#20110;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04835
&lt;/p&gt;
&lt;p&gt;
SARI&#26159;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#65292;&#32467;&#21512;&#24179;&#22343;&#31574;&#30053;&#21644;&#35782;&#21035;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064; (PLL) &#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#37117;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614; (&#37096;&#20998;&#26631;&#31614;) &#25104;&#23545;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#30495;&#27491;&#30340;&#26631;&#31614;&#12290;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064; (NPLL) &#25918;&#23485;&#20102;&#36825;&#20010;&#32422;&#26463;&#65292;&#20801;&#35768;&#19968;&#20123;&#37096;&#20998;&#26631;&#31614;&#19981;&#21253;&#21547;&#30495;&#27491;&#30340;&#26631;&#31614;&#65292;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312; NPLL &#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550; SARI&#65292;&#36890;&#36807;&#21033;&#29992;&#21152;&#26435;&#26368;&#36817;&#37051;&#31639;&#27861;&#23558;&#20266;&#26631;&#31614;&#20998;&#37197;&#32473;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#20266;&#26631;&#31614;&#19982;&#22270;&#20687;&#37197;&#23545;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#37319;&#29992;&#26631;&#31614;&#24179;&#28369;&#21644;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;&#32467;&#26524;&#26469;&#25913;&#36827;&#21644;&#25552;&#39640;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;SARI&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#24179;&#22343;&#31574;&#30053; (&#20266;&#26631;&#31614;) &#21644;&#22522;&#20110;&#35782;&#21035;&#31574;&#30053; (&#20998;&#31867;&#22120;&#35757;&#32451;)&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;SARI&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough ex
&lt;/p&gt;</description></item><item><title>&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.04647</link><description>&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65306;&#35268;&#21010;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Latent Plan Transformer: Planning as Latent Variable Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04647
&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#38271;&#26399;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#35268;&#21010;&#21464;&#24471;&#24517;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35268;&#21010;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#32570;&#20047;&#36880;&#27493;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#26469;&#36830;&#25509;&#22522;&#20110;Transformer&#30340;&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#12290;LPT&#21487;&#20197;&#36890;&#36807;&#36712;&#36857;-&#22238;&#25253;&#23545;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#65292;&#23613;&#31649;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#65292;&#33258;&#28982;&#22320;&#32858;&#38598;&#23376;&#36712;&#36857;&#20197;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#36890;&#36807;&#39044;&#26399;&#22238;&#25253;&#23545;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#35268;&#21010;&#20316;&#20026;&#25512;&#26029;&#30340;&#24605;&#24819;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#25972;&#20010;&#22238;&#21512;&#20013;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#65292;&#36215;&#21040;&#19968;&#20010;&#35745;&#21010;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LPT&#21487;&#20197;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptima
&lt;/p&gt;</description></item><item><title>InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.04617</link><description>&lt;p&gt;
InfLLM: &#25581;&#31034;LLMs&#23545;&#20110;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#26080;&#38656;&#35757;&#32451;&#30340;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04617
&lt;/p&gt;
&lt;p&gt;
InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20855;&#26377;&#28459;&#38271;&#20256;&#36755;&#36755;&#20837;&#30340;&#29616;&#23454;&#24212;&#29992;&#30340;&#22522;&#30707;&#65292;&#22914;LLM&#39537;&#21160;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22312;&#21463;&#38480;&#26368;&#22823;&#38271;&#24230;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#26080;&#27861;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#22240;&#20026;&#23384;&#22312;&#39046;&#22495;&#22806;&#21644;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#28369;&#21160;&#27880;&#24847;&#21147;&#31383;&#21475;&#21644;&#20002;&#24323;&#36828;&#36317;&#31163;&#26631;&#35760;&#65292;&#20197;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#24207;&#21015;&#20869;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#35821;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;InfLLM&#65292;&#26469;&#25581;&#31034;LLMs&#22788;&#29702;&#27969;&#24335;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InfLLM&#23558;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#23384;&#20648;&#21040;&#38468;&#21152;&#30340;&#20869;&#23384;&#21333;&#20803;&#20013;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#21046;&#26469;&#26597;&#25214;&#19982;&#27880;&#24847;&#35745;&#31639;&#30456;&#20851;&#30340;&#26631;&#35760;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;InfLLM&#20801;&#35768;LLMs&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#35821;&#20041;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.03923</link><description>&lt;p&gt;
&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Return-Aligned Decision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65288;&#21363;&#22238;&#25253;&#65289;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24212;&#29992;&#33539;&#22260;&#30340;&#25193;&#22823;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#20351;&#23454;&#38469;&#22238;&#25253;&#19982;&#25351;&#23450;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#30340;&#26234;&#33021;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20174;&#32780;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#20915;&#31574;Transformer&#65288;DT&#65289;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#20248;&#21270;&#29983;&#25104;&#20197;&#30446;&#26631;&#22238;&#25253;&#20026;&#26465;&#20214;&#30340;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#24182;&#37197;&#22791;&#20102;&#20351;&#29992;&#30446;&#26631;&#22238;&#25253;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;DT&#26088;&#22312;&#23545;&#40784;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#65292;&#20294;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#20102;DT&#20013;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22238;&#25253;&#20174;&#20256;&#32479;&#30340;&#36755;&#20837;&#24207;&#21015;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#21253;&#21547;&#22238;&#25253;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03660</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#20986;&#29616;&#20102;&#36328;&#20219;&#21153;&#32447;&#24615;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#24050;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#27969;&#36235;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20174;&#20844;&#20849;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#65288;CTL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#25105;&#20204;&#32447;&#24615;&#25554;&#20540;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#26435;&#37325;&#25554;&#20540;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#22823;&#33268;&#31561;&#20110;&#27599;&#23618;&#20013;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#29305;&#24449;&#30340;&#32447;&#24615;&#25554;&#20540;&#12290;&#36825;&#26679;&#30340;&#36328;&#20219;&#21153;&#32447;&#24615;&#22312;&#21516;&#34892;&#25991;&#29486;&#20013;&#23578;&#26410;&#34987;&#27880;&#24847;&#21040;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25903;&#25345;&#20174;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24320;&#22987;&#30340;&#24494;&#35843;&#27169;&#22411;&#19968;&#33268;&#20986;&#29616;CTL&#12290;&#25105;&#20204;&#25512;&#27979;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26412;&#36136;&#19978;&#26159;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#12289;&#21442;&#25968;&#20849;&#20139;&#31561;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, p
&lt;/p&gt;</description></item><item><title>InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.02933</link><description>&lt;p&gt;
InterpretCC: &#36866;&#20110;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02933
&lt;/p&gt;
&lt;p&gt;
InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#35299;&#37322;&#24615;&#22312;&#19977;&#20010;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;1&#65289;&#38656;&#35201;&#20154;&#31867;&#20449;&#20219;&#35299;&#37322;&#30340;&#36817;&#20284;&#65288;&#20363;&#22914;&#20107;&#21518;&#26041;&#27861;&#65289;&#65307;2&#65289;&#21066;&#24369;&#20102;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#65288;&#20363;&#22914;&#33258;&#21160;&#35782;&#21035;&#30340;&#29305;&#24449;&#25513;&#30721;&#65289;&#65307;3&#65289;&#21066;&#24369;&#20102;&#27169;&#22411;&#24615;&#33021;&#65288;&#20363;&#22914;&#20915;&#31574;&#26641;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#23545;&#20110;&#38754;&#21521;&#20154;&#31867;&#30340;&#39046;&#22495;&#65288;&#22914;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#25110;&#33258;&#28982;&#35821;&#35328;&#65289;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#20449;&#30340;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InterpretCC&#65288;&#21487;&#35299;&#37322;&#30340;&#26465;&#20214;&#35745;&#31639;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20043;&#21069;&#33258;&#36866;&#24212;&#21644;&#31232;&#30095;&#22320;&#28608;&#27963;&#29305;&#24449;&#65292;&#30830;&#20445;&#20154;&#31867;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#20026;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#20801;&#35768;&#20154;&#20204;&#31163;&#25955;&#22320;&#25351;&#23450;&#20852;&#36259;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02705</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#30340;&#34920;&#24449;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Representation Surgery for Multi-Task Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#23558;&#22810;&#20010;&#20219;&#21153;&#30340;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#39592;&#24178;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#25509;&#21512;&#24182;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25191;&#34892;MTL&#65292;&#32780;&#19981;&#26159;&#25910;&#38598;&#23427;&#20204;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;MTL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#30340;&#34920;&#31034;&#20998;&#24067;&#65292;&#25105;&#20204;&#21457;&#29616;&#21512;&#24182;&#27169;&#22411;&#24448;&#24448;&#38754;&#20020;&#34920;&#31034;&#20559;&#24046;&#30340;&#22256;&#22659;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#21512;&#24182;&#27169;&#22411;&#19982;&#20010;&#20307;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#20998;&#24067;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#21512;&#24182;MTL&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#21512;&#24182;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;Surgery&#8221;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#23427;&#20197;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#20026;&#36755;&#20837;&#65292;&#24182;&#35797;&#22270;&#36755;&#20986;&#20854;&#20013;&#21253;&#21547;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called "Surgery" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation fr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02644</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#30340;&#26041;&#27861;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Variational DAG Estimation via State Augmentation With Stochastic Permutations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02644
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#21363;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#65292;&#26159;&#19968;&#20010;&#22312;&#32479;&#35745;&#21644;&#35745;&#31639;&#19978;&#37117;&#24456;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#31561;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#24212;&#29992;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#26041;&#38754;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#22788;&#29702;&#20247;&#25152;&#21608;&#30693;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#12290;&#20174;&#27010;&#29575;&#25512;&#26029;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#65288;i&#65289;&#34920;&#31034;&#28385;&#36275;DAG&#32422;&#26463;&#30340;&#22270;&#30340;&#20998;&#24067;&#21644;&#65288;ii&#65289;&#20272;&#35745;&#24213;&#23618;&#32452;&#21512;&#31354;&#38388;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;DAG&#21644;&#25490;&#21015;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26500;&#24314;&#32852;&#21512;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#21518;&#39564;&#20272;&#35745;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;&#31163;&#25955;&#20998;&#24067;&#30340;&#36830;&#32493;&#26494;&#24347;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#33021;&#22815;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and re
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02325</link><description>&lt;p&gt;
&#21160;&#37327;&#22312;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#20013;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#20316;&#29992;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23545;&#27492;&#32570;&#20047;&#29702;&#35770;&#35299;&#37322;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;SGD&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#31243;&#24230;&#30001;&#23398;&#20064;&#29575;&#12289;&#25209;&#22823;&#23567;&#12289;&#21160;&#37327;&#22240;&#23376;&#12289;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#21450;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#30830;&#23450;&#12290;&#36825;&#19968;&#29702;&#35770;&#21457;&#29616;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#21160;&#37327;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21160;&#37327;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#20316;&#29992;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SGD&#21160;&#37327;&#24179;&#28369;&#29305;&#24615;&#30340;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;SGD&#21160;&#37327;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01922</link><description>&lt;p&gt;
&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning from Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#30528;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#24369;&#30417;&#30563;&#30340;&#21508;&#31181;&#22330;&#26223;&#21644;&#30001;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;GLWS&#65289;&#12290;GLWS&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#30340;&#20844;&#24335;&#65292;&#28789;&#27963;&#22320;&#36866;&#24212;&#20102;&#21508;&#31181;&#24369;&#30417;&#30563;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#12289;&#32858;&#21512;&#32479;&#35745;&#12289;&#25104;&#23545;&#35266;&#23519;&#21644;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;NFA&#65289;&#20197;&#21450;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;EM&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#24120;&#25152;&#38656;&#30340;&#20108;&#27425;&#25110;&#38454;&#20056;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;&#22240;&#27492;&#65292;&#20174;&#20219;&#24847;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20102;&#23545;&#23427;&#20204;&#36827;&#34892;NFA&#24314;&#27169;&#12290;GLWS&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;+
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
&lt;/p&gt;</description></item><item><title>Audio Flamingo&#26159;&#19968;&#31181;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#36798;&#21040;&#20102;&#26368;&#20248;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01831</link><description>&lt;p&gt;
Audio Flamingo: &#19968;&#31181;&#20855;&#22791;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01831
&lt;/p&gt;
&lt;p&gt;
Audio Flamingo&#26159;&#19968;&#31181;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#36798;&#21040;&#20102;&#26368;&#20248;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#29702;&#35299;&#38899;&#39057;&#8212;&#8212;&#21253;&#25324;&#38750;&#35821;&#38899;&#22768;&#38899;&#21644;&#38750;&#35328;&#35821;&#30340;&#35821;&#38899;&#8212;&#8212;&#23545;LLMs&#30340;&#22810;&#26679;&#21270;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Audio Flamingo&#30340;&#26032;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#38899;&#39057;&#29702;&#35299;&#33021;&#21147;&#12289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26816;&#32034;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#24378;&#22823;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#35757;&#32451;&#25216;&#26415;&#12289;&#26550;&#26500;&#35774;&#35745;&#21644;&#25968;&#25454;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#22791;&#36825;&#20123;&#21151;&#33021;&#12290;&#24191;&#27867;&#30340;&#38899;&#39057;&#29702;&#35299;&#20219;&#21153;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20248;&#25104;&#32489;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.01821</link><description>&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#35299;&#37322;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Ecologically rational meta-learned inference explains human category learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#24615;&#26159;&#25351;&#20154;&#31867;&#20316;&#20026;&#36866;&#24212;&#29615;&#22659;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23450;&#20041;&#21738;&#20123;&#20219;&#21153;&#22312;&#29983;&#24577;&#19978;&#26159;&#26377;&#25928;&#30340;&#20197;&#21450;&#20026;&#36825;&#20123;&#20219;&#21153;&#24314;&#31435;&#21512;&#29702;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#31867;&#21035;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#21512;&#29702;&#24615;&#20027;&#20307;&#26469;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#12290;ERMI&#22312;&#20004;&#20010;&#19981;&#21516;&#23454;&#39564;&#20013;&#20197;&#23450;&#37327;&#26041;&#24335;&#27604;&#20854;&#20182;&#19971;&#20010;&#35748;&#30693;&#27169;&#22411;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#23450;&#24615;&#19978;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#21305;&#37197;&#65306;&#65288;1&#65289;&#23427;&#21457;&#29616;&#20102;&#19982;&#20154;&#31867;&#21457;&#29616;&#22256;&#38590;&#30340;&#30456;&#21516;&#20219;&#21153;&#65292;&#65288;2&#65289;&#23427;&#21464;&#24471;&#26356;&#20381;&#36182;&#20110;&#22522;&#20110;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22788;&#29702;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#20551;&#35774;&#36716;&#31227;&#21407;&#22240;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#26410;&#30693;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#30475;&#20316;&#20998;&#24067;&#22806;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2311.18575</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#65306;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22788;&#29702;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#20551;&#35774;&#36716;&#31227;&#21407;&#22240;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#26410;&#30693;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#30475;&#20316;&#20998;&#24067;&#22806;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#26469;&#35828;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#31867;&#21035;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#20294;&#37096;&#32626;&#22312;&#26032;&#30340;&#12289;&#26410;&#30693;&#30340;&#31867;&#21035;&#19978;&#12290;&#24120;&#35265;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#21407;&#22240;&#26159;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#23646;&#24615;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#20154;&#29289;&#35782;&#21035;&#20013;&#30340;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#37319;&#29992;&#36825;&#20010;&#35774;&#32622;&#30340;&#27169;&#22411;&#65292;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#30693;&#23548;&#33268;&#36716;&#31227;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23398;&#20064;&#23545;&#36825;&#31181;&#36716;&#31227;&#40065;&#26834;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#12290;&#23613;&#31649;&#20004;&#31181;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#20851;&#38190;&#24046;&#24322;&#65292;&#20294;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#36716;&#21270;&#20026;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class distribution shifts are particularly challenging for zero-shot classifiers, which rely on representations learned from training classes but are deployed on new, unseen ones. Common causes for such shifts are changes in attributes associated with classes, such as race or gender in person identification. In this work, we propose and analyze a model that adopts this setting, assuming that the attribute responsible for the shift is unknown during training. To address the challenge of learning data representations robust to such shifts, we introduce a framework based on hierarchical sampling to construct synthetic data environments. Despite key differences between the settings, this framework allows us to formulate class distribution shifts in zero-shot learning as out-of-distribution problems. Consequently, we present an algorithm for learning robust representations, and show that our approach significantly improves generalization to diverse class distributions in both simulations an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27979;&#35797;&#26102;&#24207;&#25968;&#25454;&#20043;&#38388;&#29420;&#31435;&#24615;&#30340;&#26102;&#24207;&#20381;&#36182;&#32479;&#35745;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#20272;&#35745;&#26368;&#20339;&#20381;&#36182;&#28382;&#21518;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#26102;&#28176;&#36817;&#26377;&#25928;&#21644;&#26222;&#36941;&#19968;&#33268;&#65292;&#24182;&#19988;&#19982;&#22810;&#31181;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>https://arxiv.org/abs/1908.06486</link><description>&lt;p&gt;
&#26102;&#24207;&#25968;&#25454;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Independence Testing for Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1908.06486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27979;&#35797;&#26102;&#24207;&#25968;&#25454;&#20043;&#38388;&#29420;&#31435;&#24615;&#30340;&#26102;&#24207;&#20381;&#36182;&#32479;&#35745;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#20272;&#35745;&#26368;&#20339;&#20381;&#36182;&#28382;&#21518;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#26102;&#28176;&#36817;&#26377;&#25928;&#21644;&#26222;&#36941;&#19968;&#33268;&#65292;&#24182;&#19988;&#19982;&#22810;&#31181;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#25968;&#25454;&#22312;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#21028;&#26029;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#26159;&#21542;&#30456;&#20851;&#12290;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#23384;&#22312;&#38480;&#21046;&#65292;&#22914;&#20381;&#36182;&#21442;&#25968;&#20551;&#35774;&#12289;&#20165;&#26816;&#27979;&#32447;&#24615;&#20851;&#32852;&#12289;&#38656;&#35201;&#22810;&#20010;&#27979;&#35797;&#21644;&#20462;&#27491;&#31561;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#21442;&#25968;&#21644;&#26222;&#36941;&#19968;&#33268;&#30340;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;p&#20540;&#33192;&#32960;&#21644;&#26080;&#25928;&#30340;&#26816;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#22359;&#32622;&#25442;&#30340;&#26102;&#24207;&#20381;&#36182;&#32479;&#35745;&#37327;&#26469;&#27979;&#35797;&#26102;&#24207;&#25968;&#25454;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27979;&#35797;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#26102;&#26159;&#28176;&#36817;&#26377;&#25928;&#21644;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#20272;&#35745;&#26368;&#22823;&#21270;&#20381;&#36182;&#30340;&#26368;&#20339;&#20381;&#36182;&#28382;&#21518;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#19982;&#20016;&#23500;&#30340;&#36317;&#31163;&#21644;&#26680;&#24515;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#20860;&#23481;&#65292;&#28040;&#38500;&#20102;
&lt;/p&gt;
&lt;p&gt;
Temporal data are increasingly prevalent in modern data science. A fundamental question is whether two time-series are related or not. Existing approaches often have limitations, such as relying on parametric assumptions, detecting only linear associations, and requiring multiple tests and corrections. While many non-parametric and universally consistent dependence measures have recently been proposed, directly applying them to temporal data can inflate the p-value and result in invalid test. To address these challenges, this paper introduces the temporal dependence statistic with block permutation to test independence between temporal data. Under proper assumptions, the proposed procedure is asymptotically valid and universally consistent for testing independence between stationary time-series, and capable of estimating the optimal dependence lag that maximizes the dependence. Notably, it is compatible with a rich family of distance and kernel based dependence measures, eliminates the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#24402;&#19968;&#21270;&#27969;&#24212;&#29992;&#20110;&#26684;&#37327;&#23376;&#22330;&#35770;&#20013;&#65292;&#29983;&#25104;&#30456;&#20851;&#30340;&#26684;&#35268;&#22330;&#24577;&#38598;&#21512;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#21033;&#29992;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#35266;&#27979;&#37327;&#26102;&#30340;&#26041;&#24046;&#12290;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#20855;&#20307;&#24212;&#29992;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#26126;&#26174;&#38477;&#20302;&#20102;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10874</link><description>&lt;p&gt;
&#23558;&#27969;&#27169;&#22411;&#24212;&#29992;&#20110;&#30456;&#20851;&#26684;QCD&#24577;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Applications of flow models to the generation of correlated lattice QCD ensembles. (arXiv:2401.10874v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#24402;&#19968;&#21270;&#27969;&#24212;&#29992;&#20110;&#26684;&#37327;&#23376;&#22330;&#35770;&#20013;&#65292;&#29983;&#25104;&#30456;&#20851;&#30340;&#26684;&#35268;&#22330;&#24577;&#38598;&#21512;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#21033;&#29992;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#35266;&#27979;&#37327;&#26102;&#30340;&#26041;&#24046;&#12290;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#20855;&#20307;&#24212;&#29992;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#26126;&#26174;&#38477;&#20302;&#20102;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#29992;&#20110;&#26684;&#37327;&#23376;&#22330;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#22312;&#19981;&#21516;&#20316;&#29992;&#21442;&#25968;&#19979;&#29983;&#25104;&#32479;&#35745;&#30456;&#20851;&#30340;&#26684;&#35268;&#22330;&#24577;&#38598;&#21512;&#12290;&#26412;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#30456;&#20851;&#24615;&#26469;&#20943;&#23569;&#35745;&#31639;&#35266;&#27979;&#37327;&#26102;&#30340;&#26041;&#24046;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27531;&#24046;&#27969;&#32467;&#26500;&#65292;&#22312;&#36830;&#32493;&#26497;&#38480;&#30340;&#35268;&#33539;&#29702;&#35770;&#12289;QCD&#35266;&#27979;&#37327;&#30340;&#36136;&#37327;&#20381;&#36182;&#24615;&#21644;&#22522;&#20110;&#36153;&#26364;-&#36203;&#23572;&#26364;&#26041;&#27861;&#30340;&#24378;&#23376;&#30697;&#38453;&#20803;&#31561;&#19977;&#20010;&#27010;&#24565;&#35777;&#26126;&#24212;&#29992;&#20013;&#12290;&#22312;&#25152;&#26377;&#19977;&#31181;&#24773;&#20917;&#19979;&#65292;&#19982;&#20351;&#29992;&#19981;&#30456;&#20851;&#38598;&#21512;&#25110;&#30452;&#25509;&#37325;&#21152;&#26435;&#36827;&#34892;&#30340;&#30456;&#21516;&#35745;&#31639;&#30456;&#27604;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27969;&#30340;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learned normalizing flows can be used in the context of lattice quantum field theory to generate statistically correlated ensembles of lattice gauge fields at different action parameters. This work demonstrates how these correlations can be exploited for variance reduction in the computation of observables. Three different proof-of-concept applications are demonstrated using a novel residual flow architecture: continuum limits of gauge theories, the mass dependence of QCD observables, and hadronic matrix elements based on the Feynman-Hellmann approach. In all three cases, it is shown that statistical uncertainties are significantly reduced when machine-learned flows are incorporated as compared with the same calculations performed with uncorrelated ensembles or direct reweighting.
&lt;/p&gt;</description></item><item><title>PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09793</link><description>&lt;p&gt;
PatchAD: &#22522;&#20110;&#22359;&#30340;MLP-Mixer&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09793
&lt;/p&gt;
&lt;p&gt;
PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#20107;&#20214;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#22312;&#32570;&#20047;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#27491;&#24120;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#22815;&#36731;&#37327;&#32423;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PatchAD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#34920;&#24449;&#25552;&#21462;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PatchAD&#30001;&#22235;&#20010;&#29420;&#29305;&#30340;MLP Mixer&#32452;&#25104;&#65292;&#19987;&#38376;&#21033;&#29992;MLP&#26550;&#26500;&#23454;&#29616;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21019;&#26032;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#32531;&#35299;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32963;&#32928;&#20869;&#38236;&#35270;&#35273;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#23545;&#20110;&#26377;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#26356;&#36866;&#21512;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#24182;&#19988;&#20351;&#29992;ImageNet-1k&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#27604;&#20351;&#29992;Hyperkvasir-unlabelled&#26356;&#21512;&#36866;&#12290;</title><link>http://arxiv.org/abs/2401.06278</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32963;&#32928;&#20869;&#38236;&#35270;&#35273;&#38382;&#39064;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy. (arXiv:2401.06278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32963;&#32928;&#20869;&#38236;&#35270;&#35273;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#23545;&#20110;&#26377;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#26356;&#36866;&#21512;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#24182;&#19988;&#20351;&#29992;ImageNet-1k&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#27604;&#20351;&#29992;Hyperkvasir-unlabelled&#26356;&#21512;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32963;&#32928;&#20869;&#38236;&#65288;GIE&#65289;&#20013;&#30340;&#35270;&#35273;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#22312;ImageNet-1k&#19978;&#20197;&#26377;&#30417;&#30563;&#26041;&#24335;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#31639;&#27861;&#21644;&#19968;&#20010;&#26368;&#36817;&#30340;&#21253;&#21547;10&#19975;&#24352;&#26410;&#26631;&#35760;GIE&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65288;Hyperkvasir-unlabelled&#65289;&#30340;&#20351;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#31995;&#21015;GIE&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;ResNet50&#21644;ViT-B&#39592;&#24178;&#32593;&#32476;&#20197;&#33258;&#25105;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#26041;&#24335;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;&#38500;&#20102;&#30830;&#23450;&#27599;&#20010;&#20219;&#21153;&#26368;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#27969;&#31243;&#21644;&#39592;&#24178;&#32593;&#32476;&#26550;&#26500;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#30456;&#23545;&#20110;&#26377;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#26356;&#36866;&#21512;GIE&#35270;&#35273;&#20219;&#21153;&#30340;&#39592;&#24178;&#32593;&#32476;&#65307;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#20351;&#29992;ImageNet-1k&#36890;&#24120;&#27604;&#20351;&#29992;Hyperkvasir-unlabelled&#39044;&#35757;&#32451;&#26356;&#21512;&#36866;&#65292;&#20294;&#26377;&#19968;&#20010;&#26126;&#26174;&#30340;&#20363;&#22806;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally use image encoders pretrained in a supervised manner with ImageNet-1k as backbones. However, the use of modern self-supervised pretraining algorithms and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may allow for improvements. In this work, we study the fine-tuned performance of models with ResNet50 and ViT-B backbones pretrained in self-supervised and supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised only) in a range of GIE vision tasks. In addition to identifying the most suitable pretraining pipeline and backbone architecture for each task, out of those considered, our results suggest: that self-supervised pretraining generally produces more suitable backbones for GIE vision tasks than supervised pretraining; that self-supervised pretraining with ImageNet-1k is typically more suitable than pretraining with Hyperkvasir-unlabelled, with the notable exce
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.04234</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#36215;&#21040;&#20102;&#25913;&#36827;&#30340;&#20316;&#29992;&#65281;&#65288;arXiv&#65306;2312.04234v2 [cs.LG]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22240;&#20854;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#32780;&#38395;&#21517;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;Transformer&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#34920;&#31034;&#22312;&#21508;&#20010;&#23618;&#20043;&#38388;&#36235;&#20110;&#26080;&#27861;&#21306;&#20998;&#30340;&#20540;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#37322;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;&#24182;&#20174;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#28388;&#27874;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GFSA&#65289;&#65292;&#20197;&#23398;&#20064;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#20854;&#22797;&#26434;&#24230;&#30053;&#39640;&#20110;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GFSA&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#27169;&#24335;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#20195;&#30721;&#20998;&#31867;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#25913;&#36827;&#20102;Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
&lt;/p&gt;</description></item><item><title>CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02790</link><description>&lt;p&gt;
CausalCite&#65306;&#19968;&#31181;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02790
&lt;/p&gt;
&lt;p&gt;
CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31185;&#23398;&#30028;&#26469;&#35828;&#65292;&#35780;&#20272;&#19968;&#31687;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24341;&#29992;&#27425;&#25968;&#26159;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#19968;&#31687;&#35770;&#25991;&#30340;&#30495;&#27491;&#24433;&#21709;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;TextMatch&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#21305;&#37197;&#26694;&#26550;&#36866;&#24212;&#20110;&#39640;&#32500;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27599;&#31687;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25552;&#21462;&#30456;&#20284;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#24230;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#21512;&#25104;&#19968;&#20010;&#21453;&#20107;&#23454;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#25351;&#26631;&#31216;&#20026;CausalCite&#65292;&#20316;&#20026;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#26631;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#19982;&#31185;&#23398;&#19987;&#23478;&#23545;1K&#31687;&#35770;&#25991;&#30340;&#25253;&#21578;&#30340;&#35770;&#25991;&#24433;&#21709;&#21147;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36807;&#21435;&#35770;&#25991;&#30340;&#65288;&#32463;&#36807;&#26102;&#38388;&#32771;&#39564;&#30340;&#65289;&#22870;&#39033;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#23376;&#39046;&#22495;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#24615;&#32463;&#36807;&#23454;&#35777;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>http://arxiv.org/abs/2310.16705</link><description>&lt;p&gt;
&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#27969;&#22312;&#21464;&#20998;&#25512;&#26029;&#30340;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference. (arXiv:2310.16705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#24615;&#32463;&#36807;&#23454;&#35777;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21464;&#20998;&#21442;&#25968;&#34987;&#35843;&#25972;&#20197;&#20351;&#21464;&#20998;&#20998;&#24067;&#19982;&#30495;&#23454;&#21518;&#39564;&#23613;&#21487;&#33021;&#25509;&#36817;&#12290;&#21487;&#20197;&#36890;&#36807;&#40657;&#31665;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#26222;&#36890;&#26799;&#24230;&#19979;&#38477;&#25110;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26469;&#35299;&#20915;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#19968;&#20010;&#8220;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#8221;&#20013;&#23450;&#20041;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#20248;&#21270;&#25216;&#26415;&#65292;&#21363;&#40657;&#31665;&#21464;&#20998;&#25512;&#26029;&#21644;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#65292;&#21487;&#20197;&#37325;&#26032;&#35299;&#37322;&#20026;&#25152;&#25552;&#20986;&#30340;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#30340;&#29305;&#23450;&#23454;&#20363;&#12290;&#20026;&#20102;&#25552;&#39640;&#20248;&#21270;&#25928;&#29575;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#26469;&#25968;&#20540;&#27714;&#35299;&#31163;&#25955;&#26799;&#24230;&#27969;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theore
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08224</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#28508;&#22312;&#20108;&#36827;&#21046;&#32534;&#30721;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08224
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37197;&#22791;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#28508;&#22312;&#31354;&#38388;&#20013;&#22352;&#26631;$\vec{x}$&#30340;&#24179;&#26041;&#25351;&#25968;&#22686;&#38271;&#65292;&#35825;&#23548;&#20986;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#25105;&#20204;&#25551;&#36848;&#30340;&#29616;&#35937;&#26159;&#24050;&#30693;&#30340;&#19968;&#31181;&#34987;&#31216;&#20026;"&#31070;&#32463;&#23849;&#28291;"&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23427;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20986;&#29616;&#65292;&#24182;&#23548;&#33268;&#28508;&#22312;&#31867;&#22343;&#20540;&#23849;&#28291;&#20026;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#21152;&#36895;&#20102;&#25910;&#25947;&#21040;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#21270;&#23545;&#40784;&#21644;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65292;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#38899;&#35270;&#39057;&#20998;&#24067;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.08204</link><description>&lt;p&gt;
&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#19982;&#25239;&#36951;&#24536;&#30340;&#26412;&#22320;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments. (arXiv:2310.08204v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#21270;&#23545;&#40784;&#21644;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65292;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#38899;&#35270;&#39057;&#20998;&#24067;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#19981;&#26029;&#22320;&#20174;&#21253;&#21547;&#38899;&#35270;&#39057;&#23545;&#30340;&#35270;&#39057;&#27969;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#21516;&#26102;&#20854;&#20998;&#24067;&#38543;&#30528;&#26102;&#38388;&#19981;&#26029;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#26412;&#22320;&#21270;&#23545;&#40784;&#65306;&#24341;&#20837;&#19968;&#20010;&#23567;&#22411;&#21487;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#39044;&#27979;&#24444;&#27492;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#20196;&#29260;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21482;&#23398;&#20064;&#20855;&#26377;&#20934;&#30830;&#22810;&#27169;&#24577;&#20851;&#31995;&#30340;&#39640;&#24230;&#30456;&#20851;&#30340;&#38899;&#39057;&#35270;&#35273;&#22359;&#12290;&#65288;2&#65289;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65306;&#27604;&#36739;&#24403;&#21069;&#21644;&#36807;&#21435;&#25968;&#25454;&#23545;&#20043;&#38388;&#27599;&#20010;&#38899;&#39057;&#35270;&#39057;&#22359;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20808;&#21069;&#23398;&#20064;&#30340;&#38899;&#35270;&#39057;&#34920;&#31034;&#30340;&#24847;&#22806;&#28418;&#31227;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;FLAVA&#65288;&#25239;&#36951;&#24536;&#30340;&#26412;&#22320;&#21270;&#38899;&#35270;&#39057;&#23545;&#40784;&#65289;&#22312;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while all
&lt;/p&gt;</description></item><item><title>TimeGPT&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23427;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20511;&#37492;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26377;&#26395;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03589</link><description>&lt;p&gt;
TimeGPT-1. (arXiv:2310.03589v1 [cs.LG]) - &#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TimeGPT-1. (arXiv:2310.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03589
&lt;/p&gt;
&lt;p&gt;
TimeGPT&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23427;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20511;&#37492;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26377;&#26395;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TimeGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#24050;&#24314;&#31435;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;TimeGPT&#30340;&#38646;-shot&#25512;&#29702;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#30340;&#20854;&#20182;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20026;&#20154;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.02932</link><description>&lt;p&gt;
&#23545;&#27668;&#20505;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#65292;&#20102;&#35299;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#36171;&#20104;&#20010;&#20154;&#21644;&#31038;&#21306;&#20943;&#32531;&#21644;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#26377;&#24517;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;LLM&#23545;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#22238;&#31572;&#30340;&#21576;&#29616;&#21644;&#35748;&#35782;&#19978;&#30340;&#36866;&#24403;&#24615;&#65292;&#20026;LLM&#29983;&#25104;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#35206;&#30422;&#20102;8&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;30&#20010;&#19981;&#21516;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20363;&#23376;&#65292;&#36825;&#20010;&#39046;&#22495;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;AI&#21487;&#20197;&#34917;&#20805;&#21644;&#25552;&#21319;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#30417;&#30563;&#21327;&#35758;&#65292;&#21033;&#29992;AI&#36741;&#21161;&#24182;&#20381;&#38752;&#20855;&#26377;&#30456;&#20851;&#25945;&#32946;&#32972;&#26223;&#30340;&#35780;&#20272;&#21592;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
&lt;/p&gt;</description></item><item><title>AdaMerging&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#27169;&#22411;&#21512;&#24182;&#30340;&#31995;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21512;&#24182;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02575</link><description>&lt;p&gt;
AdaMerging: &#36866;&#24212;&#24615;&#27169;&#22411;&#21512;&#24182;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaMerging: Adaptive Model Merging for Multi-Task Learning. (arXiv:2310.02575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02575
&lt;/p&gt;
&lt;p&gt;
AdaMerging&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#27169;&#22411;&#21512;&#24182;&#30340;&#31995;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21512;&#24182;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21457;&#23637;&#34987;&#31216;&#20026;&#20219;&#21153;&#31639;&#26415;&#65292;&#25581;&#31034;&#20102;&#20960;&#20010;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#21512;&#24182;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#25509;&#28155;&#21152;&#27169;&#22411;&#24448;&#24448;&#20250;&#23548;&#33268;&#21512;&#24182;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#31181;&#19979;&#38477;&#26159;&#30001;&#20110;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#20914;&#31361;&#21644;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#25152;&#33268;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#21512;&#24182;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20351;&#29992;&#20854;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#27169;&#22411;&#21512;&#24182;&#65288;AdaMerging&#65289;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#23398;&#20064;&#27169;&#22411;&#21512;&#24182;&#30340;&#31995;&#25968;&#65292;&#21487;&#20197;&#26159;&#36880;&#20219;&#21153;&#25110;&#36880;&#23618;&#30340;&#26041;&#24335;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute MTL without necessitating a retraining process using the initial training data. Nevertheless, this direct addition of models often leads to a significant deterioration in the overall performance of the merged model. This decline occurs due to potential conflicts and intricate correlations among the multiple tasks. Consequently, the challenge emerges of how to merge pre-trained models more effectively without using their original training data. This paper introduces an innovative technique called Adaptive Model Merging (AdaMerging). This approach aims to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data. Specifically, our AdaMerging meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;</title><link>http://arxiv.org/abs/2310.01616</link><description>&lt;p&gt;
&#22810;&#25209;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65306;&#23545;&#20110;&#32500;&#24230;&#30456;&#20851;&#30340;&#36866;&#24212;&#24615;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity. (arXiv:2310.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22914;&#26524;&#31639;&#27861;&#22312;&#38382;&#39064;&#30340;&#32500;&#24230;d&#20013;&#20351;&#29992;&#30340;&#29615;&#22659;&#26597;&#35810;&#27425;&#25968;n&#26159;&#22810;&#39033;&#24335;&#30340;&#65292;&#37027;&#20040;&#23427;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#12290;&#36866;&#24212;&#24615;&#26159;&#25351;&#26597;&#35810;&#34987;&#21457;&#36865;&#21644;&#21453;&#39304;&#34987;&#22788;&#29702;&#20197;&#26356;&#26032;&#26597;&#35810;&#31574;&#30053;&#30340;&#39057;&#29575;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20801;&#35768;&#22312;K&#20010;&#25209;&#27425;&#20013;&#21457;&#36865;&#26597;&#35810;&#65292;&#22312;&#27599;&#20010;&#25209;&#27425;&#20043;&#21518;&#22788;&#29702;&#21453;&#39304;&#24182;&#26356;&#26032;&#26597;&#35810;&#12290;&#36825;&#20010;&#27169;&#22411;&#21253;&#25324;&#25972;&#20010;&#36866;&#24212;&#24615;&#35889;&#65292;&#20174;&#38750;&#33258;&#36866;&#24212;&#30340;&#8220;&#31163;&#32447;&#8221;&#65288;K=1&#65289;&#21040;&#23436;&#20840;&#33258;&#36866;&#24212;&#65288;K=n&#65289;&#30340;&#22330;&#26223;&#65292;&#20197;&#21450;&#20013;&#38388;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#21644;&#22312;d&#32500;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#24314;&#31435;&#20102;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377; &#937;(log log d) &#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;
We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy. To investigate this interplay, we employ a learning framework that allows sending queries in $K$ batches, with feedback being processed and queries updated after each batch. This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes in between. For the problems of policy evaluation and best-policy identification under $d$-dimensional linear function approximation, we establish $\Omega(\log \log d)$ lower bounds on the number of batches $K$ required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our results show that j
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16750</link><description>&lt;p&gt;
&#30524;&#20013;&#35760;&#24518;&#65306;&#25193;&#25955;&#27169;&#22411;&#21644;&#20851;&#32852;&#35760;&#24518;&#20043;&#38388;&#30340;&#31070;&#31192;&#30456;&#20284;&#20043;&#22788;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories. (arXiv:2309.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26368;&#36817;&#22312;&#35768;&#22810;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#25968;&#23398;&#25551;&#36848;&#26377;&#24456;&#22810;&#31181;&#26041;&#24335;&#65292;&#36825;&#20351;&#24471;&#20154;&#20204;&#24456;&#38590;&#23545;&#20854;&#24037;&#20316;&#21407;&#29702;&#36827;&#34892;&#31616;&#21333;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;&#21160;&#21147;&#31995;&#32479;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;DMs&#30340;&#31616;&#26126;&#27010;&#36848;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19982;&#20854;&#39640;&#24230;&#30456;&#20851;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#33021;&#37327;&#27169;&#22411;&#31867;&#21035;&#65292;&#31216;&#20026;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#30340;&#25968;&#23398;&#32852;&#31995;&#12290;&#22522;&#20110;&#33021;&#37327;&#30340;AMs&#26159;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20854;&#34892;&#20026;&#19982;&#21435;&#22122;DMs&#38750;&#24120;&#30456;&#20284;&#65292;&#20294;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#19968;&#20010;Lyapunov&#33021;&#37327;&#20989;&#25968;&#65292;&#22312;&#20854;&#19978;&#21487;&#20197;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#20197;&#21435;&#22122;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#33021;&#37327;AMs&#30340;40&#24180;&#21382;&#21490;&#65292;&#20174;&#26368;&#21021;&#30340;Hopfield&#32593;&#32476;&#24320;&#22987;&#65292;&#24182;&#35752;&#35770;&#20102;&#36890;&#36807;&#25551;&#36848;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#31243;&#24230;&#25581;&#31034;&#20986;&#26469;&#30340;AMs&#21644;DMs&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have recently set state-of-the-art on many generation benchmarks. However, there are myriad ways to describe them mathematically, which makes it difficult to develop a simple understanding of how they work. In this survey, we provide a concise overview of DMs from the perspective of dynamical systems and Ordinary Differential Equations (ODEs) which exposes a mathematical connection to the highly related yet often overlooked class of energy-based models, called Associative Memories (AMs). Energy-based AMs are a theoretical framework that behave much like denoising DMs, but they enable us to directly compute a Lyapunov energy function on which we can perform gradient descent to denoise data. We then summarize the 40 year history of energy-based AMs, beginning with the original Hopfield Network, and discuss new research directions for AMs and DMs that are revealed by characterizing the extent of their similarities and differences
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12455</link><description>&lt;p&gt;
LongDocFACTScore: &#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#23454;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12455
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#20107;&#23454;&#19968;&#33268;&#24615;&#26159;&#29983;&#25104;&#24615;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#25688;&#35201;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;ROUGE&#24471;&#20998;&#65289;&#26080;&#27861;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26377;&#38480;&#21046;&#24615;&#30340;&#20196;&#29260;&#38480;&#21046;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#22312;&#24212;&#29992;&#20110;&#38271;&#25991;&#26723;&#25968;&#25454;&#38598;&#26102;&#26159;&#21542;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;LongDocFACTScore&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#24230;&#37327;&#26631;&#20934;&#25193;&#23637;&#21040;&#20219;&#24847;&#38271;&#24230;&#30340;&#25991;&#26723;&#12290;&#35813;&#26694;&#26550;&#22312;&#19982;&#20154;&#31867;&#20107;&#23454;&#19968;&#33268;&#24615;&#24230;&#37327;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11341</link><description>&lt;p&gt;
&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#26377;&#21644;&#26032;&#21457;&#24067;&#30340;&#25991;&#31456;&#25968;&#37327;&#24222;&#22823;&#65292;&#23558;&#30740;&#31350;&#25104;&#26524;&#20998;&#31867;&#21040;&#29305;&#23450;&#19978;&#19979;&#25991;&#26631;&#31614;&#20307;&#31995;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#20016;&#23500;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#25991;&#31456;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;SciBERT&#26469;&#29983;&#25104;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#25429;&#25417;&#25991;&#31456;&#30340;&#25991;&#26412;&#20803;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;Open Graph Benchmark&#65288;OGB&#65289;ogbn-arxiv&#25968;&#25454;&#38598;&#21644;PubMed&#31958;&#23615;&#30149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23436;&#20840;&#30417;&#30563;&#30340;&#20256;&#23548;&#24335;&#33410;&#28857;&#20998;&#31867;&#23454;&#39564;&#65292;&#20998;&#21035;&#36890;&#36807;Microsoft Academic Graph&#65288;MAG&#65289;&#21644;PubMed Central&#28155;&#21152;&#20102;&#38468;&#21152;&#20803;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36793;&#24322;&#26500;&#22270;&#30456;&#23545;&#20110;&#36793;&#21516;&#26500;&#22270;&#65292;&#33021;&#22815;&#22987;&#32456;&#25552;&#39640;&#25152;&#26377;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#20351;&#31616;&#21333;&#19988;&#27973;&#23618;&#30340;GNN&#27969;&#27700;&#32447;&#33021;&#22815;&#19982;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architect
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.03426</link><description>&lt;p&gt;
&#30456;&#31561;&#30340;&#38271;&#26399;&#25928;&#30410;&#29575;&#65306;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#27010;&#24565;&#24212;&#29992;&#21040;&#39034;&#24207;&#20915;&#31574;&#20013;
&lt;/p&gt;
&lt;p&gt;
Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03426
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#21487;&#33021;&#23545;&#26102;&#38388;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38271;&#26399;&#20844;&#24179;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24573;&#30053;&#38271;&#26399;&#24433;&#21709;&#26102;&#65292;&#31616;&#21333;&#22320;&#24212;&#29992;&#38745;&#24577;&#20844;&#24179;&#24615;&#20934;&#21017;&#23454;&#38469;&#19978;&#20250;&#21152;&#21095;&#20559;&#35265;&#12290;&#20026;&#20102;&#26126;&#30830;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#20559;&#35265;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26694;&#26550;&#20013;&#21046;&#23450;&#20102;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#20182;&#20204;&#23558;&#38271;&#26399;&#20559;&#35265;&#23450;&#20041;&#20026;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#38745;&#24577;&#20559;&#35265;&#30340;&#24635;&#21644;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#22320;&#23545;&#36880;&#27493;&#20559;&#35265;&#27714;&#21644;&#21487;&#33021;&#23548;&#33268;&#34394;&#20551;&#30340;&#20844;&#24179;&#24863;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#21040;&#36716;&#25442;&#36807;&#31243;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#37325;&#35201;&#24615;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#23427;&#26126;&#30830;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38271;&#26399;&#25910;&#30410;&#30340;&#31574;&#30053;&#26799;&#24230;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12696</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TopDis&#65288;&#25299;&#25169;&#35299;&#32544;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#22810;&#23610;&#24230;&#25299;&#25169;&#25439;&#22833;&#39033;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#35299;&#32544;&#26159;&#25968;&#25454;&#34920;&#31034;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#32423;&#35748;&#30693;&#30340;&#23454;&#29616;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#22522;&#20110;VAE&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#28508;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24635;&#20307;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#23646;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#35299;&#32544;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#25968;&#25454;&#27969;&#24418;&#36941;&#21382;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25299;&#25169;&#25439;&#22833;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#65292;&#22914;MIG&#12289;FactorVAE&#24471;&#20998;&#12289;SAP&#24471;&#20998;&#21644;DCI&#35299;&#32544;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#20808;&#39564;&#30693;&#35782;&#38656;&#27714;&#21644;&#25511;&#21046;&#20998;&#24067;&#28418;&#31227;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#26041;&#38754;&#20855;&#26377;&#31361;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.10675</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#36866;&#24212;&#24615;&#31639;&#27861;&#20013;&#30340;&#26368;&#20339;&#26041;&#26696;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
An Improved Best-of-both-worlds Algorithm for Bandits with Delayed Feedback. (arXiv:2308.10675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10675
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#20808;&#39564;&#30693;&#35782;&#38656;&#27714;&#21644;&#25511;&#21046;&#20998;&#24067;&#28418;&#31227;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#26041;&#38754;&#20855;&#26377;&#31361;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#36866;&#24212;&#24615;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#22823;&#24310;&#36831;$d_{\mathrm{max}}$&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#20004;&#20010;&#24773;&#26223;&#19979;&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25913;&#36827;&#20102;Masoudian&#31561;&#20154;[2022]&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#31639;&#27861;&#21644;&#23427;&#30340;&#36951;&#25022;&#30028;&#38480;&#26159;&#22522;&#20110;&#26410;&#35299;&#20915;&#30340;&#35266;&#27979;&#27425;&#25968;&#65288;&#22312;&#34892;&#21160;&#26102;&#38388;&#35266;&#23519;&#21040;&#30340;&#25968;&#37327;&#65289;&#32780;&#19981;&#26159;&#24310;&#36831;&#25110;&#26368;&#22823;&#24310;&#36831;&#65288;&#21482;&#26377;&#24403;&#21453;&#39304;&#21040;&#36798;&#26102;&#25165;&#33021;&#35266;&#23519;&#21040;&#30340;&#25968;&#37327;&#65289;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#36129;&#29486;&#26159;&#22522;&#20110;&#26377;&#20559;&#25439;&#22833;&#20272;&#35745;&#22120;&#21644;&#36339;&#36807;&#20855;&#26377;&#36807;&#22823;&#24310;&#36831;&#35266;&#27979;&#30340;&#26032;&#22411;&#20998;&#24067;&#28418;&#31227;&#25511;&#21046;&#12290;&#21478;&#19968;&#20010;&#20027;&#35201;&#30340;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#36866;&#24212;&#24615;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#26159;&#30001;&#22312;&#36339;&#36807;&#20855;&#26377;&#36807;&#22823;&#24310;&#36831;&#35266;&#27979;&#21518;&#30340;&#32047;&#31215;&#26410;&#35299;&#20915;&#35266;&#27979;&#27425;&#25968;&#26469;&#25551;&#36848;&#30340;&#65292;&#32780;&#19981;&#26159;&#24310;&#36831;&#25110;&#26368;&#22823;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new best-of-both-worlds algorithm for bandits with variably delayed feedback. The algorithm improves on prior work by Masoudian et al. [2022] by eliminating the need in prior knowledge of the maximal delay $d_{\mathrm{max}}$ and providing tighter regret bounds in both regimes. The algorithm and its regret bounds are based on counts of outstanding observations (a quantity that is observed at action time) rather than delays or the maximal delay (quantities that are only observed when feedback arrives). One major contribution is a novel control of distribution drift, which is based on biased loss estimators and skipping of observations with excessively large delays. Another major contribution is demonstrating that the complexity of best-of-both-worlds bandits with delayed feedback is characterized by the cumulative count of outstanding observations after skipping of observations with excessively large delays, rather than the delays or the maximal delay.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12289;&#25429;&#25417;&#31283;&#20581;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08012</link><description>&lt;p&gt;
&#22522;&#20110;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling. (arXiv:2308.08012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12289;&#25429;&#25417;&#31283;&#20581;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#36890;&#24615;&#31283;&#20581;&#24615;&#26159;&#29702;&#35299;&#12289;&#20248;&#21270;&#21644;&#20462;&#22797;&#22797;&#26434;&#32593;&#32476;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#32791;&#26102;&#19988;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#27169;&#25311;&#26469;&#35780;&#20272;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#21253;&#25324;&#22312;&#26356;&#19968;&#33324;&#30340;&#36793;&#32536;&#21024;&#38500;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25915;&#20987;&#26354;&#32447;&#25429;&#25417;&#31283;&#20581;&#24615;&#32780;&#19981;&#26159;&#30452;&#25509;&#35757;&#32451;&#31283;&#20581;&#24615;&#65292;&#39044;&#27979;&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#39044;&#27979;&#33021;&#21147;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;(CNN)&#65292;&#35843;&#25972;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#37325;&#26032;&#35774;&#35745;&#25915;&#20987;&#27169;&#24335;&#65292;&#24341;&#20837;&#36866;&#24403;&#30340;&#36807;&#28388;&#35268;&#21017;&#65292;&#24182;&#23558;&#31283;&#20581;&#24615;&#30340;&#20215;&#20540;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#21152;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CNN&#26694;&#26550;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#20840;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22270;&#33258;&#21516;&#24577;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#65292;&#25214;&#21040;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;&#23618;&#20989;&#25968;&#20043;&#38388;&#30340;&#30697;&#38453;&#30340;&#29983;&#25104;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.07810</link><description>&lt;p&gt;
&#22270;&#33258;&#21516;&#24577;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Automorphism Group Equivariant Neural Networks. (arXiv:2307.07810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22270;&#33258;&#21516;&#24577;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#65292;&#25214;&#21040;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;&#23618;&#20989;&#25968;&#20043;&#38388;&#30340;&#30697;&#38453;&#30340;&#29983;&#25104;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;n&#20010;&#39030;&#28857;&#21644;&#20854;&#33258;&#21516;&#24577;&#32676;Aut(G)&#30340;&#22270;G&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;Aut(G)-&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#65292;&#20854;&#23618;&#26159;n&#32500;&#23454;&#25968;&#24352;&#37327;&#30340;&#26576;&#20123;&#24352;&#37327;&#24130;&#27425;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;n&#32500;&#23454;&#25968;&#31354;&#38388;&#30340;&#26631;&#20934;&#22522;&#19979;&#25214;&#21040;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;Aut(G)-&#31561;&#21464;&#23618;&#20989;&#25968;&#20043;&#38388;&#30340;&#30697;&#38453;&#30340;&#29983;&#25104;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
For any graph $G$ having $n$ vertices and its automorphism group $\textrm{Aut}(G)$, we provide a full characterisation of all of the possible $\textrm{Aut}(G)$-equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. In particular, we find a spanning set of matrices for the learnable, linear, $\textrm{Aut}(G)$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;(QPP)&#30340;&#36890;&#29992;&#38544;&#31169;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25351;&#23450;&#31169;&#26377;&#20449;&#24687;&#12289;&#21487;&#34892;&#30340;&#27979;&#37327;&#21644;&#22495;&#30693;&#35782;&#26041;&#38754;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#25324;&#21644;&#20811;&#26381;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;QPP&#30340;&#25805;&#20316;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;QPP&#30340;&#20984;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#21518;&#22788;&#29702;&#65292;&#25512;&#23548;&#20102;&#20445;&#35777;&#21435;&#26497;&#21270;&#26426;&#21046;QPP&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;QPP&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#23457;&#35745;&#65292;&#20197;&#35782;&#21035;&#38544;&#31169;&#20405;&#29359;&#12290;</title><link>http://arxiv.org/abs/2306.13054</link><description>&lt;p&gt;
&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#37327;&#23376;&#31995;&#32479;&#38544;&#31169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum Systems. (arXiv:2306.13054v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;(QPP)&#30340;&#36890;&#29992;&#38544;&#31169;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25351;&#23450;&#31169;&#26377;&#20449;&#24687;&#12289;&#21487;&#34892;&#30340;&#27979;&#37327;&#21644;&#22495;&#30693;&#35782;&#26041;&#38754;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#25324;&#21644;&#20811;&#26381;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;QPP&#30340;&#25805;&#20316;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;QPP&#30340;&#20984;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#21518;&#22788;&#29702;&#65292;&#25512;&#23548;&#20102;&#20445;&#35777;&#21435;&#26497;&#21270;&#26426;&#21046;QPP&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;QPP&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#23457;&#35745;&#65292;&#20197;&#35782;&#21035;&#38544;&#31169;&#20405;&#29359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#31995;&#32479;&#30340;&#36890;&#29992;&#38544;&#31169;&#26694;&#26550;&#65292;&#31216;&#20026;&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;&#65288;QPP&#65289;&#12290;&#21463;&#32463;&#20856;&#27827;&#35930;&#38544;&#31169;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#27010;&#25324;&#24182;&#35299;&#20915;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#22312;&#25351;&#23450;&#31169;&#26377;&#20449;&#24687;&#12289;&#21487;&#34892;&#30340;&#27979;&#37327;&#21644;&#22495;&#30693;&#35782;&#26041;&#38754;&#25552;&#20379;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;QPP&#21487;&#20197;&#31561;&#25928;&#22320;&#29992;Datta-Leditzky&#20449;&#24687;&#35889;&#25955;&#24230;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#39318;&#27425;&#25552;&#20379;&#20102;&#23427;&#30340;&#25805;&#20316;&#35299;&#37322;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#24046;&#24322;&#37325;&#26032;&#34920;&#36848;&#20026;&#21322;&#23450;&#35268;&#21010;&#65292;&#24182;&#25512;&#23548;&#20986;&#23427;&#30340;&#20960;&#20010;&#23646;&#24615;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#23646;&#24615;&#35777;&#26126;&#20102;QPP&#26426;&#21046;&#30340;&#20984;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#21518;&#22788;&#29702;&#12290;&#36824;&#25512;&#23548;&#20102;&#20445;&#35777;&#21435;&#26497;&#21270;&#26426;&#21046;QPP&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26222;&#36890;QPP&#26426;&#21046;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#20877;&#27425;&#20197;&#21435;&#26497;&#21270;&#26426;&#21046;&#20026;&#20363;&#30740;&#31350;&#20854;&#26126;&#30830;&#23454;&#20363;&#12290;&#28982;&#21518;&#23558;QPP&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#23457;&#35745;&#65292;&#20197;&#35782;&#21035;&#38544;&#31169;&#20405;&#29359;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a versatile privacy framework for quantum systems, termed quantum pufferfish privacy (QPP). Inspired by classical pufferfish privacy, our formulation generalizes and addresses limitations of quantum differential privacy by offering flexibility in specifying private information, feasible measurements, and domain knowledge. We show that QPP can be equivalently formulated in terms of the Datta-Leditzky information spectrum divergence, thus providing the first operational interpretation thereof. We reformulate this divergence as a semi-definite program and derive several properties of it, which are then used to prove convexity, composability, and post-processing of QPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism are also derived. We analyze the privacy-utility tradeoff of general QPP mechanisms and, again, study the depolarization mechanism as an explicit instance. The QPP framework is then applied to privacy auditing for identifying privacy violati
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#20855;&#26377;&#29616;&#23454;&#20551;&#35774;&#30340;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#21464;&#25104;&#20102;&#19968;&#20010;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#26412;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02931</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery using Bayesian Model Selection. (arXiv:2306.02931v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02931
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#29616;&#23454;&#20551;&#35774;&#30340;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#21464;&#25104;&#20102;&#19968;&#20010;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#26412;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#20004;&#20010;&#21464;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#19988;&#27809;&#26377;&#20854;&#20182;&#20551;&#35774;&#65292;&#26080;&#27861;&#25512;&#26029;&#21738;&#20010;&#21464;&#37327;&#26159;&#24341;&#36215;&#21478;&#19968;&#20010;&#21464;&#37327;&#30340;&#21407;&#22240;&#12290;&#22823;&#37096;&#20998;&#22240;&#26524;&#25991;&#29486;&#32858;&#28966;&#20110;&#38024;&#23545;&#24378;&#20551;&#35774;&#30340;&#25968;&#25454;&#38598;(&#22914;&#21152;&#24615;&#22122;&#22768;&#25110;&#21442;&#25968;&#35745;&#25968;&#38480;&#21046;)&#20445;&#35777;&#22240;&#26524;&#26041;&#21521;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#34987;&#27979;&#35797;&#20110;&#36829;&#21453;&#20551;&#35774;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#22914;&#20309;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#20351;&#29992;&#22240;&#26524;&#20551;&#35774;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#20855;&#26377;&#29616;&#23454;&#20551;&#35774;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32534;&#30721;&#29420;&#31435;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#23548;&#33268;&#22240;&#26524;&#26041;&#21521;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#24615;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#25104;&#20026;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20309;&#22312;&#24050;&#30693;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#21644;&#28789;&#27963;&#30340;&#27169;&#22411;&#31867;&#19978;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
With only observational data on two variables, and without other assumptions, it is not possible to infer which one causes the other. Much of the causal literature has focused on guaranteeing identifiability of causal direction in statistical models for datasets where strong assumptions hold, such as additive noise or restrictions on parameter count. These methods are then subsequently tested on realistic datasets, most of which violate their assumptions. Building on previous attempts, we show how to use causal assumptions within the Bayesian framework. This allows us to specify models with realistic assumptions, while also encoding independent causal mechanisms, leading to an asymmetry between the causal directions. Identifying causal direction then becomes a Bayesian model selection problem. We analyse why Bayesian model selection works for known identifiable cases and flexible model classes, while also providing correctness guarantees about its behaviour. To demonstrate our approach
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31181;&#21487;&#33021;&#8220;&#35268;&#33539;&#19981;&#27491;&#30830;&#8221;&#27169;&#22411;&#30340;&#36890;&#29992;&#21327;&#35758;&#65292;&#8220;&#25554;&#20214;&#24335;&#34920;&#29616;&#20248;&#21270;&#8221;&#12290;</title><link>http://arxiv.org/abs/2305.18728</link><description>&lt;p&gt;
&#25554;&#20214;&#21270;&#34920;&#29616;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Plug-in Performative Optimization. (arXiv:2305.18728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18728
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31181;&#21487;&#33021;&#8220;&#35268;&#33539;&#19981;&#27491;&#30830;&#8221;&#27169;&#22411;&#30340;&#36890;&#29992;&#21327;&#35758;&#65292;&#8220;&#25554;&#20214;&#24335;&#34920;&#29616;&#20248;&#21270;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#39044;&#27979;&#20855;&#26377;&#34920;&#29616;&#24615;&#26102;&#65292;&#36873;&#25321;&#21738;&#20010;&#39044;&#27979;&#22120;&#37096;&#32626;&#23558;&#24433;&#21709;&#26410;&#26469;&#35266;&#27979;&#30340;&#20998;&#24067;&#12290;&#22312;&#34920;&#29616;&#24615;&#23398;&#20064;&#20013;&#65292;&#24635;&#20307;&#30446;&#26631;&#26159;&#25214;&#21040;&#20855;&#26377;&#20302;&#8220;&#34920;&#29616;&#24615;&#39118;&#38505;&#8221;&#30340;&#39044;&#27979;&#22120;&#65292;&#21363;&#22312;&#20854;&#24341;&#23548;&#30340;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#20248;&#21270;&#34920;&#29616;&#24615;&#39118;&#38505;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#36172;&#24466;&#31639;&#27861;&#21644;&#20854;&#20182;&#26080;&#23548;&#25968;&#26041;&#27861;&#65292;&#22312;&#34920;&#29616;&#24615;&#21453;&#39304;&#20013;&#19981;&#30693;&#36947;&#20219;&#20309;&#32467;&#26500;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#26497;&#24930;&#12290;&#34917;&#20805;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#21453;&#39304;&#20013;&#30340;&#26174;&#24335;&#8220;&#27169;&#22411;&#8221;&#65292;&#20363;&#22914;&#25112;&#30053;&#20998;&#31867;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36895;&#29575;&#20851;&#38190;&#20381;&#36182;&#20110;&#21453;&#39304;&#27169;&#22411;&#30340;&#35268;&#33539;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21551;&#21160;&#20102;&#23545;&#22312;&#34920;&#29616;&#24615;&#39044;&#27979;&#20013;&#20351;&#29992;&#21487;&#33021;&#30340;&#8220;&#35268;&#33539;&#19981;&#27491;&#30830;&#8221;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#30340;&#36890;&#29992;&#21327;&#35758;&#65292;&#31216;&#20026;&#8220;&#25554;&#20214;&#24335;&#34920;&#29616;&#20248;&#21270;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
When predictions are performative, the choice of which predictor to deploy influences the distribution of future observations. The overarching goal in learning under performativity is to find a predictor that has low \emph{performative risk}, that is, good performance on its induced distribution. One family of solutions for optimizing the performative risk, including bandits and other derivative-free methods, is agnostic to any structure in the performative feedback, leading to exceedingly slow convergence rates. A complementary family of solutions makes use of explicit \emph{models} for the feedback, such as best-response models in strategic classification, enabling significantly faster rates. However, these rates critically rely on the feedback model being well-specified. In this work we initiate a study of the use of possibly \emph{misspecified} models in performative prediction. We study a general protocol for making use of models, called \emph{plug-in performative optimization}, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#33258;&#23450;&#20041;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#20302;&#32500;&#32467;&#26500;&#21487;&#20197;&#22312;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13536</link><description>&lt;p&gt;
&#29992;&#20302;&#32500;&#21442;&#25968;&#23376;&#31354;&#38388;&#34920;&#31034;&#36755;&#20837;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Representing Input Transformations by Low-Dimensional Parameter Subspaces. (arXiv:2305.13536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#33258;&#23450;&#20041;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#20302;&#32500;&#32467;&#26500;&#21487;&#20197;&#22312;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#23545;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#21464;&#25442;&#65288;&#22914;&#26059;&#36716;&#12289;&#32553;&#25918;&#21644;&#24179;&#31227;&#65289;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#38500;&#38750;&#23427;&#20204;&#20855;&#26377;&#29305;&#23450;&#30340;&#19981;&#21464;&#32467;&#26500;&#25110;&#32463;&#36807;&#29305;&#23450;&#30340;&#35757;&#32451;&#21518;&#65288;&#20363;&#22914;&#20174;&#25968;&#25454;&#22686;&#24378;&#20013;&#23398;&#20064;&#25152;&#38656;&#30340;&#40065;&#26834;&#24615;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#21363;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#23376;&#31354;&#38388;&#21487;&#37197;&#32622;&#32593;&#32476;&#26469;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#23427;&#20204;&#22312;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#30340;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#12289;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#20013;&#30340;&#32467;&#26500;&#21644;&#24778;&#20154;&#30340;&#20302;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#36328;&#19981;&#21516;&#39046;&#22495;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#36716;&#31227;&#36755;&#20837;&#21464;&#25442;&#30693;&#35782;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#26356;&#20581;&#22766;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models lack robustness to simple input transformations such as rotation, scaling, and translation, unless they feature a particular invariant architecture or undergo specific training, e.g., learning the desired robustness from data augmentations. Alternatively, input transformations can be treated as a domain shift problem, and solved by post-deployment model adaptation. Although a large number of methods deal with transformed inputs, the fundamental relation between input transformations and optimal model weights is unknown. In this paper, we put forward the configuration subspace hypothesis that model weights optimal for parameterized continuous transformations can reside in low-dimensional linear subspaces. We introduce subspace-configurable networks to learn these subspaces and observe their structure and surprisingly low dimensionality on all tested transformations, datasets and architectures from computer vision and audio signal processing domains. Our findings enable effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#35299;&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#65288;SBC&#65289;&#30340;GSBC&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#38408;&#20540;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#12289;&#26465;&#20214;&#38543;&#26426;&#37319;&#26679;&#21644;$\ell_\infty$&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#24182;&#33021;&#22815;&#20998;&#26512;&#30830;&#23450;&#39044;&#26399;&#30340;&#35299;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#32780;&#25918;&#26494;&#30340;&#22122;&#22768;SBC&#20013;&#31526;&#21495;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13957</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#30340;&#20998;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Factorizers for Distributed Sparse Block Codes. (arXiv:2303.13957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#35299;&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#65288;SBC&#65289;&#30340;GSBC&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#38408;&#20540;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#12289;&#26465;&#20214;&#38543;&#26426;&#37319;&#26679;&#21644;$\ell_\infty$&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#24182;&#33021;&#22815;&#20998;&#26512;&#30830;&#23450;&#39044;&#26399;&#30340;&#35299;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#32780;&#25918;&#26494;&#30340;&#22122;&#22768;SBC&#20013;&#31526;&#21495;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#65288;SBC&#65289;&#21033;&#29992;&#22266;&#23450;&#23485;&#24230;&#30340;&#21521;&#37327;&#23545;&#31526;&#21495;&#25968;&#25454;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#21644;&#25805;&#20316;&#65292;&#20855;&#26377;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#22312;&#19981;&#24517;&#25628;&#23547;&#25152;&#26377;&#21487;&#33021;&#30340;&#32452;&#21512;&#30340;&#24773;&#20917;&#19979;&#23558;&#36825;&#20123;&#25968;&#25454;&#32467;&#26500;&#25286;&#20998;&#25104;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;&#24403;&#20351;&#29992;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26597;&#35810;&#21521;&#37327;&#26102;&#65292;&#22122;&#22768;SBC&#20013;&#30340;&#31526;&#21495;&#34920;&#31034;&#30001;&#20110;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#32780;&#25918;&#26494;&#65292;&#36825;&#20351;&#24471;&#36825;&#31181;&#20998;&#35299;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#31934;&#24230;&#30340;&#26041;&#27861;&#26469;&#20998;&#35299;&#19968;&#31181;&#26356;&#28789;&#27963;&#12289;&#22240;&#27492;&#26356;&#26222;&#36941;&#30340;SBC&#24418;&#24335;&#65292;&#31216;&#20026;GSBC&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#22240;&#23376;&#24341;&#20837;&#20102;&#22522;&#20110;&#38408;&#20540;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#12289;&#26465;&#20214;&#38543;&#26426;&#37319;&#26679;&#21644;$\ell_\infty$&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#23427;&#30340;&#38543;&#26426;&#37319;&#26679;&#26426;&#21046;&#19982;&#21472;&#21152;&#25628;&#32034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20998;&#26512;&#30830;&#23450;&#39044;&#26399;&#30340;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed sparse block codes (SBCs) exhibit compact representations for encoding and manipulating symbolic data structures using fixed-with vectors. One major challenge however is to disentangle, or factorize, such data structures into their constituent elements without having to search through all possible combinations. This factorization becomes more challenging when queried by noisy SBCs wherein symbol representations are relaxed due to perceptual uncertainty and approximations made when modern neural networks are used to generate the query vectors. To address these challenges, we first propose a fast and highly accurate method for factorizing a more flexible and hence generalized form of SBCs, dubbed GSBCs. Our iterative factorizer introduces a threshold-based nonlinear activation, a conditional random sampling, and an $\ell_\infty$-based similarity metric. Its random sampling mechanism in combination with the search in superposition allows to analytically determine the expected 
&lt;/p&gt;</description></item><item><title>CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.06213</link><description>&lt;p&gt;
CHGNN: &#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network. (arXiv:2303.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06213
&lt;/p&gt;
&lt;p&gt;
CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHGNN is a semi-supervised contrastive hypergraph learning network that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. It includes an adaptive hypergraph view generator, an improved hypergraph encoder, and a joint loss function.
&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#21487;&#20197;&#27169;&#25311;&#24212;&#29992;&#31243;&#24207;&#20013;&#21457;&#29616;&#30340;&#25968;&#25454;&#23545;&#35937;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36229;&#22270;&#23398;&#20064;&#30740;&#31350;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#25193;&#23637;&#21040;&#36229;&#22270;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#29305;&#24449;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;CHGNN&#65292;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;CHGNN&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#65292;&#37319;&#29992;&#33258;&#21160;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#23398;&#20064;&#26368;&#23567;&#20805;&#20998;&#35270;&#22270;&#30340;&#25200;&#21160;&#27010;&#29575;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;CHGNN&#21253;&#21547;&#19968;&#20010;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#65292;&#32771;&#34385;&#21040;&#36229;&#36793;&#30340;&#21516;&#36136;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#31532;&#19977;&#65292;CHGNN&#37197;&#22791;&#20102;&#19968;&#20010;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#35270;&#22270;&#29983;&#25104;&#22120;&#30340;&#30456;&#20284;&#24615;&#25439;&#22833;&#12289;&#33410;&#28857;&#20998;&#31867;&#25439;&#22833;&#21644;&#36229;&#36793;&#21516;&#36136;&#24615;&#25439;&#22833;&#65292;&#27880;&#20837;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#35299;&#20915;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#65292;&#38024;&#23545;&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#36275;&#22815;&#22823;&#30340;&#39044;&#31639;&#19979;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#37117;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#20248;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2302.07510</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Stochastic Rising Bandits. (arXiv:2302.07510v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#35299;&#20915;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#65292;&#38024;&#23545;&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#36275;&#22815;&#22823;&#30340;&#39044;&#31639;&#19979;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#37117;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#20248;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426; (SRB) &#27169;&#22411;&#25551;&#36848;&#20102;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#21487;&#36873;&#36873;&#39033;&#30340;&#39044;&#26399;&#22870;&#21169;&#27599;&#27425;&#36873;&#25321;&#37117;&#20250;&#22686;&#21152;&#12290;&#36825;&#20010;&#35774;&#32622;&#28085;&#30422;&#20102;&#35768;&#22810;&#22330;&#26223;&#65292;&#20854;&#20013;&#21487;&#29992;&#36873;&#39033;&#26159;&#23398;&#20064;&#23454;&#20307;&#65292;&#20854;&#34920;&#29616;&#65288;&#26399;&#26395;&#65289;&#38543;&#26102;&#38388;&#25913;&#21892;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20294;&#26412;&#25991;&#19987;&#27880;&#20110; SRB &#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035; (BAI) &#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#23450;&#36718;&#25968;&#30340;&#22266;&#23450;&#39044;&#31639;&#65292;&#25105;&#20204;&#35201;&#22312;&#35782;&#21035;&#36807;&#31243;&#32467;&#26463;&#26102;&#25552;&#20379;&#20851;&#20110;&#26368;&#20339;&#36873;&#39033;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#21363; R-UCBE&#65292;&#23427;&#37319;&#29992;&#31867;&#20284;&#20110; UCB &#30340;&#26041;&#27861;&#65292;&#21644; R-SR&#65292;&#23427;&#37319;&#29992;&#36880;&#27493;&#25298;&#32477;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#39044;&#31639;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#20445;&#35777;&#33021;&#22815;&#27491;&#30830;&#35782;&#21035;&#26368;&#20248;&#36873;&#39033;&#30340;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545; SRB &#30340; BAI &#38382;&#39064;&#30340;&#38543;&#26426;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#20204;&#35777;&#26126;&#20102; R-UCBE &#30340;&#19978;&#30028;&#21644; R-SR &#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected rewards of the available options increase every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper, focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Density-Softmax&#30340;&#24555;&#36895;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23494;&#24230;&#20989;&#25968;&#19982;softmax&#32467;&#21512;&#26469;&#25552;&#39640;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;</title><link>http://arxiv.org/abs/2302.06495</link><description>&lt;p&gt;
Density-Softmax: &#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24555;&#36895;&#30830;&#23450;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts. (arXiv:2302.06495v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Density-Softmax&#30340;&#24555;&#36895;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23494;&#24230;&#20989;&#25968;&#19982;softmax&#32467;&#21512;&#26469;&#25552;&#39640;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#23384;&#22312;&#36739;&#22823;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#27010;&#29575;&#26041;&#27861;&#34429;&#28982;&#33021;&#32531;&#35299;&#27492;&#38382;&#39064;&#20294;&#35745;&#31639;&#25928;&#29575;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;Density-Softmax&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23494;&#24230;&#20989;&#25968;&#19982;softmax&#32467;&#21512;&#65292;&#20197;&#24555;&#36895;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#24335;&#25552;&#39640;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#30340;&#20284;&#28982;&#20540;&#65292;&#22312;&#27979;&#35797;&#26102;&#22312;&#36828;&#31163;&#35757;&#32451;&#26679;&#26412;&#26102;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#39564;&#19978;&#65292;Density-Softmax&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#26631;&#20934;softmax&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#31354;&#38388;&#26469;&#26500;&#24314;&#20581;&#24247;&#25351;&#26631;&#65292;&#21487;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#30340;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2208.13288</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#26377;&#20449;&#24687;&#20581;&#24247;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Informative Health Indicators Through Unsupervised Contrastive Learning. (arXiv:2208.13288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#31354;&#38388;&#26469;&#26500;&#24314;&#20581;&#24247;&#25351;&#26631;&#65292;&#21487;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#30340;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#30417;&#27979;&#23545;&#20110;&#23433;&#20840;&#39640;&#25928;&#22320;&#36816;&#33829;&#24037;&#19994;&#36164;&#20135;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36817;&#24180;&#26469;&#23545;&#31283;&#20581;&#20581;&#24247;&#25351;&#26631;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#25351;&#26631;&#33021;&#22815;&#25552;&#20379;&#23454;&#26102;&#23450;&#37327;&#30340;&#24037;&#19994;&#36164;&#20135;&#20581;&#24247;&#29366;&#24577;&#20449;&#24687;&#65292;&#26159;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#23398;&#20064;&#20581;&#24247;&#25351;&#26631;&#30340;&#26032;&#26041;&#27861;&#12290;&#36816;&#34892;&#26102;&#38388;&#20316;&#20026;&#36164;&#20135;&#36864;&#21270;&#29366;&#24577;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#31354;&#38388;&#65292;&#27979;&#37327;&#19982;&#20581;&#24247;&#29366;&#24577;&#30340;&#36317;&#31163;&#26469;&#26500;&#24314;&#20581;&#24247;&#25351;&#26631;&#12290;&#20026;&#20102;&#31361;&#26174;&#25152;&#25552;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; - &#39118;&#36710;&#30952;&#24202;&#26696;&#20363;&#30740;&#31350;&#21644;&#30495;&#23454;&#26465;&#20214;&#30417;&#27979;&#20219;&#21153;&#20013;&#30340;&#30952;&#24202;&#30952;&#24202;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition monitoring is essential to operate industrial assets safely and efficiently. To achieve this goal, the development of robust health indicators has recently attracted significant attention. These indicators, which provide quantitative real-time insights into the health status of industrial assets over time, serve as valuable tools for fault detection and prognostics. In this study, we propose a novel and universal approach to learn health indicators based on unsupervised contrastive learning. Operational time acts as a proxy for the asset's degradation state, enabling the learning of a contrastive feature space that facilitates the construction of a health indicator by measuring the distance to the healthy condition. To highlight the universality of the proposed approach, we assess the proposed contrastive learning framework in two distinct tasks - wear assessment and fault detection - across two different case studies: a milling machines case study and a real condition monito
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.11749</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26410;&#30693;&#25968;&#37327;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#28304;&#20998;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#26410;&#30693;&#25968;&#37327;&#20449;&#21495;&#30340;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#36755;&#20986;&#19982;&#30446;&#26631;&#23545;&#40784;&#24341;&#36215;&#30340;&#25490;&#21015;&#38382;&#39064;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20004;&#27493;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;&#26377;&#38745;&#38899;&#36890;&#36947;&#30340;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#28151;&#21512;&#30340;&#36752;&#23556;&#33337;&#22122;&#22768;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36798;&#21040;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.09736</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24352;&#37327;&#21644;&#30697;&#38453;&#20302;&#31209;&#20540;&#20989;&#25968;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning. (arXiv:2201.09736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#65288;VF&#65289;&#30340;&#36817;&#20284;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;VF&#20272;&#35745;&#22312;&#32500;&#24230;&#28798;&#38590;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20154;&#20204;&#37319;&#29992;&#20102;&#31616;&#27905;&#30340;&#21442;&#25968;&#27169;&#22411;&#26469;&#36817;&#20284;VF&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#32447;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#19978;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#31616;&#27905;&#30340;&#38750;&#21442;&#25968;&#8221;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#20197;&#22312;&#32447;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#20272;&#35745;VF&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;VF&#24448;&#24448;&#26159;&#22810;&#32500;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#24352;&#37327;&#65288;&#22810;&#32500;&#25968;&#32452;&#65289;&#34920;&#31034;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;VF&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;PARAFAC&#20998;&#35299;&#26469;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-function (VF) approximation is a central problem in Reinforcement Learning (RL). Classical non-parametric VF estimation suffers from the curse of dimensionality. As a result, parsimonious parametric models have been adopted to approximate VFs in high-dimensional spaces, with most efforts being focused on linear and neural-network-based approaches. Differently, this paper puts forth a a \emph{parsimonious non-parametric} approach, where we use \emph{stochastic low-rank algorithms} to estimate the VF matrix in an online and model-free fashion. Furthermore, as VFs tend to be multi-dimensional, we propose replacing the classical VF matrix representation with a tensor (multi-way array) representation and, then, use the PARAFAC decomposition to design an online model-free tensor low-rank algorithm. Different versions of the algorithms are proposed, their complexity is analyzed, and their performance is assessed numerically using standardized RL environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#21830;&#19994;GAN&#30340;&#29983;&#20135;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#29256;&#26435;&#20445;&#25252;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#25152;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#20174;&#30446;&#26631;GAN&#21644;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#22797;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#22797;&#21512;&#27169;&#22411;&#20013;&#20135;&#29983;&#25351;&#32441;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29256;&#26435;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#26696;&#21551;&#21457;&#20102;&#19968;&#20123;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#25152;&#38656;&#35201;&#30340;&#19981;&#21516;&#23433;&#20840;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
&lt;/p&gt;</description></item></channel></rss>