<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#26469;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;D-PPO&#31639;&#27861;&#30456;&#36739;&#20110;PPO&#31639;&#27861;&#22312;Atari 2600&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.20380</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Dropout&#31574;&#30053;&#65306;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;
Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#26469;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;D-PPO&#31639;&#27861;&#30456;&#36739;&#20110;PPO&#31639;&#27861;&#22312;Atari 2600&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20854;&#20013;&#65292;&#20027;&#27969;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22914;PPO&#21644;TRPO&#24341;&#20837;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#20801;&#35768;&#37325;&#29992;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#21152;&#65292;&#38388;&#25509;&#24433;&#21709;&#20102;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#23427;&#21487;&#20197;&#38543;&#26367;&#20195;&#30446;&#26631;&#30340;&#22686;&#21152;&#32780;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#65292;&#20197;&#36991;&#20813;&#37325;&#35201;&#24615;&#37319;&#26679;&#24341;&#36215;&#30340;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#36807;&#24230;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20027;&#27969;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23558;Dropout&#25216;&#26415;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;D-PPO&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;Atari 2600&#28216;&#25103;&#19978;&#23545;D-PPO&#21644;PPO&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#65288;D-SGDA&#65289;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20998;&#24067;&#24335;&#32467;&#26500;&#19981;&#20250;&#30772;&#22351;D-SGDA&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20854;&#27867;&#21270;&#24615;&#33021;&#21487;&#20197;&#23218;&#32654;&#21407;&#22987;&#30340;SGDA&#12290;</title><link>http://arxiv.org/abs/2310.20369</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm. (arXiv:2310.20369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#65288;D-SGDA&#65289;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20998;&#24067;&#24335;&#32467;&#26500;&#19981;&#20250;&#30772;&#22351;D-SGDA&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20854;&#27867;&#21270;&#24615;&#33021;&#21487;&#20197;&#23218;&#32654;&#21407;&#22987;&#30340;SGDA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#29992;&#25968;&#25454;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#38271;&#65292;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20998;&#24067;&#24335;&#26041;&#24335;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20197;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#24067;&#24335;&#26497;&#23567;&#26497;&#22823;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#23545;&#20854;&#27867;&#21270;&#24615;&#33021;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#65288;D-SGDA&#65289;&#22312;&#20984;&#20985;&#21644;&#38750;&#20984;&#38750;&#20985;&#24773;&#20917;&#19979;&#30340;&#21407;&#22987;-&#23545;&#20598;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23545;&#20998;&#24067;&#24335;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#31934;&#32454;&#21270;&#25506;&#31350;&#65292;&#24182;&#34920;&#26126;&#20998;&#24067;&#24335;&#32467;&#26500;&#24182;&#19981;&#20250;&#30772;&#22351;D-SGDA&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20854;&#27867;&#21270;&#24615;&#33021;&#21487;&#20197;&#23218;&#32654;&#21407;&#22987;&#30340;SGDA&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20998;&#26512;&#20102;&#19981;&#21516;&#25299;&#25169;&#32467;&#26500;&#23545;D-SGDA&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#30340;&#24433;&#21709;&#65292;&#36229;&#36234;&#20102;&#19968;&#20123;&#24494;&#19981;&#36275;&#36947;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20303;&#23429;&#30005;&#21147;&#36127;&#33655;&#36718;&#24275;&#36827;&#34892;&#32858;&#31867;&#65292;&#22686;&#24378;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#12290;&#36890;&#36807;&#20351;&#29992;&#20262;&#25958;&#23478;&#24237;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#21644;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22235;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#27010;&#29575;&#20998;&#31867;&#38382;&#39064;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20367</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#29992;&#20110;&#23545;&#20303;&#23429;&#30005;&#21147;&#36127;&#33655;&#36718;&#24275;&#36827;&#34892;&#32858;&#31867;&#20197;&#22686;&#24378;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs. (arXiv:2310.20367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20303;&#23429;&#30005;&#21147;&#36127;&#33655;&#36718;&#24275;&#36827;&#34892;&#32858;&#31867;&#65292;&#22686;&#24378;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#12290;&#36890;&#36807;&#20351;&#29992;&#20262;&#25958;&#23478;&#24237;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#21644;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22235;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#27010;&#29575;&#20998;&#31867;&#38382;&#39064;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#34893;&#29983;&#20986;&#30340;&#36127;&#33655;&#26354;&#32447;&#32463;&#24120;&#34987;&#29992;&#20110;&#20998;&#26512;&#26085;&#24120;&#33021;&#28304;&#28040;&#32791;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#31561;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#24037;&#20316;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#35782;&#21035;&#20855;&#26377;&#31867;&#20284;&#28040;&#32791;&#34892;&#20026;&#30340;&#26368;&#21512;&#36866;&#30340;&#28040;&#36153;&#32773;&#32676;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#21033;&#29992;&#20262;&#25958;&#36817;5000&#25143;&#23478;&#24237;&#30340;&#25968;&#25454;&#23454;&#29616;&#26368;&#20339;&#36127;&#33655;&#36718;&#24275;&#12290;&#20855;&#20307;&#24212;&#29992;&#20102;&#22235;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#21253;&#25324;K-means&#12289;K-medoids&#12289;&#23618;&#27425;&#20957;&#32858;&#32858;&#31867;&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#31354;&#38388;&#32858;&#31867;&#12290;&#36824;&#21033;&#29992;&#32463;&#39564;&#20998;&#26512;&#21644;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#27010;&#29575;&#20998;&#31867;&#38382;&#39064;&#65292;&#20998;&#31867;&#22120;&#27169;&#25311;&#32858;&#31867;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#26159;&#21542;&#38656;&#35201;&#26356;&#22810;&#24490;&#29615;&#26816;&#27979;&#22120;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20132;&#36890;&#27969;&#29702;&#35770;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#35777;&#25454;&#23398;&#20064;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20366</link><description>&lt;p&gt;
&#20174;&#24490;&#29615;&#26816;&#27979;&#22120;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#26412;&#36136;&#65306;&#32593;&#32476;&#32423;&#20132;&#36890;&#39044;&#27979;&#26159;&#21542;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?. (arXiv:2310.20366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#26159;&#21542;&#38656;&#35201;&#26356;&#22810;&#24490;&#29615;&#26816;&#27979;&#22120;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20132;&#36890;&#27969;&#29702;&#35770;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#35777;&#25454;&#23398;&#20064;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#32593;&#32476;&#32423;&#20132;&#36890;&#26465;&#20214;&#39044;&#27979;&#19968;&#30452;&#21463;&#21040;&#23494;&#20999;&#20851;&#27880;&#12290;&#23613;&#31649;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#21644;&#20132;&#36890;&#25968;&#25454;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#39044;&#27979;&#20934;&#30830;&#24615;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#20132;&#36890;&#39044;&#27979;&#22312;&#23454;&#36341;&#20013;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12289;&#20132;&#36890;&#21160;&#24577;&#30340;&#22266;&#26377;&#19981;&#21487;&#39044;&#27979;&#24615;&#20197;&#21450;&#36827;&#19968;&#27493;&#25913;&#36827;&#20132;&#36890;&#39044;&#27979;&#26159;&#21542;&#38656;&#35201;&#26356;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#26412;&#25991;&#38024;&#23545;&#21518;&#32773;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24490;&#29615;&#26816;&#27979;&#22120;&#25968;&#25454;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#25506;&#32034;&#24490;&#29615;&#25968;&#25454;&#26679;&#26412;&#22312;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#26102;&#30340;&#30495;&#27491;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#35774;&#35745;&#23558;&#20132;&#36890;&#27969;&#29702;&#35770;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#30830;&#20445;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#35777;&#25454;&#23398;&#20064;&#26469;&#37327;&#21270;&#19968;&#27425;&#20256;&#36882;&#20013;&#19981;&#21516;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network-level traffic condition forecasting has been intensively studied for decades. Although prediction accuracy has been continuously improved with emerging deep learning models and ever-expanding traffic data, traffic forecasting still faces many challenges in practice. These challenges include the robustness of data-driven models, the inherent unpredictability of traffic dynamics, and whether further improvement of traffic forecasting requires more sensor data. In this paper, we focus on this latter question and particularly on data from loop detectors. To answer this, we propose an uncertainty-aware traffic forecasting framework to explore how many samples of loop data are truly effective for training forecasting models. Firstly, the model design combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Secondly, evidential learning is employed to quantify different sources of uncertainty in a single pass. The e
&lt;/p&gt;</description></item><item><title>CAFE&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#29305;&#24449;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#20914;&#31361;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20363</link><description>&lt;p&gt;
CAFE: &#20914;&#31361;&#24863;&#30693;&#30340;&#29305;&#24449;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
CAFE: Conflict-Aware Feature-wise Explanations. (arXiv:2310.20363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20363
&lt;/p&gt;
&lt;p&gt;
CAFE&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#29305;&#24449;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#20914;&#31361;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#30830;&#23450;&#21333;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;CAFE&#65288;&#20914;&#31361;&#24863;&#30693;&#30340;&#29305;&#24449;&#35299;&#37322;&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19977;&#20010;&#23616;&#38480;&#24615;&#65306;&#23545;&#20914;&#31361;&#29305;&#24449;&#24433;&#21709;&#30340;&#24573;&#35270;&#65292;&#23545;&#20559;&#24046;&#39033;&#24433;&#21709;&#30340;&#32570;&#20047;&#32771;&#34385;&#65292;&#20197;&#21450;&#23545;&#28608;&#27963;&#20989;&#25968;&#23616;&#37096;&#21464;&#21270;&#36807;&#20110;&#25935;&#24863;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;CAFE&#25552;&#20379;&#20102;&#38450;&#27490;&#39640;&#20272;&#31070;&#32463;&#20803;&#36755;&#20837;&#24433;&#21709;&#30340;&#20445;&#25252;&#26426;&#21046;&#65292;&#24182;&#21333;&#29420;&#36861;&#36394;&#36755;&#20837;&#29305;&#24449;&#21644;&#20559;&#24046;&#30340;&#27491;&#21521;&#21644;&#36127;&#21521;&#24433;&#21709;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31283;&#20581;&#24615;&#21644;&#21457;&#29616;&#29305;&#24449;&#20914;&#31361;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CAFE&#22312;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#19978;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#20914;&#31361;&#29305;&#24449;&#65292;&#24182;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20339;&#30340;&#20840;&#38754;&#21487;&#20449;&#24230;&#65292;&#32780;&#35745;&#31639;&#24615;&#33021;&#20063;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computation
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.20360</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#65306;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory. (arXiv:2310.20360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#24102;&#26377;&#25209;&#24402;&#19968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#22522;&#26412;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#12289;&#21152;&#36895;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20960;&#20010;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#31215;&#20998;&#65289;&#12289;&#20248;&#21270;&#29702;&#35770;&#65288;&#21253;&#25324;Kurdyka-Lojasiewicz&#19981;&#31561;&#24335;&#65289;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#20070;&#30340;&#26368;&#21518;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#19968;&#20123;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Galerkin&#26041;&#27861;&#12290;&#24076;&#26395;&#26412;&#20070;&#33021;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19977;&#32500;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#19979;&#32930;18&#20010;&#32908;&#32905;&#30340;&#26041;&#27861;&#65292;&#20197;&#36741;&#21161;&#24418;&#24577;&#27979;&#37327;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28151;&#21512;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#32908;&#32905;&#32452;&#32455;&#26080;&#27861;&#20998;&#36776;&#21644;&#36718;&#24275;&#38590;&#20197;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20355</link><description>&lt;p&gt;
&#32908;&#32905;&#20307;&#31215;&#23450;&#37327;&#21270;&#65306;&#22522;&#20110;&#35299;&#21078;&#20808;&#39564;&#25351;&#23548;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Muscle volume quantification: guiding transformers with anatomical priors. (arXiv:2310.20355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19977;&#32500;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#19979;&#32930;18&#20010;&#32908;&#32905;&#30340;&#26041;&#27861;&#65292;&#20197;&#36741;&#21161;&#24418;&#24577;&#27979;&#37327;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28151;&#21512;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#32908;&#32905;&#32452;&#32455;&#26080;&#27861;&#20998;&#36776;&#21644;&#36718;&#24275;&#38590;&#20197;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#32905;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#23450;&#37327;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#19981;&#20165;&#29992;&#20110;&#36816;&#21160;&#39046;&#22495;&#65292;&#36824;&#29992;&#20110;&#36864;&#34892;&#24615;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#38543;&#35775;&#12290;&#38500;&#20102;&#20307;&#31215;&#22806;&#65292;&#36890;&#36807;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#20998;&#21106;&#24863;&#20852;&#36259;&#30340;&#32908;&#32905;&#65292;&#36824;&#21487;&#20197;&#25552;&#21462;&#20854;&#20182;&#24418;&#29366;&#26631;&#24535;&#29289;&#12290;&#23613;&#31649;&#38750;&#24120;&#32791;&#26102;&#65292;&#20294;&#22312;&#36825;&#31867;&#27979;&#37327;&#20013;&#65292;&#25163;&#24037;&#20998;&#21106;&#20173;&#28982;&#26159;&#37329;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#21106;&#19977;&#32500;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#19979;&#32930;&#30340;18&#20010;&#32908;&#32905;&#65292;&#20197;&#36741;&#21161;&#24418;&#24577;&#27979;&#37327;&#20998;&#26512;&#12290;&#30001;&#20110;&#22312;MR&#22270;&#20687;&#20013;&#35266;&#23519;&#26102;&#65292;&#19981;&#21516;&#32908;&#32905;&#30340;&#32452;&#32455;&#26080;&#27861;&#20998;&#36776;&#12290;&#22240;&#27492;&#65292;&#32908;&#32905;&#20998;&#21106;&#31639;&#27861;&#19981;&#33021;&#20381;&#36182;&#22806;&#35266;&#65292;&#21482;&#33021;&#20381;&#36182;&#36718;&#24275;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#36718;&#24275;&#24456;&#38590;&#26816;&#27979;&#65292;&#24182;&#19988;&#20854;&#21402;&#24230;&#22312;&#21463;&#35797;&#32773;&#20043;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#22522;&#20110;&#28151;&#21512;&#26550;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#31181;&#28151;&#21512;&#26550;&#26500;&#30340;&#34892;&#20026;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20350</link><description>&lt;p&gt;
&#23558;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#32467;&#21512;&#65292;&#23454;&#29616;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36741;&#21161;&#26426;&#22120;&#20154;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#26159;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26222;&#36866;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#22312;&#35266;&#27979;&#33021;&#21147;&#26377;&#38480;&#21644;&#21033;&#29992;&#22810;&#25351;&#25163;&#36827;&#34892;&#28789;&#27963;&#25235;&#21462;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#24555;&#36895;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#30001;&#22522;&#20110;&#21333;&#20010;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#29289;&#20307;&#24418;&#29366;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#32452;&#25104;&#12290;&#24418;&#29366;&#23436;&#25104;&#32593;&#32476;&#22522;&#20110;VQDIF&#65292;&#22312;&#20219;&#24847;&#26597;&#35810;&#28857;&#19978;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#20540;&#12290;&#20316;&#20026;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#25163;&#23039;&#21183;&#65292;&#28982;&#21518;&#22238;&#24402;&#27599;&#20010;&#23039;&#21183;&#30340;&#25163;&#25351;&#20851;&#33410;&#37197;&#32622;&#12290;&#20851;&#38190;&#22240;&#32032;&#26159;&#36275;&#22815;&#30340;&#25968;&#25454;&#30495;&#23454;&#24615;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22256;&#38590;&#24773;&#20917;&#30340;&#29305;&#27530;&#20851;&#27880;&#12290;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;CLIP&#27169;&#22411;&#20013;&#22686;&#21152;&#36866;&#37197;&#22120;&#23618;&#65292;&#24182;&#20351;&#29992;&#21442;&#25968;&#20445;&#30041;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20348</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning with Pre-trained Vision-Language Models. (arXiv:2310.20348v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;CLIP&#27169;&#22411;&#20013;&#22686;&#21152;&#36866;&#37197;&#22120;&#23618;&#65292;&#24182;&#20351;&#29992;&#21442;&#25968;&#20445;&#30041;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23558;&#20854;&#36866;&#24212;&#21644;&#21033;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#19968;&#27493;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#20013;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#23618;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#21518;&#25110;&#25991;&#26412;&#32534;&#30721;&#22120;&#20043;&#21069;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65306;&#32447;&#24615;&#36866;&#37197;&#22120;&#12289;&#33258;&#27880;&#24847;&#21147;&#36866;&#37197;&#22120;&#65292;&#20998;&#21035;&#20316;&#29992;&#20110;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#19988;&#20351;&#29992;Prompt Tuning&#20462;&#25913;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#20445;&#30041;&#30340;&#36866;&#37197;&#22120;&#23618;&#26041;&#27861;&#65292;&#20351;&#29992;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#26469;&#26356;&#22909;&#22320;&#20445;&#25345;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;--&#21482;&#26377;&#19968;&#20010;&#32447;&#24615;&#36866;&#37197;&#22120;&#23618;&#19988;&#20855;&#26377;&#21442;&#25968;&#20445;&#30041;--&#21487;&#20197;&#20135;&#29983;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of large-scale pre-trained models, interest in adapting and exploiting them for continual learning scenarios has grown.  In this paper, we propose an approach to exploiting pre-trained vision-language models (e.g. CLIP) that enables further adaptation instead of only using zero-shot learning of new tasks. We augment a pre-trained CLIP model with additional layers after the Image Encoder or before the Text Encoder. We investigate three different strategies: a Linear Adapter, a Self-attention Adapter, each operating on the image embedding, and Prompt Tuning which instead modifies prompts input to the CLIP text encoder. We also propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to better maintain stability and plasticity during incremental learning. Our experiments demonstrate that the simplest solution -- a single Linear Adapter layer with parameter retention -- produces the best results. Experiments on several conve
&lt;/p&gt;</description></item><item><title>GACE&#26159;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#20960;&#20309;&#32447;&#32034;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;&#40657;&#30418;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26816;&#27979;&#22120;&#20013;&#23454;&#29616;&#20102;&#25345;&#32493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.20319</link><description>&lt;p&gt;
GACE: &#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;LiDAR&#25968;&#25454;&#40657;&#30418;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#32622;&#20449;&#24230;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data. (arXiv:2310.20319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20319
&lt;/p&gt;
&lt;p&gt;
GACE&#26159;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#20960;&#20309;&#32447;&#32034;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;&#40657;&#30418;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26816;&#27979;&#22120;&#20013;&#23454;&#29616;&#20102;&#25345;&#32493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#36827;&#34892;&#32622;&#20449;&#24230;&#20272;&#35745;&#26102;&#24448;&#24448;&#24573;&#30053;&#20102;&#20174;&#29289;&#20307;&#25552;&#35758;&#20013;&#33719;&#24471;&#30340;&#22522;&#26412;&#20960;&#20309;&#20449;&#24687;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#65292;&#24448;&#24448;&#37319;&#29992;&#20102;&#20174;2D&#22270;&#20687;&#39046;&#22495;&#37319;&#32435;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;2D&#22270;&#20687;&#39046;&#22495;&#20013;&#20960;&#20309;&#19978;&#19979;&#25991;&#24456;&#23569;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#20013;&#65292;&#32508;&#21512;&#32771;&#34385;&#29289;&#20307;&#23646;&#24615;&#21450;&#20854;&#21608;&#22260;&#29615;&#22659;&#23545;&#20110;&#21306;&#20998;&#30495;&#27491;&#30340;&#21644;&#38169;&#35823;&#30340;&#26816;&#27979;&#32467;&#26524;&#65292;&#20363;&#22914;&#19968;&#32452;&#36974;&#25377;&#30340;&#34892;&#20154;&#65292;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GACE&#65292;&#19968;&#31181;&#30452;&#35266;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#32473;&#23450;&#40657;&#30418;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#32858;&#21512;&#26816;&#27979;&#32467;&#26524;&#30340;&#20960;&#20309;&#32447;&#32034;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#23427;&#20204;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22240;&#27492;&#25913;&#36827;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#36825;&#26679;&#23601;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#19978;&#25345;&#32493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#25152;&#26377;&#35780;&#20272;&#30340;&#26816;&#27979;&#22120;&#20013;&#65292;GACE&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely-used LiDAR-based 3D object detectors often neglect fundamental geometric information readily available from the object proposals in their confidence estimation. This is mostly due to architectural design choices, which were often adopted from the 2D image domain, where geometric context is rarely available. In 3D, however, considering the object properties and its surroundings in a holistic way is important to distinguish between true and false positive detections, e.g. occluded pedestrians in a group. To address this, we present GACE, an intuitive and highly efficient method to improve the confidence estimation of a given black-box 3D object detector. We aggregate geometric cues of detections and their spatial relationships, which enables us to properly assess their plausibility and consequently, improve the confidence estimation. This leads to consistent performance gains over a variety of state-of-the-art detectors. Across all evaluated detectors, GACE proves to be especially
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;Transformer&#36827;&#34892;&#38646;&#26679;&#26412;&#22240;&#26524;&#21457;&#29616;&#12290;&#36890;&#36807;&#35745;&#31639;&#26368;&#28145;&#27880;&#24847;&#23618;&#20013;&#30456;&#24212;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#20026;Transformer&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#22240;&#26524;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.20307</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;Transformer&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Causal Interpretation of Self-Attention in Pre-Trained Transformers. (arXiv:2310.20307v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;Transformer&#36827;&#34892;&#38646;&#26679;&#26412;&#22240;&#26524;&#21457;&#29616;&#12290;&#36890;&#36807;&#35745;&#31639;&#26368;&#28145;&#27880;&#24847;&#23618;&#20013;&#30456;&#24212;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#20026;Transformer&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#22240;&#26524;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#33258;&#27880;&#24847;&#21147;&#35299;&#37322;&#20026;&#19968;&#31181;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#31526;&#21495;&#24207;&#21015;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#26426;&#21046;&#12290;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#20026;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35299;&#37322;&#20173;&#28982;&#26377;&#25928;&#12290;&#26681;&#25454;&#36825;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#26368;&#28145;&#27880;&#24847;&#23618;&#20013;&#30456;&#24212;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#26469;&#20272;&#35745;&#36755;&#20837;&#31526;&#21495;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#20110;&#32422;&#26463;&#30340;&#31639;&#27861;&#23398;&#20064;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35762;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;Transformer&#21487;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#22240;&#26524;&#21457;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#20004;&#20010;&#20219;&#21153;&#20013;Transformer&#30340;&#32467;&#26524;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#26469;&#31034;&#33539;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks:
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23616;&#37096;&#24046;&#20998;&#20998;&#31867;&#38544;&#31169;&#65288;LDCP&#65289;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#65292;&#26088;&#22312;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;Sphynx&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#32593;&#32476;&#30340;&#25277;&#35937;&#26469;&#39564;&#35777;LDCP&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#20013;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#32593;&#32476;&#21644;&#36880;&#20010;&#39564;&#35777;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20299</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23616;&#37096;&#24046;&#20998;&#20998;&#31867;&#38544;&#31169;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verification of Neural Networks Local Differential Classification Privacy. (arXiv:2310.20299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23616;&#37096;&#24046;&#20998;&#20998;&#31867;&#38544;&#31169;&#65288;LDCP&#65289;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#65292;&#26088;&#22312;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;Sphynx&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#32593;&#32476;&#30340;&#25277;&#35937;&#26469;&#39564;&#35777;LDCP&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#20013;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#32593;&#32476;&#21644;&#36880;&#20010;&#39564;&#35777;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#12290;&#33267;&#20170;&#27809;&#26377;&#39564;&#35777;&#22120;&#33021;&#22815;&#25512;&#26029;&#21442;&#19982;&#35757;&#32451;&#38598;&#30340;&#20010;&#20307;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#23646;&#24615;&#65292;&#31216;&#20026;&#23616;&#37096;&#24046;&#20998;&#20998;&#31867;&#38544;&#31169;&#65288;LDCP&#65289;&#65292;&#23558;&#23616;&#37096;&#40065;&#26834;&#24615;&#25193;&#23637;&#21040;&#36866;&#29992;&#20110;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#37051;&#22495;&#65292;&#22914;&#26524;&#19968;&#20010;&#20998;&#31867;&#22120;&#26159;LDCP&#30340;&#65292;&#26080;&#35770;&#23427;&#26159;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#35757;&#32451;&#36824;&#26159;&#30465;&#30053;&#20219;&#20309;&#19968;&#20010;&#26465;&#30446;&#35757;&#32451;&#65292;&#23427;&#37117;&#33021;&#23545;&#25152;&#26377;&#36755;&#20837;&#36827;&#34892;&#30456;&#21516;&#30340;&#20998;&#31867;&#12290;&#19968;&#20010;&#22825;&#30495;&#30340;&#31639;&#27861;&#26159;&#38750;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#35757;&#32451;&#22823;&#37327;&#30340;&#32593;&#32476;&#65292;&#24182;&#19988;&#23545;&#32473;&#23450;&#37051;&#22495;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#36827;&#34892;&#21333;&#29420;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Sphynx&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#19968;&#23567;&#32452;&#32593;&#32476;&#20013;&#20197;&#39640;&#27010;&#29575;&#35745;&#31639;&#20986;&#25152;&#26377;&#32593;&#32476;&#30340;&#25277;&#35937;&#65292;&#24182;&#30452;&#25509;&#22312;&#25277;&#35937;&#32593;&#32476;&#19978;&#39564;&#35777;LDCP&#12290;&#25361;&#25112;&#26159;&#21452;&#37325;&#30340;&#65306;&#32593;&#32476;&#21442;&#25968;&#19981;&#36981;&#24490;&#24050;&#30693;&#30340;&#20998;&#24067;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are susceptible to privacy attacks. To date, no verifier can reason about the privacy of individuals participating in the training set. We propose a new privacy property, called local differential classification privacy (LDCP), extending local robustness to a differential privacy setting suitable for black-box classifiers. Given a neighborhood of inputs, a classifier is LDCP if it classifies all inputs the same regardless of whether it is trained with the full dataset or whether any single entry is omitted. A naive algorithm is highly impractical because it involves training a very large number of networks and verifying local robustness of the given neighborhood separately for every network. We propose Sphynx, an algorithm that computes an abstraction of all networks, with a high probability, from a small set of networks, and verifies LDCP directly on the abstract network. The challenge is twofold: network parameters do not adhere to a known distribution probability, ma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#32622;&#30340;&#28145;&#24230;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#35299;&#20915;&#22797;&#20301;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20287</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#32622;&#28145;&#24230;&#38598;&#21512;&#20195;&#29702;&#23454;&#29616;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23433;&#20840;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents. (arXiv:2310.20287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20287
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#32622;&#30340;&#28145;&#24230;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#35299;&#20915;&#22797;&#20301;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#36182;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;&#20248;&#20808;&#32423;&#20559;&#35265;&#8221;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#36825;&#20123;&#20989;&#25968;&#36924;&#36817;&#22120;&#20542;&#21521;&#20110;&#20248;&#20808;&#32771;&#34385;&#26089;&#26399;&#30340;&#32463;&#39564;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20248;&#20808;&#32423;&#20559;&#35265;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#20445;&#30041;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26041;&#24335;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#20998;&#25110;&#20840;&#37096;&#36827;&#34892;&#21608;&#26399;&#24615;&#37325;&#32622;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#37325;&#32622;&#26041;&#27861;&#21518;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#23849;&#28291;&#65292;&#36825;&#20174;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37325;&#32622;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#38598;&#21512;&#23398;&#20064;&#26469;&#35299;&#20915;&#26222;&#36890;&#37325;&#32622;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#21253;&#25324;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#39564;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#26469;&#38477;&#20302;&#35745;&#31639;&#37327;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.20285</link><description>&lt;p&gt;
&#36890;&#36807;&#20197;&#35745;&#31639;&#20026;&#20195;&#20215;&#21152;&#36895;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generalized Linear Models by Trading off Computation for Uncertainty. (arXiv:2310.20285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#26469;&#38477;&#20302;&#35745;&#31639;&#37327;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLMs&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#20998;&#31867;&#12289;&#26377;&#24207;&#21644;&#36830;&#32493;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;GLMs&#30340;&#31934;&#30830;&#25512;&#26029;&#20195;&#20215;&#22826;&#39640;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#36896;&#25104;&#30340;&#36817;&#20284;&#35823;&#24046;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#27809;&#26377;&#34987;&#32771;&#34385;&#22312;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#36845;&#20195;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#23545;&#36825;&#20010;&#35823;&#24046;&#24314;&#27169;&#12290;&#23427;&#20204;&#38750;&#24120;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#30828;&#20214;&#65292;&#26377;&#25928;&#22320;&#22238;&#25910;&#35745;&#31639;&#24182;&#21387;&#32553;&#20449;&#24687;&#65292;&#20197;&#20943;&#23569;GLMs&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#22823;&#22411;&#20998;&#31867;&#38382;&#39064;&#19978;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26126;&#30830;&#22320;&#23558;&#20943;&#23569;&#35745;&#31639;&#19982;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#26435;&#34913;&#26469;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20280</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#22312;BizITOps&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data. (arXiv:2310.20280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19994;&#21153;&#36807;&#31243;&#30340;&#25928;&#29575;&#20381;&#36182;&#20110;&#19994;&#21153;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;Biz-KPIs&#65289;&#65292;&#32780;IT&#25925;&#38556;&#21487;&#33021;&#23545;&#20854;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;BizITOps&#25968;&#25454;&#23558;Biz-KPIs&#21644;IT&#20107;&#20214;&#36890;&#36947;&#34701;&#21512;&#25104;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25552;&#21069;&#39044;&#27979;Biz-KPIs&#21487;&#20197;&#36890;&#36807;&#20027;&#21160;&#30340;&#32416;&#27491;&#25514;&#26045;&#25552;&#39640;&#25928;&#29575;&#21644;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;BizITOps&#25968;&#25454;&#36890;&#24120;&#23637;&#31034;&#20986;Biz-KPIs&#21644;IT&#20107;&#20214;&#20043;&#38388;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#38656;&#35201;&#26377;&#25928;&#35299;&#32806;&#12290;&#24403;&#20351;&#29992;&#29616;&#26377;&#30340;&#22810;&#21464;&#37327;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;AutoMixer&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#26032;&#39062;&#30340;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#12290;AutoMixer&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#36890;&#36947;&#21387;&#32553;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#36827;&#30340;TSMixer&#27169;&#22411;&#38598;&#25104;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36825;&#31181;&#34701;&#21512;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;TSM
&lt;/p&gt;
&lt;p&gt;
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#20135;&#21697;&#27604;&#36739;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.20274</link><description>&lt;p&gt;
&#20174;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Extracting Entities of Interest from Comparative Product Reviews. (arXiv:2310.20274v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#20135;&#21697;&#27604;&#36739;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21508;&#31181;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;&#29992;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#20135;&#21697;&#27604;&#36739;&#20449;&#24687;&#12290;&#20219;&#20309;&#19968;&#20010;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#37117;&#26377;&#19977;&#20010;&#37325;&#35201;&#30340;&#20449;&#24687;&#23454;&#20307;&#65306;&#34987;&#27604;&#36739;&#20135;&#21697;&#30340;&#21517;&#31216;&#65292;&#29992;&#25143;&#35266;&#28857;&#65288;&#35859;&#35789;&#65289;&#20197;&#21450;&#34987;&#27604;&#36739;&#30340;&#29305;&#24449;&#25110;&#26041;&#38754;&#12290;&#25152;&#26377;&#36825;&#20123;&#20449;&#24687;&#23454;&#20307;&#24444;&#27492;&#20381;&#36182;&#24182;&#21463;&#21040;&#35780;&#35770;&#35821;&#35328;&#35268;&#21017;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#24456;&#22909;&#22320;&#36890;&#36807;LSTM&#36827;&#34892;&#25429;&#25417;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#20943;&#23567;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20258</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#20943;&#23567;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#30340;&#20989;&#25968;&#35780;&#20272;&#26469;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#23545;&#32467;&#26500;&#21270;&#25110;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20248;&#21270;&#19981;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#36825;&#23548;&#33268;&#20102;&#28508;&#22312;&#30340;&#24046;&#36317;&#65292;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20851;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoBO&#65289;&#65292;&#23427;&#19987;&#27880;&#20110;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20854;&#29305;&#28857;&#26159;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#21644;&#30446;&#26631;&#20989;&#25968;&#20869;&#30340;&#36317;&#31163;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#26368;&#23567;&#21270;&#26377;&#24076;&#26395;&#21306;&#22495;&#21608;&#22260;&#30340;&#28508;&#22312;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in disc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#21183;&#20808;&#39564;&#30340;&#36328;&#39046;&#22495;&#21160;&#20316;&#37325;&#23450;&#21521;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21478;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#26550;&#30340;&#35282;&#33394;&#30340;&#29616;&#26377;&#21160;&#20316;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#21160;&#20316;&#65292;&#20026;&#20165;&#20855;&#26377;&#23039;&#21183;&#25968;&#25454;&#30340;&#35282;&#33394;&#29983;&#25104;&#21512;&#29702;&#30340;&#21160;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#25110;&#22122;&#22768;&#25968;&#25454;&#38598;&#24773;&#20917;&#19979;&#34920;&#29616;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20249</link><description>&lt;p&gt;
Pose-to-Motion: &#22522;&#20110;&#23039;&#21183;&#20808;&#39564;&#30340;&#36328;&#39046;&#22495;&#21160;&#20316;&#37325;&#23450;&#21521;
&lt;/p&gt;
&lt;p&gt;
Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior. (arXiv:2310.20249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#21183;&#20808;&#39564;&#30340;&#36328;&#39046;&#22495;&#21160;&#20316;&#37325;&#23450;&#21521;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21478;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#26550;&#30340;&#35282;&#33394;&#30340;&#29616;&#26377;&#21160;&#20316;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#21160;&#20316;&#65292;&#20026;&#20165;&#20855;&#26377;&#23039;&#21183;&#25968;&#25454;&#30340;&#35282;&#33394;&#29983;&#25104;&#21512;&#29702;&#30340;&#21160;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#25110;&#22122;&#22768;&#25968;&#25454;&#38598;&#24773;&#20917;&#19979;&#34920;&#29616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#39046;&#22495;&#65292;&#21019;&#36896;&#36924;&#30495;&#30340;&#35282;&#33394;&#21160;&#20316;&#19968;&#30452;&#26159;&#19968;&#20010;&#30446;&#26631;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#20316;&#21512;&#25104;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#21160;&#20316;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#24456;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#33719;&#24471;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23039;&#21183;&#25968;&#25454;&#26356;&#26131;&#33719;&#21462;&#65292;&#22240;&#20026;&#38745;&#24577;&#30340;&#35282;&#33394;&#23039;&#21183;&#26356;&#23481;&#26131;&#21019;&#24314;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26368;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26367;&#20195;&#25968;&#25454;&#28304;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31070;&#32463;&#21160;&#20316;&#21512;&#25104;&#26041;&#27861;&#36890;&#36807;&#37325;&#23450;&#21521;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#21478;&#19968;&#20010;&#20855;&#26377;&#26497;&#20854;&#19981;&#21516;&#39592;&#26550;&#30340;&#35282;&#33394;&#30340;&#29616;&#26377;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#21160;&#20316;&#65292;&#20026;&#20165;&#20855;&#26377;&#23039;&#21183;&#25968;&#25454;&#30340;&#35282;&#33394;&#29983;&#25104;&#21512;&#29702;&#30340;&#21160;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#28304;&#35282;&#33394;&#30340;&#21160;&#20316;&#29305;&#24449;&#19982;&#30446;&#26631;&#35282;&#33394;&#30340;&#23039;&#21183;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#24182;&#19988;&#22312;&#23039;&#21183;&#25968;&#25454;&#38598;&#24456;&#23567;&#25110;&#24102;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#20174;&#20960;&#20010;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#21040;&#22823;&#35268;&#27169;&#30495;&#23454;&#25968;&#25454;&#38598;&#31561;&#21508;&#31181;&#24773;&#20917;&#30340;&#23454;&#39564;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating believable motions for various characters has long been a goal in computer graphics. Current learning-based motion synthesis methods depend on extensive motion datasets, which are often challenging, if not impossible, to obtain. On the other hand, pose data is more accessible, since static posed characters are easier to create and can even be extracted from images using recent advancements in computer vision. In this paper, we utilize this alternative data source and introduce a neural motion synthesis approach through retargeting. Our method generates plausible motions for characters that have only pose data by transferring motion from an existing motion capture dataset of another character, which can have drastically different skeletons. Our experiments show that our method effectively combines the motion features of the source character with the pose features of the target character, and performs robustly with small or noisy pose data sets, ranging from a few artist-created
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#21644;&#31354;&#38388;&#35821;&#20041;&#22270;&#23545;&#22522;&#20110;&#36712;&#36857;&#35760;&#24405;&#30340;&#20056;&#23458;&#32858;&#31867;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#33021;&#22312;&#19968;&#27493;&#20013;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#37327;&#65292;&#24182;&#20445;&#30041;&#20102;&#22810;&#32500;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.20224</link><description>&lt;p&gt;
&#36873;&#25321;&#19968;&#20010;&#34920;&#65306;&#22522;&#20110;&#22270;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#20056;&#23458;&#36712;&#36857;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering. (arXiv:2310.20224v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#21644;&#31354;&#38388;&#35821;&#20041;&#22270;&#23545;&#22522;&#20110;&#36712;&#36857;&#35760;&#24405;&#30340;&#20056;&#23458;&#32858;&#31867;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#33021;&#22312;&#19968;&#27493;&#20013;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#37327;&#65292;&#24182;&#20445;&#30041;&#20102;&#22810;&#32500;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36712;&#36857;&#35760;&#24405;&#30340;&#20056;&#23458;&#32858;&#31867;&#23545;&#20110;&#20132;&#36890;&#36816;&#33829;&#21830;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#20056;&#23458;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#21253;&#25324;&#27599;&#20010;&#20056;&#23458;&#20869;&#37096;&#30340;&#22810;&#27425;&#20986;&#34892;&#20197;&#21450;&#27599;&#27425;&#20986;&#34892;&#30340;&#22810;&#32500;&#20449;&#24687;&#65292;&#26080;&#27861;&#36731;&#26494;&#22320;&#32858;&#31867;&#20056;&#23458;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20934;&#30830;&#25351;&#23450;&#32858;&#31867;&#25968;&#37327;&#30340;&#36215;&#22987;&#20540;&#12290;&#26368;&#21518;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#32771;&#34385;&#31354;&#38388;&#35821;&#20041;&#22270;&#65292;&#22914;&#22320;&#29702;&#37051;&#36817;&#24615;&#21644;&#20301;&#32622;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20445;&#30041;&#22810;&#32500;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#33021;&#20197;&#32479;&#19968;&#30340;&#19968;&#27493;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#32858;&#31867;&#65292;&#20855;&#26377;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#37327;&#30340;&#33021;&#21147;&#12290;&#31354;&#38388;&#22270;&#34987;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#20197;&#36830;&#25509;&#35821;&#20041;&#37051;&#23621;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#24352;&#37327;&#29256;&#26412;&#30340;Coll...
&lt;/p&gt;
&lt;p&gt;
Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Coll
&lt;/p&gt;</description></item><item><title>STDA-Meta&#26159;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20223</link><description>&lt;p&gt;
STDA-Meta: &#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction. (arXiv:2310.20223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20223
&lt;/p&gt;
&lt;p&gt;
STDA-Meta&#26159;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22478;&#24066;&#30340;&#21457;&#23637;&#65292;&#20132;&#36890;&#25317;&#22581;&#25104;&#20026;&#19968;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#20132;&#36890;&#39044;&#27979;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#20132;&#36890;&#39044;&#27979;&#26159;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#23398;&#20064;&#30340;&#19968;&#31181;&#29305;&#23450;&#24212;&#29992;&#65292;&#22914;&#20986;&#31199;&#36710;&#35843;&#24230;&#12289;&#22825;&#27668;&#39044;&#27979;&#21644;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#19968;&#20123;&#20256;&#24863;&#22120;&#19981;&#36275;&#30340;&#26032;&#20852;&#22478;&#24066;&#24182;&#19981;&#36866;&#29992;&#65292;&#24182;&#19988;&#25968;&#25454;&#31232;&#32570;&#20351;&#39044;&#27979;&#24615;&#33021;&#26356;&#24046;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#23398;&#20064;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#32780;&#20132;&#36890;&#39044;&#27979;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#22270;&#32467;&#26500;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#22270;&#30340;&#21160;&#24577;&#24615;&#26080;&#27861;&#28385;&#36275;&#31354;&#38388;-&#26102;&#38388;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#35201;&#27714;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20256;&#32479;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the development of cities, traffic congestion becomes an increasingly pressing issue, and traffic prediction is a classic method to relieve that issue. Traffic prediction is one specific application of spatio-temporal prediction learning, like taxi scheduling, weather prediction, and ship trajectory prediction. Against these problems, classical spatio-temporal prediction learning methods including deep learning, require large amounts of training data. In reality, some newly developed cities with insufficient sensors would not hold that assumption, and the data scarcity makes predictive performance worse. In such situation, the learning method on insufficient data is known as few-shot learning (FSL), and the FSL of traffic prediction remains challenges. On the one hand, graph structures' irregularity and dynamic nature of graphs cannot hold the performance of spatio-temporal learning method. On the other hand, conventional domain adaptation methods cannot work well on insufficient tr
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Transformer&#30340;&#38271;&#26399;&#31995;&#21015;&#39044;&#27979;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;Transformer&#26550;&#26500;&#21450;&#20854;&#25913;&#36827;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12289;&#26377;&#25928;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.20218</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38271;&#26399;&#31995;&#21015;&#39044;&#27979;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review for Transformer-based Long-term Series Forecasting. (arXiv:2310.20218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20218
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38271;&#26399;&#31995;&#21015;&#39044;&#27979;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;Transformer&#26550;&#26500;&#21450;&#20854;&#25913;&#36827;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12289;&#26377;&#25928;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;Transformer&#26550;&#26500;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37319;&#29992;&#12290;Transformer&#34987;&#35777;&#26126;&#26159;&#25552;&#21462;&#38271;&#24207;&#21015;&#20869;&#37096;&#20803;&#32032;&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#26368;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21508;&#31181;&#21464;&#20307;&#20351;&#24471;Transformer&#26550;&#26500;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;Transformer&#26550;&#26500;&#21450;&#20854;&#21518;&#32493;&#25913;&#36827;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32972;&#26223;&#19979;&#26377;&#25928;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#26415;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20999;&#25442;&#21327;&#35758;(DHO)&#65292;&#36890;&#36807;&#39044;&#27979;&#33021;&#21147;&#36339;&#36807;&#27979;&#37327;&#25253;&#21578;&#38454;&#27573;&#65292;&#31616;&#21270;&#20999;&#25442;&#36807;&#31243;&#24182;&#28040;&#38500;&#35775;&#38382;&#24310;&#36831;&#65292;&#21516;&#26102;&#22312;&#32593;&#32476;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#36234;&#65292;&#23637;&#31034;&#20102;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.20215</link><description>&lt;p&gt;
LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#20999;&#25442;&#21327;&#35758;&#23398;&#20064;&#65306;&#35775;&#38382;&#24310;&#36831;&#21644;&#30896;&#25758;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization. (arXiv:2310.20215v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20999;&#25442;&#21327;&#35758;(DHO)&#65292;&#36890;&#36807;&#39044;&#27979;&#33021;&#21147;&#36339;&#36807;&#27979;&#37327;&#25253;&#21578;&#38454;&#27573;&#65292;&#31616;&#21270;&#20999;&#25442;&#36807;&#31243;&#24182;&#28040;&#38500;&#35775;&#38382;&#24310;&#36831;&#65292;&#21516;&#26102;&#22312;&#32593;&#32476;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#36234;&#65292;&#23637;&#31034;&#20102;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (DRL) &#30340;&#20999;&#25442; (HO) &#21327;&#35758;&#65292;&#31216;&#20026;DHO&#65292;&#19987;&#38376;&#38024;&#23545;&#20302;&#36712;&#36947;&#21355;&#26143;&#32593;&#32476;&#30340;&#38271;&#20256;&#25773;&#24310;&#36831;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;DHO&#22312;&#39044;&#23450;&#30340;LEO&#21355;&#26143;&#36712;&#36857;&#27169;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;HO&#36807;&#31243;&#20013;&#21033;&#29992;&#20854;&#39044;&#27979;&#33021;&#21147;&#36339;&#36807;&#27979;&#37327;&#25253;&#21578;(MR)&#38454;&#27573;&#65292;&#31616;&#21270;&#20102;&#36807;&#31243;&#24182;&#28040;&#38500;&#20102;MR&#38454;&#27573;&#20135;&#29983;&#30340;&#20256;&#25773;&#24310;&#36831;&#65292;&#21516;&#26102;&#20173;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;HO&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;DHO&#22312;&#19981;&#21516;&#32593;&#32476;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;HO&#21327;&#35758;&#65292;&#21253;&#25324;&#35775;&#38382;&#24310;&#36831;&#12289;&#30896;&#25758;&#29575;&#21644;&#20999;&#25442;&#25104;&#21151;&#29575;&#65292;&#23637;&#31034;&#20102;DHO&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#35775;&#38382;&#24310;&#36831;&#21644;&#30896;&#25758;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;&#21508;&#31181;DRL&#31639;&#27861;&#23545;DHO&#36827;&#34892;&#35757;&#32451;&#30340;&#24615;&#33021;&#21644;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#26657;&#20934;&#24230;&#37327;&#26041;&#27861;&#65292;&#32479;&#19968;&#21644;&#25512;&#24191;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#20013;&#24120;&#35265;&#30340;&#26657;&#20934;&#24418;&#24335;&#12290;&#36825;&#20123;&#24230;&#37327;&#21487;&#20197;&#20135;&#29983;&#21487;&#24494;&#30340;&#26679;&#26412;&#20272;&#35745;&#65292;&#26131;&#20110;&#32435;&#20837;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#26657;&#20934;&#24230;&#37327;&#26469;&#20248;&#21270;&#20915;&#31574;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20211</link><description>&lt;p&gt;
&#20998;&#24067;&#21305;&#37197;&#26657;&#20934;&#65306;&#21487;&#35757;&#32451;&#30340;&#26680;&#26657;&#20934;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Calibration by Distribution Matching: Trainable Kernel Calibration Metrics. (arXiv:2310.20211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#26657;&#20934;&#24230;&#37327;&#26041;&#27861;&#65292;&#32479;&#19968;&#21644;&#25512;&#24191;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#20013;&#24120;&#35265;&#30340;&#26657;&#20934;&#24418;&#24335;&#12290;&#36825;&#20123;&#24230;&#37327;&#21487;&#20197;&#20135;&#29983;&#21487;&#24494;&#30340;&#26679;&#26412;&#20272;&#35745;&#65292;&#26131;&#20110;&#32435;&#20837;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#26657;&#20934;&#24230;&#37327;&#26469;&#20248;&#21270;&#20915;&#31574;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#30830;&#20445;&#27010;&#29575;&#39044;&#27979;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#27714;&#39044;&#27979;&#27010;&#29575;&#19982;&#32463;&#39564;&#39057;&#29575;&#30456;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19987;&#38376;&#29992;&#20110;&#20107;&#21518;&#20877;&#26657;&#20934;&#65292;&#21487;&#33021;&#20250;&#24694;&#21270;&#39044;&#27979;&#30340;&#23574;&#38160;&#24615;&#12290;&#22522;&#20110;&#23558;&#26657;&#20934;&#35270;&#20026;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#30340;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26680;&#30340;&#26657;&#20934;&#24230;&#37327;&#65292;&#32479;&#19968;&#21644;&#25512;&#24191;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#20013;&#24120;&#35265;&#30340;&#26657;&#20934;&#24418;&#24335;&#12290;&#36825;&#20123;&#24230;&#37327;&#21487;&#20197;&#20135;&#29983;&#21487;&#24494;&#30340;&#26679;&#26412;&#20272;&#35745;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#23558;&#26657;&#20934;&#30446;&#26631;&#32435;&#20837;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#26426;&#21046;&#26469;&#23450;&#21046;&#20915;&#31574;&#20219;&#21153;&#30340;&#26657;&#20934;&#24230;&#37327;&#65292;&#24182;&#24378;&#21046;&#20934;&#30830;&#30340;&#25439;&#22833;&#20272;&#35745;&#21644;&#26080;&#36951;&#25022;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#25552;&#39640;&#22312;&#19968;&#31995;&#21015;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26657;&#20934;&#24615;&#12289;&#23574;&#38160;&#24615;&#21644;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#37325;&#35201;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21551;&#21457;&#24335;&#25512;&#23548;&#20986;Taylor&#19968;&#38454;&#36817;&#20284;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#21644;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#26377;&#26631;&#31614;&#26679;&#26412;&#30340;&#38656;&#27714;&#65292;&#24182;&#20351;&#24471;&#25152;&#26377;&#26679;&#26412;&#23545;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#36129;&#29486;&#30456;&#20284;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#20808;&#21069;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.20203</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#37325;&#35201;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Importance Estimation with Random Gradient for Neural Network Pruning. (arXiv:2310.20203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#37325;&#35201;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21551;&#21457;&#24335;&#25512;&#23548;&#20986;Taylor&#19968;&#38454;&#36817;&#20284;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#21644;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#26377;&#26631;&#31614;&#26679;&#26412;&#30340;&#38656;&#27714;&#65292;&#24182;&#20351;&#24471;&#25152;&#26377;&#26679;&#26412;&#23545;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#36129;&#29486;&#30456;&#20284;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#20808;&#21069;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#31070;&#32463;&#20803;&#37325;&#35201;&#24615;&#20272;&#35745;&#34987;&#29992;&#20110;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#28608;&#27963;&#25110;&#26799;&#24230;&#20449;&#24687;&#65292;&#35201;&#20040;&#20004;&#32773;&#20860;&#26377;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#31070;&#32463;&#20803;&#25110;&#21367;&#31215;&#26680;&#30340;&#20840;&#23616;&#37325;&#35201;&#24615;&#65292;&#36825;&#35201;&#27714;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#25512;&#23548;&#20986;&#31867;&#20284;&#20110;Taylor&#19968;&#38454;&#65288;TaylorFO&#65289;&#36817;&#20284;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;TaylorFO-abs&#21644;TaylorFO-sq&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39069;&#22806;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#36825;&#20123;&#37325;&#35201;&#24615;&#20272;&#35745;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#20256;&#25773;&#38543;&#26426;&#26799;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#26377;&#26631;&#31614;&#30340;&#26679;&#26412;&#30340;&#38656;&#27714;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26368;&#21518;&#19968;&#23618;&#30340;&#26799;&#24230;&#24133;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20351;&#25152;&#26377;&#26679;&#26412;&#23545;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#36129;&#29486;&#30456;&#20284;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#39069;&#22806;&#30340;&#25216;&#26415;&#22312;CIFAR-100&#21644;STL-10&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#26102;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. F
&lt;/p&gt;</description></item><item><title>FedRec+&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#38544;&#31169;&#24615;&#21644;&#35299;&#20915;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#29305;&#24449;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#20266;&#39033;&#30446;&#30340;&#34394;&#25311;&#35780;&#20998;&#65292;&#20943;&#23569;&#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#26469;&#20272;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20193</link><description>&lt;p&gt;
FedRec+:&#22686;&#24378;&#38544;&#31169;&#24615;&#21644;&#35299;&#20915;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems. (arXiv:2310.20193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20193
&lt;/p&gt;
&lt;p&gt;
FedRec+&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#38544;&#31169;&#24615;&#21644;&#35299;&#20915;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#29305;&#24449;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#20266;&#39033;&#30446;&#30340;&#34394;&#25311;&#35780;&#20998;&#65292;&#20943;&#23569;&#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#26469;&#20272;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20445;&#25252;&#38544;&#31169;&#21644;&#38477;&#20302;&#36793;&#32536;&#29992;&#25143;&#30340;&#36890;&#20449;&#25104;&#26412;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#22312;&#36991;&#20813;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#25968;&#25454;&#20132;&#25442;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30740;&#31350;&#34920;&#26126;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#26681;&#25454;&#20004;&#36718;&#29992;&#25143;&#19978;&#20256;&#30340;&#26799;&#24230;&#30340;&#38750;&#38646;&#26799;&#24230;&#21464;&#21270;&#26469;&#25512;&#26029;&#29992;&#25143;&#30340;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#25512;&#33616;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRec+&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#38544;&#31169;&#24615;&#24182;&#35299;&#20915;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;FedRec+&#21033;&#29992;&#22522;&#20110;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#26368;&#20248;&#23376;&#38598;&#36873;&#25321;&#26469;&#29983;&#25104;&#20266;&#39033;&#30446;&#30340;&#36817;&#20284;&#26368;&#20339;&#34394;&#25311;&#35780;&#20998;&#65292;&#20165;&#21033;&#29992;&#29992;&#25143;&#30340;&#26412;&#22320;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#22122;&#22768;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;Wasserstein&#36317;&#31163;&#26469;&#20272;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20302;&#20809;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#21033;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#21644;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#36716;&#25442;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#23433;&#38450;&#21644;&#30417;&#25511;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.20190</link><description>&lt;p&gt;
&#20302;&#20809;&#26465;&#20214;&#19979;&#25913;&#21892;&#35270;&#35273;&#20219;&#21153;&#30340;&#21487;&#35265;&#20809;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Visible to Thermal image Translation for improving visual task in low light conditions. (arXiv:2310.20190v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20302;&#20809;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#21033;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#21644;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#36716;&#25442;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#23433;&#38450;&#21644;&#30417;&#25511;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#20809;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#24456;&#38590;&#23436;&#25104;&#19968;&#20123;&#35270;&#35273;&#20219;&#21153;&#65292;&#22914;&#34892;&#20154;&#26816;&#27979;&#21644;&#22270;&#20687;&#32763;&#35793;&#12290;&#28909;&#32418;&#22806;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#28909;&#21464;&#21270;&#21487;&#20197;&#29992;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#21644;&#19968;&#20010;&#26816;&#27979;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#21487;&#35265;&#20809;&#22270;&#20687;&#32763;&#35793;&#25104;&#28909;&#32418;&#22806;&#22270;&#20687;&#65292;&#24182;&#23558;&#29983;&#25104;&#30340;&#28909;&#32418;&#22806;&#22270;&#20687;&#19982;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;Parrot Anafi Thermal&#26080;&#20154;&#26426;&#22312;&#20004;&#20010;&#19981;&#21516;&#20301;&#32622;&#25910;&#38598;&#20102;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;&#32593;&#32476;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20102;&#39044;&#22788;&#29702;&#12289;&#22686;&#24378;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GAN&#23558;&#21487;&#35265;&#20809;&#35757;&#32451;&#25968;&#25454;&#36716;&#25442;&#20026;&#28909;&#32418;&#22806;&#25968;&#25454;&#26159;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#29616;&#22312;&#21487;&#20197;&#26356;&#24555;&#36895;&#12289;&#26356;&#32463;&#27982;&#22320;&#29983;&#25104;&#28909;&#32418;&#22806;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#23433;&#38450;&#21644;&#30417;&#25511;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several visual tasks, such as pedestrian detection and image-to-image translation, are challenging to accomplish in low light using RGB images. Heat variation of objects in thermal images can be used to overcome this. In this work, an end-to-end framework, which consists of a generative network and a detector network, is proposed to translate RGB image into Thermal ones and compare generated thermal images with real data. We have collected images from two different locations using the Parrot Anafi Thermal drone. After that, we created a two-stream network, preprocessed, augmented, the image data, and trained the generator and discriminator models from scratch. The findings demonstrate that it is feasible to translate RGB training data to thermal data using GAN. As a result, thermal data can now be produced more quickly and affordably, which is useful for security and surveillance applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20187</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#38477;&#27700;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#38450;&#21361;&#38505;&#22825;&#27668;&#20107;&#20214;&#65292;&#30830;&#20445;&#20805;&#36275;&#30340;&#23616;&#22320;&#38477;&#27700;&#39044;&#25253;&#25552;&#21069;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20840;&#29699;&#21464;&#26262;&#24341;&#36215;&#30340;&#27668;&#20505;&#21464;&#21270;&#22686;&#21152;&#20102;&#20934;&#30830;&#39044;&#27979;&#20005;&#37325;&#38477;&#27700;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#12290;&#38477;&#27700;&#21518;&#22788;&#29702;&#21253;&#25324;&#65288;i&#65289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#22312;&#22823;&#27668;&#29289;&#29702;&#39046;&#22495;&#30340;&#36974;&#34109;&#21464;&#37327;&#37325;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#23398;&#20064;&#21040;&#38477;&#27700;&#20998;&#21106;&#20219;&#21153;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#21306;&#22495;NWP&#20013;&#30340;&#38477;&#27700;&#26657;&#27491;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20178</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#21457;&#29616;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning to Discover Skills through Guidance. (arXiv:2310.20178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25216;&#33021;&#20559;&#31163;&#20854;&#21021;&#22987;&#36712;&#36857;&#20250;&#21463;&#21040;&#37325;&#22823;&#24809;&#32602;&#12290;&#20026;&#20102;&#22686;&#24378;&#25506;&#32034;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#22870;&#21169;&#26469;&#26368;&#22823;&#21270;&#29366;&#24577;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#25110;&#29109;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#30340;&#25928;&#26524;&#38543;&#30528;&#29615;&#22659;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#32780;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;DISCO-DANCE&#65292;&#23427;&#36873;&#25321;&#20855;&#26377;&#36798;&#21040;&#26410;&#25506;&#32034;&#29366;&#24577;&#28508;&#21147;&#26368;&#39640;&#30340;&#24341;&#23548;&#25216;&#33021;&#65292;&#24341;&#23548;&#20854;&#20182;&#25216;&#33021;&#36981;&#24490;&#24341;&#23548;&#25216;&#33021;&#65292;&#28982;&#21518;&#20998;&#25955;&#24341;&#23548;&#25216;&#33021;&#20197;&#26368;&#22823;&#21270;&#22312;&#26410;&#25506;&#32034;&#29366;&#24577;&#20013;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#23548;&#33322;&#22522;&#20934;&#21644;&#19968;&#20010;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#65292;DISCO-DANCE&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#22522;&#32447;&#12290;DISCO-DANCE&#30340;&#23450;&#24615;&#21487;&#35270;&#21270;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;CBS-GPT&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#29983;&#25104;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20172</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#29983;&#25104;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;
&lt;/p&gt;
&lt;p&gt;
Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer. (arXiv:2310.20172v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;CBS-GPT&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#29983;&#25104;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#24341;&#21147;&#27874;&#25506;&#27979;&#26159;&#26410;&#26469;&#21313;&#24180;&#26368;&#21463;&#26399;&#24453;&#30340;&#24341;&#21147;&#27874;&#25506;&#27979;&#39033;&#30446;&#20043;&#19968;&#65292;&#23558;&#25506;&#27979;&#21040;&#20016;&#23500;&#30340;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31354;&#38388;&#24341;&#21147;&#27874;&#27874;&#24418;&#30340;&#31934;&#30830;&#39044;&#27979;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#20108;&#20195;&#26102;&#24310;&#24178;&#28041;&#65288;TDI 2.0&#65289;&#24341;&#36215;&#30340;&#27874;&#24418;&#22797;&#26434;&#24615;&#22686;&#21152;&#32780;&#24102;&#26469;&#30340;&#25968;&#25454;&#22788;&#29702;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CBS-GPT&#65288;Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer&#65289;&#30340;&#21487;&#35299;&#37322;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#12290;&#23545;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#65292;&#35757;&#32451;&#20102;&#19977;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#36229;&#22823;&#36136;&#37327;&#40657;&#27934;&#20108;&#36827;&#21046;&#65288;MBHB&#65289;&#12289;&#26497;&#31471;&#36136;&#37327;&#27604;&#34701;&#21512;&#65288;EMRIs&#65289;&#21644;&#26143;&#31995;&#20108;&#36827;&#21046;&#65288;GB&#65289;&#30340;&#27874;&#24418;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;98%&#12289;91%&#21644;99%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;CBS-GPT&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#65292;&#20854;&#38544;&#34255;&#21442;&#25968;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#27874;&#24418;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#19981;&#36830;&#32493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex ins
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#26032;&#39062;&#19988;&#30452;&#35266;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#27973;&#20113;&#27169;&#25311;&#20013;&#28082;&#28404;&#23610;&#23544;&#30340;&#32452;&#32455;&#21644;&#28436;&#21270;&#29305;&#24449;&#12290;&#23545;&#27604;&#19981;&#21516;&#27668;&#28342;&#33014;&#27987;&#24230;&#30340;&#27169;&#25311;&#65292;&#25105;&#20204;&#21457;&#29616;&#28082;&#28404;&#35889;&#30340;&#28436;&#21270;&#22312;&#19981;&#21516;&#30340;&#27668;&#28342;&#33014;&#27700;&#24179;&#19979;&#24456;&#30456;&#20284;&#65292;&#20294;&#36895;&#24230;&#19981;&#21516;&#65292;&#36825;&#20026;&#27668;&#28342;&#33014;&#19982;&#20113;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.20168</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#21487;&#35270;&#21270;&#27973;&#20113;&#27169;&#25311;&#20013;&#30340;&#28082;&#28404;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds. (arXiv:2310.20168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#26032;&#39062;&#19988;&#30452;&#35266;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#27973;&#20113;&#27169;&#25311;&#20013;&#28082;&#28404;&#23610;&#23544;&#30340;&#32452;&#32455;&#21644;&#28436;&#21270;&#29305;&#24449;&#12290;&#23545;&#27604;&#19981;&#21516;&#27668;&#28342;&#33014;&#27987;&#24230;&#30340;&#27169;&#25311;&#65292;&#25105;&#20204;&#21457;&#29616;&#28082;&#28404;&#35889;&#30340;&#28436;&#21270;&#22312;&#19981;&#21516;&#30340;&#27668;&#28342;&#33014;&#27700;&#24179;&#19979;&#24456;&#30456;&#20284;&#65292;&#20294;&#36895;&#24230;&#19981;&#21516;&#65292;&#36825;&#20026;&#27668;&#28342;&#33014;&#19982;&#20113;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#23616;&#37096;&#28082;&#28404;&#32423;&#21035;&#20132;&#20114;&#30340;&#28145;&#20837;&#20998;&#26512;&#23545;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#20113;&#20013;&#24494;&#29289;&#29702;&#36807;&#31243;&#21450;&#20854;&#23545;&#20840;&#29699;&#27668;&#20505;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#22823;&#28065;&#27169;&#25311;&#65288;Large Eddy Simulations&#65292;LES&#65289;&#20013;&#28082;&#28404;&#23610;&#23544;&#20998;&#24067;&#30340;&#39640;&#31934;&#24230;&#27169;&#25311;&#25361;&#25112;&#24403;&#21069;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#30001;&#20110;&#23427;&#20204;&#28041;&#21450;&#19977;&#20010;&#31354;&#38388;&#32500;&#24230;&#12289;&#26102;&#38388;&#21644;&#36830;&#32493;&#33539;&#22260;&#30340;&#28082;&#28404;&#23610;&#23544;&#65292;&#23548;&#33268;&#32500;&#24230;&#38750;&#24120;&#39640;&#12290;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;Variational Autoencoders&#65292;VAEs&#65289;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#26032;&#39062;&#19988;&#30452;&#35266;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#23637;&#31034;&#28082;&#28404;&#23610;&#23544;&#30340;&#32452;&#32455;&#21644;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21270;&#29305;&#24449;&#12290;&#36825;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#19988;&#35753;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27668;&#28342;&#33014;&#27987;&#24230;&#30340;&#27169;&#25311;&#26469;&#30740;&#31350;&#27668;&#28342;&#33014;&#19982;&#20113;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28082;&#28404;&#35889;&#30340;&#28436;&#21270;&#22312;&#19981;&#21516;&#30340;&#27668;&#28342;&#33014;&#27700;&#24179;&#19979;&#24456;&#30456;&#20284;&#65292;&#20294;&#36895;&#24230;&#19981;&#21516;&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#34920;&#26126;&#38477;&#27700;&#21487;&#33021;&#21463;&#21040;&#27668;&#28342;&#33014;&#27987;&#24230;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#23545;&#20113;&#24494;&#29289;&#29702;&#36807;&#31243;&#30340;&#26032;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thorough analysis of local droplet-level interactions is crucial to better understand the microphysical processes in clouds and their effect on the global climate. High-accuracy simulations of relevant droplet size distributions from Large Eddy Simulations (LES) of bin microphysics challenge current analysis techniques due to their high dimensionality involving three spatial dimensions, time, and a continuous range of droplet sizes. Utilizing the compact latent representations from Variational Autoencoders (VAEs), we produce novel and intuitive visualizations for the organization of droplet sizes and their evolution over time beyond what is possible with clustering techniques. This greatly improves interpretation and allows us to examine aerosol-cloud interactions by contrasting simulations with different aerosol concentrations. We find that the evolution of the droplet spectrum is similar across aerosol levels but occurs at different paces. This similarity suggests that precipitation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;robust Bayesian Optimization&#31639;&#27861;&#65292;AIRBO&#65292;&#23427;&#33021;&#22815;&#22312;&#20219;&#24847;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#26377;&#25928;&#35782;&#21035;&#20986;&#34920;&#29616;&#19968;&#33268;&#33391;&#22909;&#30340;&#40065;&#26834;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.20145</link><description>&lt;p&gt;
&#39640;&#25928;robust Bayesian Optimization&#23545;&#20110;&#20219;&#24847;&#19981;&#30830;&#23450;&#36755;&#20837;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs. (arXiv:2310.20145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;robust Bayesian Optimization&#31639;&#27861;&#65292;AIRBO&#65292;&#23427;&#33021;&#22815;&#22312;&#20219;&#24847;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#26377;&#25928;&#35782;&#21035;&#20986;&#34920;&#29616;&#19968;&#33268;&#33391;&#22909;&#30340;&#40065;&#26834;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) &#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;BO&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#38543;&#26426;&#24615;&#65292;&#22914;&#21152;&#24037;&#35823;&#24046;&#12289;&#25191;&#34892;&#22122;&#22768;&#25110;&#19978;&#19979;&#25991;&#21464;&#24322;&#65292;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#20250;&#20986;&#29616;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#20250;&#20351;&#36755;&#20837;&#22312;&#35780;&#20272;&#20043;&#21069;&#20559;&#31163;&#39044;&#26399;&#20540;&#65292;&#23548;&#33268;&#26368;&#32456;&#32467;&#26524;&#30340;&#24615;&#33021;&#27874;&#21160;&#36739;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;robust Bayesian Optimization&#31639;&#27861;&#65292;AIRBO&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#35782;&#21035;&#22312;&#20219;&#24847;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#34920;&#29616;&#19968;&#33268;&#33391;&#22909;&#30340;&#40065;&#26834;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;(MMD)&#36171;&#33021;&#39640;&#26031;&#36807;&#31243;&#65292;&#30452;&#25509;&#24314;&#27169;&#20219;&#24847;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;Nystrom&#36924;&#36817;&#21152;&#36895;&#21518;&#39564;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;MMD&#20272;&#35745;&#35823;&#24046;&#19979;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21512;&#25104;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic function
&lt;/p&gt;</description></item><item><title>EELBERT&#26159;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#22238;&#24402;&#21644;&#26174;&#33879;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#23567;&#12290;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#22312;GLUE&#24471;&#20998;&#19978;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#30456;&#24046;4%&#65292;&#24182;&#19988;&#20307;&#31215;&#21482;&#26377;&#20854;15&#20493;&#20043;&#19968;&#65288;1.2MB&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.20144</link><description>&lt;p&gt;
EELBERT:&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20144
&lt;/p&gt;
&lt;p&gt;
EELBERT&#26159;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#22238;&#24402;&#21644;&#26174;&#33879;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#23567;&#12290;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#22312;GLUE&#24471;&#20998;&#19978;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#30456;&#24046;4%&#65292;&#24182;&#19988;&#20307;&#31215;&#21482;&#26377;&#20854;15&#20493;&#20043;&#19968;&#65288;1.2MB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EELBERT&#65292;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#65289;&#30340;&#26041;&#27861;&#65292;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#26368;&#23567;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#26367;&#25442;&#20026;&#21160;&#24577;&#30340;&#65292;&#21363;&#21363;&#26102;&#35745;&#31639;&#30340;&#23884;&#20837;&#23454;&#29616;&#26469;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#36755;&#20837;&#23884;&#20837;&#23618;&#21344;&#27169;&#22411;&#22823;&#23567;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;BERT&#21464;&#20307;&#65292;&#29992;&#23884;&#20837;&#35745;&#31639;&#20989;&#25968;&#26367;&#25442;&#35813;&#23618;&#26377;&#21161;&#20110;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;BERT&#21464;&#20307;&#65288;EELBERT&#65289;&#19982;&#20256;&#32479;BERT&#27169;&#22411;&#30456;&#27604;&#20165;&#20855;&#26377;&#26368;&#23567;&#30340;&#22238;&#24402;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20986;&#25105;&#20204;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#65292;&#20854;GLUE&#24471;&#20998;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#39640;4&#65285;&#65292;&#21516;&#26102;&#20307;&#31215;&#23567;15&#20493;&#65288;1.2MB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20141</link><description>&lt;p&gt;
&#23545;&#27604;&#24046;&#24322;&#24615;&#39044;&#27979;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#25512;&#29702;&#26410;&#26469;&#26159;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#26680;&#24515;&#12290;&#20363;&#22914;&#65292;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23398;&#20064;&#34920;&#31034;&#20197;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#35775;&#38382;&#30340;&#29366;&#24577;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#23545;&#27604;&#24615;&#39044;&#27979;&#32534;&#30721;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#23398;&#20064;&#32534;&#30721;&#38271;&#26399;&#20381;&#36182;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#20943;&#23569;&#23398;&#20064;&#26410;&#26469;&#20107;&#20214;&#39044;&#27979;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#23548;&#20986;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#25552;&#39640;2&#20493;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#38543;&#26426;&#29615;&#22659;&#12290;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#32422;&#20026;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;"&#30456;&#37051;&#20551;&#35774;"&#30697;&#38453;&#21644;&#24341;&#20837;&#26679;&#26412;&#26465;&#20214;&#30340;&#20551;&#35774;&#31283;&#23450;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#20445;&#35777;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#20449;&#24687;&#35770;&#30028;&#38480;&#65292;&#24182;&#35299;&#20915;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#20449;&#24687;&#35770;&#30028;&#38480;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20102</link><description>&lt;p&gt;
&#26679;&#26412;&#26465;&#20214;&#30340;&#20551;&#35774;&#31283;&#23450;&#24615;&#25552;&#21319;&#20102;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds. (arXiv:2310.20102v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20102
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;"&#30456;&#37051;&#20551;&#35774;"&#30697;&#38453;&#21644;&#24341;&#20837;&#26679;&#26412;&#26465;&#20214;&#30340;&#20551;&#35774;&#31283;&#23450;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#20445;&#35777;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#20449;&#24687;&#35770;&#30028;&#38480;&#65292;&#24182;&#35299;&#20915;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#20449;&#24687;&#35770;&#30028;&#38480;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;"&#30456;&#37051;&#20551;&#35774;"&#30697;&#38453;&#30340;&#26500;&#36896;&#21644;&#19968;&#31181;&#26032;&#30340;&#31283;&#23450;&#24615;&#27010;&#24565;&#8212;&#8212;&#26679;&#26412;&#26465;&#20214;&#30340;&#20551;&#35774;&#31283;&#23450;&#24615;&#65288;SCH&#31283;&#23450;&#24615;&#65289;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#27604;&#20808;&#21069;&#20449;&#24687;&#35770;&#30028;&#38480;&#26356;&#20934;&#30830;&#30340;&#30028;&#38480;&#65292;&#22312;&#21508;&#31181;&#23398;&#20064;&#22330;&#26223;&#20013;&#26377;&#25152;&#25913;&#21892;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#30028;&#38480;&#35299;&#20915;&#20102;&#26368;&#36817;Haghifam&#31561;&#20154;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#38382;&#39064;&#19978;&#30340;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#35770;&#30028;&#38480;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new information-theoretic generalization guarantees through the a novel construction of the "neighboring-hypothesis" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability. Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#23398;&#20064;&#31639;&#27861;RCL&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#27493;&#20999;&#25442;&#25104;&#26412;&#21644;&#21453;&#39304;&#24310;&#36831;&#30340;&#24179;&#28369;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23558;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19982;&#21487;&#20449;&#30340;&#19987;&#23478;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32422;&#26463;&#25237;&#24433;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;RCL&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#19987;&#23478;&#31639;&#27861;&#19979;&#23454;&#29616;$(1+\lambda)$-&#31454;&#20105;&#24615;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#21270;&#24863;&#30693;&#30340;&#26041;&#24335;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;RCL&#22312;&#30005;&#21160;&#20132;&#36890;&#30340;&#30005;&#27744;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#21644;&#24179;&#22343;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.20098</link><description>&lt;p&gt;
&#24179;&#28369;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#21453;&#39304;&#24310;&#36831;&#30340;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning for Smoothed Online Convex Optimization with Feedback Delay. (arXiv:2310.20098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20098
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#23398;&#20064;&#31639;&#27861;RCL&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#27493;&#20999;&#25442;&#25104;&#26412;&#21644;&#21453;&#39304;&#24310;&#36831;&#30340;&#24179;&#28369;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23558;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19982;&#21487;&#20449;&#30340;&#19987;&#23478;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32422;&#26463;&#25237;&#24433;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;RCL&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#19987;&#23478;&#31639;&#27861;&#19979;&#23454;&#29616;$(1+\lambda)$-&#31454;&#20105;&#24615;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#21270;&#24863;&#30693;&#30340;&#26041;&#24335;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;RCL&#22312;&#30005;&#21160;&#20132;&#36890;&#30340;&#30005;&#27744;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#21644;&#24179;&#22343;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#27493;&#38750;&#32447;&#24615;&#20999;&#25442;&#25104;&#26412;&#21644;&#21453;&#39304;&#24310;&#36831;&#30340;&#24179;&#28369;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;SOCO&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22686;&#24378;&#22312;&#32447;&#31639;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#32422;&#26463;&#23398;&#20064;&#65288;RCL&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#32422;&#26463;&#25237;&#24433;&#23558;&#19981;&#21487;&#20449;&#30340;ML&#39044;&#27979;&#19982;&#21487;&#20449;&#30340;&#19987;&#23478;&#22312;&#32447;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;ML&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RCL&#33021;&#22815;&#20445;&#35777;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#19987;&#23478;&#65292;&#28385;&#36275;$(1+\lambda)$-&#31454;&#20105;&#24615;&#65292;&#20854;&#20013;$\lambda&gt;0$&#65292;&#21516;&#26102;&#20197;&#40065;&#26834;&#21270;&#24863;&#30693;&#30340;&#26041;&#24335;&#26126;&#30830;&#35757;&#32451;ML&#27169;&#22411;&#20197;&#25913;&#21892;&#24179;&#22343;&#24773;&#20917;&#24615;&#33021;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;RCL&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#27493;&#20999;&#25442;&#25104;&#26412;&#21644;&#21453;&#39304;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;ML&#22686;&#24378;&#31639;&#27861;&#12290;&#25105;&#20204;&#20197;&#30005;&#21160;&#20132;&#36890;&#30340;&#30005;&#27744;&#31649;&#29702;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;RCL&#22312;&#40065;&#26834;&#24615;&#21644;&#24179;&#22343;&#24615;&#33021;&#19978;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a challenging form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step nonlinear switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically,we prove that RCL is able to guarantee$(1+\lambda)$-competitiveness against any given expert for any$\lambda&gt;0$, while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly,RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay.We demonstrate the improvement of RCL in both robustness and average performance using battery management for electrifying transportationas a case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#26174;&#33879;&#20943;&#23569;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#36136;&#37327;&#35299;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20092</link><description>&lt;p&gt;
&#36229;&#36234;U&#65306;&#20351;&#25193;&#25955;&#27169;&#22411;&#26356;&#24555;&#26356;&#36731;
&lt;/p&gt;
&lt;p&gt;
Beyond U: Making Diffusion Models Faster &amp; Lighter. (arXiv:2310.20092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#26174;&#33879;&#20943;&#23569;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#36136;&#37327;&#35299;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#21644;&#20998;&#23376;&#35774;&#35745;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21019;&#32426;&#24405;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#22791;&#36825;&#20123;&#33021;&#21147;&#65292;&#20294;&#20854;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#36870;&#21521;&#21435;&#22122;&#36807;&#31243;&#20013;&#65292;&#20173;&#28982;&#38754;&#20020;&#30528;&#24930;&#25910;&#25947;&#36895;&#24230;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#26469;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#23545;&#21435;&#22122;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#30340;&#21442;&#25968;&#32422;&#20026;&#26631;&#20934;&#21435;&#22122;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;DDPM&#65289;&#20013;&#26631;&#20934;U-Net&#30340;&#22235;&#20998;&#20043;&#19968;&#65292;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#32422;&#20026;&#26631;&#20934;U-Net&#30340;30%&#12290;&#27492;&#22806;&#65292;&#22312;&#30456;&#31561;&#26465;&#20214;&#19979;&#27979;&#37327;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#27604;&#22522;&#20934;&#27169;&#22411;&#24555;70&#65285;&#65292;&#21516;&#26102;&#25910;&#25947;&#21040;&#26356;&#22909;&#30340;&#36136;&#37327;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a family of generative models that yield record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse denoising process, remains a challenge due to slow convergence rates and high computational costs. In this work, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with denoising probabilistic diffusion models, our framework operates with approximately a quarter of the parameters and 30% of the Floating Point Operations (FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models (DDPMs). Furthermore, our model is up to 70% faster in inference than the baseline models when measured in equal conditions while converging to better quality solutio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#38500;&#21464;&#20998;&#25512;&#26029;&#19982;Wasserstein&#26799;&#24230;&#27969;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24067;&#38647;&#26031;-&#29926;&#29791;&#26031;&#22374;&#26799;&#24230;&#27969;&#21487;&#20197;&#37325;&#26032;&#34920;&#31034;&#20026;&#27431;&#27663;&#26799;&#24230;&#27969;&#65292;&#25552;&#20986;&#20102;&#36335;&#24452;&#23548;&#25968;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#33976;&#39311;&#36807;&#31243;&#26469;&#25299;&#23637;&#35813;&#26041;&#27861;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;f-&#25955;&#24230;&#21644;&#38750;&#39640;&#26031;&#21464;&#20998;&#26063;&#12290;</title><link>http://arxiv.org/abs/2310.20090</link><description>&lt;p&gt;
&#28040;&#38500;&#21464;&#20998;&#25512;&#26029;&#19982;Wasserstein&#26799;&#24230;&#27969;&#20043;&#38388;&#30340;&#40511;&#27807;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows. (arXiv:2310.20090v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#38500;&#21464;&#20998;&#25512;&#26029;&#19982;Wasserstein&#26799;&#24230;&#27969;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24067;&#38647;&#26031;-&#29926;&#29791;&#26031;&#22374;&#26799;&#24230;&#27969;&#21487;&#20197;&#37325;&#26032;&#34920;&#31034;&#20026;&#27431;&#27663;&#26799;&#24230;&#27969;&#65292;&#25552;&#20986;&#20102;&#36335;&#24452;&#23548;&#25968;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#33976;&#39311;&#36807;&#31243;&#26469;&#25299;&#23637;&#35813;&#26041;&#27861;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;f-&#25955;&#24230;&#21644;&#38750;&#39640;&#26031;&#21464;&#20998;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#21464;&#20998;&#26063;&#21442;&#25968;&#31354;&#38388;&#20869;&#36827;&#34892;&#20248;&#21270;&#26469;&#36817;&#20284;&#30446;&#26631;&#20998;&#24067;&#30340;&#25216;&#26415;&#12290;&#32780;Wasserstein&#26799;&#24230;&#27969;&#25551;&#36848;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#30340;&#31354;&#38388;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#20854;&#20013;&#19981;&#19968;&#23450;&#23384;&#22312;&#21442;&#25968;&#23494;&#24230;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#24067;&#38647;&#26031;-&#29926;&#29791;&#26031;&#22374;&#26799;&#24230;&#27969;&#21487;&#20197;&#37325;&#26032;&#34920;&#31034;&#20026;&#27431;&#27663;&#26799;&#24230;&#27969;&#65292;&#20854;&#21069;&#21521;&#27431;&#25289;&#26041;&#26696;&#26159;&#26631;&#20934;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26799;&#24230;&#27969;&#30340;&#21521;&#37327;&#22330;&#36890;&#36807;&#36335;&#24452;&#23548;&#25968;&#26799;&#24230;&#20272;&#35745;&#22120;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#36335;&#24452;&#23548;&#25968;&#26799;&#24230;&#30340;&#26367;&#20195;&#35270;&#35282;&#65292;&#23558;&#20854;&#26694;&#26550;&#21270;&#20026;&#23545;Wasserstein&#26799;&#24230;&#27969;&#30340;&#33976;&#39311;&#36807;&#31243;&#12290;&#33976;&#39311;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#21547;f-&#25955;&#24230;&#21644;&#38750;&#39640;&#26031;&#21464;&#20998;&#26063;&#12290;&#36825;&#31181;&#25193;&#23637;&#20135;&#29983;&#20102;&#19968;&#20010;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20082</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficient Subgraph GNNs by Learning Effective Selection Policies. (arXiv:2310.20082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#21487;&#35777;&#26126;&#34920;&#36798;&#21147;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;&#23376;&#22270;&#20013;&#23398;&#20064;&#22270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35768;&#22810;&#23376;&#22270;&#19978;&#25191;&#34892;&#20449;&#24687;&#20256;&#36882;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#32771;&#34385;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#36873;&#25321;&#19968;&#20010;&#23567;&#30340;&#23376;&#22270;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35777;&#26126;&#23384;&#22312;&#19968;&#20123;WL-&#38590;&#21306;&#20998;&#30340;&#22270;&#26063;&#65292;&#36825;&#20123;&#22270;&#26063;&#23384;&#22312;&#21487;&#20197;&#35782;&#21035;&#35813;&#26063;&#20013;&#25152;&#26377;&#22270;&#30340;&#26377;&#25928;&#23376;&#22270;&#36873;&#25321;&#31574;&#30053;&#65292;&#26469;&#35299;&#37322;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Policy-Learn&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#36845;&#20195;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#36873;&#25321;&#23376;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#24120;&#29992;&#30340;&#38543;&#26426;&#31574;&#30053;&#21644;&#35299;&#20915;&#30456;&#21516;&#38382;&#39064;&#30340;&#20808;&#21069;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#33021;&#22815;&#23398;&#20064;&#21040;&#19978;&#36848;&#39640;&#25928;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Policy-Learn&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called Policy-Learn, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Our experimental results demonstrate that Policy-Learn outperforms existing basel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODE&#65289;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#32858;&#21464;&#21453;&#24212;&#22534;&#31561;&#31163;&#23376;&#20307;&#30340;&#32806;&#21512;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#21644;&#20869;&#37096;&#24863;&#24212;&#21160;&#21147;&#23398;&#65292;&#20197;&#35299;&#20915;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20079</link><description>&lt;p&gt;
&#28151;&#21512;&#29289;&#29702;&#21644;&#31070;&#32463;ODE&#29992;&#20110;&#39044;&#27979;&#25176;&#21345;&#39532;&#20811;&#32858;&#21464;&#21453;&#24212;&#22534;&#31561;&#31163;&#23376;&#20307;&#24863;&#24212;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors. (arXiv:2310.20079v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODE&#65289;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#32858;&#21464;&#21453;&#24212;&#22534;&#31561;&#31163;&#23376;&#20307;&#30340;&#32806;&#21512;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#21644;&#20869;&#37096;&#24863;&#24212;&#21160;&#21147;&#23398;&#65292;&#20197;&#35299;&#20915;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#34987;&#31216;&#20026;&#25176;&#21345;&#39532;&#20811;&#30340;&#32858;&#21464;&#21453;&#24212;&#22534;&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#33021;&#28304;&#26469;&#28304;&#32780;&#22791;&#21463;&#26399;&#24453;&#65292;&#20294;&#20026;&#20102;&#20351;&#20854;&#32463;&#27982;&#21487;&#34892;&#65292;&#38656;&#35201;&#22312;&#31561;&#31163;&#23376;&#20307;&#25511;&#21046;&#21644;&#22788;&#29702;&#22833;&#25511;&#31561;&#20107;&#20214;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;&#24212;&#29992;&#26356;&#20808;&#36827;&#30340;&#25511;&#21046;&#31639;&#27861;&#38754;&#20020;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#65292;&#21363;&#38656;&#35201;&#26356;&#22909;&#30340;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#65292;&#30446;&#21069;&#22522;&#20110;&#29289;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#37117;&#23384;&#22312;&#38382;&#39064;&#12290;&#21069;&#32773;&#21463;&#21040;&#35745;&#31639;&#25104;&#26412;&#21644;&#24314;&#27169;&#31561;&#38590;&#39064;&#30340;&#38480;&#21046;&#65292;&#21518;&#32773;&#21463;&#21040;&#25968;&#25454;&#30456;&#23545;&#21294;&#20047;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23558;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODE&#65289;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#30340;&#23376;&#38598;&#65292;&#21363;&#32806;&#21512;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#21644;&#20869;&#37096;&#24863;&#24212;&#21160;&#21147;&#23398;&#12290;&#30001;&#20110;&#31070;&#32463;ODE&#26694;&#26550;&#33021;&#22815;&#33258;&#28982;&#22320;&#21253;&#21547;&#22522;&#20110;&#29289;&#29702;&#30340;&#20559;&#32622;&#65292;&#25105;&#20204;&#22312;Alcator C-Mod&#32858;&#21464;&#21453;&#24212;&#22534;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#22522;&#20110;&#29289;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#31070;&#32463;ODE&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fusion reactors known as tokamaks hold promise as a firm energy source, advances in plasma control, and handling of events where control of plasmas is lost, are needed for them to be economical. A significant bottleneck towards applying more advanced control algorithms is the need for better plasma simulation, where both physics-based and data-driven approaches currently fall short. The former is bottle-necked by both computational cost and the difficulty of modelling plasmas, and the latter is bottle-necked by the relative paucity of data. To address this issue, this work applies the neural ordinary differential equations (ODE) framework to the problem of predicting a subset of plasma dynamics, namely the coupled plasma current and internal inductance dynamics. As the neural ODE framework allows for the natural inclusion of physics-based inductive biases, we train both physics-based and neural network models on data from the Alcator C-Mod fusion reactor and find that a model tha
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#37096;&#20998;&#24352;&#37327;&#21270;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;BERT&#21644;ViT&#31561;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#23618;&#21387;&#32553;&#21644;&#37096;&#20998;&#24352;&#37327;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25171;&#30772;&#20102;&#24352;&#37327;&#20998;&#35299;&#39046;&#22495;&#30340;&#26032;&#23616;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.20077</link><description>&lt;p&gt;
&#37096;&#20998;&#24352;&#37327;&#21270;&#21464;&#21387;&#22120;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Partial Tensorized Transformers for Natural Language Processing. (arXiv:2310.20077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20077
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#37096;&#20998;&#24352;&#37327;&#21270;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;BERT&#21644;ViT&#31561;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#23618;&#21387;&#32553;&#21644;&#37096;&#20998;&#24352;&#37327;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25171;&#30772;&#20102;&#24352;&#37327;&#20998;&#35299;&#39046;&#22495;&#30340;&#26032;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24320;&#21019;&#20102;&#26032;&#23616;&#38754;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#23384;&#20648;&#21644;&#21442;&#25968;&#38656;&#27714;&#36890;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24352;&#37327;&#21015;&#20998;&#35299;&#23545;&#25552;&#39640;BERT&#21644;ViT&#31561;&#21464;&#21387;&#22120;&#35270;&#35273;&#35821;&#35328;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#21387;&#32553;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23884;&#20837;&#23618;&#21387;&#32553;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#37096;&#20998;&#24352;&#37327;&#21270;&#65288;PTNN&#65289;&#36890;&#36807;&#19968;&#31181;&#31639;&#27861;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;PTNN&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#21518;&#26399;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25171;&#30772;&#20102;&#24352;&#37327;&#20998;&#35299;&#39046;&#22495;&#30340;&#26032;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Meek&#20998;&#31163;&#22120;&#21450;&#20854;&#22312;&#30446;&#26631;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;Meek&#20998;&#31163;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#20986;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#23547;&#25214;&#23567;&#35268;&#27169;&#30340;&#20998;&#31163;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#30446;&#26631;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.20075</link><description>&lt;p&gt;
Meek&#20998;&#31163;&#22120;&#21450;&#20854;&#22312;&#30446;&#26631;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Meek Separators and Their Applications in Targeted Causal Discovery. (arXiv:2310.20075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Meek&#20998;&#31163;&#22120;&#21450;&#20854;&#22312;&#30446;&#26631;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;Meek&#20998;&#31163;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#20986;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#23547;&#25214;&#23567;&#35268;&#27169;&#30340;&#20998;&#31163;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#30446;&#26631;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24178;&#39044;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#23613;&#31649;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#20110;&#24674;&#22797;&#25972;&#20010;&#22240;&#26524;&#22270;&#65292;&#20294;&#23454;&#38469;&#19978;&#23384;&#22312;&#19968;&#20123;&#22330;&#26223;&#65292;&#20165;&#23398;&#20064;&#22240;&#26524;&#22270;&#30340;&#37096;&#20998;&#21363;&#21487;&#28385;&#36275;&#38656;&#27714;&#12290;&#36825;&#34987;&#31216;&#20026;&#8220;&#30446;&#26631;&#22240;&#26524;&#21457;&#29616;&#8221;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#20010;&#36825;&#26679;&#30340;&#38382;&#39064;&#65306;&#23376;&#38598;&#25628;&#32034;&#21644;&#22240;&#26524;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#23613;&#37327;&#20943;&#23569;&#24178;&#39044;&#30340;&#27425;&#25968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Meek&#20998;&#31163;&#22120;&#65292;&#23427;&#26159;&#19968;&#20010;&#23376;&#38598;&#65292;&#22312;&#24178;&#39044;&#26102;&#23558;&#21097;&#20313;&#30340;&#26410;&#23450;&#21521;&#36793;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#36830;&#36890;&#20998;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#23547;&#25214;&#23567;&#35268;&#27169;&#30340;Meek&#20998;&#31163;&#22120;&#12290;&#36825;&#26679;&#30340;&#36807;&#31243;&#26377;&#21161;&#20110;&#35774;&#35745;&#21508;&#31181;&#22522;&#20110;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38543;&#26426;&#31639;&#27861;&#65292;&#20998;&#21035;&#23545;&#23376;&#38598;&#25628;&#32034;&#21644;&#22240;&#26524;&#21305;&#37197;&#23454;&#29616;&#23545;&#25968;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Learning causal structures from interventional data is a fundamental problem with broad applications across various fields. While many previous works have focused on recovering the entire causal graph, in practice, there are scenarios where learning only part of the causal graph suffices. This is called $targeted$ causal discovery. In our work, we focus on two such well-motivated problems: subset search and causal matching. We aim to minimize the number of interventions in both cases.  Towards this, we introduce the $Meek~separator$, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches. In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22810;&#20219;&#21153;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20072</link><description>&lt;p&gt;
&#29992;&#25351;&#23548;&#35843;&#25972;&#23454;&#29616;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22810;&#20219;&#21153;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;NLP&#39046;&#22495;&#19968;&#20010;&#38590;&#20197;&#36798;&#21040;&#30340;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#27169;&#25311;&#20154;&#31867;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#35780;&#20272;&#26631;&#20934;&#19978;&#30340;&#21028;&#26029;&#12290;&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;HEAP&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#20154;&#31867;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;HEAP&#19978;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#35780;&#20272;&#24615;&#33021;&#65292;&#23613;&#31649;&#26377;&#20123;&#35780;&#20272;&#26631;&#20934;&#30340;&#23398;&#20064;&#24182;&#19981;&#37027;&#20040;&#23481;&#26131;&#12290;&#27492;&#22806;&#65292;&#22810;&#20219;&#21153;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#23545;&#20110;&#26410;&#26469;&#32570;&#20047;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#20219;&#21153;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FOCAL&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#20174;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#29305;&#24449;&#12290;&#23427;&#36890;&#36807;&#20351;&#27599;&#20010;&#27169;&#24577;&#37117;&#32534;&#30721;&#21040;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#31361;&#20986;&#20849;&#20139;&#29305;&#24449;&#21644;&#19987;&#23646;&#29305;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#24863;&#30693;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#21644;&#19987;&#23646;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.20071</link><description>&lt;p&gt;
FOCAL: &#22312;&#22240;&#23376;&#21270;&#27491;&#20132;&#28508;&#31354;&#38388;&#20013;&#30340;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space. (arXiv:2310.20071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FOCAL&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#20174;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#29305;&#24449;&#12290;&#23427;&#36890;&#36807;&#20351;&#27599;&#20010;&#27169;&#24577;&#37117;&#32534;&#30721;&#21040;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#31361;&#20986;&#20849;&#20139;&#29305;&#24449;&#21644;&#19987;&#23646;&#29305;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#24863;&#30693;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#21644;&#19987;&#23646;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FOCAL&#30340;&#26032;&#22411;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#20174;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#23545;&#27604;&#26694;&#26550;&#20027;&#35201;&#20381;&#36182;&#20110;&#24863;&#30693;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#23545;&#29702;&#35299;&#24213;&#23618;&#24863;&#30693;&#29289;&#29702;&#23398;&#33267;&#20851;&#37325;&#35201;&#30340;&#19987;&#23646;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#27604;&#26694;&#26550;&#27809;&#26377;&#36866;&#24403;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#23616;&#37096;&#24615;&#12290;FOCAL&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20855;&#20307;&#36129;&#29486;&#22914;&#19979;&#65306;&#39318;&#20808;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#65292;&#23558;&#27599;&#20010;&#27169;&#24577;&#32534;&#30721;&#21040;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#35813;&#28508;&#31354;&#38388;&#30001;&#20849;&#20139;&#29305;&#24449;&#21644;&#24444;&#27492;&#27491;&#20132;&#30340;&#19987;&#23646;&#29305;&#24449;&#32452;&#25104;&#12290;&#20849;&#20139;&#31354;&#38388;&#36890;&#36807;&#27169;&#24577;&#21305;&#37197;&#30446;&#26631;&#24378;&#35843;&#36328;&#24863;&#30693;&#27169;&#24577;&#30340;&#29305;&#24449;&#27169;&#24335;&#19968;&#33268;&#24615;&#12290;&#30456;&#21453;&#65292;&#19987;&#23646;&#31354;&#38388;&#36890;&#36807;&#19968;&#20010;&#30446;&#26631;&#25552;&#21462;&#27169;&#24577;&#29420;&#21344;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#24418;&#21464;&#21644;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#24515;&#33039;&#34180;&#22721;&#32467;&#26500;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#29289;&#29702;&#27169;&#25311;&#65292;&#20943;&#23569;&#21518;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.20065</link><description>&lt;p&gt;
LinFlo-Net: &#19968;&#31181;&#29983;&#25104;&#24515;&#33039;&#27169;&#25311;&#32593;&#26684;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart. (arXiv:2310.20065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#24418;&#21464;&#21644;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#24515;&#33039;&#34180;&#22721;&#32467;&#26500;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#29289;&#29702;&#27169;&#25311;&#65292;&#20943;&#23569;&#21518;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24739;&#32773;&#25104;&#20687;&#25968;&#25454;&#20013;&#33258;&#21160;&#29983;&#25104;&#20154;&#31867;&#24515;&#33039;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#24182;&#19988;&#29305;&#21035;&#24378;&#35843;&#20854;&#33021;&#22815;&#29983;&#25104;&#34180;&#22721;&#24515;&#33039;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#19968;&#20010;&#27169;&#26495;&#32593;&#26684;&#24418;&#21464;&#21040;&#32473;&#23450;&#22270;&#20687;&#30340;&#24515;&#33039;&#32467;&#26500;&#19978;&#12290;&#19982;&#20197;&#21069;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#26368;&#23567;&#21270;&#32593;&#26684;&#33258;&#31359;&#36879;&#65292;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#24418;&#21464;&#30340;&#34920;&#38754;&#32593;&#26684;&#20043;&#38388;&#30340;&#23567;&#36317;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20004;&#38454;&#27573;&#30340;&#24418;&#21464;&#36807;&#31243;&#21644;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#21160;&#21147;&#23398;&#23548;&#20986;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#24809;&#32602;&#34920;&#38754;&#25509;&#35302;&#21644;&#30456;&#20114;&#28183;&#36879;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20135;&#29983;&#19981;&#33258;&#30456;&#20132;&#30340;&#32593;&#26684;&#12290;&#20135;&#29983;&#30340;&#32593;&#26684;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#25311;&#20013;&#21487;&#30452;&#25509;&#20351;&#29992;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#21518;&#22788;&#29702;&#21644;&#28165;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning model to automatically generate computer models of the human heart from patient imaging data with an emphasis on its capability to generate thin-walled cardiac structures. Our method works by deforming a template mesh to fit the cardiac structures to the given image. Compared with prior deep learning methods that adopted this approach, our framework is designed to minimize mesh self-penetration, which typically arises when deforming surface meshes separated by small distances. We achieve this by using a two-stage diffeomorphic deformation process along with a novel loss function derived from the kinematics of motion that penalizes surface contact and interpenetration. Our model demonstrates comparable accuracy with state-of-the-art methods while additionally producing meshes free of self-intersections. The resultant meshes are readily usable in physics based simulation, minimizing the need for post-processing and cleanup.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#29992;&#21435;&#22122;&#32593;&#32476;&#22312;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#19979;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20064</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#30450;&#30446;&#30340;&#22810;&#20998;&#24067;&#22122;&#22768;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
A Scalable Training Strategy for Blind Multi-Distribution Noise Removal. (arXiv:2310.20064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20064
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#29992;&#21435;&#22122;&#32593;&#32476;&#22312;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#19979;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#24320;&#21457;&#36890;&#29992;&#30340;&#21435;&#22122;&#21644;&#21435;&#20266;&#24433;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#22266;&#23450;&#30340;&#32593;&#32476;&#26435;&#37325;&#65292;&#19968;&#20010;&#20219;&#21153;&#65288;&#20363;&#22914;&#21435;&#38500;&#27850;&#26494;&#22122;&#22768;&#65289;&#30340;&#19987;&#38376;&#21270;&#19982;&#21478;&#19968;&#20010;&#20219;&#21153;&#65288;&#20363;&#22914;&#21435;&#38500;&#26001;&#28857;&#22122;&#22768;&#65289;&#30340;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#22825;&#28982;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65306;&#38543;&#30528;&#35268;&#26684;&#31354;&#38388;&#30340;&#32500;&#24230;&#22686;&#21152;&#65288;&#21363;&#38656;&#35201;&#25551;&#36848;&#22122;&#22768;&#20998;&#24067;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#65289;&#65292;&#38656;&#35201;&#35757;&#32451;&#30340;&#21807;&#19968;&#35268;&#26684;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22343;&#21248;&#37319;&#26679;&#36825;&#20010;&#31354;&#38388;&#20250;&#23548;&#33268;&#32593;&#32476;&#22312;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35268;&#26684;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#38382;&#39064;&#35268;&#26684;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21363;&#20351;&#22823;&#35823;&#24046;&#20063;&#23545;&#24635;&#20307;&#22343;&#26041;&#35823;&#24046;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, developing general-purpose universal denoising and artifact-removal networks remains largely an open problem: Given fixed network weights, one inherently trades-off specialization at one task (e.g.,~removing Poisson noise) for performance at another (e.g.,~removing speckle noise). In addition, training such a network is challenging due to the curse of dimensionality: As one increases the dimensions of the specification-space (i.e.,~the number of parameters needed to describe the noise distribution) the number of unique specifications one needs to train for grows exponentially. Uniformly sampling this space will result in a network that does well at very challenging problem specifications but poorly at easy problem specifications, where even large errors will have a small effect on the overall mean squared error.  In this work we propose training denoising networks using an adaptive-sampling/active-learning strategy. Our work improves upon a recently proposed un
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31995;&#32479;&#65292;&#20351;&#30495;&#23454;&#25968;&#25454;&#30340;&#36129;&#29486;&#32773;&#33021;&#22815;&#21442;&#19982;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.20062</link><description>&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation. (arXiv:2310.20062v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20062
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31995;&#32479;&#65292;&#20351;&#30495;&#23454;&#25968;&#25454;&#30340;&#36129;&#29486;&#32773;&#33021;&#22815;&#21442;&#19982;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#26041;&#24335;&#22312;&#38477;&#20302;&#38544;&#31169;&#39118;&#38505;&#30340;&#21516;&#26102;&#21457;&#25381;&#25968;&#25454;&#20215;&#20540;&#12290;&#21512;&#25104;&#25968;&#25454;&#30340;&#28508;&#21147;&#19981;&#20165;&#23616;&#38480;&#20110;&#38544;&#31169;&#21451;&#22909;&#30340;&#25968;&#25454;&#21457;&#24067;&#65292;&#36824;&#21253;&#25324;&#22312;&#22521;&#35757;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31561;&#20351;&#29992;&#26696;&#20363;&#20013;&#34917;&#20805;&#30495;&#23454;&#25968;&#25454;&#65292;&#20351;&#20854;&#26356;&#20844;&#24179;&#12289;&#26356;&#33021;&#25269;&#25239;&#20998;&#24067;&#36716;&#21464;&#31561;&#12290;&#23545;&#20110;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#32479;&#35745;&#20445;&#35777;&#20197;&#21450;&#26356;&#22909;&#22320;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#31639;&#27861;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36127;&#36131;&#20219;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#35828;&#65292;&#20165;&#20851;&#27880;&#36825;&#20123;&#31639;&#27861;&#26041;&#38754;&#26159;&#19981;&#22815;&#30340;&#65292;&#32780;&#24212;&#35813;&#32771;&#34385;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#30340;&#25972;&#20307;&#35270;&#35282;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20801;&#35768;&#30495;&#23454;&#25968;&#25454;&#30340;&#36129;&#29486;&#32773;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#20540;&#24471;&#20449;&#36182;&#30340;&#20013;&#24515;&#30340;&#24773;&#20917;&#19979;&#33258;&#20027;&#21442;&#19982;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#12289;&#36890;&#29992;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Synthetic data is emerging as a promising way to harness the value of data, while reducing privacy risks. The potential of synthetic data is not limited to privacy-friendly data release, but also includes complementing real data in use-cases such as training machine learning algorithms that are more fair and robust to distribution shifts etc. There is a lot of interest in algorithmic advances in synthetic data generation for providing better privacy and statistical guarantees and for its better utilisation in machine learning pipelines. However, for responsible and trustworthy synthetic data generation, it is not sufficient to focus only on these algorithmic aspects and instead, a holistic view of the synthetic data generation pipeline must be considered. We build a novel system that allows the contributors of real data to autonomously participate in differentially private synthetic data generation without relying on a trusted centre. Our modular, general and scalable solution is based
&lt;/p&gt;</description></item><item><title>AdaSub&#26159;&#19968;&#31181;&#20351;&#29992;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#25628;&#32034;&#30340;&#23376;&#31354;&#38388;&#32500;&#24230;&#26469;&#31649;&#29702;&#35745;&#31639;&#24320;&#38144;&#21644;&#31639;&#27861;&#25928;&#29575;&#12290;&#21021;&#27493;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;AdaSub&#22312;&#26102;&#38388;&#21644;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.20060</link><description>&lt;p&gt;
AdaSub&#65306;&#20351;&#29992;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces. (arXiv:2310.20060v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20060
&lt;/p&gt;
&lt;p&gt;
AdaSub&#26159;&#19968;&#31181;&#20351;&#29992;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#25628;&#32034;&#30340;&#23376;&#31354;&#38388;&#32500;&#24230;&#26469;&#31649;&#29702;&#35745;&#31639;&#24320;&#38144;&#21644;&#31639;&#27861;&#25928;&#29575;&#12290;&#21021;&#27493;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;AdaSub&#22312;&#26102;&#38388;&#21644;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;AdaSub&#65292;&#19968;&#31181;&#22522;&#20110;&#20302;&#32500;&#33258;&#36866;&#24212;&#23450;&#20041;&#30340;&#20108;&#38454;&#20449;&#24687;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#12290;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#20108;&#38454;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#20294;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35745;&#31639;Hessian&#30697;&#38453;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20351;&#20854;&#19981;&#23454;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#25628;&#32034;&#30340;&#23376;&#31354;&#38388;&#32500;&#24230;&#26469;&#31649;&#29702;&#35745;&#31639;&#24320;&#38144;&#21644;&#31639;&#27861;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#22312;GitHub&#19978;&#20813;&#36153;&#25552;&#20379;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;AdaSub&#22312;&#36798;&#21040;&#32473;&#23450;&#31934;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AdaSub, a stochastic optimization algorithm that computes a search direction based on second-order information in a low-dimensional subspace that is defined adaptively based on available current and past information. Compared to first-order methods, second-order methods exhibit better convergence characteristics, but the need to compute the Hessian matrix at each iteration results in excessive computational expenses, making them impractical. To address this issue, our approach enables the management of computational expenses and algorithm efficiency by enabling the selection of the subspace dimension for the search. Our code is freely available on GitHub, and our preliminary numerical results demonstrate that AdaSub surpasses popular stochastic optimizers in terms of time and number of iterations required to reach a given accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Hamiltonian Monte Carlo&#26041;&#27861;&#20272;&#35745;&#26368;&#20248;PAC-Bayes&#30028;&#38480;&#65292;&#30740;&#31350;&#20102;&#36890;&#36807;&#38480;&#21046;&#21518;&#39564;&#20998;&#24067;&#20026;&#22240;&#23376;&#21270;&#39640;&#26031;&#20998;&#24067;&#22312;&#20248;&#21270;PAC-Bayes&#30028;&#38480;&#26041;&#38754;&#25152;&#22833;&#21435;&#30340;&#32039;&#33268;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#33719;&#24471;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23384;&#22312;&#26174;&#33879;&#30340;&#32039;&#33268;&#24230;&#24046;&#36317;&#65292;&#39640;&#36798;5-6&#65285;&#12290;</title><link>http://arxiv.org/abs/2310.20053</link><description>&lt;p&gt;
&#20351;&#29992;Hamiltonian Monte Carlo&#26041;&#27861;&#20272;&#35745;&#26368;&#20248;PAC-Bayes&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo. (arXiv:2310.20053v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Hamiltonian Monte Carlo&#26041;&#27861;&#20272;&#35745;&#26368;&#20248;PAC-Bayes&#30028;&#38480;&#65292;&#30740;&#31350;&#20102;&#36890;&#36807;&#38480;&#21046;&#21518;&#39564;&#20998;&#24067;&#20026;&#22240;&#23376;&#21270;&#39640;&#26031;&#20998;&#24067;&#22312;&#20248;&#21270;PAC-Bayes&#30028;&#38480;&#26041;&#38754;&#25152;&#22833;&#21435;&#30340;&#32039;&#33268;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#33719;&#24471;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23384;&#22312;&#26174;&#33879;&#30340;&#32039;&#33268;&#24230;&#24046;&#36317;&#65292;&#39640;&#36798;5-6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PAC-Bayes&#25991;&#29486;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#22312;&#20248;&#21270;PAC-Bayes&#30028;&#38480;&#26102;&#65292;&#36890;&#36807;&#38480;&#21046;&#21518;&#39564;&#20998;&#24067;&#20026;&#22240;&#23376;&#21270;&#39640;&#26031;&#20998;&#24067;&#65292;&#25105;&#20204;&#22833;&#21435;&#20102;&#22810;&#23569;&#32039;&#33268;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#29420;&#31435;&#20110;&#25968;&#25454;&#30340;PAC-Bayes&#30028;&#38480;&#26469;&#35843;&#26597;&#27492;&#38382;&#39064;&#65292;&#20351;&#29992;&#26368;&#20248;&#30340;&#21518;&#39564;&#19982;&#20351;&#29992;MFVI&#24471;&#21040;&#30340;&#30028;&#38480;&#36827;&#34892;&#27604;&#36739;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;Hamiltonian Monte Carlo&#20174;&#26368;&#20248;&#30340;Gibbs&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#29992;&#28909;&#21147;&#23398;&#31215;&#20998;&#20272;&#35745;&#20854;&#19982;&#20808;&#39564;&#30340;KL&#25955;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#33719;&#24471;&#39640;&#27010;&#29575;&#30028;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#32039;&#33268;&#24230;&#24046;&#36317;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#36798;5-6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important yet underexplored question in the PAC-Bayes literature is how much tightness we lose by restricting the posterior family to factorized Gaussian distributions when optimizing a PAC-Bayes bound. We investigate this issue by estimating data-independent PAC-Bayes bounds using the optimal posteriors, comparing them to bounds obtained using MFVI. Concretely, we (1) sample from the optimal Gibbs posterior using Hamiltonian Monte Carlo, (2) estimate its KL divergence from the prior with thermodynamic integration, and (3) propose three methods to obtain high-probability bounds under different assumptions. Our experiments on the MNIST dataset reveal significant tightness gaps, as much as 5-6\% in some cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#20854;&#19982;&#20256;&#32479;softmax&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.20051</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expressibility of Polynomial based Attention Scheme. (arXiv:2310.20051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#20854;&#19982;&#20256;&#32479;softmax&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26174;&#33879;&#25913;&#21892;&#20102;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36825;&#20123;&#27169;&#22411;&#24433;&#21709;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#25945;&#32946;&#65292;&#25552;&#39640;&#20102;&#29983;&#20135;&#21147;&#12289;&#20915;&#31574;&#36807;&#31243;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#20869;&#23481;&#26102;&#65292;transformer&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#20351;&#24471;&#22312;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#25110;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#39640;&#25928;&#22320;&#20351;&#29992;&#23427;&#20204;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#26368;&#36817;&#30001;[KMZ23]&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#26367;&#25442;softmax&#20197;&#21152;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25216;&#26415;&#65292;&#20294;&#23545;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#19981;&#23436;&#21892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22810;&#39033;&#24335;&#27880;&#24847;&#21147;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#39033;&#24335;&#27880;&#24847;&#21147;&#33021;&#21147;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.  In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20049</link><description>&lt;p&gt;
SURF: GNN&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#23545;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#28085;&#30422;&#20102;&#20174;&#31616;&#21333;&#38400;&#38376;&#21040;&#22797;&#26434;&#28065;&#36718;&#26426;&#26800;&#30340;&#33539;&#22260;&#12290;&#20934;&#30830;&#27714;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#26041;&#31243;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#30495;&#27491;&#29702;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#27867;&#21270;&#32780;&#38750;&#25554;&#20540;&#12290;&#27867;&#21270;&#26159;&#36890;&#29992;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12289;&#20998;&#36776;&#29575;&#25110;&#28909;&#21147;&#23398;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SURF&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#27979;&#35797;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;SURF&#21253;&#25324;&#21508;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#36817;&#20284;&#26041;&#27861;&#21644;&#21033;&#29992;&#23545;&#31216;&#31354;&#38388;&#30340;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#35745;&#31639;&#21644;&#22312;&#39640;&#32500;&#20219;&#21153;&#19978;&#30340;&#25193;&#23637;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20030</link><description>&lt;p&gt;
&#32553;&#25918;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Riemannian Diffusion Models. (arXiv:2310.20030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#36817;&#20284;&#26041;&#27861;&#21644;&#21033;&#29992;&#23545;&#31216;&#31354;&#38388;&#30340;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#35745;&#31639;&#21644;&#22312;&#39640;&#32500;&#20219;&#21153;&#19978;&#30340;&#25193;&#23637;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#20174;&#26631;&#20934;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23398;&#20064;&#22312;&#19968;&#33324;&#27969;&#24418;&#19978;&#30340;&#20998;&#24067;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#39069;&#22806;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#20351;&#24471;&#25193;&#25955;&#36807;&#28193;&#39033;&#26080;&#27861;&#29992;&#38381;&#24335;&#34920;&#36798;&#24335;&#34920;&#31034;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#20351;&#29992;&#31895;&#30053;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#38477;&#20302;&#24615;&#33021;&#24182;&#38459;&#27490;&#22312;&#39640;&#32500;&#24230;&#19979;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#36825;&#20123;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20960;&#20010;&#23454;&#29992;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22823;&#22810;&#25968;&#30456;&#20851;&#27969;&#24418;&#26159;&#23545;&#31216;&#31354;&#38388;&#65292;&#36825;&#20123;&#23545;&#31216;&#31354;&#38388;&#22312;&#35745;&#31639;&#19978;&#26356;&#23481;&#26131;&#22788;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#21644;&#32452;&#21512;&#21508;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#30456;&#20851;&#30340;&#37327;&#21040;&#39640;&#31934;&#24230;&#12290;&#22312;&#20302;&#32500;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26657;&#27491;&#20135;&#29983;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#20351;&#25193;&#25955;&#33021;&#22815;&#19982;&#20854;&#20182;&#26041;&#27861;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#38750;&#24179;&#20961;&#27969;&#24418;&#19978;&#25193;&#23637;&#21040;&#39640;&#32500;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian diffusion models draw inspiration from standard Euclidean space diffusion models to learn distributions on general manifolds. Unfortunately, the additional geometric complexity renders the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation. By leveraging and combining various ans\"{a}tze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we mo
&lt;/p&gt;</description></item><item><title>GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20025</link><description>&lt;p&gt;
GOPlan:&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20025
&lt;/p&gt;
&lt;p&gt;
GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#20026;&#20174;&#22810;&#26679;&#21270;&#21644;&#22810;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36890;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20027;&#23548;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#20173;&#28982;&#21463;&#38480;&#20110;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#23545;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;Goal-conditioned Offline Planning&#65288;GOPlan&#65289;&#65292;&#21253;&#25324;&#65288;1&#65289;&#39044;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#22810;&#27169;&#24577;&#21160;&#20316;&#20998;&#24067;&#30340;&#20808;&#39564;&#31574;&#30053;&#65307;&#65288;2&#65289;&#21033;&#29992;&#35268;&#21010;&#30340;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#20026;&#24494;&#35843;&#31574;&#30053;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20808;&#39564;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#20998;&#31163;&#30340;&#24102;&#20248;&#21183;&#26435;&#37325;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21160;&#20316;&#30340;&#32570;&#28857;&#12290;&#20026;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#65292;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23481;&#38169;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#30340;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#25925;&#38556;&#21644;&#21518;&#25925;&#38556;&#30340;&#20004;&#20010;&#19981;&#21516;&#39044;&#27979;&#36335;&#24452;&#39044;&#27979;&#20856;&#22411;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#20339;&#31574;&#30053;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2310.20024</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#30340;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23481;&#38169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach. (arXiv:2310.20024v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23481;&#38169;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#30340;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#25925;&#38556;&#21644;&#21518;&#25925;&#38556;&#30340;&#20004;&#20010;&#19981;&#21516;&#39044;&#27979;&#36335;&#24452;&#39044;&#27979;&#20856;&#22411;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#20339;&#31574;&#30053;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#20013;&#30340;&#25925;&#38556;&#21487;&#33021;&#20250;&#20005;&#37325;&#25200;&#20081;&#20854;&#25299;&#25169;&#32467;&#26500;&#65292;&#23548;&#33268;&#20854;&#23376;&#38598;&#20043;&#38388;&#22833;&#21435;&#36830;&#25509;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#65292;&#36827;&#34892;&#26368;&#20339;&#25299;&#25169;&#21512;&#25104;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#19988;&#32791;&#26102;&#30340;&#65292;&#38590;&#20197;&#23454;&#26102;&#23436;&#25104;&#12290;&#21482;&#26377;&#24403;&#20219;&#20309;&#25925;&#38556;&#21457;&#29983;&#21518;&#25299;&#25169;&#24674;&#22797;&#30340;&#27010;&#29575;&#36229;&#36807;&#19981;&#21487;&#24674;&#22797;&#30340;&#27010;&#29575;&#26102;&#65292;&#25165;&#24212;&#35813;&#25191;&#34892;&#25299;&#25169;&#37325;&#26032;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#27492;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#36125;&#21494;&#26031;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#25925;&#38556;&#21069;&#21644;&#25925;&#38556;&#21518;&#39044;&#27979;&#36335;&#24452;&#26469;&#39044;&#27979;&#20856;&#22411;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#36335;&#24452;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#19982;&#25991;&#29486;&#20013;&#26368;&#20339;&#30340;&#24403;&#21069;&#31574;&#30053;&#30456;&#27604;&#65292;&#26126;&#30830;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#25299;&#25169;&#65288;&#19981;&#65289;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#38382;&#39064;&#26041;&#38754;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faults occurring in ad-hoc robot networks may fatally perturb their topologies leading to disconnection of subsets of those networks. Optimal topology synthesis is generally resource-intensive and time-consuming to be done in real time for large ad-hoc robot networks. One should only perform topology re-computations if the probability of topology recoverability after the occurrence of any fault surpasses that of its irrecoverability. We formulate this problem as a binary classification problem. Then, we develop a two-pathway data-driven model based on Bayesian Gaussian mixture models that predicts the solution to a typical problem by two different pre-fault and post-fault prediction pathways. The results, obtained by the integration of the predictions of those pathways, clearly indicate the success of our model in solving the topology (ir)recoverability prediction problem compared to the best of current strategies found in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#22810;&#23610;&#24230;&#36974;&#25377;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24322;&#24120;&#28857;&#65292;&#24182;&#19988;&#22312;Dark Energy Survey Instrument&#20013;&#30340;&#26143;&#31995;&#20809;&#35889;&#24322;&#24120;&#28857;&#19978;&#21462;&#24471;&#20102;&#26356;&#26131;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20012</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#29305;&#24449;&#24402;&#22240;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiscale Feature Attribution for Outliers. (arXiv:2310.20012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#22810;&#23610;&#24230;&#36974;&#25377;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24322;&#24120;&#28857;&#65292;&#24182;&#19988;&#22312;Dark Energy Survey Instrument&#20013;&#30340;&#26143;&#31995;&#20809;&#35889;&#24322;&#24120;&#28857;&#19978;&#21462;&#24471;&#20102;&#26356;&#26131;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#35782;&#21035;&#28023;&#37327;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#24120;&#28857;&#65292;&#27604;&#20154;&#24037;&#26816;&#26597;&#35201;&#24555;&#24471;&#22810;&#12290;&#20294;&#26159;&#65292;&#19968;&#26086;&#21457;&#29616;&#36825;&#20123;&#24322;&#24120;&#28857;&#65292;&#24456;&#24555;&#23601;&#20250;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064;&#65306;&#21738;&#20123;&#29305;&#24449;&#20351;&#24471;&#36825;&#20010;&#36755;&#20837;&#25104;&#20026;&#24322;&#24120;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;&#36870;&#22810;&#23610;&#24230;&#36974;&#25377;&#65292;&#19987;&#20026;&#24322;&#24120;&#28857;&#35774;&#35745;&#65292;&#22240;&#20026;&#25105;&#20204;&#23545;&#35201;&#35782;&#21035;&#30340;&#29305;&#24449;&#31867;&#22411;&#20102;&#35299;&#24456;&#23569;&#65292;&#24182;&#19988;&#39044;&#35745;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#22240;&#20026;&#24322;&#24120;&#30340;&#27979;&#35797;&#25968;&#25454;&#24456;&#21487;&#33021;&#36229;&#36807;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;Dark Energy Survey Instrument&#26816;&#27979;&#21040;&#30340;&#26143;&#31995;&#20809;&#35889;&#24322;&#24120;&#28857;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#32467;&#26524;&#27604;&#20854;&#20182;&#24402;&#22240;&#26041;&#27861;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques can automatically identify outliers in massive datasets, much faster and more reproducible than human inspection ever could. But finding such outliers immediately leads to the question: which features render this input anomalous? We propose a new feature attribution method, Inverse Multiscale Occlusion, that is specifically designed for outliers, for which we have little knowledge of the type of features we want to identify and expect that the model performance is questionable because anomalous test data likely exceed the limits of the training data. We demonstrate our method on outliers detected in galaxy spectra from the Dark Energy Survey Instrument and find its results to be much more interpretable than alternative attribution approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.20007</link><description>&lt;p&gt;
&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20013;&#27748;&#26222;&#26862;&#37319;&#26679;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#65292;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#31163;&#25955;&#30340;&#20195;&#29702;&#29615;&#22659;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#19968;&#33268;&#24615;&#23545;&#20449;&#24687;&#27604;&#36827;&#34892;&#20102;&#31934;&#30830;&#20998;&#26512;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#20026;$\widetilde{O}(H\sqrt{d_{l_1}T})$&#65292;&#20854;&#20013;$H$&#20026;&#22238;&#21512;&#38271;&#24230;&#65292;$d_{l_1}$&#20026;&#29615;&#22659;&#31354;&#38388;&#30340;Kolmogorov $l_1$&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#25214;&#21040;&#20102;$d_{l_1}$&#30340;&#20855;&#20307;&#30028;&#38480;&#65292;&#27604;&#22914;&#34920;&#26684;&#12289;&#32447;&#24615;&#21644;&#26377;&#38480;&#28151;&#21512;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
&lt;/p&gt;</description></item><item><title>PolyThrottle&#26159;&#19968;&#31181;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#33410;&#33021;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#22791;&#19978;&#30340;&#30828;&#20214;&#20803;&#32032;&#37197;&#32622;&#65292;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;36%&#30340;&#33021;&#37327;&#33410;&#30465;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.19991</link><description>&lt;p&gt;
PolyThrottle: &#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#33410;&#33021;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices. (arXiv:2310.19991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19991
&lt;/p&gt;
&lt;p&gt;
PolyThrottle&#26159;&#19968;&#31181;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#33410;&#33021;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#22791;&#19978;&#30340;&#30828;&#20214;&#20803;&#32032;&#37197;&#32622;&#65292;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;36%&#30340;&#33021;&#37327;&#33410;&#30465;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37096;&#32626;&#65292;&#20854;&#33021;&#37327;&#38656;&#27714;&#20063;&#30456;&#24212;&#22686;&#38271;&#12290;&#23613;&#31649;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#65292;&#20294;&#26159;ML&#39537;&#21160;&#31995;&#32479;&#30340;&#36830;&#32493;&#36816;&#34892;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#35774;&#22791;&#19978;&#30340;&#30828;&#20214;&#20803;&#32032;&#37197;&#32622;&#65288;&#22914;GPU&#12289;&#20869;&#23384;&#21644;CPU&#39057;&#29575;&#65289;&#22312;&#24120;&#35268;&#24494;&#35843;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20013;&#22914;&#20309;&#24433;&#21709;&#33021;&#37327;&#28040;&#32791;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PolyThrottle&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20197;&#33410;&#33021;&#30340;&#26041;&#24335;&#23545;&#21508;&#20010;&#30828;&#20214;&#32452;&#20214;&#30340;&#37197;&#32622;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25581;&#31034;&#20102;&#33021;&#37327;&#24615;&#33021;&#24179;&#34913;&#30340;&#26032;&#39062;&#26041;&#38754;&#65292;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20026;&#27969;&#34892;&#27169;&#22411;&#33410;&#30465;&#39640;&#36798;36%&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;PolyThrottle&#21487;&#20197;&#22312;&#28385;&#36275;&#24212;&#29992;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#24555;&#36895;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31105;&#24524;&#25628;&#32034;&#30340;&#31616;&#21333;&#23398;&#20064;&#21551;&#21457;&#24335;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19990</link><description>&lt;p&gt;
&#25581;&#31034;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30340;&#23616;&#38480;&#24615;: &#20320;&#26159;&#26368;&#24378;&#22823;&#30340;&#28201;&#39034;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?. (arXiv:2310.19990v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31105;&#24524;&#25628;&#32034;&#30340;&#31616;&#21333;&#23398;&#20064;&#21551;&#21457;&#24335;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30456;&#32467;&#21512;&#24050;&#32463;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#21464;&#24471;&#27969;&#34892;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#20294;&#23427;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#24037;&#31243;&#25237;&#20837;&#19979;&#23637;&#29616;&#20986;&#20102;&#24456;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20123;&#25972;&#21512;&#23581;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#20013;&#31561;&#22797;&#26434;&#24615;&#21644;&#24369;&#22522;&#32447;&#30340;&#24773;&#20917;&#22312;&#20934;&#30830;&#35780;&#20272;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#26102;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20854;&#27425;&#65292;&#32570;&#20047;&#28040;&#34701;&#30740;&#31350;&#20351;&#24471;&#20934;&#30830;&#22320;&#37327;&#21270;&#21644;&#24402;&#22240;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#25913;&#36827;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#21518;&#65292;&#22312;&#19981;&#21516;&#20998;&#24067;&#19979;&#23398;&#20064;&#21551;&#21457;&#24335;&#30340;&#27867;&#21270;&#24615;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#34987;&#35782;&#21035;&#20986;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#31105;&#24524;&#25628;&#32034;&#30340;&#31616;&#21333;&#23398;&#20064;&#21551;&#21457;&#24335;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has exhibited promising outcomes with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heurist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#34920;&#29616;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#32593;&#32476;&#25628;&#32034;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23376;&#31867;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19986</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#25628;&#32034;&#21644;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24369;&#20915;&#31574;&#36793;&#30028;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models. (arXiv:2310.19986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#34920;&#29616;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#32593;&#32476;&#25628;&#32034;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23376;&#31867;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#20247;&#25152;&#21608;&#30693;&#23384;&#22312;&#20262;&#29702;&#21644;&#36816;&#33829;&#38382;&#39064;&#65292;&#28982;&#32780;&#25105;&#20204;&#30446;&#30585;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20225;&#19994;&#33268;&#21147;&#20110;&#22312;&#25935;&#24863;&#24212;&#29992;&#20013;&#37096;&#32626;&#23427;&#20204;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;ML&#27169;&#22411;&#23545;&#20110;&#20195;&#34920;&#23569;&#25968;&#32676;&#20307;&#30340;&#26679;&#26412;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#36825;&#20351;&#24471;&#24369;&#21183;&#32676;&#20307;&#22788;&#20110;&#26356;&#19981;&#21033;&#21644;&#19981;&#20844;&#24179;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32593;&#32476;&#25628;&#32034;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#21028;&#21035;&#27169;&#22411;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;ImageNet&#30340;&#20154;&#31867;&#23376;&#26641;&#23376;&#38598;&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#23545;&#26576;&#20123;&#20195;&#34920;&#24369;&#21183;&#32676;&#20307;&#65288;&#20363;&#22914;&#65292;&#26377;&#33394;&#20154;&#31181;&#22899;&#24615;&#21307;&#29983;&#65289;&#30340;&#31867;&#21035;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#33021;&#22815;&#65306;&#65288;1&#65289;&#35782;&#21035;&#36825;&#20123;&#31867;&#21035;&#30340;&#24369;&#20915;&#31574;&#36793;&#30028;&#65307;&#65288;2&#65289;&#20026;&#35895;&#27468;&#26500;&#24314;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;DALL-E 2&#21644;&#31283;&#23450;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) technologies are known to be riddled with ethical and operational problems, however, we are witnessing an increasing thrust by businesses to deploy them in sensitive applications. One major issue among many is that ML models do not perform equally well for underrepresented groups. This puts vulnerable populations in an even disadvantaged and unfavorable position. We propose an approach that leverages the power of web search and generative models to alleviate some of the shortcomings of discriminative models. We demonstrate our method on an image classification problem using ImageNet's People Subtree subset, and show that it is effective in enhancing robustness and mitigating bias in certain classes that represent vulnerable populations (e.g., female doctor of color). Our new method is able to (1) identify weak decision boundaries for such classes; (2) construct search queries for Google as well as text for generating images through DALL-E 2 and Stable Diffusion; a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;Frank-Wolfe&#31639;&#27861;&#26469;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31232;&#30095;&#36755;&#20837;&#25968;&#25454;&#19978;&#26377;&#25928;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.19978</link><description>&lt;p&gt;
&#36890;&#36807;&#26356;&#24555;&#30340;Frank-Wolfe&#36845;&#20195;&#26041;&#27861;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;LASSO&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#30340;&#25193;&#23637;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations. (arXiv:2310.19978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;Frank-Wolfe&#31639;&#27861;&#26469;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31232;&#30095;&#36755;&#20837;&#25968;&#25454;&#19978;&#26377;&#25928;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#26041;&#27861;&#21487;&#20197;&#22312;&#31232;&#30095;&#36755;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#22238;&#24402;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;$ L_1 $&#24809;&#32602;&#32447;&#24615;&#22238;&#24402;&#30340;Frank-Wolfe&#31639;&#27861;&#36866;&#24212;&#20110;&#31232;&#30095;&#36755;&#20837;&#65292;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;$ \mathcal {O}&#65288;TDS + TNS&#65289;$&#20943;&#23569;&#21040;$ \mathcal {O}&#65288;NS + T \sqrt {D} \log {D} + TS ^ 2&#65289;$&#65292;&#20854;&#20013;$ T $&#26159;&#36845;&#20195;&#27425;&#25968;&#65292;&#32780;$ N $&#26159;&#25968;&#25454;&#38598;&#30340;&#34892;&#25968;&#65292;$ D $&#26159;&#29305;&#24449;&#25968;&#65292;$ S $&#26159;&#31232;&#30095;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#23558;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#22810;&#36798;$ 2,200 \times $&#65292;&#36825;&#21462;&#20915;&#20110;&#38544;&#31169;&#21442;&#25968;$ \epsilon $&#30340;&#20540;&#21644;&#25968;&#25454;&#38598;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from $\mathcal{O}( T D S + T N S)$ to $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our results demonstrate that this procedure can reduce runtime by a factor of up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$ and the sparsity of the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#25913;&#36827;&#20102;&#27927;&#29260;&#27169;&#22411;&#21644;DP-GD&#20013;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#38544;&#31169;&#36793;&#30028;&#65292;&#25240;&#34935;&#20989;&#25968;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#20248;&#20110;$(\epsilon,\delta)$-DP&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38543;&#26426;&#21021;&#22987;&#21270;&#21487;&#20197;&#22686;&#24378;DP-GD&#30340;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19973</link><description>&lt;p&gt;
&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#32479;&#19968;&#22686;&#24378;&#28151;&#21512;&#26426;&#21046;&#30340;&#38544;&#31169;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. (arXiv:2310.19973v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#25913;&#36827;&#20102;&#27927;&#29260;&#27169;&#22411;&#21644;DP-GD&#20013;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#38544;&#31169;&#36793;&#30028;&#65292;&#25240;&#34935;&#20989;&#25968;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#20248;&#20110;$(\epsilon,\delta)$-DP&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38543;&#26426;&#21021;&#22987;&#21270;&#21487;&#20197;&#22686;&#24378;DP-GD&#30340;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20250;&#20135;&#29983;&#35768;&#22810;&#38543;&#26426;&#24615;&#65292;&#22914;&#38543;&#26426;&#21021;&#22987;&#21270;&#12289;&#38543;&#26426;&#25209;&#27425;&#25277;&#26679;&#21644;&#27927;&#29260;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#38543;&#26426;&#24615;&#20250;&#23548;&#33268;&#38590;&#20197;&#20998;&#26512;&#30340;&#28151;&#21512;&#20998;&#24067;&#65292;&#25152;&#20197;&#22312;&#35777;&#26126;&#24046;&#20998;&#38544;&#31169;&#36793;&#30028;&#26102;&#24456;&#38590;&#23558;&#20854;&#32435;&#20837;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#27927;&#29260;&#27169;&#22411;&#21644;&#19968;&#27425;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#65288;DP-GD&#65289;&#20013;&#29992;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#38544;&#31169;&#36793;&#30028;&#65292;&#37319;&#29992;$f$-DP&#26041;&#27861;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27927;&#29260;&#27169;&#22411;&#30340;&#25240;&#34935;&#20989;&#25968;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#20248;&#20110;&#22522;&#20110;$(\epsilon,\delta)$-DP&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#19968;&#27425;&#36845;&#20195;&#30340;DP-GD&#30340;&#38544;&#31169;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#25240;&#34935;&#20989;&#25968;&#30340;&#25968;&#20540;&#35745;&#31639;&#34920;&#26126;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#21487;&#20197;&#22686;&#24378;DP-GD&#30340;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#28151;&#21512;&#26426;&#21046;&#30340;$f$-DP&#20445;&#35777;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#19968;&#31181;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an ine
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;&#65292;&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#21487;&#20197;&#25913;&#21892;&#36716;&#35786;&#65292;&#20026;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;&#30149;&#24773;&#24694;&#21270;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.19967</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#20197;&#25913;&#21892;&#36716;&#35786;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records. (arXiv:2310.19967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;&#65292;&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#21487;&#20197;&#25913;&#21892;&#36716;&#35786;&#65292;&#20026;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;&#30149;&#24773;&#24694;&#21270;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#65288;IA&#65289;&#23545;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#21307;&#38498;&#36716;&#35786;&#20998;&#27969;&#20197;&#21450;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;IA&#30142;&#30149;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#38480;&#30340;&#21307;&#30103;&#36164;&#28304;&#19979;&#12290;&#25163;&#21160;&#35780;&#20272;&#27969;&#31243;&#26159;&#30446;&#21069;&#23454;&#36341;&#20013;&#26368;&#24120;&#35265;&#30340;&#26089;&#26399;&#26816;&#27979;IA&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#21644;&#20302;&#25928;&#29575;&#12290;&#27599;&#20010;&#20174;&#19968;&#33324;&#23454;&#36341;&#65288;GP&#65289;&#21040;&#21307;&#38498;&#30340;&#36716;&#35786;&#37117;&#38656;&#35201;&#35780;&#20272;&#22823;&#37327;&#30340;&#20020;&#24202;&#20449;&#24687;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#37325;&#22797;&#35780;&#20272;&#20219;&#21153;&#21644;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;IA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IA&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#34880;&#28082;&#27979;&#35797;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#34880;&#28082;&#27979;&#35797;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#22312;&#36716;&#35786;&#26102;&#21487;&#29992;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;IA&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34701;&#21512;&#21644;
&lt;/p&gt;
&lt;p&gt;
Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and en
&lt;/p&gt;</description></item><item><title>ExPT&#26159;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#24212;&#29992;&#20110;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#21644;&#26399;&#26395;&#36755;&#20986;&#65292;&#29983;&#25104;&#26368;&#20248;&#30340;&#36755;&#20837;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#25110;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19961</link><description>&lt;p&gt;
ExPT: &#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ExPT: Synthetic Pretraining for Few-Shot Experimental Design. (arXiv:2310.19961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19961
&lt;/p&gt;
&lt;p&gt;
ExPT&#26159;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#24212;&#29992;&#20110;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#21644;&#26399;&#26395;&#36755;&#20986;&#65292;&#29983;&#25104;&#26368;&#20248;&#30340;&#36755;&#20837;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#25110;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#35774;&#35745;&#26159;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#35774;&#35745;&#35780;&#20272;&#30340;&#26102;&#38388;&#12289;&#37329;&#38065;&#21644;&#23433;&#20840;&#25104;&#26412;&#65292;&#26679;&#26412;&#25928;&#29575;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#65292;&#35201;&#20040;&#20381;&#36182;&#23545;&#36807;&#21435;&#23454;&#39564;&#30340;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#65292;&#20854;&#20013;&#21482;&#26377;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#36755;&#20837;&#35774;&#35745;&#26679;&#26412;&#21450;&#20854;&#30456;&#24212;&#30340;&#25968;&#20540;&#21487;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#30475;&#20316;&#26159;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#26681;&#25454;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#21644;&#26399;&#26395;&#30340;&#36755;&#20986;&#26465;&#20214;&#29983;&#25104;&#26368;&#20248;&#30340;&#36755;&#20837;&#35774;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23454;&#39564;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;ExPT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#21512;&#25104;&#39044;&#35757;&#32451;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#22312;ExPT&#20013;&#65292;&#25105;&#20204;&#21482;&#20551;&#35774;&#23545;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26377;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#22352;&#26631;&#21644;&#25299;&#25169;&#35825;&#23548;&#30340;&#32858;&#31867;&#22312;&#19968;&#31181;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#25299;&#25169;&#20449;&#24687;&#65292;&#20197;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#33021;&#22312;&#26102;&#38388;&#21644;&#36816;&#21160;&#24207;&#21015;&#20998;&#26512;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.19960</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#22352;&#26631;&#20026;&#36816;&#21160;&#25968;&#25454;&#36827;&#34892;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Topological Learning for Motion Data via Mixed Coordinates. (arXiv:2310.19960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#22352;&#26631;&#21644;&#25299;&#25169;&#35825;&#23548;&#30340;&#32858;&#31867;&#22312;&#19968;&#31181;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#25299;&#25169;&#20449;&#24687;&#65292;&#20197;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#33021;&#22312;&#26102;&#38388;&#21644;&#36816;&#21160;&#24207;&#21015;&#20998;&#26512;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#23398;&#21487;&#20197;&#39640;&#25928;&#22320;&#25552;&#21462;&#25968;&#25454;&#38598;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#26412;&#25991;&#35797;&#22270;&#23558;&#25299;&#25169;&#20449;&#24687;&#24341;&#20837;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#36801;&#31227;&#23398;&#20064;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#29615;&#24418;&#22352;&#26631;&#30340;&#26694;&#26550;&#25193;&#23637;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#20540;&#22352;&#26631;&#26694;&#26550;&#65292;&#20197;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32447;&#24615;&#36235;&#21183;&#12290;&#36890;&#36807;&#25299;&#25169;&#35825;&#23548;&#30340;&#32858;&#31867;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#26680;&#24515;&#22312;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#65292;&#26159;&#20174;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20013;&#26377;&#25928;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#36825;&#20010;&#26680;&#24515;&#19981;&#20165;&#34701;&#20837;&#20102;&#25299;&#25169;&#32467;&#26500;&#20449;&#24687;&#65292;&#36824;&#20801;&#35768;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#22788;&#29702;&#26102;&#38388;&#21644;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology can extract the structural information in a dataset efficiently. In this paper, we attempt to incorporate topological information into a multiple output Gaussian process model for transfer learning purposes. To achieve this goal, we extend the framework of circular coordinates into a novel framework of mixed valued coordinates to take linear trends in the time series into consideration.  One of the major challenges to learn from multiple time series effectively via a multiple output Gaussian process model is constructing a functional kernel. We propose to use topologically induced clustering to construct a cluster based kernel in a multiple output Gaussian process model. This kernel not only incorporates the topological structural information, but also allows us to put forward a unified framework using topological information in time and motion series.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21098;&#26525;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25512;&#23548;&#20986;&#20102;&#27844;&#38706;&#20449;&#24687;&#30340;&#19978;&#38480;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.19958</link><description>&lt;p&gt;
PriPrune: &#22312;&#21098;&#26525;&#32852;&#37030;&#23398;&#20064;&#20013;&#37327;&#21270;&#21644;&#20445;&#25252;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21098;&#26525;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25512;&#23548;&#20986;&#20102;&#27844;&#38706;&#20449;&#24687;&#30340;&#19978;&#38480;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#36890;&#36807;&#20165;&#20132;&#25442;&#27169;&#22411;&#26356;&#26032;&#32780;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#33539;&#20363;&#65292;&#32780;&#19981;&#38656;&#35201;&#35774;&#22791;&#20849;&#20139;&#20182;&#20204;&#30340;&#23616;&#37096;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#35774;&#22791;&#22312;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#19968;&#27493;&#20174;&#27169;&#22411;&#21098;&#26525;&#20013;&#21463;&#30410; - &#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#33539;&#20363;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36890;&#36807;&#20351;&#23616;&#37096;&#27169;&#22411;&#26356;&#31895;&#31961;&#65292;&#21098;&#26525;&#39044;&#35745;&#22312;FL&#29615;&#22659;&#20013;&#20063;&#20250;&#25552;&#20379;&#19968;&#23450;&#30340;&#38544;&#31169;&#25915;&#20987;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#27492;&#20445;&#25252;&#36827;&#34892;&#27491;&#24335;&#25110;&#23454;&#39564;&#24615;&#30340;&#29305;&#24449;&#21270;&#65292;&#24182;&#19988;&#19981;&#28165;&#26970;&#23427;&#26159;&#21542;&#36275;&#20197;&#25269;&#24481;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21098;&#26525;FL&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#38556;&#36827;&#34892;&#20102;&#39318;&#27425;&#35843;&#26597;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#21098;&#26525;FL&#27169;&#22411;&#27844;&#38706;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21457;&#29616;&#36827;&#34892;&#20102;&#34917;&#20805;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.  In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with compreh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.19957</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65306;&#26426;&#36935;&#21644;&#25361;&#25112;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges. (arXiv:2310.19957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#23450;&#20301;&#31995;&#32479;&#12289;&#36965;&#24863;&#21644;&#35745;&#31639;&#27169;&#25311;&#30340;&#36827;&#27493;&#65292;&#26469;&#33258;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22823;&#37327;&#26102;&#31354;&#25968;&#25454;&#27491;&#22312;&#20197;&#36234;&#26469;&#36234;&#24555;&#30340;&#36895;&#24230;&#34987;&#25910;&#38598;&#12290;&#28085;&#30422;&#30340;&#24212;&#29992;&#39046;&#22495;&#21253;&#25324;&#22320;&#29699;&#31185;&#23398;&#12289;&#20892;&#19994;&#12289;&#26234;&#24935;&#22478;&#24066;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#12290;&#36825;&#31181;&#26032;&#20852;&#30340;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#31354;&#22823;&#25968;&#25454;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#35299;&#20915;&#20197;&#24448;&#26080;&#27861;&#23454;&#29616;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#20363;&#22914;&#65292;&#36965;&#24863;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#22320;&#29699;&#22270;&#20687;&#22823;&#25968;&#25454;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#20247;&#22810;&#22303;&#22320;&#35206;&#30422;&#21644;&#22303;&#22320;&#21033;&#29992;&#24314;&#27169;&#20219;&#21153;&#12290;&#27839;&#28023;&#27169;&#25311;&#23398;&#23478;&#21487;&#20197;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#26367;&#20195;&#27169;&#22411;&#21152;&#36895;&#25968;&#20540;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#26102;&#31354;&#22823;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#32473;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#23637;&#26395;&#20102;&#26102;&#31354;&#22823;&#25968;&#25454;&#30340;&#21508;&#31181;&#31867;&#22411;&#65292;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#26032;&#30740;&#31350;&#26426;&#20250;&#65292;&#24182;&#21015;&#20030;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advancements in GPS, remote sensing, and computational simulation, an enormous volume of spatiotemporal data is being collected at an increasing speed from various application domains, spanning Earth sciences, agriculture, smart cities, and public safety. Such emerging geospatial and spatiotemporal big data, coupled with recent advances in deep learning technologies, foster new opportunities to solve problems that have not been possible before. For instance, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the distinctive characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19944</link><description>&lt;p&gt;
&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conditional Unscented Autoencoders for Trajectory Prediction. (arXiv:2310.19944v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#26159;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#20013;&#26368;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#23427;&#23558;&#39550;&#39542;&#29615;&#22659;&#21644;&#30495;&#23454;&#26410;&#26469;&#20851;&#31995;&#24314;&#31435;&#22312;&#27010;&#29575;&#38544;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#27492;&#31354;&#38388;&#29983;&#25104;&#39044;&#27979;&#12290;&#26412;&#25991;&#23545;CVAE&#30340;&#20851;&#38190;&#32452;&#20214;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21457;&#29616;&#21464;&#21270;&#37319;&#26679;&#36807;&#31243;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#32447;&#24615;&#37319;&#26679;&#33021;&#22815;&#26356;&#36866;&#21512;&#20110;&#36712;&#36857;&#39044;&#27979;&#65292;&#32780;&#38543;&#26426;&#37319;&#26679;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20854;&#20182;&#25913;&#36827;&#65292;&#21253;&#25324;&#26356;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#38544;&#31354;&#38388;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#12289;&#21487;&#33021;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;CVAE&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;INTERACTION&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#29289;&#36136;&#30693;&#35782;&#33719;&#21462;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20854;&#23398;&#20064;&#36712;&#36857;&#19982;&#20799;&#31461;&#30340;&#21457;&#23637;&#36712;&#36857;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.19943</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#29289;&#36136;&#30693;&#35782;&#33719;&#21462;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Acquisition of Physical Knowledge in Generative Neural Networks. (arXiv:2310.19943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#29289;&#36136;&#30693;&#35782;&#33719;&#21462;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20854;&#23398;&#20064;&#36712;&#36857;&#19982;&#20799;&#31461;&#30340;&#21457;&#23637;&#36712;&#36857;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23401;&#23376;&#20204;&#30340;&#25104;&#38271;&#65292;&#20182;&#20204;&#23545;&#21608;&#22260;&#29289;&#29702;&#36807;&#31243;&#30340;&#30452;&#35266;&#29702;&#35299;&#36880;&#28176;&#21457;&#23637;&#36215;&#26469;&#12290;&#20182;&#20204;&#30340;&#29289;&#29702;&#29702;&#35299;&#20197;&#21457;&#23637;&#36712;&#36857;&#30340;&#26041;&#24335;&#21576;&#29616;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#34987;&#24191;&#27867;&#26144;&#23556;&#20986;&#26469;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#29289;&#36136;&#29702;&#35299;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#30740;&#31350;&#28145;&#24230;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20799;&#31461;&#21457;&#23637;&#36712;&#36857;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20801;&#35768;&#25105;&#20204;&#26816;&#39564;&#20154;&#31867;&#21457;&#23637;&#30340;&#20004;&#20010;&#19981;&#21516;&#20551;&#35774; - &#38543;&#26426;&#20248;&#21270;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19968;&#20123;&#29289;&#29702;&#36807;&#31243;&#65292;&#20294;&#23427;&#20204;&#22312;&#20004;&#20010;&#20551;&#35774;&#19979;&#30340;&#23398;&#20064;&#36712;&#36857;&#24182;&#19981;&#19982;&#20799;&#31461;&#30340;&#21457;&#23637;&#36712;&#36857;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Split-NER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#20998;&#25104;&#25552;&#21462;&#23454;&#20307;&#25552;&#21450;&#36328;&#24230;&#21644;&#36328;&#24230;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#21033;&#29992;&#38382;&#31572;&#27169;&#22411;&#35299;&#20915;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.19942</link><description>&lt;p&gt;
Split-NER: &#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#20998;&#31867;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications. (arXiv:2310.19942v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Split-NER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#20998;&#25104;&#25552;&#21462;&#23454;&#20307;&#25552;&#21450;&#36328;&#24230;&#21644;&#36328;&#24230;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#21033;&#29992;&#38382;&#31572;&#27169;&#22411;&#35299;&#20915;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#20998;&#25104;&#20004;&#20010;&#36923;&#36753;&#23376;&#20219;&#21153;&#65306;&#65288;1&#65289;&#25552;&#21462;&#23454;&#20307;&#25552;&#21450;&#36328;&#24230;&#65292;&#26080;&#35770;&#23454;&#20307;&#31867;&#22411;&#22914;&#20309;&#65307;&#65288;2&#65289;&#23558;&#36328;&#24230;&#20998;&#31867;&#20026;&#23454;&#20307;&#31867;&#22411;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#37117;&#24418;&#24335;&#21270;&#20026;&#38382;&#31572;&#38382;&#39064;&#65292;&#24182;&#20135;&#29983;&#20004;&#20010;&#21487;&#20197;&#20998;&#21035;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#30340;&#26356;&#36731;&#30340;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#20004;&#27493;&#27861;&#26082;&#26377;&#25928;&#21448;&#33410;&#30465;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;SplitNER&#22312;OntoNotes5.0&#12289;WNUT17&#21644;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#22522;&#32447;&#65292;&#24182;&#22312;BioNLP13CG&#19978;&#34920;&#29616;&#30456;&#24403;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#19982;QA&#22522;&#32447;&#23545;&#29031;&#30456;&#27604;&#65292;&#23427;&#22312;&#35757;&#32451;&#26102;&#20943;&#23569;&#20102;&#26174;&#33879;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#20998;&#21035;&#23545;&#36328;&#24230;&#26816;&#27979;&#21644;&#20998;&#31867;&#36827;&#34892;BERT&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/c3sr/split-ner&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dropout&#30340;DNN&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Lyapunov&#30340;&#23454;&#26102;&#26435;&#37325;&#35843;&#25972;&#23450;&#24459;&#21644;Dropout&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36319;&#36394;&#35823;&#24046;&#25913;&#36827;&#21644;&#31934;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.19938</link><description>&lt;p&gt;
&#22522;&#20110;Lyapunov&#30340;Dropout&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;Lb-DDNN&#65289;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller. (arXiv:2310.19938v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dropout&#30340;DNN&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Lyapunov&#30340;&#23454;&#26102;&#26435;&#37325;&#35843;&#25972;&#23450;&#24459;&#21644;Dropout&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36319;&#36394;&#35823;&#24046;&#25913;&#36827;&#21644;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22522;&#20110;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#21487;&#20197;&#29992;&#26469;&#34917;&#20607;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;DNN&#20063;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#21644;&#20849;&#36866;&#24212;&#12290;Dropout&#27491;&#21017;&#21270;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#33410;&#28857;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#20849;&#36866;&#24212;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dropout&#30340;DNN&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#12290;&#35813;Dropout&#25216;&#26415;&#20801;&#35768;&#23545;DNN&#27599;&#20010;&#23618;&#20013;&#30340;&#26435;&#37325;&#36827;&#34892;&#38543;&#26426;&#36873;&#25321;&#30340;&#21435;&#28608;&#27963;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Lyapunov&#30340;&#23454;&#26102;&#26435;&#37325;&#35843;&#25972;&#23450;&#24459;&#65292;&#29992;&#20110;&#26356;&#26032;DNN&#25152;&#26377;&#23618;&#32423;&#30340;&#26435;&#37325;&#20197;&#23454;&#29616;&#22312;&#32447;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#36827;&#34892;&#20102;&#38750;&#20809;&#28369;&#30340;&#22522;&#20110;Lyapunov&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#30830;&#20445;&#36319;&#36394;&#35823;&#24046;&#30340;&#28176;&#36817;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20110;Dropout&#30340;DNN&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#30340;&#36319;&#36394;&#35823;&#24046;&#25913;&#36827;&#20102;38.32%&#65292;&#31934;&#24230;&#25913;&#36827;&#20102;53.67%&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN)-based adaptive controllers can be used to compensate for unstructured uncertainties in nonlinear dynamic systems. However, DNNs are also very susceptible to overfitting and co-adaptation. Dropout regularization is an approach where nodes are randomly dropped during training to alleviate issues such as overfitting and co-adaptation. In this paper, a dropout DNN-based adaptive controller is developed. The developed dropout technique allows the deactivation of weights that are stochastically selected for each individual layer within the DNN. Simultaneously, a Lyapunov-based real-time weight adaptation law is introduced to update the weights of all layers of the DNN for online unsupervised learning. A non-smooth Lyapunov-based stability analysis is performed to ensure asymptotic convergence of the tracking error. Simulation results of the developed dropout DNN-based adaptive controller indicate a 38.32% improvement in the tracking error, a 53.67% improvement in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#24072;&#29983;&#26550;&#26500;&#22312;&#23569;&#26631;&#27880;&#23398;&#20064;&#35774;&#32622;&#20013;&#36991;&#20813;&#20102;&#20381;&#36182;&#25935;&#24863;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#22312;&#26631;&#27880;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.19936</link><description>&lt;p&gt;
&#38754;&#21521;&#23569;&#26631;&#27880;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#65306;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?. (arXiv:2310.19936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#24072;&#29983;&#26550;&#26500;&#22312;&#23569;&#26631;&#27880;&#23398;&#20064;&#35774;&#32622;&#20013;&#36991;&#20813;&#20102;&#20381;&#36182;&#25935;&#24863;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#22312;&#26631;&#27880;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19987;&#38376;&#30340;&#21644;&#23494;&#38598;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#30446;&#26631;&#26816;&#27979;&#65292;&#26631;&#35760;&#25968;&#25454;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#25104;&#26412;&#36739;&#39640;&#65292;&#22240;&#27492;&#23569;&#26679;&#26412;&#21644;&#21322;&#30417;&#30563;&#27169;&#22411;&#25104;&#20026;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#23569;&#26679;&#26412;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#30456;&#20284;&#25968;&#37327;&#30340;&#21442;&#25968;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#27604;&#22522;&#20110;&#21367;&#31215;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#26368;&#26032;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#27809;&#26377;&#37027;&#20040;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;Deformable DETR&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#24072;&#29983;&#26550;&#26500;&#22312;&#23569;&#26631;&#27880;&#23398;&#20064;&#35774;&#32622;&#20013;&#36991;&#20813;&#20102;&#20381;&#36182;&#25935;&#24863;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;COVO&#21644;Pascal VOC&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#29305;&#21035;&#26159;&#26631;&#27880;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#36129;&#29486;&#23558;&#24320;&#21551;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilitie
&lt;/p&gt;</description></item><item><title>"Sim2Real"&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#30495;&#23454;&#29615;&#22659;&#35266;&#27979;&#25968;&#25454;&#31232;&#30095;&#24615;&#20043;&#38388;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20877;&#20998;&#26512;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#23545;&#29615;&#22659;&#31070;&#32463;&#36807;&#31243;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2310.19932</link><description>&lt;p&gt;
&#29615;&#22659;&#31070;&#32463;&#36807;&#31243;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Sim2Real for Environmental Neural Processes. (arXiv:2310.19932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19932
&lt;/p&gt;
&lt;p&gt;
"Sim2Real"&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#30495;&#23454;&#29615;&#22659;&#35266;&#27979;&#25968;&#25454;&#31232;&#30095;&#24615;&#20043;&#38388;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20877;&#20998;&#26512;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#23545;&#29615;&#22659;&#31070;&#32463;&#36807;&#31243;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22825;&#27668;&#27169;&#22411;&#24050;&#32463;&#26377;&#20102;&#24555;&#36895;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#25968;&#20540;&#25968;&#25454;&#21516;&#21270;&#31995;&#32479;&#30340;&#32593;&#26684;&#21270;&#20877;&#20998;&#26512;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#20877;&#20998;&#26512;&#25968;&#25454;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#23545;&#29289;&#29702;&#23450;&#24459;&#30340;&#20551;&#35774;&#21644;&#20302;&#26102;&#31354;&#20998;&#36776;&#29575;&#12290;&#20877;&#20998;&#26512;&#32467;&#26524;&#19982;&#30495;&#23454;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#36317;&#24341;&#21457;&#20102;&#30452;&#25509;&#22312;&#22825;&#27668;&#35266;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#23545;&#20998;&#25955;&#21644;&#31232;&#30095;&#30340;&#29615;&#22659;&#35266;&#27979;&#36827;&#34892;&#24314;&#27169;&#38656;&#35201;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#21367;&#31215;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;(ConvCNP)&#12290;ConvCNP&#21487;&#20197;&#23398;&#20064;&#22312;&#32593;&#26684;&#21270;&#21644;&#38750;&#32593;&#26684;&#21270;&#19978;&#19979;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#65292;&#20197;&#22312;&#30446;&#26631;&#20301;&#32622;&#19978;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#35266;&#27979;&#30340;&#31232;&#30095;&#24615;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;ConvCNP&#65289;&#26469;&#35828;&#26159;&#20010;&#25361;&#25112;&#12290;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#8220;Sim2Real&#8221;&#65306;&#22312;&#20877;&#20998;&#26512;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML)-based weather models have recently undergone rapid improvements. These models are typically trained on gridded reanalysis data from numerical data assimilation systems. However, reanalysis data comes with limitations, such as assumptions about physical laws and low spatiotemporal resolution. The gap between reanalysis and reality has sparked growing interest in training ML models directly on observations such as weather stations. Modelling scattered and sparse environmental observations requires scalable and flexible ML architectures, one of which is the convolutional conditional neural process (ConvCNP). ConvCNPs can learn to condition on both gridded and off-the-grid context data to make uncertainty-aware predictions at target locations. However, the sparsity of real observations presents a challenge for data-hungry deep learning models like the ConvCNP. One potential solution is 'Sim2Real': pre-training on reanalysis and fine-tuning on observational data. We an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#36890;&#36807;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#38271;&#26399;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#20248;&#21270;&#22256;&#38590;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#26799;&#24230;&#26041;&#24046;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19927</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65306;&#29702;&#35770;&#21644;&#23454;&#36341;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms. (arXiv:2310.19927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19927
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#36890;&#36807;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#38271;&#26399;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#20248;&#21270;&#22256;&#38590;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#26799;&#24230;&#26041;&#24046;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#24212;&#29992;&#20110;&#38271;&#26399;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#21487;&#33021;&#20250;&#36935;&#21040;&#28151;&#20081;&#21644;&#38750;&#24179;&#28369;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#23548;&#33268;&#26799;&#24230;&#26041;&#24046;&#29190;&#28856;&#65292;&#20174;&#32780;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#36825;&#19982;&#20256;&#32479;&#35266;&#24565;&#30456;&#21453;&#65292;&#21363;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#22312;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31561;&#38382;&#39064;&#20013;&#20855;&#26377;&#36739;&#20302;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#23547;&#25214;&#35299;&#20915;&#20248;&#21270;&#22256;&#38590;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25351;&#20986;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#24179;&#28369;&#24615;&#26159;&#24433;&#21709;&#26799;&#24230;&#20272;&#35745;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#29190;&#28856;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding var
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36817;&#20284;&#30830;&#23450;&#19968;&#31867;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#29983;&#25104;&#21106;&#24179;&#38754;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#25903;&#30028;&#38480;&#31639;&#27861;&#20013;&#36816;&#34892;CGLP&#30340;&#35745;&#31639;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19920</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#19968;&#31867;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#29983;&#25104;&#21106;&#24179;&#38754;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving a Class of Cut-Generating Linear Programs via Machine Learning. (arXiv:2310.19920v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36817;&#20284;&#30830;&#23450;&#19968;&#31867;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#29983;&#25104;&#21106;&#24179;&#38754;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#25903;&#30028;&#38480;&#31639;&#27861;&#20013;&#36816;&#34892;CGLP&#30340;&#35745;&#31639;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21106;&#24179;&#38754;&#30340;&#32447;&#24615;&#35268;&#21010;&#65288;CGLP&#65289;&#22312;&#20135;&#29983;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21487;&#34892;&#21306;&#22495;&#30340;&#26377;&#25928;&#19981;&#31561;&#24335;&#26102;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#24403;&#23558;&#20854;&#25554;&#20837;&#20998;&#25903;&#30028;&#38480;&#31639;&#27861;&#20013;&#26102;&#65292;&#20174;CGLP&#33719;&#24471;&#30340;&#21106;&#24179;&#38754;&#26377;&#21161;&#20110;&#21152;&#32039;&#26494;&#24347;&#24182;&#25913;&#36827;&#23545;&#20598;&#19979;&#30028;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#20505;&#36873;&#25968;&#30446;&#24222;&#22823;&#19988;&#32570;&#20047;&#20851;&#20110;&#21738;&#20123;&#33410;&#28857;&#21487;&#29983;&#25104;&#26377;&#25928;&#21106;&#24179;&#38754;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#20998;&#25903;&#30028;&#38480;&#26641;&#30340;&#33410;&#28857;&#19978;&#36816;&#34892;CGLP&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;CGLP&#23545;&#25913;&#36827;&#23545;&#20598;&#19979;&#30028;&#26377;&#28508;&#22312;&#24433;&#21709;&#65292;&#20294;&#22312;&#40664;&#35748;&#35774;&#32622;&#30340;&#20998;&#25903;&#21106;&#31639;&#27861;&#20013;&#32463;&#24120;&#36991;&#20813;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#30830;&#23450;&#19968;&#20010;CGLP&#31867;&#30340;&#26368;&#20248;&#20540;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#21106;&#24179;&#38754;&#26159;&#21542;&#21487;&#20197;&#22312;&#20998;&#25903;&#30028;&#38480;&#26641;&#30340;&#33410;&#28857;&#19978;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;CGLP&#36716;&#21270;&#20026;&#30446;&#26631;&#21521;&#37327;&#30340;&#25351;&#31034;&#20989;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#36890;&#36807;&#21512;&#20316;&#20249;&#20276;&#26469;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cut-generating linear programs (CGLPs) play a key role as a separation oracle to produce valid inequalities for the feasible region of mixed-integer programs. When incorporated inside branch-and-bound, the cutting planes obtained from CGLPs help to tighten relaxations and improve dual bounds. However, running the CGLPs at the nodes of the branch-and-bound tree is computationally cumbersome due to the large number of node candidates and the lack of a priori knowledge on which nodes admit useful cutting planes. As a result, CGLPs are often avoided at default settings of branch-and-cut algorithms despite their potential impact on improving dual bounds. In this paper, we propose a novel framework based on machine learning to approximate the optimal value of a CGLP class that determines whether a cutting plane can be generated at a node of the branch-and-bound tree. Translating the CGLP as an indicator function of the objective function vector, we show that it can be approximated through co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21162;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#25511;&#21046;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#21319;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19919</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#20215;&#20540;&#26368;&#22823;&#21270;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Strategies through Value Maximization in Neural Networks. (arXiv:2310.19919v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21162;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#25511;&#21046;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#21319;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#23398;&#20064;&#20195;&#29702;&#38754;&#20020;&#35832;&#22810;&#23398;&#20064;&#36873;&#25321;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#20219;&#21153;&#20998;&#24067;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#35838;&#31243;&#12290;&#20102;&#35299;&#22914;&#20309;&#36827;&#34892;&#36825;&#20123;&#20803;&#23398;&#20064;&#36873;&#25321;&#21487;&#20197;&#25552;&#20379;&#23545;&#29983;&#29289;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#25511;&#21046;&#21151;&#33021;&#30340;&#35268;&#33539;&#35299;&#37322;&#65292;&#24182;&#25913;&#36827;&#24037;&#31243;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20248;&#21270;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#30446;&#21069;&#20173;&#28982;&#25361;&#25112;&#30528;&#35745;&#31639;&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#29615;&#22659;&#20013;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21162;&#21147;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#23436;&#20840;&#35268;&#33539;&#21270;&#30340;&#30446;&#26631;&#19978;&#39640;&#25928;&#22320;&#20248;&#21270;&#25511;&#21046;&#20449;&#21495;&#65306;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#25240;&#29616;&#32047;&#31215;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#30340;&#24179;&#22343;&#21160;&#21147;&#26041;&#31243;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#35745;&#31639;&#30340;&#21487;&#34892;&#24615;&#65292;&#35813;&#26041;&#31243;&#36866;&#29992;&#20110;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#23481;&#20102;&#19968;&#31995;&#21015;&#20803;&#23398;&#20064;&#21644;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24418;&#25104;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified n
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19917</link><description>&lt;p&gt;
&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#20559;&#35265;&#26816;&#27979;&#21644;&#32531;&#35299;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22312;&#21307;&#30103;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#32508;&#36848;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#12290;&#26041;&#27861;&#65306;&#36981;&#24490;Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)&#20934;&#21017;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#20174;PubMed&#12289;Web of Science&#21644;&#30005;&#27668;&#21644;&#30005;&#23376;&#24037;&#31243;&#24072;&#23398;&#20250;&#20013;&#26816;&#32034;&#20102;2010&#24180;1&#26376;1&#26085;&#33267;2022&#24180;10&#26376;31&#26085;&#26399;&#38388;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;&#32467;&#26524;&#65306;&#22312;&#26816;&#32034;&#21040;&#30340;252&#31687;&#25991;&#31456;&#20013;&#65292;&#26377;20&#31687;&#31526;&#21512;&#26368;&#32456;&#32508;&#36848;&#30340;&#32435;&#20837;&#26631;&#20934;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#20845;&#31181;&#20559;&#35265;&#20013;&#30340;&#20116;&#31181;&#65306;&#20843;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#36873;&#25321;&#20559;&#35265;&#65307;&#20845;&#39033;&#30740;&#31350;&#38024;&#23545;&#38544;&#24615;&#20559;&#35265;&#65307;&#20116;&#39033;&#30740;&#31350;&#23545;&#28151;&#26434;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#22235;&#39033;&#30740;&#31350;&#23545;&#27979;&#37327;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#20004;&#39033;&#30740;&#31350;&#23545;&#31639;&#27861;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#26041;&#38754;&#65292;&#26377;&#21313;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPCR-BERT&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20102;G&#34507;&#30333;&#20598;&#32852;&#21463;&#20307;(GPCR)&#30340;&#39034;&#24207;&#35774;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#32467;&#21512;&#21475;&#34955;&#20013;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#19968;&#20123;&#20445;&#23432;&#30340;&#22522;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.19915</link><description>&lt;p&gt;
GPCR-BERT:&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;G&#34507;&#30333;&#20598;&#32852;&#21463;&#20307;(GPCR)&#30340;&#39034;&#24207;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models. (arXiv:2310.19915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPCR-BERT&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20102;G&#34507;&#30333;&#20598;&#32852;&#21463;&#20307;(GPCR)&#30340;&#39034;&#24207;&#35774;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#32467;&#21512;&#21475;&#34955;&#20013;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#19968;&#20123;&#20445;&#23432;&#30340;&#22522;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20013;Transformer&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20852;&#36215;&#65292;&#23545;&#27835;&#30103;&#26041;&#27861;&#30340;&#35774;&#35745;&#21644;&#29702;&#35299;&#30340;&#31185;&#23398;&#30028;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#34507;&#30333;&#36136;&#24207;&#21015;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#21033;&#29992;&#26368;&#36817;&#22312;LLMs&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25105;&#20204;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#35775;&#38382;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GPCR-BERT&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;G&#34507;&#30333;&#20598;&#32852;&#21463;&#20307;(GPCRs)&#30340;&#39034;&#24207;&#35774;&#35745;&#12290;GPCRs&#26159;FDA&#25209;&#20934;&#30340;&#33647;&#29289;&#20013;&#36229;&#36807;&#19977;&#20998;&#20043;&#19968;&#30340;&#38774;&#28857;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#27688;&#22522;&#37240;&#24207;&#21015;&#12289;&#37197;&#20307;&#36873;&#25321;&#24615;&#21644;&#26500;&#35937;&#22522;&#24207;(&#22914;NPxxY&#12289;CWxP&#12289;E/DRY)&#20043;&#38388;&#30340;&#20851;&#31995;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#27169;&#22411;(Prot-Bert)&#24182;&#36890;&#36807;&#21464;&#24322;&#39044;&#27979;&#20219;&#21153;&#23545;&#22522;&#24207;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#32467;&#21512;&#21475;&#34955;&#20013;&#27531;&#22522;&#20043;&#38388;&#30340;&#20960;&#31181;&#20851;&#31995;&#21644;&#19968;&#20123;&#20445;&#23432;&#30340;&#22522;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of Transformers and Large Language Models (LLMs) in Chemistry and Biology, new avenues for the design and understanding of therapeutics have opened up to the scientific community. Protein sequences can be modeled as language and can take advantage of recent advances in LLMs, specifically with the abundance of our access to the protein sequence datasets. In this paper, we developed the GPCR-BERT model for understanding the sequential design of G Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of FDA-approved pharmaceuticals. However, there is a lack of comprehensive understanding regarding the relationship between amino acid sequence, ligand selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with prediction tasks of variations in the motifs, we were able to shed light on several relationships between residues in the binding pocket and some of the conserved mot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#22330;&#37325;&#24314;&#31639;&#27861;&#65292;&#32467;&#21512;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#23431;&#23449;&#21021;&#22987;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.19910</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20223;&#30495;&#25512;&#26029;&#30340;&#23431;&#23449;&#21021;&#22987;&#26465;&#20214;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bayesian Simulation-based Inference for Cosmological Initial Conditions. (arXiv:2310.19910v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#22330;&#37325;&#24314;&#31639;&#27861;&#65292;&#32467;&#21512;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#23431;&#23449;&#21021;&#22987;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#37325;&#24314;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#39046;&#22495;&#30340;&#22330;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23427;&#38656;&#35201;&#32771;&#34385;&#21040;&#38750;&#32447;&#24615;&#36716;&#25442;&#12289;&#31354;&#38388;&#32467;&#26500;&#30340;&#28151;&#21512;&#21644;&#22122;&#22768;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23558;&#22330;&#26144;&#23556;&#21040;&#35266;&#27979;&#25968;&#25454;&#30340;&#27491;&#21521;&#27169;&#25311;&#22120;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#30340;&#22810;&#21151;&#33021;&#36125;&#21494;&#26031;&#22330;&#37325;&#24314;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#33258;&#22238;&#24402;&#24314;&#27169;&#36827;&#34892;&#25913;&#36827;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#36866;&#29992;&#20110;&#36890;&#29992;&#30340;&#65288;&#38750;&#21487;&#24494;&#20998;&#65289;&#27491;&#21521;&#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#20801;&#35768;&#20174;&#28508;&#22312;&#22330;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#24212;&#29992;&#20013;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65306;&#20174;&#26202;&#26399;&#23494;&#24230;&#22330;&#24674;&#22797;&#23431;&#23449;&#21021;&#22987;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing astrophysical and cosmological fields from observations is challenging. It requires accounting for non-linear transformations, mixing of spatial structure, and noise. In contrast, forward simulators that map fields to observations are readily available for many applications. We present a versatile Bayesian field reconstruction algorithm rooted in simulation-based inference and enhanced by autoregressive modeling. The proposed technique is applicable to generic (non-differentiable) forward simulators and allows sampling from the posterior for the underlying field. We show first promising results on a proof-of-concept application: the recovery of cosmological initial conditions from late-time density fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#22909;&#22320;&#36873;&#25321;&#39592;&#24178;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#30740;&#31350;&#36827;&#23637;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.19909</link><description>&lt;p&gt;
&#39592;&#24178;&#32593;&#32476;&#20043;&#25112;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks. (arXiv:2310.19909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#22909;&#22320;&#36873;&#25321;&#39592;&#24178;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#30740;&#31350;&#36827;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#36890;&#24120;&#30001;&#19968;&#20010;&#39592;&#24178;&#32593;&#32476;&#26500;&#25104;&#65292;&#21363;&#39044;&#35757;&#32451;&#25110;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#20960;&#24180;&#21069;&#65292;&#20351;&#29992;ImageNet&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#40664;&#35748;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#21457;&#23637;&#20986;&#29616;&#20102;&#20351;&#29992;&#21508;&#31181;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#20247;&#22810;&#39592;&#24178;&#32593;&#32476;&#12290;&#34429;&#28982;&#36825;&#31181;&#36873;&#25321;&#20016;&#23500;&#24615;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#24456;&#38590;&#20570;&#20986;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;&#39592;&#24178;&#32593;&#32476;&#20043;&#25112;&#65288;BoB&#65289;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#31283;&#23450;&#25193;&#25955;&#39592;&#24178;&#31561;&#31561;&#65292;&#20197;&#21450;&#38024;&#23545;&#20174;&#20998;&#31867;&#21040;&#30446;&#26631;&#26816;&#27979;&#21040;OOD&#27867;&#21270;&#31561;&#22810;&#26679;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#36825;&#20010;&#36873;&#25321;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;BoB&#36890;&#36807;&#25581;&#31034;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20026;&#30740;&#31350;&#31038;&#21306;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19906</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#21644;&#23545;&#20854;&#39044;&#27979;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#36825;&#20652;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;&#21407;&#22411;&#30340;&#20351;&#29992;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#21407;&#22411;&#26469;&#26263;&#31034;&#24433;&#21709;&#39044;&#27979;&#30340;&#35757;&#32451;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#32473;&#21407;&#22411;&#25552;&#20379;&#26469;&#33258;&#25972;&#20010;&#22270;&#30340;&#36807;&#22810;&#20449;&#24687;&#65292;&#23548;&#33268;&#20851;&#38190;&#23376;&#32467;&#26500;&#30340;&#25490;&#38500;&#25110;&#26080;&#20851;&#23376;&#32467;&#26500;&#30340;&#21253;&#21547;&#65292;&#36825;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#31216;&#20026;&#35299;&#37322;&#24615;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048; (PGIB)&#65292;&#23558;&#21407;&#22411;&#23398;&#20064;&#32435;&#20837;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65292;&#20026;&#21407;&#22411;&#25552;&#20379;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;Transformer&#65288;MIST&#65289;&#65292;&#21033;&#29992;Convolutional Attention Mixing&#65288;CAM&#65289;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;Transformer&#22312;&#22810;&#27169;&#24577;&#23610;&#23544;&#20013;&#25429;&#25417;&#20687;&#32032;&#23616;&#37096;&#19978;&#19979;&#25991;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19898</link><description>&lt;p&gt;
MIST: &#20855;&#26377;&#21367;&#31215;&#27880;&#24847;&#21147;&#28151;&#21512;&#65288;CAM&#65289;&#35299;&#30721;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;Transformer
&lt;/p&gt;
&lt;p&gt;
MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder. (arXiv:2310.19898v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;Transformer&#65288;MIST&#65289;&#65292;&#21033;&#29992;Convolutional Attention Mixing&#65288;CAM&#65289;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;Transformer&#22312;&#22810;&#27169;&#24577;&#23610;&#23544;&#20013;&#25429;&#25417;&#20687;&#32032;&#23616;&#37096;&#19978;&#19979;&#25991;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#26159;Transformer&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#25429;&#25417;&#20687;&#32032;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;Transformer&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22810;&#27169;&#24577;&#23610;&#23544;&#20013;&#25429;&#25417;&#20687;&#32032;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;Transformer&#65288;MIST&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#27880;&#24847;&#21147;&#28151;&#21512;&#65288;CAM&#65289;&#35299;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;MIST&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#36724;&#35270;&#35273;Transformer&#65288;MaxViT&#65289;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;CAM&#35299;&#30721;&#22120;&#23545;&#32534;&#30721;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;&#22312;CAM&#35299;&#30721;&#22120;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#28151;&#21512;&#22120;&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#21387;&#32553;&#28608;&#21169;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#25429;&#25417;&#25152;&#26377;&#31354;&#38388;&#32500;&#24230;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#31354;&#38388;&#20449;&#24687;&#30340;&#33719;&#21462;&#65292;&#36824;&#20351;&#29992;&#20102;&#28145;&#23618;&#21644;&#27973;&#23618;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the common and promising deep learning approaches used for medical image segmentation is transformers, as they can capture long-range dependencies among the pixels by utilizing self-attention. Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#35270;&#35273;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#25935;&#24863;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30740;&#31350;&#32593;&#32476;&#30340;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#21512;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#33539;&#22260;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#32423;&#21035;&#38598;&#36941;&#21382;&#31639;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19982;&#32473;&#23450;&#28304;&#22270;&#20687;&#30456;&#20284;&#20294;&#23646;&#20110;&#30456;&#21516;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.19889</link><description>&lt;p&gt;
&#25506;&#32034;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#30450;&#28857;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Exploring Geometry of Blind Spots in Vision Models. (arXiv:2310.19889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#35270;&#35273;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#25935;&#24863;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30740;&#31350;&#32593;&#32476;&#30340;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#21512;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#33539;&#22260;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#32423;&#21035;&#38598;&#36941;&#21382;&#31639;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19982;&#32473;&#23450;&#28304;&#22270;&#20687;&#30456;&#20284;&#20294;&#23646;&#20110;&#30456;&#21516;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#23545;&#25509;&#36817;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#65292;&#28145;&#24230;&#32593;&#32476;&#20063;&#21487;&#33021;&#20986;&#29616;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#22823;&#24133;&#25200;&#21160;&#19981;&#25935;&#24863;&#30340;&#24773;&#20917;&#65292;&#32780;&#36825;&#24182;&#19981;&#20250;&#23548;&#33268;&#32593;&#32476;&#28608;&#27963;&#21457;&#29983;&#26126;&#26174;&#25913;&#21464;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#22312;CNNs&#21644;Transformers&#31561;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#19981;&#25935;&#24863;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#30740;&#31350;&#36825;&#20123;&#32593;&#32476;&#8220;&#31561;&#32622;&#20449;&#24230;&#8221;&#32423;&#21035;&#38598;&#21512;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#33539;&#22260;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#21035;&#38598;&#36941;&#21382;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#26799;&#24230;&#30340;&#27491;&#20132;&#20998;&#37327;&#26469;&#36845;&#20195;&#22320;&#25506;&#32034;&#19982;&#36755;&#20837;&#31354;&#38388;&#20013;&#39640;&#32622;&#20449;&#24230;&#21306;&#22495;&#12290;&#32473;&#23450;&#19968;&#20010;&#28304;&#22270;&#20687;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#31639;&#27861;&#26469;&#35782;&#21035;&#19982;&#28304;&#22270;&#20687;&#23646;&#20110;&#30456;&#21516;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#30340;&#36755;&#20837;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#24863;&#30693;&#19978;&#19982;&#20219;&#24847;&#22270;&#20687;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary image
&lt;/p&gt;</description></item><item><title>BTRec&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#36712;&#36857;&#25512;&#33616;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#36807;&#21435;&#30340;POI&#35775;&#38382;&#20449;&#24687;&#65292;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;BERT&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;POI&#34892;&#31243;&#39044;&#27979;&#65292;&#20174;&#32780;&#20026;&#26053;&#28216;&#32773;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#25512;&#33616;&#34892;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.19886</link><description>&lt;p&gt;
BTRec: &#22522;&#20110;BERT&#30340;&#20010;&#24615;&#21270;&#26053;&#28216;&#36712;&#36857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
BTRec: BERT-Based Trajectory Recommendation for Personalized Tours. (arXiv:2310.19886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19886
&lt;/p&gt;
&lt;p&gt;
BTRec&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#36712;&#36857;&#25512;&#33616;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#36807;&#21435;&#30340;POI&#35775;&#38382;&#20449;&#24687;&#65292;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;BERT&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;POI&#34892;&#31243;&#39044;&#27979;&#65292;&#20174;&#32780;&#20026;&#26053;&#28216;&#32773;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#25512;&#33616;&#34892;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26053;&#28216;&#32773;&#26469;&#35828;&#65292;&#25317;&#26377;&#19968;&#20010;&#31934;&#24515;&#35268;&#21010;&#30340;&#34892;&#31243;&#21644;&#30456;&#20851;&#25512;&#33616;&#26159;&#24230;&#36807;&#24841;&#24555;&#20551;&#26399;&#30340;&#20851;&#38190;&#65292;&#23588;&#20854;&#26159;&#24403;&#20182;&#20204;&#35775;&#38382;&#38476;&#29983;&#22478;&#24066;&#26102;&#12290;&#35768;&#22810;&#26053;&#28216;&#25512;&#33616;&#24037;&#20855;&#21482;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#22240;&#32032;&#65292;&#22914;&#28909;&#38376;&#26223;&#28857;&#21644;&#36335;&#24452;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#24635;&#26159;&#19982;&#31995;&#32479;&#30340;&#20010;&#20307;&#29992;&#25143;&#20445;&#25345;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BTREC&#65288;&#22522;&#20110;BERT&#30340;&#36712;&#36857;&#25512;&#33616;&#65289;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#23427;&#20174;POIBERT&#23884;&#20837;&#31639;&#27861;&#25193;&#23637;&#21040;&#20351;&#29992;BERT&#26694;&#26550;&#25512;&#33616;&#20010;&#24615;&#21270;POI&#34892;&#31243;&#12290;&#25105;&#20204;&#30340;BTREC&#31639;&#27861;&#23558;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#36807;&#21435;&#30340;POI&#35775;&#38382;&#20449;&#24687;&#21512;&#24182;&#21040;&#20462;&#25913;&#21518;&#30340;BERT&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#32473;&#20986;&#19968;&#23545;&#20986;&#21457;&#22320;&#21644;&#30446;&#30340;&#22320;POI&#30340;&#20010;&#24615;&#21270;POI&#34892;&#31243;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#35775;&#38382;POI&#30340;&#26053;&#34892;&#34892;&#31243;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
An essential task for tourists having a pleasant holiday is to have a well-planned itinerary with relevant recommendations, especially when visiting unfamiliar cities. Many tour recommendation tools only take into account a limited number of factors, such as popular Points of Interest (POIs) and routing constraints. Consequently, the solutions they provide may not always align with the individual users of the system. We propose an iterative algorithm in this paper, namely: BTREC (BERT-based Trajectory Recommendation), that extends from the POIBERT embedding algorithm to recommend personalized itineraries on POIs using the BERT framework. Our BTREC algorithm incorporates users' demographic information alongside past POI visits into a modified BERT language model to recommend a personalized POI itinerary prediction given a pair of source and destination POIs. Our recommendation system can create a travel itinerary that maximizes POIs visited, while also taking into account user preferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23398;&#20064;&#20855;&#26377;&#26377;&#30028;&#38376;&#22797;&#26434;&#24230;&#30340;&#37327;&#23376;&#24577;&#21644;&#37193;&#31639;&#31526;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#30456;&#24212;&#30340;&#38376;&#22797;&#26434;&#24230;&#32447;&#24615;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#23384;&#22312;&#25351;&#25968;&#20851;&#31995;&#65292;&#36825;&#19968;&#32467;&#26524;&#38480;&#21046;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19882</link><description>&lt;p&gt;
&#23398;&#20064;&#26377;&#30028;&#38376;&#22797;&#26434;&#24230;&#30340;&#37327;&#23376;&#24577;&#21644;&#37193;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
Learning quantum states and unitaries of bounded gate complexity. (arXiv:2310.19882v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23398;&#20064;&#20855;&#26377;&#26377;&#30028;&#38376;&#22797;&#26434;&#24230;&#30340;&#37327;&#23376;&#24577;&#21644;&#37193;&#31639;&#31526;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#30456;&#24212;&#30340;&#38376;&#22797;&#26434;&#24230;&#32447;&#24615;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#23384;&#22312;&#25351;&#25968;&#20851;&#31995;&#65292;&#36825;&#19968;&#32467;&#26524;&#38480;&#21046;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37327;&#23376;&#29366;&#24577;&#37325;&#26500;&#38750;&#24120;&#22256;&#38590;&#65292;&#20294;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#26500;&#32773;&#26469;&#35828;&#65292;&#22823;&#22810;&#25968;&#29366;&#24577;&#30340;&#20852;&#36259;&#19981;&#22823;&#12290;&#37492;&#20110;&#33258;&#28982;&#30028;&#20013;&#20986;&#29616;&#30340;&#29366;&#24577;&#21644;&#37193;&#31639;&#31526;&#37117;&#20855;&#26377;&#26377;&#30028;&#30340;&#38376;&#22797;&#26434;&#24230;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#38382;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#23558;&#30001;$G$&#20010;&#20004;&#37327;&#23376;&#27604;&#29305;&#38376;&#29983;&#25104;&#30340;&#29366;&#24577;&#23398;&#20064;&#21040;&#23567;&#30340;&#36857;&#36317;&#31163;&#65292;&#38656;&#35201;&#21644;&#20805;&#20998;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;$G$&#32447;&#24615;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23398;&#20064;&#30001;$G$&#20010;&#38376;&#29983;&#25104;&#30340;&#37193;&#31639;&#31526;&#21040;&#23567;&#30340;&#24179;&#22343;&#35823;&#24046;&#30340;&#26368;&#20248;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;$G$&#32447;&#24615;&#27604;&#20363;&#12290;&#34429;&#28982;&#21487;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21512;&#29702;&#30340;&#23494;&#30721;&#23398;&#29468;&#24819;&#19979;&#65292;&#23398;&#20064;&#38376;&#22797;&#26434;&#24230;&#20026;$G$&#30340;&#29366;&#24577;&#21644;&#37193;&#31639;&#31526;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24517;&#39035;&#19982;$G$&#25351;&#25968;&#27604;&#20363;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#22914;&#20309;&#30830;&#23450;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While quantum state tomography is notoriously hard, most states hold little interest to practically-minded tomographers. Given that states and unitaries appearing in Nature are of bounded gate complexity, it is natural to ask if efficient learning becomes possible. In this work, we prove that to learn a state generated by a quantum circuit with $G$ two-qubit gates to a small trace distance, a sample complexity scaling linearly in $G$ is necessary and sufficient. We also prove that the optimal query complexity to learn a unitary generated by $G$ gates to a small average-case error scales linearly in $G$. While sample-efficient learning can be achieved, we show that under reasonable cryptographic conjectures, the computational complexity for learning states and unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate how these results establish fundamental limitations on the expressivity of quantum machine learning models and provide new perspectives on no-free-lun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#30340;&#24230;&#37327;&#27969;&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#22312;&#40654;&#26364;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27969;&#21160;&#12290;&#20854;&#24212;&#29992;&#20110;&#25968;&#20540;Calabi-Yau&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#29305;&#24449;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19870</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#24230;&#37327;&#27969;
&lt;/p&gt;
&lt;p&gt;
Metric Flows with Neural Networks. (arXiv:2310.19870v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#30340;&#24230;&#37327;&#27969;&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#22312;&#40654;&#26364;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27969;&#21160;&#12290;&#20854;&#24212;&#29992;&#20110;&#25968;&#20540;Calabi-Yau&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#29305;&#24449;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#30001;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#35825;&#23548;&#30340;&#40654;&#26364;&#24230;&#37327;&#31354;&#38388;&#20013;&#27969;&#21160;&#30340;&#29702;&#35770;&#12290;&#36825;&#37096;&#20998;&#26159;&#21463;&#21040;&#36817;&#26399;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;Calabi-Yau&#24230;&#37327;&#30340;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#20063;&#26159;&#30001;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#31354;&#38388;&#20013;&#27969;&#21160;&#30340;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#30456;&#24212;&#30340;&#24230;&#37327;&#27969;&#21160;&#26041;&#31243;&#65292;&#20854;&#30001;&#24230;&#37327;&#31070;&#32463;&#20999;&#21521;&#26680;&#23450;&#20041;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38750;&#23616;&#37096;&#23545;&#35937;&#65292;&#20250;&#38543;&#26102;&#38388;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#32467;&#26500;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#26680;&#23558;&#21464;&#24471;&#22266;&#23450;&#19988;&#21160;&#24577;&#31616;&#21270;&#12290;&#38468;&#21152;&#20551;&#35774;&#21487;&#23548;&#33268;&#27969;&#21160;&#20013;&#30340;&#23616;&#37096;&#24615;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;Perelman&#20851;&#20110;&#35299;&#20915;3D Poincar&#233;&#29468;&#24819;&#20013;&#20351;&#29992;&#30340;Ricci&#27969;&#30340;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#25968;&#20540;Calabi-Yau&#24230;&#37327;&#65292;&#21253;&#25324;&#20851;&#20110;&#29305;&#24449;&#23398;&#20064;&#37325;&#35201;&#24615;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#22312;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#23545;&#24328;&#21644;&#23545;&#25239;&#24615;&#24191;&#20041;&#33406;&#30053;&#29305;&#31995;&#25968;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#25506;&#32034;-&#21033;&#29992;&#24179;&#34913;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#24182;&#19988;&#25104;&#21151;&#22788;&#29702;&#20102;&#29366;&#24577;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#23398;&#20064;&#20855;&#26377;&#28508;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#23545;&#25239;&#24615;&#21338;&#24328;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20302;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.19861</link><description>&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#22312;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#20989;&#25968;&#36817;&#20284;&#21644;&#37096;&#20998;&#35266;&#27979;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling for Competitive RL: Function Approximation and Partial Observation. (arXiv:2310.19861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#22312;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#23545;&#24328;&#21644;&#23545;&#25239;&#24615;&#24191;&#20041;&#33406;&#30053;&#29305;&#31995;&#25968;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#25506;&#32034;-&#21033;&#29992;&#24179;&#34913;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#24182;&#19988;&#25104;&#21151;&#22788;&#29702;&#20102;&#29366;&#24577;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#23398;&#20064;&#20855;&#26377;&#28508;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#23545;&#25239;&#24615;&#21338;&#24328;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20302;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#33324;&#20989;&#25968;&#36817;&#20284;&#30340;&#32972;&#26223;&#19979;&#65292;&#29992;&#20110;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#12290;&#38024;&#23545;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#33258;&#23545;&#24328;&#21644;&#23545;&#25239;&#23398;&#20064;&#20004;&#20010;&#20851;&#38190;&#24773;&#26223;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#33258;&#23545;&#24328;&#21644;&#23545;&#25239;&#24615;&#24191;&#20041;&#33406;&#30053;&#29305;&#31995;&#25968;(GEC)&#20316;&#20026;&#20989;&#25968;&#36817;&#20284;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#25429;&#25417;&#21338;&#24328;&#20013;&#30340;&#25506;&#32034;-&#21033;&#29992;&#24179;&#34913;&#12290;&#22522;&#20110;&#33258;&#23545;&#24328;GEC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#33258;&#23545;&#24328;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#20004;&#20010;&#29609;&#23478;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#65292;&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#29366;&#24577;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#22871;&#19982;&#23545;&#25163;&#30340;&#23545;&#25239;&#31574;&#30053;&#30456;&#36866;&#24212;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#21338;&#24328;&#27169;&#22411;&#12290;&#32467;&#21512;&#23545;&#25239;GEC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#28508;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#23545;&#25239;&#24615;&#21338;&#24328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20026;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#20302;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#30830;&#24674;&#22797;&#33410;&#28857;&#23646;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#21644;&#33410;&#28857;&#23646;&#24615;&#20449;&#24687;&#20132;&#25442;&#65292;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#32593;&#32476;&#20449;&#24687;&#38656;&#35201;&#26356;&#23569;&#21487;&#38752;&#30340;&#23646;&#24615;&#20449;&#24687;&#30340;&#31934;&#30830;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2310.19854</link><description>&lt;p&gt;
&#33410;&#28857;&#23646;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#31934;&#30830;&#24674;&#22797;&#19982;Bregman&#30828;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model. (arXiv:2310.19854v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#30830;&#24674;&#22797;&#33410;&#28857;&#23646;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#21644;&#33410;&#28857;&#23646;&#24615;&#20449;&#24687;&#20132;&#25442;&#65292;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#32593;&#32476;&#20449;&#24687;&#38656;&#35201;&#26356;&#23569;&#21487;&#38752;&#30340;&#23646;&#24615;&#20449;&#24687;&#30340;&#31934;&#30830;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#32858;&#31867;&#35299;&#20915;&#20102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#36830;&#25509;&#27169;&#24335;&#30340;&#33410;&#28857;&#38598;&#65288;&#31038;&#21306;&#65289;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#33410;&#28857;&#36824;&#20855;&#26377;&#19982;&#32858;&#31867;&#32467;&#26500;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#32852;&#21512;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#65288;&#36793;&#65289;&#21644;&#33410;&#28857;&#20449;&#24687;&#65288;&#23646;&#24615;&#65289;&#26469;&#35774;&#35745;&#39640;&#24615;&#33021;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#22312;&#32593;&#32476;&#21644;&#33410;&#28857;&#23646;&#24615;&#30340;&#19968;&#33324;&#27169;&#22411;&#19979;&#65292;&#35813;&#24037;&#20316;&#24314;&#31435;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#20934;&#21017;&#65292;&#29992;&#20110;&#31934;&#30830;&#24674;&#22797;&#31038;&#21306;&#26631;&#31614;&#65292;&#24182;&#30830;&#23450;&#20102;&#30001;&#27169;&#22411;&#30340;Chernoff-Hellinger&#25955;&#24230;&#30830;&#23450;&#30340;&#30456;&#21464;&#12290;&#36825;&#20010;&#20934;&#21017;&#26174;&#31034;&#20102;&#32593;&#32476;&#21644;&#23646;&#24615;&#20449;&#24687;&#22914;&#20309;&#20132;&#25442;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#65288;&#20363;&#22914;&#65292;&#26356;&#21487;&#38752;&#30340;&#32593;&#32476;&#20449;&#24687;&#38656;&#35201;&#26356;&#19981;&#21487;&#38752;&#30340;&#23646;&#24615;&#20449;&#24687;&#65289;&#12290;&#35813;&#24037;&#20316;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#32858;&#31867;&#31639;&#27861;&#65292;&#26368;&#22823;&#21270;&#32852;&#21512;&#20284;&#28982;&#65292;&#20551;&#35774;&#32593;&#32476;&#20132;&#20114;&#21644;&#33410;&#28857;&#23646;&#24615;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network clustering tackles the problem of identifying sets of nodes (communities) that have similar connection patterns. However, in many scenarios, nodes also have attributes that are correlated with the clustering structure. Thus, network information (edges) and node information (attributes) can be jointly leveraged to design high-performance clustering algorithms. Under a general model for the network and node attributes, this work establishes an information-theoretic criterion for the exact recovery of community labels and characterizes a phase transition determined by the Chernoff-Hellinger divergence of the model. The criterion shows how network and attribute information can be exchanged in order to have exact recovery (e.g., more reliable network information requires less reliable attribute information). This work also presents an iterative clustering algorithm that maximizes the joint likelihood, assuming that the probability distribution of network interactions and node attrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20013;&#27688;&#22522;&#37240;&#31361;&#21464;&#30340;&#25928;&#24212;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20391;&#38142;&#26500;&#35937;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#32473;&#20986;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30028;&#38754;&#19978;&#31361;&#21464;&#30340;&#32467;&#26500;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#31361;&#21464;&#25928;&#24212;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19849</link><description>&lt;p&gt;
&#36890;&#36807;&#20391;&#38142;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#31361;&#21464;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model. (arXiv:2310.19849v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20013;&#27688;&#22522;&#37240;&#31361;&#21464;&#30340;&#25928;&#24212;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20391;&#38142;&#26500;&#35937;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#32473;&#20986;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30028;&#38754;&#19978;&#31361;&#21464;&#30340;&#32467;&#26500;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#31361;&#21464;&#25928;&#24212;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#20851;&#38190;&#29983;&#29289;&#36807;&#31243;&#20381;&#36182;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#12290;&#39044;&#27979;&#27688;&#22522;&#37240;&#31361;&#21464;&#23545;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#23545;&#20110;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#33647;&#29289;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32467;&#21512;&#33021;&#30340;&#23454;&#39564;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32473;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;SidechainDiff&#65292;&#23427;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#23454;&#39564;&#34507;&#30333;&#36136;&#32467;&#26500;&#12290;SidechainDiff&#21033;&#29992;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20391;&#38142;&#26500;&#35937;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#32473;&#20986;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30028;&#38754;&#19978;&#31361;&#21464;&#30340;&#32467;&#26500;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#31361;&#21464;&#25928;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;SidechainDiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#20391;&#38142;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#20391;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many crucial biological processes rely on networks of protein-protein interactions. Predicting the effect of amino acid mutations on protein-protein binding is vital in protein engineering and therapeutic discovery. However, the scarcity of annotated experimental data on binding energy poses a significant challenge for developing computational approaches, particularly deep learning-based methods. In this work, we propose SidechainDiff, a representation learning-based approach that leverages unlabelled experimental protein structures. SidechainDiff utilizes a Riemannian diffusion model to learn the generative process of side-chain conformations and can also give the structural context representations of mutations on the protein-protein interface. Leveraging the learned representations, we achieve state-of-the-art performance in predicting the mutational effects on protein-protein binding. Furthermore, SidechainDiff is the first diffusion-based generative model for side-chains, distingui
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#38750;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34920;&#31034;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#21407;&#21017;&#36827;&#34892;&#25506;&#32034;&#26469;&#38477;&#20302;&#36951;&#25022;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#12289;&#25968;&#25454;&#30456;&#20851;&#30340;&#27979;&#37327;&#36873;&#25321;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#23569;&#30340;&#26679;&#26412;&#21644;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.19848</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#39537;&#21160;&#30340;&#39640;&#25928;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration in Continuous-time Model-based Reinforcement Learning. (arXiv:2310.19848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19848
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#38750;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34920;&#31034;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#21407;&#21017;&#36827;&#34892;&#25506;&#32034;&#26469;&#38477;&#20302;&#36951;&#25022;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#12289;&#25968;&#25454;&#30456;&#20851;&#30340;&#27979;&#37327;&#36873;&#25321;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#23569;&#30340;&#26679;&#26412;&#21644;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#32771;&#34385;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#65292;&#21363;&#20351;&#24213;&#23618;&#31995;&#32479;&#36890;&#24120;&#26159;&#36830;&#32493;&#26102;&#38388;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#34920;&#31034;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#25429;&#25417;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#20048;&#35266;&#21407;&#21017;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#34920;&#26126;&#20102;&#27979;&#37327;&#36873;&#25321;&#31574;&#30053;&#65288;MSS&#65289;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#22312;&#36830;&#32493;&#26102;&#38388;&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#24517;&#39035;&#20915;&#23450;&#22914;&#20309;&#36827;&#34892;&#25506;&#32034;&#65292;&#36824;&#24517;&#39035;&#20915;&#23450;&#20309;&#26102;&#35266;&#23519;&#24213;&#23618;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#23545;ODE&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36873;&#25321;&#24120;&#35265;&#30340;MSS&#65292;&#22914;&#31561;&#36317;&#37319;&#26679;&#65292;&#36951;&#25022;&#26159;&#27425;&#32447;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#25968;&#25454;&#30456;&#20851;&#30340;&#23454;&#29992;MSS&#65292;&#22312;&#19982;GP&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#26102;&#65292;&#20063;&#33021;&#20197;&#26356;&#23569;&#30340;&#26679;&#26412;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21327;&#21516;&#20316;&#29992;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the measurement selection strategy(MSS), since in continuous time we not only must decide how to explore, but also when to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an adaptive, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.19845</link><description>&lt;p&gt;
&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#65306;&#20197;XGBoost&#22312;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#20013;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22403;&#22334;&#37038;&#20214;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#21644;&#21830;&#19994;&#30028;&#30340;&#20851;&#27880;&#12290;Twitter&#24050;&#25104;&#20026;&#20256;&#25773;&#22403;&#22334;&#37038;&#20214;&#20869;&#23481;&#30340;&#39318;&#36873;&#23186;&#20171;&#12290;&#35768;&#22810;&#30740;&#31350;&#21162;&#21147;&#35797;&#22270;&#24212;&#23545;&#31038;&#20132;&#32593;&#32476;&#22403;&#22334;&#37038;&#20214;&#12290;Twitter&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#24120;&#65292;&#30456;&#20851;&#30740;&#31350;&#24037;&#20316;&#20851;&#27880;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#20027;&#35201;&#25361;&#25112;&#65292;&#25110;&#32773;&#20135;&#29983;&#40657;&#30418;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#35813;&#31639;&#27861;&#21021;&#22987;&#21270;&#20102;&#19968;&#20010;eXtreme Gradient Boosting&#20998;&#31867;&#22120;&#65292;&#24182;&#20943;&#23569;&#20102;&#25512;&#25991;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;50&#27425;&#37325;&#22797;&#30340;10&#20493;&#20998;&#23618;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#20351;&#29992;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#36827;&#34892;&#20998;&#26512;&#12290;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#24179;&#22343;&#36798;&#21040;82.32&#65285;&#21644;92.67&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#26497;&#38480;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#30005;&#35805;&#33829;&#38144;&#36807;&#31243;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#21644;&#25104;&#26412;&#25935;&#24863;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30005;&#35805;&#33829;&#38144;&#25968;&#25454;&#21644;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#23545;&#23458;&#25143;&#24847;&#24895;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26500;&#24314;&#20986;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.19843</link><description>&lt;p&gt;
&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#26497;&#38480;&#22686;&#24378;&#27169;&#22411;&#23545;&#30005;&#35805;&#33829;&#38144;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65306;&#29305;&#24449;&#36873;&#25321;&#21644;&#25104;&#26412;&#25935;&#24863;&#30340;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach. (arXiv:2310.19843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#26497;&#38480;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#30005;&#35805;&#33829;&#38144;&#36807;&#31243;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#21644;&#25104;&#26412;&#25935;&#24863;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30005;&#35805;&#33829;&#38144;&#25968;&#25454;&#21644;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#23545;&#23458;&#25143;&#24847;&#24895;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26500;&#24314;&#20986;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#30452;&#25509;&#33829;&#38144;&#27963;&#21160;&#37117;&#26159;&#36890;&#36807;&#34394;&#25311;&#26041;&#24335;&#32780;&#19981;&#26159;&#38754;&#23545;&#38754;&#36827;&#34892;&#30340;&#65292;&#36825;&#21152;&#24555;&#20102;&#20154;&#38469;&#20132;&#24448;&#25216;&#24039;&#30340;&#34928;&#36864;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#20225;&#19994;&#19968;&#30452;&#22312;&#21162;&#21147;&#24863;&#30693;&#21644;&#20419;&#36827;&#23458;&#25143;&#25509;&#21463;&#33829;&#38144;&#25552;&#26696;&#30340;&#20542;&#21521;&#12290;&#25968;&#23383;&#36716;&#22411;&#21644;&#22686;&#21152;&#30340;&#34394;&#25311;&#23384;&#22312;&#36843;&#20351;&#20225;&#19994;&#23547;&#27714;&#26032;&#30340;&#33829;&#38144;&#30740;&#31350;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#30005;&#35805;&#33829;&#38144;&#25968;&#25454;&#24314;&#27169;&#23458;&#25143;&#21150;&#29702;&#23450;&#26399;&#23384;&#27454;&#30340;&#24847;&#24895;&#65292;&#24182;&#25214;&#20986;&#23458;&#25143;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#12290;&#20351;&#29992;&#33889;&#33796;&#29273;&#38134;&#34892;&#30340;&#30495;&#23454;&#25968;&#25454;&#21644;&#22269;&#23478;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#26469;&#24314;&#27169;&#30005;&#35805;&#33829;&#38144;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#36873;&#25321;&#26368;&#20339;&#30340;&#21306;&#20998;&#29305;&#24449;&#21644;&#35843;&#25972;&#20998;&#31867;&#22120;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#32467;&#26500;&#21270;&#12289;&#20219;&#24847;&#38271;&#38899;&#20048;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#24314;&#35758;&#38899;&#20048;&#30340;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.19842</link><description>&lt;p&gt;
&#38899;&#20048;&#24418;&#24335;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Musical Form Generation. (arXiv:2310.19842v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#32467;&#26500;&#21270;&#12289;&#20219;&#24847;&#38271;&#38899;&#20048;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#24314;&#35758;&#38899;&#20048;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#24341;&#20154;&#20837;&#32988;&#30340;&#38899;&#20048;&#65292;&#20294;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#12290;&#38899;&#20048;&#20013;&#30340;&#21464;&#21270;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#65292;&#23548;&#33268;&#20316;&#21697;&#32570;&#20047;&#32467;&#26500;&#12290;&#36229;&#36807;&#19968;&#20998;&#38047;&#30340;&#20316;&#21697;&#21487;&#33021;&#21464;&#24471;&#19981;&#36830;&#36143;&#25110;&#37325;&#22797;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#32467;&#26500;&#21270;&#12289;&#20219;&#24847;&#38271;&#38899;&#20048;&#20316;&#21697;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#22312;&#36825;&#20123;&#29255;&#27573;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#12290;&#30830;&#23450;&#38899;&#20048;&#30340;&#39640;&#23618;&#26500;&#25104;&#30340;&#29983;&#25104;&#19982;&#21019;&#24314;&#32454;&#33410;&#30340;&#20302;&#23618;&#27425;&#32454;&#33410;&#30340;&#29983;&#25104;&#26159;&#19981;&#21516;&#30340;&#12290;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#24314;&#35758;&#38899;&#20048;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent generative models can produce engaging music, their utility is limited. The variation in the music is often left to chance, resulting in compositions that lack structure. Pieces extending beyond a minute can become incoherent or repetitive. This paper introduces an approach for generating structured, arbitrarily long musical pieces. Central to this approach is the creation of musical segments using a conditional generative model, with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#20998;&#26512;&#23433;&#20840;&#27675;&#22260;&#65292;&#36890;&#36807;&#32858;&#31867;&#21496;&#26426;&#32676;&#20307;&#30340;&#23433;&#20840;&#27675;&#22260;&#24863;&#30693;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#32452;&#32455;&#30340;&#21592;&#24037;&#29305;&#28857;&#65292;&#24182;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2310.19841</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#26041;&#27861;&#22312;&#23433;&#20840;&#27675;&#22260;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#30740;&#31350;&#21496;&#26426;&#32676;&#20307;&#22312;&#23433;&#20840;&#27675;&#22260;&#24863;&#30693;&#20013;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions. (arXiv:2310.19841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19841
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#20998;&#26512;&#23433;&#20840;&#27675;&#22260;&#65292;&#36890;&#36807;&#32858;&#31867;&#21496;&#26426;&#32676;&#20307;&#30340;&#23433;&#20840;&#27675;&#22260;&#24863;&#30693;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#32452;&#32455;&#30340;&#21592;&#24037;&#29305;&#28857;&#65292;&#24182;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#34892;&#19994;&#65292;&#29305;&#21035;&#26159;&#21345;&#36710;&#37096;&#38376;&#65292;&#23481;&#26131;&#21457;&#29983;&#24037;&#20316;&#22330;&#25152;&#20107;&#25925;&#21644;&#27515;&#20129;&#12290;&#28041;&#21450;&#22823;&#22411;&#21345;&#36710;&#30340;&#20107;&#25925;&#21344;&#21040;&#20102;&#25972;&#20307;&#20132;&#36890;&#20107;&#25925;&#27515;&#20129;&#20154;&#25968;&#30340;&#30456;&#24403;&#27604;&#20363;&#12290;&#35748;&#35782;&#21040;&#23433;&#20840;&#27675;&#22260;&#22312;&#20107;&#25925;&#39044;&#38450;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#20102;&#35299;&#20854;&#22240;&#32032;&#24182;&#22312;&#32452;&#32455;&#20013;&#27979;&#37327;&#20854;&#24433;&#21709;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#22411;&#23433;&#20840;&#27675;&#22260;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#23433;&#20840;&#27675;&#22260;&#24863;&#30693;&#23545;&#21592;&#24037;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#26159;&#21019;&#26032;&#30340;&#65292;&#24182;&#19988;&#23578;&#26410;&#22312;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#26681;&#25454;&#21496;&#26426;&#30340;&#23433;&#20840;&#27675;&#22260;&#24863;&#30693;&#26469;&#35782;&#21035;&#19981;&#21516;&#30340;&#32858;&#31867;&#32676;&#20307;&#65292;&#21487;&#20197;&#24110;&#21161;&#32452;&#32455;&#23545;&#20854;&#21592;&#24037;&#36827;&#34892;&#30011;&#20687;&#65292;&#24182;&#21046;&#23450;&#26356;&#26377;&#24433;&#21709;&#21147;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#27809;&#26377;&#24191;&#27867;&#24212;&#29992;&#32858;&#31867;&#26041;&#27861;&#21487;&#33021;&#26159;&#22240;&#20026;&#38590;&#20197;&#35299;&#37322;&#24433;&#21709;&#21592;&#24037;&#32858;&#31867;&#25104;&#21592;&#36164;&#26684;&#30340;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#23433;&#20840;&#30456;&#20851;&#30740;&#31350;&#24182;&#26410;&#27604;&#36739;&#22810;&#20010;&#32858;&#31867;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
The transportation industry, particularly the trucking sector, is prone to workplace accidents and fatalities. Accidents involving large trucks accounted for a considerable percentage of overall traffic fatalities. Recognizing the crucial role of safety climate in accident prevention, researchers have sought to understand its factors and measure its impact within organizations. While existing data-driven safety climate studies have made remarkable progress, clustering employees based on their safety climate perception is innovative and has not been extensively utilized in research. Identifying clusters of drivers based on their safety climate perception allows the organization to profile its workforce and devise more impactful interventions. The lack of utilizing the clustering approach could be due to difficulties interpreting or explaining the factors influencing employees' cluster membership. Moreover, existing safety-related studies did not compare multiple clustering algorithms, r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29983;&#25104;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#36793;&#30028;&#26694;&#65292;&#25552;&#20379;&#20102;&#23545;&#21307;&#23398;&#25104;&#20687;&#35786;&#26029;&#30340;&#35299;&#37322;&#65292;&#24182;&#35797;&#22270;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#20013;&#36793;&#30028;&#26694;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19835</link><description>&lt;p&gt;
CrossEAI: &#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#36793;&#30028;&#26694;
&lt;/p&gt;
&lt;p&gt;
CrossEAI: Using Explainable AI to generate better bounding boxes for Chest X-ray images. (arXiv:2310.19835v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19835
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29983;&#25104;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#36793;&#30028;&#26694;&#65292;&#25552;&#20379;&#20102;&#23545;&#21307;&#23398;&#25104;&#20687;&#35786;&#26029;&#30340;&#35299;&#37322;&#65292;&#24182;&#35797;&#22270;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#20013;&#36793;&#30028;&#26694;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#24212;&#29992;&#26681;&#25454;&#27861;&#24459;&#27861;&#35268;&#21644;&#36131;&#20219;&#35201;&#27714;&#21521;&#24739;&#32773;&#21644;&#21307;&#29983;&#25552;&#20379;&#35299;&#37322;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#38598;&#25104;&#26799;&#24230;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#20351;&#29992;LIME&#30340;&#27169;&#22411;&#36924;&#36817;&#65292;&#25110;&#32773;&#20351;&#29992;&#31070;&#32463;&#20803;&#28608;&#27963;&#21644;&#23618;&#23548;&#30005;&#26469;&#25552;&#20379;&#23545;&#26576;&#20123;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#35786;&#26029;&#20013;&#65292;&#30142;&#30149;&#20998;&#31867;&#36890;&#24120;&#33021;&#22815;&#36798;&#21040;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#29983;&#25104;&#30340;&#36793;&#30028;&#26694;&#30340;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#35201;&#20302;&#24471;&#22810;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#36793;&#30028;&#26694;&#29983;&#25104;&#26041;&#38754;&#24456;&#23569;&#26377;&#25913;&#36827;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#36793;&#30028;&#26694;&#36890;&#24120;&#27604;&#30495;&#23454;&#20540;&#26356;&#22823;&#65292;&#24182;&#21253;&#21547;&#20027;&#35201;&#30340;&#38750;&#30142;&#30149;&#21306;&#22495;&#12290;&#26412;&#25991;&#21033;&#29992;&#20107;&#21518;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20026;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#29983;&#25104;&#36793;&#30028;&#26694;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability is critical for deep learning applications in healthcare which are mandated to provide interpretations to both patients and doctors according to legal regulations and responsibilities. Explainable AI methods, such as feature importance using integrated gradients, model approximation using LIME, or neuron activation and layer conductance to provide interpretations for certain health risk predictions. In medical imaging diagnosis, disease classification usually achieves high accuracy, but generated bounding boxes have much lower Intersection over Union (IoU). Different methods with self-supervised or semi-supervised learning strategies have been proposed, but few improvements have been identified for bounding box generation. Previous work shows that bounding boxes generated by these methods are usually larger than ground truth and contain major non-disease area. This paper utilizes the advantages of post-hoc AI explainable methods to generate bounding boxes for chest x-ray
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#22522;&#20110;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26469;&#29702;&#35299;&#20915;&#31574;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36879;&#26126;&#24230;&#12289;&#36866;&#24212;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#23436;&#20840;&#31163;&#32447;&#36816;&#34892;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#38382;&#39064;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20316;&#20026;&#23457;&#35745;&#21644;&#20998;&#26512;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19831</link><description>&lt;p&gt;
&#27169;&#20223;&#35299;&#37322;&#65306;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#29702;&#35299;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning. (arXiv:2310.19831v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#22522;&#20110;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26469;&#29702;&#35299;&#20915;&#31574;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36879;&#26126;&#24230;&#12289;&#36866;&#24212;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#23436;&#20840;&#31163;&#32447;&#36816;&#34892;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#38382;&#39064;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20316;&#20026;&#23457;&#35745;&#21644;&#20998;&#26512;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#23545;&#20110;&#36879;&#26126;&#24230;&#21644;&#20915;&#31574;&#30340;&#38382;&#36131;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#24314;&#27169;&#20915;&#31574;&#32773;&#30340;&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#8212;&#8212;&#27809;&#26377;&#35775;&#38382;&#24213;&#23618;&#29366;&#24577;&#30340;&#26435;&#38480;&#65292;&#27809;&#26377;&#20102;&#35299;&#29615;&#22659;&#21160;&#24577;&#30340;&#30693;&#35782;&#65292;&#20063;&#27809;&#26377;&#36827;&#34892;&#23454;&#26102;&#23454;&#39564;&#30340;&#23481;&#38169;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#23398;&#20064;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#34892;&#20026;&#34920;&#31034;&#65292;&#23427;&#20855;&#26377;&#65288;1&#65289;&#35774;&#35745;&#19978;&#30340;&#36879;&#26126;&#24230;&#65292;&#65288;2&#65289;&#36866;&#24212;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#65288;3&#65289;&#23436;&#20840;&#31163;&#32447;&#36816;&#34892;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#20851;&#38190;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#65288;"Interpole"&#65289;&#65292;&#23427;&#21516;&#26102;&#20272;&#35745;&#19968;&#20010;&#20195;&#29702;&#20154;&#30340;&#65288;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#65289;&#32622;&#20449;&#26356;&#26032;&#36807;&#31243;&#20197;&#21450;&#20182;&#20204;&#65288;&#21487;&#33021;&#27425;&#20248;&#30340;&#65289;&#20449;&#24565;&#21160;&#20316;&#26144;&#23556;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#38382;&#39064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#23457;&#35745;&#21644;&#20998;&#26512;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker's policy is challenging -- with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision-making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning ("Interpole") that jointly estimates an agent's (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer's disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, qua
&lt;/p&gt;</description></item><item><title>FuXi-Extreme&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#24674;&#22797;&#31934;&#32454;&#23610;&#24230;&#30340;&#32454;&#33410;&#65292;&#25552;&#39640;&#20102;&#26497;&#31471;&#38477;&#38632;&#21644;&#39118;&#26292;&#39044;&#25253;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19822</link><description>&lt;p&gt;
FuXi-Extreme: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#26497;&#31471;&#38477;&#38632;&#21644;&#39118;&#26292;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model. (arXiv:2310.19822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19822
&lt;/p&gt;
&lt;p&gt;
FuXi-Extreme&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#24674;&#22797;&#31934;&#32454;&#23610;&#24230;&#30340;&#32454;&#33410;&#65292;&#25552;&#39640;&#20102;&#26497;&#31471;&#38477;&#38632;&#21644;&#39118;&#26292;&#39044;&#25253;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;FuXi&#31561;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;ML&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#32479;&#35745;&#39044;&#25253;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#38754;&#20020;&#19968;&#20010;&#20849;&#21516;&#30340;&#25361;&#25112;&#65306;&#38543;&#30528;&#39044;&#25253;&#26102;&#38388;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#29983;&#25104;&#36234;&#26469;&#36234;&#24179;&#28369;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#23548;&#33268;&#23545;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#24378;&#24230;&#20302;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FuXi-Extreme&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#24674;&#22797;&#30001;FuXi&#27169;&#22411;&#22312;5&#22825;&#39044;&#25253;&#20013;&#29983;&#25104;&#30340;&#34920;&#38754;&#39044;&#25253;&#25968;&#25454;&#20013;&#30340;&#26356;&#32454;&#33410;&#23610;&#24230;&#30340;&#32454;&#33410;&#12290;&#23545;&#26497;&#31471;&#38477;&#27700;&#65288;$\textrm{TP}$&#65289;&#12289;10&#31859;&#39118;&#36895;&#65288;$\textrm{WS10}$&#65289;&#21644;2&#31859;&#28201;&#24230;&#65288;$\textrm{T2M}$&#65289;&#30340;&#35780;&#20272;&#35828;&#26126;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant advancements in the development of machine learning (ML) models for weather forecasting have produced remarkable results. State-of-the-art ML-based weather forecast models, such as FuXi, have demonstrated superior statistical forecast performance in comparison to the high-resolution forecasts (HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF). However, ML models face a common challenge: as forecast lead times increase, they tend to generate increasingly smooth predictions, leading to an underestimation of the intensity of extreme weather events. To address this challenge, we developed the FuXi-Extreme model, which employs a denoising diffusion probabilistic model (DDPM) to restore finer-scale details in the surface forecast data generated by the FuXi model in 5-day forecasts. An evaluation of extreme total precipitation ($\textrm{TP}$), 10-meter wind speed ($\textrm{WS10}$), and 2-meter temperature ($\textrm{T2M}$) illustrates the superior performance 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#33258;&#36866;&#24212;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#39118;&#38505;&#24230;&#37327;&#21644;&#37325;&#21551;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#27874;&#21160;&#24615;&#39046;&#22495;&#20013;&#31616;&#21333;&#22870;&#21169;&#26368;&#22823;&#21270;&#26041;&#27861;&#30340;&#19981;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19821</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#38750;&#24179;&#31283;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#39118;&#38505;&#35268;&#36991;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Risk-Averse Framework for Non-Stationary Stochastic Multi-Armed Bandits. (arXiv:2310.19821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19821
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#33258;&#36866;&#24212;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#39118;&#38505;&#24230;&#37327;&#21644;&#37325;&#21551;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#27874;&#21160;&#24615;&#39046;&#22495;&#20013;&#31616;&#21333;&#22870;&#21169;&#26368;&#22823;&#21270;&#26041;&#27861;&#30340;&#19981;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20856;&#22411;&#30340;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#36890;&#24120;&#26159;&#22312;&#19968;&#23450;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#24635;&#21644;&#12290;&#28982;&#32780;&#65292;&#24403;&#25552;&#20379;&#39069;&#22806;&#30340;&#29615;&#22659;&#29305;&#23450;&#30693;&#35782;&#26102;&#65292;&#36873;&#25321;&#19968;&#20010;&#33021;&#22815;&#36798;&#21040;&#26368;&#20248;&#30340;&#31574;&#30053;&#19981;&#20877;&#36866;&#29992;&#12290;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#25110;&#37329;&#34701;&#31561;&#39640;&#27874;&#21160;&#24615;&#39046;&#22495;&#65292;&#31616;&#21333;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#26041;&#27861;&#24448;&#24448;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#23398;&#20064;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21508;&#31181;&#39118;&#38505;&#24230;&#37327;&#65292;&#23558;&#22810;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#26063;&#26144;&#23556;&#21040;&#39118;&#38505;&#25935;&#24863;&#35774;&#32622;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#31639;&#27861;&#37197;&#22791;&#20102;&#37325;&#21551;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#65288;R-BOCPD&#65289;&#31639;&#27861;&#65292;&#24182;&#26045;&#21152;&#20102;&#65288;&#21487;&#35843;&#33410;&#30340;&#65289;&#24378;&#21046;&#32456;&#27490;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a typical stochastic multi-armed bandit problem, the objective is often to maximize the expected sum of rewards over some time horizon $T$. While the choice of a strategy that accomplishes that is optimal with no additional information, it is no longer the case when provided additional environment-specific knowledge. In particular, in areas of high volatility like healthcare or finance, a naive reward maximization approach often does not accurately capture the complexity of the learning problem and results in unreliable solutions. To tackle problems of this nature, we propose a framework of adaptive risk-aware strategies that operate in non-stationary environments. Our framework incorporates various risk measures prevalent in the literature to map multiple families of multi-armed bandit algorithms into a risk-sensitive setting. In addition, we equip the resulting algorithms with the Restarted Bayesian Online Change-Point Detection (R-BOCPD) algorithm and impose a (tunable) forced ex
&lt;/p&gt;</description></item><item><title>NetDistiller&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26435;&#37325;&#20849;&#20139;&#25945;&#24072;&#27169;&#22411;&#30340;&#23376;&#32593;&#32476;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21644;&#19981;&#30830;&#23450;&#24615;-aware&#30340;&#33976;&#39311;&#26469;&#25552;&#39640;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19820</link><description>&lt;p&gt;
NetDistiller: &#36890;&#36807;&#21407;&#22320;&#33976;&#39311;&#22686;&#24378;&#24494;&#22411;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation. (arXiv:2310.19820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19820
&lt;/p&gt;
&lt;p&gt;
NetDistiller&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26435;&#37325;&#20849;&#20139;&#25945;&#24072;&#27169;&#22411;&#30340;&#23376;&#32593;&#32476;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21644;&#19981;&#30830;&#23450;&#24615;-aware&#30340;&#33976;&#39311;&#26469;&#25552;&#39640;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476; (TNNs) &#38754;&#20020;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#35774;&#22791;&#22312;&#20869;&#23384;&#12289;&#35745;&#31639;&#12289;&#24102;&#23485;&#21644;&#30005;&#28304;&#26041;&#38754;&#37117;&#26377;&#20005;&#26684;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NetDistiller&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;TNNs&#30475;&#20316;&#26159;&#36890;&#36807;&#25193;&#23637;TNN&#36890;&#36947;&#25968;&#32780;&#26500;&#36896;&#30340;&#26435;&#37325;&#20849;&#20139;&#25945;&#24072;&#27169;&#22411;&#30340;&#23376;&#32593;&#32476;&#65292;&#25552;&#39640;TNNs&#30340;&#21487;&#23454;&#29616;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30446;&#26631;TNN&#27169;&#22411;&#19982;&#26435;&#37325;&#20849;&#20139;&#25945;&#24072;&#27169;&#22411;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26041;&#27861;&#21253;&#25324;(1)&#36890;&#36807;&#26799;&#24230;&#25163;&#26415;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#26799;&#24230;&#20914;&#31361;&#65292;&#20197;&#21450;(2)&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#33976;&#39311;&#26469;&#20943;&#36731;&#25945;&#24072;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;NetDistiller&#22312;&#25552;&#39640;TNNs&#21487;&#23454;&#29616;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/GATECH-EIC/NetDistiller&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployments of TNNs on edge devices which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as sub-networks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN model is jointly trained with the weight-sharing teacher model via (1) gradient surgery to tackle the gradient conflicts between them and (2) uncertainty-aware distillation to mitigate the overfitting of the teacher model. Extensive experiments across diverse tasks validate NetDistiller's effectiveness in boosting TNNs' achievable accuracy over state-of-the-art methods. Our code is available at https://github.com/GATECH-EIC/NetDistiller.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#20449;&#20219;&#21644;&#30693;&#35782;&#30340;&#24418;&#25104;&#33267;&#20851;&#37325;&#35201;&#65292;&#21482;&#26377;&#22312;&#27491;&#30830;&#29305;&#24449;&#19979;&#36328;&#24773;&#26223;&#33391;&#22909;&#36816;&#34892;&#30340;&#31639;&#27861;&#25165;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.19819</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#65306;&#20026;&#20160;&#20040;&#40065;&#26834;&#24615;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Knowledge: Why Robustness Matters. (arXiv:2310.19819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19819
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#20449;&#20219;&#21644;&#30693;&#35782;&#30340;&#24418;&#25104;&#33267;&#20851;&#37325;&#35201;&#65292;&#21482;&#26377;&#22312;&#27491;&#30830;&#29305;&#24449;&#19979;&#36328;&#24773;&#26223;&#33391;&#22909;&#36816;&#34892;&#30340;&#31639;&#27861;&#25165;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#23545;&#20854;&#36755;&#20986;&#26377;&#20449;&#24515;&#12290;&#20449;&#24515;&#36890;&#24120;&#20197;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#37322;&#65292;&#21363;&#27169;&#22411;&#20135;&#29983;&#39640;&#27604;&#20363;&#27491;&#30830;&#36755;&#20986;&#26102;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21487;&#38752;&#24615;&#19981;&#33021;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#27169;&#22411;&#20381;&#36182;&#38169;&#35823;&#29305;&#24449;&#25110;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#21464;&#21270;&#12290;&#25105;&#35748;&#20026;&#65292;&#23545;&#20449;&#20219;&#30340;&#35748;&#35782;&#32500;&#24230;&#21487;&#20197;&#36890;&#36807;&#30693;&#35782;&#30340;&#27010;&#24565;&#26469;&#29702;&#35299;&#65292;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#21462;&#20915;&#20110;&#20854;&#29992;&#25143;&#26159;&#21542;&#33021;&#22815;&#30830;&#35748;&#20854;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#12290;&#30693;&#35782;&#35201;&#27714;&#20449;&#24565;&#22522;&#20110;&#27491;&#30830;&#30340;&#21407;&#22240;&#24418;&#25104;&#65292;&#24182;&#19988;&#23545;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21482;&#26377;&#22312;&#36328;&#21453;&#20107;&#23454;&#24773;&#26223;&#20013;&#33391;&#22909;&#36816;&#34892;&#65292;&#24182;&#22522;&#20110;&#27491;&#30830;&#29305;&#24449;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#25165;&#33021;&#25552;&#20379;&#30693;&#35782;&#12290;&#25105;&#35748;&#20026;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#25105;&#20204;&#24212;&#35813;&#20851;&#24515;&#20687;&#21487;&#35299;&#37322;&#24615;&#36825;&#26679;&#30340;&#27169;&#22411;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trusting machine learning algorithms requires having confidence in their outputs. Confidence is typically interpreted in terms of model reliability, where a model is reliable if it produces a high proportion of correct outputs. However, model reliability does not address concerns about the robustness of machine learning models, such as models relying on the wrong features or variations in performance based on context. I argue that the epistemic dimension of trust can instead be understood through the concept of knowledge, where the trustworthiness of an algorithm depends on whether its users are in the position to know that its outputs are correct. Knowledge requires beliefs to be formed for the right reasons and to be robust to error, so machine learning algorithms can only provide knowledge if they work well across counterfactual scenarios and if they make decisions based on the right features. This, I argue, can explain why we should care about model properties like interpretability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25805;&#20316;&#26356;&#22823;&#36755;&#20837;&#22270;&#20687;&#65288;256x256&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23581;&#35797;&#21306;&#20998;&#19981;&#21516;GPU&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#29992;&#25143;&#36873;&#25321;&#21512;&#36866;&#30340;GPU&#20197;&#23454;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#24615;&#33021;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2310.19816</link><description>&lt;p&gt;
&#22522;&#20110;SVBRDF&#25552;&#21462;&#27169;&#22411;&#30340;GPU&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking GPUs on SVBRDF Extractor Model. (arXiv:2310.19816v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25805;&#20316;&#26356;&#22823;&#36755;&#20837;&#22270;&#20687;&#65288;256x256&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23581;&#35797;&#21306;&#20998;&#19981;&#21516;GPU&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#29992;&#25143;&#36873;&#25321;&#21512;&#36866;&#30340;GPU&#20197;&#23454;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#24615;&#33021;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#29087;&#65292;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#24840;&#21457;&#24191;&#27867;&#12290;&#21516;&#26102;&#65292;&#24066;&#22330;&#19978;&#19981;&#21516;&#31867;&#22411;&#30340;GPU&#20063;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#20026;&#29992;&#25143;&#36873;&#25321;&#21512;&#36866;&#30340;GPU&#24102;&#26469;&#20102;&#22256;&#25200;&#12290;&#29992;&#25143;&#22914;&#20309;&#36873;&#25321;GPU&#20197;&#23454;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#24615;&#33021;&#21602;&#65311;GPU&#26550;&#26500;&#30340;&#20998;&#26512;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#29616;&#26377;&#30340;GPU&#24615;&#33021;&#35780;&#20272;&#24037;&#20316;&#24182;&#26410;&#30740;&#31350;&#23545;&#20110;&#22788;&#29702;&#26356;&#22823;&#36755;&#20837;&#22270;&#20687;&#65288;256x256&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;GPU&#22312;&#25805;&#20316;&#36739;&#22823;&#36755;&#20837;&#22270;&#20687;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the maturity of deep learning, its use is emerging in every field. Also, as different types of GPUs are becoming more available in the markets, it creates a difficult decision for users. How can users select GPUs to achieve optimal performance for a specific task? Analysis of GPU architecture is well studied, but existing works that benchmark GPUs do not study tasks for networks with significantly larger input. In this work, we tried to differentiate the performance of different GPUs on neural network models that operate on bigger input images (256x256).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25299;&#25169;&#21464;&#21270;&#21644;&#31574;&#30053;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#35757;&#32451;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#20302;&#33021;&#32791;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.19815</link><description>&lt;p&gt;
&#35757;&#32451;&#26080;&#38656;&#28014;&#28857;&#31934;&#24230;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training binary neural networks without floating point precision. (arXiv:2310.19815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25299;&#25169;&#21464;&#21270;&#21644;&#31574;&#30053;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#35757;&#32451;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#20302;&#33021;&#32791;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#35757;&#32451;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#20302;&#33021;&#32791;&#30340;&#29305;&#28857;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25299;&#25169;&#21464;&#21270;&#21644;&#31574;&#30053;&#35757;&#32451;&#65292;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#26159;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main goal of this work is to improve the efficiency of training binary neural networks, which are low latency and low energy networks. The main contribution of this work is the proposal of two solutions comprised of topology changes and strategy training that allow the network to achieve near the state-of-the-art performance and efficient training. The time required for training and the memory required in the process are two factors that contribute to efficient training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#35013;&#31665;&#38382;&#39064;&#12290;&#36890;&#36807;&#23398;&#20064;&#26799;&#24230;&#22330;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#23545;&#35937;&#26377;&#25928;&#24615;&#32422;&#26463;&#21644;&#30896;&#25758;&#36991;&#20813;&#31561;&#25361;&#25112;&#65292;&#24182;&#29983;&#25104;&#26368;&#20248;&#30340;&#35013;&#31665;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.19814</link><description>&lt;p&gt;
&#23398;&#20064;&#26799;&#24230;&#22330;&#29992;&#20110;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#19981;&#35268;&#21017;&#35013;&#31665;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Gradient Fields for Scalable and Generalizable Irregular Packing. (arXiv:2310.19814v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#35013;&#31665;&#38382;&#39064;&#12290;&#36890;&#36807;&#23398;&#20064;&#26799;&#24230;&#22330;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#23545;&#35937;&#26377;&#25928;&#24615;&#32422;&#26463;&#21644;&#30896;&#25758;&#36991;&#20813;&#31561;&#25361;&#25112;&#65292;&#24182;&#29983;&#25104;&#26368;&#20248;&#30340;&#35013;&#31665;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#31665;&#38382;&#39064;&#22312;&#29289;&#27969;&#12289;&#21046;&#36896;&#12289;&#24067;&#23616;&#35774;&#35745;&#21644;&#22270;&#38598;&#29983;&#25104;&#31561;&#39046;&#22495;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#23427;&#28041;&#21450;&#23558;&#19981;&#35268;&#21017;&#24418;&#29366;&#30340;&#29289;&#21697;&#25490;&#21015;&#22909;&#65292;&#20197;&#26368;&#23567;&#21270;&#28010;&#36153;&#24182;&#36991;&#20813;&#37325;&#21472;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#35013;&#31665;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#35013;&#31665;&#38382;&#39064;&#21046;&#23450;&#20026;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#19981;&#35268;&#21017;&#35013;&#31665;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#35937;&#26377;&#25928;&#24615;&#32422;&#26463;&#21644;&#30896;&#25758;&#36991;&#20813;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#26799;&#24230;&#22330;&#12290;&#36825;&#20123;&#26799;&#24230;&#22330;&#32534;&#30721;&#20102;&#32422;&#26463;&#28385;&#36275;&#19982;&#22810;&#36793;&#24418;&#31354;&#38388;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#25945;&#24072;&#26696;&#20363;&#20013;&#23398;&#20064;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#26799;&#24230;&#22330;&#24341;&#23548;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;&#21040;&#31895;&#31890;&#24230;&#30340;&#25913;&#36827;&#26426;&#21046;&#29983;&#25104;&#35013;&#31665;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The packing problem, also known as cutting or nesting, has diverse applications in logistics, manufacturing, layout design, and atlas generation. It involves arranging irregularly shaped pieces to minimize waste while avoiding overlap. Recent advances in machine learning, particularly reinforcement learning, have shown promise in addressing the packing problem. In this work, we delve deeper into a novel machine learning-based approach that formulates the packing problem as conditional generative modeling. To tackle the challenges of irregular packing, including object validity constraints and collision avoidance, our method employs the score-based diffusion model to learn a series of gradient fields. These gradient fields encode the correlations between constraint satisfaction and the spatial relationships of polygons, learned from teacher examples. During the testing phase, packing solutions are generated using a coarse-to-fine refinement mechanism guided by the learned gradient field
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36951;&#20256;&#25913;&#36827;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#26469;&#25552;&#39640;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20351;&#29992;LLM&#32534;&#36753;&#30340;&#34917;&#19969;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#25968;&#37327;&#39640;&#36798;75&#65285;&#65292;&#20294;&#30456;&#27604;&#36739;&#26631;&#20934;&#32534;&#36753;&#65292;LLMs&#25214;&#21040;&#30340;&#34917;&#19969;&#36739;&#23569;&#22810;&#26679;&#21270;&#12290;&#23613;&#31649;LLM&#22686;&#24378;&#30340;GI&#25214;&#21040;&#20102;&#35768;&#22810;&#25913;&#36827;&#30340;&#34917;&#19969;&#65292;&#20294;&#26368;&#22909;&#30340;&#25913;&#36827;&#34917;&#19969;&#26159;&#36890;&#36807;&#26631;&#20934;GI&#25214;&#21040;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.19813</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36951;&#20256;&#25913;&#36827;&#31361;&#21464;
&lt;/p&gt;
&lt;p&gt;
Enhancing Genetic Improvement Mutations Using Large Language Models. (arXiv:2310.19813v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36951;&#20256;&#25913;&#36827;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#26469;&#25552;&#39640;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20351;&#29992;LLM&#32534;&#36753;&#30340;&#34917;&#19969;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#25968;&#37327;&#39640;&#36798;75&#65285;&#65292;&#20294;&#30456;&#27604;&#36739;&#26631;&#20934;&#32534;&#36753;&#65292;LLMs&#25214;&#21040;&#30340;&#34917;&#19969;&#36739;&#23569;&#22810;&#26679;&#21270;&#12290;&#23613;&#31649;LLM&#22686;&#24378;&#30340;GI&#25214;&#21040;&#20102;&#35768;&#22810;&#25913;&#36827;&#30340;&#34917;&#19969;&#65292;&#20294;&#26368;&#22909;&#30340;&#25913;&#36827;&#34917;&#19969;&#26159;&#36890;&#36807;&#26631;&#20934;GI&#25214;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#31243;&#24207;&#20462;&#22797;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36951;&#20256;&#25913;&#36827;&#65288;GI&#65289;&#31561;&#22522;&#20110;&#25628;&#32034;&#30340;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;LLMs&#20316;&#20026;GI&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#20197;&#25913;&#36827;&#25628;&#32034;&#36807;&#31243;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Gin Java GI&#24037;&#20855;&#21253;&#65292;&#20197;&#35843;&#29992;OpenAI&#30340;API&#20026;JCodec&#24037;&#20855;&#29983;&#25104;&#32534;&#36753;&#12290;&#25105;&#20204;&#20351;&#29992;5&#31181;&#19981;&#21516;&#30340;&#32534;&#36753;&#31867;&#22411;&#38543;&#26426;&#25277;&#26679;&#32534;&#36753;&#31354;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;LLM&#32534;&#36753;&#65292;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#34917;&#19969;&#25968;&#37327;&#39640;&#20110;&#20351;&#29992;&#26631;&#20934;&#25554;&#20837;&#32534;&#36753;&#30340;&#34917;&#19969;&#25968;&#37327;&#39640;&#36798;75&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#26631;&#20934;&#32534;&#36753;&#30456;&#27604;&#65292;LLMs&#25214;&#21040;&#30340;&#34917;&#19969;&#36890;&#24120;&#36739;&#23569;&#22810;&#26679;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#36816;&#34892;GI&#20197;&#23547;&#25214;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;&#23613;&#31649;LLM&#22686;&#24378;&#30340;GI&#25214;&#21040;&#20102;&#35768;&#22810;&#25913;&#36827;&#30340;&#34917;&#19969;&#65292;&#20294;&#26368;&#22909;&#30340;&#25913;&#36827;&#34917;&#19969;&#26159;&#36890;&#36807;&#26631;&#20934;GI&#25214;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19812</link><description>&lt;p&gt;
&#33041;&#35299;&#30721;&#65306;&#36208;&#21521;&#23454;&#26102;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20116;&#24180;&#20013;&#65292;&#29983;&#25104;&#24335;&#21644;&#22522;&#30784;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20351;&#29992;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23545;&#22823;&#33041;&#27963;&#21160;&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#35273;&#30693;&#35273;&#65292;&#29616;&#22312;&#21487;&#20197;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#35299;&#30721;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#26377;&#38480;&#65288;&#32422;&#20026;0.5 Hz&#65289;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#65288;&#32422;&#20026;5000 Hz&#65289;&#27979;&#37327;&#33041;&#27963;&#21160;&#30340;&#31070;&#32463;&#24433;&#20687;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;MEG&#35299;&#30721;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#21644;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i&#65289;&#20174;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;ii&#65289;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;MEG&#27169;&#22359;&#20197;&#21450;iii&#65289;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;MEG&#35299;&#30721;&#22120;&#22312;&#32463;&#20856;&#32447;&#24615;&#35299;&#30721;&#22120;&#19978;&#26174;&#31034;&#20986;7&#20493;&#30340;&#22270;&#20687;&#26816;&#32034;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#21518;&#26399;&#33041;&#37096;
&lt;/p&gt;
&lt;p&gt;
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#25968;&#25454;&#27969;&#30740;&#31350;&#30340;&#21382;&#21490;&#32972;&#26223;&#65292;&#24182;&#23558;&#26426;&#22120;&#23398;&#20064;&#23545;&#25968;&#25454;&#27969;&#30340;&#24120;&#35265;&#20551;&#35774;&#25918;&#32622;&#22312;&#20854;&#21382;&#21490;&#32972;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.19811</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#30340;&#21382;&#21490;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Historical Context for Data Streams. (arXiv:2310.19811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#25968;&#25454;&#27969;&#30740;&#31350;&#30340;&#21382;&#21490;&#32972;&#26223;&#65292;&#24182;&#23558;&#26426;&#22120;&#23398;&#20064;&#23545;&#25968;&#25454;&#27969;&#30340;&#24120;&#35265;&#20551;&#35774;&#25918;&#32622;&#22312;&#20854;&#21382;&#21490;&#32972;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#27963;&#36291;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20851;&#20110;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#36890;&#24120;&#23545;&#19982;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#30456;&#20851;&#30340;&#20005;&#26684;&#20551;&#35774;&#36827;&#34892;&#25506;&#35752;&#65292;&#21253;&#25324;&#23545;&#27969;&#25366;&#25496;&#31639;&#27861;&#27599;&#20010;&#23454;&#20363;&#21482;&#33021;&#26816;&#26597;&#19968;&#27425;&#65292;&#24182;&#19988;&#38543;&#26102;&#20934;&#22791;&#22909;&#32473;&#20986;&#39044;&#27979;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#25968;&#25454;&#27969;&#30740;&#31350;&#30340;&#21382;&#21490;&#32972;&#26223;&#65292;&#24182;&#23558;&#26426;&#22120;&#23398;&#20064;&#23545;&#25968;&#25454;&#27969;&#30340;&#24120;&#35265;&#20551;&#35774;&#25918;&#32622;&#22312;&#20854;&#21382;&#21490;&#32972;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning from data streams is an active and growing research area. Research on learning from streaming data typically makes strict assumptions linked to computational resource constraints, including requirements for stream mining algorithms to inspect each instance not more than once and be ready to give a prediction at any time. Here we review the historical context of data streams research placing the common assumptions used in machine learning over data streams in their historical context.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#20102;&#24433;&#21709;&#24503;&#40657;&#20848;&#24555;&#36895;&#24052;&#22763;&#31995;&#32479;&#20934;&#28857;&#24615;&#30340;&#22240;&#32032;&#65292;&#24182;&#26500;&#24314;&#20102;&#20934;&#30830;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20844;&#20132;&#32447;&#36335;&#26159;&#21542;&#31526;&#21512;&#20934;&#26102;&#24615;&#26631;&#20934;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#24433;&#21709;&#20844;&#20132;&#32447;&#36335;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.19810</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20844;&#20849;&#27773;&#36710;&#36816;&#36755;&#20998;&#26512;&#20013;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Advantages of Machine Learning in Bus Transport Analysis. (arXiv:2310.19810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#20102;&#24433;&#21709;&#24503;&#40657;&#20848;&#24555;&#36895;&#24052;&#22763;&#31995;&#32479;&#20934;&#28857;&#24615;&#30340;&#22240;&#32032;&#65292;&#24182;&#26500;&#24314;&#20102;&#20934;&#30830;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20844;&#20132;&#32447;&#36335;&#26159;&#21542;&#31526;&#21512;&#20934;&#26102;&#24615;&#26631;&#20934;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#24433;&#21709;&#20844;&#20132;&#32447;&#36335;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#20102;&#24433;&#21709;&#24503;&#40657;&#20848;&#24555;&#36895;&#24052;&#22763;&#31995;&#32479;&#20934;&#28857;&#24615;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;2020&#24180;&#21040;2022&#24180;&#30340;&#24503;&#40657;&#20848;&#24066;&#25919;&#24220;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#31639;&#27861;&#21644;Python&#30340;Sci Kit Learn&#21644;Stats Models&#24211;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#20219;&#20309;&#32473;&#23450;&#26085;&#26399;&#20844;&#20132;&#32447;&#36335;&#26159;&#21542;&#31526;&#21512;&#20934;&#26102;&#24615;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#27599;&#20010;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#30830;&#23450;&#23427;&#25152;&#32771;&#34385;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#22240;&#32032;&#12290;&#36825;&#39033;&#35843;&#26597;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#26174;&#33879;&#24433;&#21709;&#20844;&#20132;&#32447;&#36335;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#20026;&#25913;&#36827;&#20854;&#24615;&#33021;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Machine Learning is an innovative method that aims to mimic human learning by using past experiences. In this study, we utilize supervised machine learning algorithms to analyze the factors that contribute to the punctuality of Tehran BRT bus system. We gather publicly available datasets of 2020 to 2022 from Municipality of Tehran to train and test our models. By employing various algorithms and leveraging Python's Sci Kit Learn and Stats Models libraries, we construct accurate models capable of predicting whether a bus route will meet the prescribed standards for on-time performance on any given day. Furthermore, we delve deeper into the decision-making process of each algorithm to determine the most influential factor it considers. This investigation allows us to uncover the key feature that significantly impacts the effectiveness of bus routes, providing valuable insights for improving their performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19809</link><description>&lt;p&gt;
MgNO:&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
MgNO: Efficient Parameterization of Linear Operators via Multigrid. (arXiv:2310.19809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#26469;&#36827;&#34892;&#31639;&#23376;&#23398;&#20064;&#12290;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#27604;&#65292;&#23558;&#31070;&#32463;&#31639;&#23376;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#31639;&#23376;&#23618;&#20013;&#31532;$i$&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20986;&#65292;&#35760;&#20316;$\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$&#12290;&#20854;&#20013;&#65292;$\mathcal W_{ij}$&#34920;&#31034;&#36830;&#25509;&#31532;$j$&#20010;&#36755;&#20837;&#31070;&#32463;&#20803;&#21644;&#31532;$i$&#20010;&#36755;&#20986;&#31070;&#32463;&#20803;&#30340;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#32780;&#20559;&#24046;$\mathcal B_{ij}$&#37319;&#29992;&#20989;&#25968;&#24418;&#24335;&#32780;&#38750;&#26631;&#37327;&#24418;&#24335;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31070;&#32463;&#20803;&#65288;Banach&#31354;&#38388;&#65289;&#20043;&#38388;&#26377;&#25928;&#21442;&#25968;&#21270;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;MgNO&#24341;&#20837;&#20102;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#26082;&#20855;&#22791;&#20102;&#25968;&#23398;&#20005;&#23494;&#24615;&#65292;&#21448;&#20855;&#22791;&#20102;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;MgNO&#28040;&#38500;&#20102;&#23545;&#20256;&#32479;&#30340;lifting&#21644;projecting&#25805;&#20316;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting ope
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26680;&#30340;&#35282;&#24230;&#35770;&#36848;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#34892;&#20026;&#24230;&#37327;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#19982;MICo&#36317;&#31163;&#31561;&#20215;&#12290;&#27492;&#22806;&#65292;&#26680;&#30340;&#35270;&#35282;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#30028;&#23450;&#20215;&#20540;&#20989;&#25968;&#24046;&#24322;&#21644;&#23884;&#20837;&#21040;&#20302;&#22833;&#30495;&#35823;&#24046;&#30340;&#27431;&#27663;&#31354;&#38388;&#20013;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;&#34892;&#20026;&#24230;&#37327;&#26500;&#24314;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19804</link><description>&lt;p&gt;
&#20174;&#26680;&#30340;&#35282;&#24230;&#30475;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#34892;&#20026;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Kernel Perspective on Behavioural Metrics for Markov Decision Processes. (arXiv:2310.19804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26680;&#30340;&#35282;&#24230;&#35770;&#36848;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#34892;&#20026;&#24230;&#37327;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#19982;MICo&#36317;&#31163;&#31561;&#20215;&#12290;&#27492;&#22806;&#65292;&#26680;&#30340;&#35270;&#35282;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#30028;&#23450;&#20215;&#20540;&#20989;&#25968;&#24046;&#24322;&#21644;&#23884;&#20837;&#21040;&#20302;&#22833;&#30495;&#35823;&#24046;&#30340;&#27431;&#27663;&#31354;&#38388;&#20013;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;&#34892;&#20026;&#24230;&#37327;&#26500;&#24314;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#34892;&#20026;&#24230;&#37327;&#26159;&#26500;&#24314;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#27491;&#23450;&#26680;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#34892;&#20026;&#24230;&#37327;&#30340;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#35270;&#35282;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#65292;&#21487;&#20197;&#34987;&#35777;&#26126;&#19982;&#26368;&#36817;&#24341;&#20837;&#30340;MICo&#36317;&#31163;&#65288;Castro&#31561;&#20154;&#65292;2021&#24180;&#65289;&#31561;&#20215;&#12290;&#26680;&#30340;&#35270;&#35282;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#36825;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#32467;&#26524;&#21253;&#25324;&#36890;&#36807;&#25105;&#20204;&#30340;&#24230;&#37327;&#26469;&#30028;&#23450;&#20215;&#20540;&#20989;&#25968;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#24230;&#37327;&#21487;&#20197;&#34987;&#35777;&#26126;&#23884;&#20837;&#21040;&#20855;&#26377;&#20302;&#22833;&#30495;&#35823;&#24046;&#30340;&#26377;&#38480;&#32500;&#27431;&#27663;&#31354;&#38388;&#20013;&#12290;&#36825;&#26159;&#22312;&#20351;&#29992;&#34892;&#20026;&#24230;&#37327;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#26102;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective further enables us to provide new theoretical results, which has so far eluded prior work. These include bounding value function differences by means of our metric, and the demonstration that our metric can be provably embedded into a finite-dimensional Euclidean space with low distortion error. These are two crucial properties when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.19802</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#38543;&#26426;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21442;&#25968;&#21270;&#27010;&#29575;&#27169;&#22411;&#65288;PPM&#65289;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#20174;&#26412;&#36136;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#28909;&#21147;&#23398;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#21442;&#25968;&#65288;&#35760;&#20026;$\Theta$&#65289;&#19982;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#65288;&#35760;&#20026;$X$&#65289;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20248;&#21270;&#22120;&#30340;&#20316;&#29992;&#26159;&#39537;&#21160;&#36825;&#20004;&#20010;&#23376;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#33021;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#26679;&#26412;$X$&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;$\Theta$&#30340;&#29109;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#20102;&#19968;&#20010;&#28909;&#24211;&#65292;&#26377;&#25928;&#22320;&#23384;&#20648;&#20102;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#28909;&#24211;&#30340;&#35282;&#33394;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#19988;&#19968;&#33268;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28909;&#21147;&#23398;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
&lt;/p&gt;</description></item><item><title>SyMPox&#26159;&#19968;&#20010;&#22522;&#20110;&#30151;&#29366;&#30340;&#33258;&#21160;&#29492;&#30168;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;XGBoost&#31639;&#27861;&#20998;&#26512;&#30151;&#29366;&#27169;&#24335;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#29492;&#30168;&#35786;&#26029;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.19801</link><description>&lt;p&gt;
&#22522;&#20110;&#30151;&#29366;&#30340;&#33258;&#21160;&#29492;&#30168;&#26816;&#27979;&#31995;&#32479;SyMPox&#65306;&#20351;&#29992;XGBoost&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost. (arXiv:2310.19801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19801
&lt;/p&gt;
&lt;p&gt;
SyMPox&#26159;&#19968;&#20010;&#22522;&#20110;&#30151;&#29366;&#30340;&#33258;&#21160;&#29492;&#30168;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;XGBoost&#31639;&#27861;&#20998;&#26512;&#30151;&#29366;&#27169;&#24335;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#29492;&#30168;&#35786;&#26029;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29492;&#30168;&#26159;&#19968;&#31181;&#20154;&#30044;&#20849;&#24739;&#30149;&#12290;&#21040;2023&#24180;6&#26376;10&#26085;&#65292;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#30830;&#35748;&#20102;&#32422;87000&#20363;&#29492;&#30168;&#30149;&#20363;&#12290;&#30446;&#21069;&#26368;&#24120;&#29992;&#30340;&#37492;&#23450;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#20687;&#35782;&#21035;&#25216;&#26415;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#19988;&#20165;&#20379;&#23569;&#25968;&#20154;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyMPox&#30340;&#29420;&#31435;&#24212;&#29992;&#65292;&#22522;&#20110;&#30151;&#29366;&#35786;&#26029;&#29492;&#30168;&#30149;&#20363;&#12290;SyMPox&#21033;&#29992;&#24378;&#22823;&#30340;XGBoost&#31639;&#27861;&#20998;&#26512;&#30151;&#29366;&#27169;&#24335;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;SyMPox&#20351;&#29992;Gradio&#26694;&#26550;&#24320;&#21457;&#65292;&#20026;&#20010;&#20154;&#25552;&#20379;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#30151;&#29366;&#24182;&#33719;&#24471;&#21487;&#38752;&#30340;&#29492;&#30168;&#35786;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monkeypox is a zoonotic disease. About 87000 cases of monkeypox were confirmed by the World Health Organization until 10th June 2023. The most prevalent methods for identifying this disease are image-based recognition techniques. Still, they are not too fast and could only be available to a few individuals. This study presents an independent application named SyMPox, developed to diagnose Monkeypox cases based on symptoms. SyMPox utilizes the robust XGBoost algorithm to analyze symptom patterns and provide accurate assessments. Developed using the Gradio framework, SyMPox offers a user-friendly platform for individuals to assess their symptoms and obtain reliable Monkeypox diagnoses.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19786</link><description>&lt;p&gt;
&#20174;&#22806;&#37096;&#21040;Swap&#36951;&#25022;2.0&#65306;&#38024;&#23545;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#32422;&#21270;&#21644;&#26080;&#30693;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#21040;&#22806;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#34892;&#20026;&#31354;&#38388;&#30340;&#26377;&#38480;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23384;&#22312;&#26576;&#20010;&#20551;&#35774;&#31867;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#65292;&#23601;&#24517;&#28982;&#23384;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#26080;Swap&#36951;&#25022;&#31639;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#21487;&#20197;&#20445;&#35777;&#22312;$\log(N)^{O(1/\epsilon)}$&#36718;&#21518;&#65292;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(N)$&#30340;&#24773;&#20917;&#19979;&#65292;Swap&#36951;&#25022;&#34987;&#38480;&#23450;&#20026;$\epsilon$&#65292;&#32780;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#38656;&#35201;$O(N/\epsilon^2)$&#36718;&#21644;&#33267;&#23569;$\Omega(N^2)$&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20276;&#38543;&#30528;&#19968;&#20010;&#30456;&#20851;&#30340;&#19979;&#30028;&#65292;&#19982;[BM07]&#19981;&#21516;&#65292;&#36825;&#20010;&#19979;&#30028;&#36866;&#29992;&#20110;&#26080;&#30693;&#21644;$\ell_1$-&#21463;&#38480;&#30340;&#23545;&#25163;&#21644;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#19979;&#30028;&#30340;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19708</link><description>&lt;p&gt;
&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#29992;&#26041;&#27861;&#65306;&#19968;&#31181;&#20016;&#23500;&#22810;&#24425;&#30340;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#21644;&#26415;&#35821;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20123;&#26415;&#35821;&#32463;&#24120;&#22312;&#21307;&#23398;&#25110;&#24037;&#19994;&#39046;&#22495;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#36890;&#24120;&#24456;&#38590;&#35299;&#37322;&#23558;&#36890;&#29992;&#35821;&#35328;&#19982;&#19987;&#38376;&#26415;&#35821;&#28151;&#21512;&#20351;&#29992;&#30340;&#28151;&#21512;&#35821;&#38899;&#12290;&#36825;&#23545;&#20110;&#22312;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#20869;&#25805;&#20316;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#25110;&#27425;&#32423;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#35813;&#31574;&#30053;&#28041;&#21450;&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#65292;&#20197;&#25351;&#31034;&#20854;&#19982;&#36890;&#29992;&#25110;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#22686;&#24378;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#28041;&#21450;&#19978;&#33394;&#21333;&#35789;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38598;&#25104;&#26415;&#35821;&#21040;&#35821;&#35328;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#38024;&#23545;&#27969;&#24418;&#20540;&#25968;&#25454;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27969;&#24418;&#19978;&#25805;&#20316;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#26469;&#30452;&#25509;&#20272;&#35745;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31616;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19561</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#22312;&#27969;&#24418;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Non-parametric regression for robot learning on manifolds. (arXiv:2310.19561v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#38024;&#23545;&#27969;&#24418;&#20540;&#25968;&#25454;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27969;&#24418;&#19978;&#25805;&#20316;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#26469;&#30452;&#25509;&#20272;&#35745;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31616;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#20154;&#23398;&#20064;&#24037;&#20855;&#37117;&#26159;&#38024;&#23545;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#35774;&#35745;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#35768;&#22810;&#24212;&#29992;&#28041;&#21450;&#21040;&#27969;&#24418;&#20540;&#25968;&#25454;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#20363;&#23376;&#26159;&#23039;&#24577;&#65307;&#23427;&#21487;&#20197;&#34920;&#31034;&#20026;3x3&#30340;&#26059;&#36716;&#30697;&#38453;&#25110;&#22235;&#20803;&#25968;&#65292;&#20854;&#31354;&#38388;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#27969;&#24418;&#12290;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#65292;&#27969;&#24418;&#20540;&#25968;&#25454;&#36890;&#24120;&#36890;&#36807;&#23558;&#27969;&#24418;&#19982;&#21512;&#36866;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30456;&#20851;&#32852;&#26469;&#22788;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#23884;&#20837;&#27969;&#24418;&#25110;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#25110;&#22810;&#20010;&#20999;&#31354;&#38388;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#19981;&#39640;&#21644;&#31639;&#27861;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24418;&#20869;&#30452;&#25509;&#36827;&#34892;&#22238;&#24402;&#30340;&#8220;&#22266;&#26377;&#8221;&#26041;&#27861;&#12290;&#23427;&#28041;&#21450;&#23545;&#27969;&#24418;&#19978;&#30340;&#36866;&#24403;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#25805;&#20316;&#65292;&#23558;&#20854;&#21442;&#25968;&#20316;&#20026;&#39044;&#27979;&#21464;&#37327;&#65288;&#22914;&#26102;&#38388;&#65289;&#30340;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#8220;&#23616;&#37096;&#20284;&#28982;&#8221;&#26041;&#27861;&#26469;&#38750;&#21442;&#25968;&#20272;&#35745;&#35813;&#20989;&#25968;&#65292;&#20854;&#20013;&#21253;&#21547;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#21629;&#21517;&#20026;&#26680;&#21270;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the tools available for robot learning were designed for Euclidean data. However, many applications in robotics involve manifold-valued data. A common example is orientation; this can be represented as a 3-by-3 rotation matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In robot learning, manifold-valued data are often handled by relating the manifold to a suitable Euclidean space, either by embedding the manifold or by projecting the data onto one or several tangent spaces. These approaches can result in poor predictive accuracy, and convoluted algorithms. In this paper, we propose an "intrinsic" approach to regression that works directly within the manifold. It involves taking a suitable probability distribution on the manifold, letting its parameter be a function of a predictor variable, such as time, then estimating that function non-parametrically via a "local likelihood" method that incorporates a kernel. We name the method kernelised likelihood esti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#23545;&#36793;&#32536;&#38598;&#21512;&#19978;&#30340;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Hodge&#20998;&#35299;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#30340;&#26080;&#25955;&#24230;&#21644;&#26080;&#26059;&#24230;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#23427;&#20204;&#26469;&#34920;&#31034;&#20219;&#24847;&#36793;&#32536;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#27969;&#21160;&#25968;&#25454;&#25512;&#26029;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.19450</link><description>&lt;p&gt;
Hodge-Compositional &#36793;&#32536;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hodge-Compositional Edge Gaussian Processes. (arXiv:2310.19450v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#23545;&#36793;&#32536;&#38598;&#21512;&#19978;&#30340;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Hodge&#20998;&#35299;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#30340;&#26080;&#25955;&#24230;&#21644;&#26080;&#26059;&#24230;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#23427;&#20204;&#26469;&#34920;&#31034;&#20219;&#24847;&#36793;&#32536;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#27969;&#21160;&#25968;&#25454;&#25512;&#26029;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#38598;&#21512;&#30340;2-&#22797;&#24418;&#32467;&#26500;&#65288;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20854;&#20013;&#36793;&#32536;&#21487;&#24418;&#25104;&#19977;&#35282;&#38754;&#65289;&#30340;&#20989;&#25968;&#24314;&#27169;&#30340;&#26377;&#21407;&#21017;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#23398;&#20064;&#32593;&#32476;&#19978;&#30340;&#27969;&#21160;&#31867;&#22411;&#25968;&#25454;&#65292;&#20854;&#20013;&#36793;&#32536;&#27969;&#21487;&#20197;&#36890;&#36807;&#31163;&#25955;&#30340;&#25955;&#24230;&#21644;&#26059;&#24230;&#26469;&#34920;&#24449;&#12290;&#20511;&#37492;Hodge&#20998;&#35299;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#30340;&#26080;&#25955;&#24230;&#21644;&#26080;&#26059;&#28216;&#30340;&#36793;&#32536;GPs&#12290;&#28982;&#21518;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#21019;&#24314;Hodge-&#32452;&#21512;&#36793;&#32536;GPs&#65292;&#36825;&#20123;GPs&#36275;&#22815;&#34920;&#36798;&#20219;&#20309;&#36793;&#32536;&#20989;&#25968;&#12290;&#36825;&#20123;GPs&#20415;&#20110;&#23545;&#36793;&#32536;&#20989;&#25968;&#30340;&#19981;&#21516;Hodge&#20998;&#37327;&#36827;&#34892;&#30452;&#25509;&#21644;&#29420;&#31435;&#30340;&#23398;&#20064;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#20013;&#25429;&#25417;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#31361;&#26174;&#23427;&#20204;&#30340;&#23454;&#38469;&#28508;&#21147;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#36135;&#24065;&#20817;&#25442;&#12289;&#28023;&#27915;&#27969;&#21160;&#21644;&#20379;&#27700;&#32593;&#32476;&#20013;&#30340;&#27969;&#21160;&#25968;&#25454;&#25512;&#26029;&#65292;&#24182;&#23558;&#20854;&#19982;&#26367;&#20195;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#20986;&#21475;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#30340;&#31283;&#20581;&#24615;, &#21457;&#29616;&#22797;&#26434;&#30340;&#26426;&#21046;&#26356;&#26131;&#21463;&#21040;&#24694;&#24847;&#20943;&#36895;&#30340;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#35757;&#32451;&#26080;&#25928;&#65292;&#20294;&#23545;&#35805;&#27169;&#22411;&#30340;&#36755;&#20837;&#28165;&#27927;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.19152</link><description>&lt;p&gt;
BERT&#22833;&#21435;&#32784;&#24515;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#19981;&#33021;&#20445;&#25345;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
BERT Lost Patience Won't Be Robust to Adversarial Slowdown. (arXiv:2310.19152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#20986;&#21475;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#30340;&#31283;&#20581;&#24615;, &#21457;&#29616;&#22797;&#26434;&#30340;&#26426;&#21046;&#26356;&#26131;&#21463;&#21040;&#24694;&#24847;&#20943;&#36895;&#30340;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#35757;&#32451;&#26080;&#25928;&#65292;&#20294;&#23545;&#35805;&#27169;&#22411;&#30340;&#36755;&#20837;&#28165;&#27927;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#22810;&#20986;&#21475;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#23457;&#26680;&#20854;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20943;&#36895;&#25915;&#20987;&#65292;&#29983;&#25104;&#32469;&#36807;&#26089;&#26399;&#36864;&#20986;&#28857;&#30340;&#33258;&#28982;&#24694;&#24847;&#25991;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;WAFFLE&#25915;&#20987;&#20316;&#20026;&#24037;&#20855;&#65292;&#23545;&#19977;&#31181;&#22810;&#20986;&#21475;&#26426;&#21046;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#20102;&#36825;&#19977;&#31181;&#26041;&#27861;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#20379;&#30340;&#35745;&#31639;&#33410;&#30465;&#25928;&#26524;&#12290;&#26426;&#21046;&#36234;&#22797;&#26434;&#65292;&#36234;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#20943;&#36895;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#23545;&#25200;&#21160;&#30340;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#20102;&#35821;&#35328;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#25105;&#20204;&#30340;&#25915;&#20987;&#29983;&#25104;&#30340;&#24120;&#35265;&#25200;&#21160;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;&#30340;&#24694;&#24847;&#25991;&#26412;&#25915;&#20987;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#25239;&#35757;&#32451;&#22312;&#25171;&#36133;&#25105;&#20204;&#30340;&#20943;&#36895;&#25915;&#20987;&#26041;&#38754;&#26159;&#26080;&#25928;&#30340;&#65292;&#20294;&#20351;&#29992;&#23545;&#35805;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#36827;&#34892;&#36755;&#20837;&#28165;&#27927;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#20989;&#25968;&#26469;&#23454;&#29616;&#34892;&#20026;&#19968;&#33268;&#24615;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#35774;&#35745;&#24072;&#30340;&#21551;&#21457;&#21644;&#39046;&#22495;&#30693;&#35782;&#19982;&#29615;&#22659;&#30340;&#20027;&#35201;&#22870;&#21169;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#30830;&#23450;&#20102;&#26368;&#26377;&#25928;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#20197;&#36991;&#20813;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.19007</link><description>&lt;p&gt;
&#34892;&#20026;&#19968;&#33268;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavior Alignment via Reward Function Optimization. (arXiv:2310.19007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#20989;&#25968;&#26469;&#23454;&#29616;&#34892;&#20026;&#19968;&#33268;&#24615;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#35774;&#35745;&#24072;&#30340;&#21551;&#21457;&#21644;&#39046;&#22495;&#30693;&#35782;&#19982;&#29615;&#22659;&#30340;&#20027;&#35201;&#22870;&#21169;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#30830;&#23450;&#20102;&#26368;&#26377;&#25928;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#20197;&#36991;&#20813;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26469;&#26377;&#25928;&#22320;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21521;&#29305;&#23450;&#34892;&#20026;&#30340;&#26041;&#21521;&#21457;&#23637;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#30830;&#23450;&#38750;&#31232;&#30095;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#24182;&#36991;&#20813;&#26080;&#24847;&#20013;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#31616;&#21333;&#22320;&#20462;&#25913;&#22870;&#21169;&#32467;&#26500;&#20197;&#25552;&#20379;&#26356;&#23494;&#38598;&#21644;&#26356;&#39057;&#32321;&#30340;&#21453;&#39304;&#21487;&#33021;&#20250;&#23548;&#33268;&#24847;&#22806;&#30340;&#32467;&#26524;&#65292;&#24182;&#20419;&#20351;&#34892;&#20026;&#19982;&#35774;&#35745;&#32773;&#30340;&#39044;&#26399;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;&#34429;&#28982;&#36890;&#24120;&#24314;&#35758;&#20351;&#29992;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23427;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20250;&#20005;&#37325;&#24433;&#21709;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21452;&#23618;&#30446;&#26631;&#26469;&#23398;&#20064;"&#34892;&#20026;&#19968;&#33268;&#24615;&#22870;&#21169;&#20989;&#25968;"&#12290;&#36825;&#20123;&#20989;&#25968;&#36890;&#36807;&#23558;&#21453;&#26144;&#35774;&#35745;&#24072;&#21551;&#21457;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#36741;&#21161;&#22870;&#21169;&#19982;&#29615;&#22659;&#30340;&#20027;&#35201;&#22870;&#21169;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#30830;&#23450;&#20102;&#26368;&#26377;&#25928;&#30340;&#28151;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \emph{behavior alignment reward functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#32852;&#21512;&#22810;&#22270;&#23398;&#20064;&#21644;&#22270;&#20449;&#21495;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#33410;&#28857;&#20391;&#20449;&#24687;&#25972;&#21512;&#21040;&#20449;&#21495;&#30340;&#20998;&#21306;&#21644;&#22270;&#23398;&#20064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.19005</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#22810;&#22270;&#20449;&#21495;&#23398;&#20064;&#21644;&#22270;&#20449;&#21495;&#32858;&#31867;&#30340;&#32852;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals. (arXiv:2310.19005v1 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19005
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#32852;&#21512;&#22810;&#22270;&#23398;&#20064;&#21644;&#22270;&#20449;&#21495;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#33410;&#28857;&#20391;&#20449;&#24687;&#25972;&#21512;&#21040;&#20449;&#21495;&#30340;&#20998;&#21306;&#21644;&#22270;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#23398;&#20064;&#65288;GL&#65289;&#30740;&#31350;&#20174;&#33410;&#28857;&#35266;&#27979;&#65288;&#21363;&#22270;&#20449;&#21495;&#65289;&#20013;&#25512;&#26029;&#20986;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#28151;&#21512;&#24418;&#24335;&#65292;&#28041;&#21450;&#19981;&#21516;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;&#36825;&#31181;&#24322;&#36136;&#24615;&#38656;&#35201;&#32852;&#21512;&#32858;&#31867;&#21644;&#23398;&#20064;&#22810;&#20010;&#22270;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26377;&#21487;&#29992;&#30340;&#33410;&#28857;&#20391;&#21327;&#21464;&#37327;&#65288;&#21363;&#26680;&#20989;&#25968;&#65289;&#65292;&#24517;&#39035;&#21152;&#20197;&#21512;&#24182;&#65292;&#32780;&#36825;&#22312;&#29616;&#26377;&#30340;&#22270;&#20449;&#21495;&#32858;&#31867;&#26041;&#27861;&#20013;&#23578;&#26410;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#21463;&#20016;&#23500;&#30340;K-means&#26694;&#26550;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26680;&#30340;&#31639;&#27861;&#65292;&#23558;&#33410;&#28857;&#20391;&#20449;&#24687;&#25972;&#21512;&#21040;&#20449;&#21495;&#30340;&#20998;&#21306;&#21644;&#27599;&#20010;&#38598;&#32676;&#30340;&#22270;&#23398;&#20064;&#20013;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#20043;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is concerned with the inference of a graph's topology from nodal observations, i.e., graph signals. However, data is often in mixed form, relating to different underlying structures. This heterogeneity necessitates the joint clustering and learning of multiple graphs. In many real-life applications, there are available node-side covariates (i.e., kernels) that imperatively should be incorporated, which has not been addressed by the rare graph signal clustering approaches. To this end and inspired by the rich K-means framework, we propose a novel kernel-based algorithm to incorporate this node-side information as we jointly partition the signals and learn a graph for each cluster. Numerical experiments demonstrate its effectiveness over the state-of-the-art.
&lt;/p&gt;</description></item><item><title>LoRAShear&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#21098;&#26525;&#21644;&#21160;&#24577;&#24494;&#35843;&#65292;&#26377;&#25928;&#20943;&#23569;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#24182;&#19988;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18356</link><description>&lt;p&gt;
LoRAShear: &#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. (arXiv:2310.18356v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18356
&lt;/p&gt;
&lt;p&gt;
LoRAShear&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#21098;&#26525;&#21644;&#21160;&#24577;&#24494;&#35843;&#65292;&#26377;&#25928;&#20943;&#23569;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#24182;&#19988;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26684;&#23616;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoRAShear&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#21098;&#26525;LLMs&#24182;&#24674;&#22797;&#30693;&#35782;&#12290;LoRAShear&#39318;&#20808;&#22312;LoRA&#27169;&#22359;&#19978;&#21019;&#24314;&#20381;&#36182;&#22270;&#65292;&#20197;&#21457;&#29616;&#26368;&#23567;&#21024;&#38500;&#32467;&#26500;&#24182;&#20998;&#26512;&#30693;&#35782;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;LoRA&#36866;&#37197;&#22120;&#19978;&#36827;&#34892;&#28176;&#36827;&#24335;&#32467;&#26500;&#21098;&#26525;&#65292;&#24182;&#23454;&#29616;&#20869;&#22312;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#20887;&#20313;&#32467;&#26500;&#20013;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#24674;&#22797;&#21098;&#26525;&#26399;&#38388;&#20002;&#22833;&#30340;&#30693;&#35782;&#65292;LoRAShear&#20180;&#32454;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#26041;&#26696;&#65292;&#20351;&#29992;&#21160;&#24577;&#25968;&#25454;&#36866;&#37197;&#22120;&#65292;&#20197;&#26377;&#25928;&#32553;&#23567;&#19982;&#23436;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#20351;&#29992;&#19968;&#22359;GPU&#22312;&#20960;&#22825;&#20869;&#65292;LoRAShear&#23558;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#26377;&#25928;&#20943;&#23569;&#20102;20%&#65292;&#20165;&#26377;1.0%&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance d
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKDF&#30340;&#26080;&#25968;&#25454;&#33976;&#39311;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#25552;&#39640;&#33016;&#37096;&#30142;&#30149;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#22120;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#65292;FedKDF&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#20854;&#22522;&#30784;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.18346</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#33976;&#39311;&#25552;&#39640;&#20102;&#32852;&#37030;&#33016;&#37096;&#30142;&#30149;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Data-Free Distillation Improves Efficiency and Privacy in Federated Thorax Disease Analysis. (arXiv:2310.18346v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18346
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKDF&#30340;&#26080;&#25968;&#25454;&#33976;&#39311;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#25552;&#39640;&#33016;&#37096;&#30142;&#30149;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#22120;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#65292;FedKDF&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#20854;&#22522;&#30784;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#12289;&#22810;&#20013;&#24515;&#21644;&#22810;&#25195;&#25551;&#20202;&#29615;&#22659;&#20013;&#65292;&#33016;&#37096;&#30142;&#30149;&#20998;&#26512;&#21463;&#21040;&#20005;&#26684;&#30340;&#38544;&#31169;&#25919;&#31574;&#30340;&#38480;&#21046;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#25968;&#30340;FL&#21487;&#33021;&#21463;&#21040;&#39640;&#36890;&#20449;&#25104;&#26412;&#12289;&#25968;&#25454;&#27844;&#28431;&#21644;&#24322;&#36136;&#24615;&#31561;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#22522;&#20110;&#33976;&#39311;&#30340;FL&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20381;&#36182;&#20110;&#19968;&#20010;&#20195;&#29702;&#25968;&#25454;&#38598;&#65292;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#36890;&#24120;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#33976;&#39311;&#30340;FL&#26041;&#27861;FedKDF&#12290;&#22312;FedKDF&#20013;&#65292;&#26381;&#21153;&#22120;&#20351;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#22120;&#20174;&#19981;&#21516;&#23458;&#25143;&#31471;&#32858;&#21512;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20854;&#31169;&#26377;&#25968;&#25454;&#25110;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;FedKDF&#23558;&#23458;&#25143;&#31471;&#30340;&#39044;&#27979;&#22120;&#32452;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#22120;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;FedKDF&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#39640;&#25928;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#33016;&#37096;&#30142;&#30149;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thorax disease analysis in large-scale, multi-centre, and multi-scanner settings is often limited by strict privacy policies. Federated learning (FL) offers a potential solution, while traditional parameter-based FL can be limited by issues such as high communication costs, data leakage, and heterogeneity. Distillation-based FL can improve efficiency, but it relies on a proxy dataset, which is often impractical in clinical practice. To address these challenges, we introduce a data-free distillation-based FL approach FedKDF. In FedKDF, the server employs a lightweight generator to aggregate knowledge from different clients without requiring access to their private data or a proxy dataset. FedKDF combines the predictors from clients into a single, unified predictor, which is further optimized using the learned knowledge in the lightweight generator. Our empirical experiments demonstrate that FedKDF offers a robust solution for efficient, privacy-preserving federated thorax disease analys
&lt;/p&gt;</description></item><item><title>AllTogether&#26159;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#36890;&#36807;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#23545;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#22312;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26041;&#38754;&#65292;&#20248;&#20110;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.18331</link><description>&lt;p&gt;
AllTogether&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20351;&#29992;&#25340;&#25509;&#25552;&#31034;&#36827;&#34892;Web&#23548;&#33322;&#30340;&#25928;&#26524;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models. (arXiv:2310.18331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18331
&lt;/p&gt;
&lt;p&gt;
AllTogether&#26159;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#36890;&#36807;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#23545;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#22312;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26041;&#38754;&#65292;&#20248;&#20110;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#29992;&#20110;Web&#23548;&#33322;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#20195;&#29702;&#65292;&#23427;&#20204;&#35299;&#37322;&#30446;&#26631;&#24182;&#19982;Web&#39029;&#38754;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#20013;&#20351;&#29992;&#25340;&#25509;&#25552;&#31034;&#30340;&#25928;&#26524;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AllTogether&#65292;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;LLMs&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#26469;&#33258;&#24320;&#28304;Llama-2&#21644;&#21487;&#35775;&#38382;&#30340;GPT&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#21644;&#25351;&#20196;&#24494;&#35843;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;Web&#23548;&#33322;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#19988;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#27604;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26356;&#26377;&#25928;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;</title><link>http://arxiv.org/abs/2310.16945</link><description>&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection&#65288;CATE&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#22240;&#26524;Q&#38598;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#26159;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#29992;&#20110;CATE&#20272;&#35745;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#27169;&#22411;&#36873;&#25321;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21033;&#20110;&#20855;&#26377;&#21452;&#37325;&#40065;&#26834;&#24615;&#36136;&#30340;&#20195;&#29702;&#25439;&#22833;&#24230;&#37327;&#21644;&#27169;&#22411;&#38598;&#25104;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#19981;&#22815;&#12290;&#30452;&#25509;&#24212;&#29992;&#20808;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20250;&#30001;&#20110;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;&#20027;&#35201;CATE&#38598;&#25104;&#26041;&#27861;&#30340;&#36951;&#25022;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#30340;Q&#38598;&#25104;&#30340;&#26032;&#30340;CATE&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#22240;&#26524;Q&#38598;&#25104;&#22312;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#30340;&#36951;&#25022;&#29575;&#19978;&#36798;&#21040;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20248;&#20540;&#20026;$\frac{\log(M)}{n}$&#65288;&#20854;&#20013;$M$&#20026;&#27169;&#22411;&#25968;&#65292;$n$&#20026;&#26679;&#26412;&#25968;&#65289;&#65292;&#21152;&#19978;&#39640;&#38454;&#20272;&#35745;&#35823;&#24046;&#39033;
&lt;/p&gt;
&lt;p&gt;
Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.15848</link><description>&lt;p&gt;
&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#20844;&#24179;&#24615;&#12289;&#38544;&#31169;&#21644;&#27861;&#35268;&#20934;&#21017;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#36827;&#20837;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#20102;&#24778;&#20154;&#30340;&#25913;&#36827;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21487;&#20449;&#24615;&#23384;&#22312;&#20005;&#37325;&#25285;&#24551;&#12290;&#31185;&#23398;&#30028;&#33268;&#21147;&#20110;&#24320;&#21457;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#20854;&#24320;&#21457;&#36807;&#31243;&#20013;&#20005;&#37325;&#20381;&#36182;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#34892;&#20026;&#30446;&#26631;&#12290;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#32570;&#38519;&#37117;&#26377;&#21487;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#31639;&#27861;&#30340;&#32570;&#38519;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#31639;&#27861;&#30340;&#21518;&#26399;&#35780;&#20272;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24615;&#65292;&#32780;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21333;&#29420;&#32771;&#34385;&#25968;&#25454;&#32452;&#20214;&#20197;&#29702;&#35299;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand it
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14336</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Rules for Scalable Data Representation and Classification. (arXiv:2310.14336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14336
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65288;&#22914;&#20915;&#31574;&#26641;&#65289;&#22312;&#38656;&#35201;&#39640;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#36879;&#26126;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#33391;&#22909;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31163;&#25955;&#30340;&#21442;&#25968;&#21644;&#32467;&#26500;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#22312;&#20248;&#21270;&#26041;&#38754;&#24456;&#38590;&#24212;&#23545;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#38598;&#25104;&#26041;&#27861;&#21644;&#27169;&#31946;/&#36719;&#35268;&#21017;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#33719;&#24471;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#65292;&#31216;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#34920;&#31034;&#23398;&#20064;&#22120;&#65288;RRL&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#19981;&#21487;&#24494;&#20998;&#30340;RRL&#65292;&#25105;&#20204;&#23558;&#20854;&#26144;&#23556;&#21040;&#36830;&#32493;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#26799;&#24230;&#23884;&#20837;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#30452;&#25509;&#20248;&#21270;&#31163;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#21152;RRL&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#21028;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discr
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#27969;&#24418;&#19978;&#35745;&#31639;&#27010;&#29575;&#23494;&#24230;&#24182;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#34920;&#31034;&#20013;&#23384;&#22312;&#30528;&#19982;&#27969;&#24418;&#20851;&#32852;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12743</link><description>&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;
&lt;/p&gt;
&lt;p&gt;
Canonical normalizing flows for manifold learning. (arXiv:2310.12743v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#27969;&#24418;&#19978;&#35745;&#31639;&#27010;&#29575;&#23494;&#24230;&#24182;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#34920;&#31034;&#20013;&#23384;&#22312;&#30528;&#19982;&#27969;&#24418;&#20851;&#32852;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#27969;&#26159;&#19968;&#31867;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#20551;&#35774;&#25968;&#25454;&#20855;&#26377;&#20302;&#32500;&#27969;&#24418;&#25551;&#36848;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#36825;&#31181;&#27969;&#24418;&#23884;&#20837;&#21040;&#25968;&#25454;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;&#19968;&#26086;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#27491;&#30830;&#23545;&#40784;&#27969;&#24418;&#65292;&#27969;&#24418;&#19978;&#30340;&#27010;&#29575;&#23494;&#24230;&#23601;&#26159;&#21487;&#35745;&#31639;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#12290;&#33258;&#28982;&#22320;&#65292;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#38656;&#35201;&#26159;&#21333;&#23556;&#26144;&#23556;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#24314;&#27169;&#30340;&#27969;&#24418;&#19978;&#23545;&#23494;&#24230;&#36827;&#34892;&#23545;&#20934;&#65292;&#24182;&#22312;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#26102;&#39640;&#25928;&#35745;&#31639;&#23494;&#24230;&#20307;&#31215;&#21464;&#21270;&#39033;&#12290;&#28982;&#32780;&#65292;&#38500;&#38750;&#21333;&#23556;&#26144;&#23556;&#22312;&#35299;&#26512;&#19978;&#39044;&#23450;&#20041;&#65292;&#21542;&#21017;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#19981;&#19968;&#23450;&#26159;&#25968;&#25454;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#28508;&#22312;&#32500;&#24230;&#32463;&#24120;&#20250;&#23398;&#20064;&#21040;&#19982;&#27969;&#24418;&#30456;&#20851;&#24182;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an efficient representation of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis with degenerat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#21464;&#21270;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12289;&#35789;&#27719;&#37325;&#21472;&#21644;&#35821;&#35328;&#29305;&#24615;&#26159;&#24433;&#21709;&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.10385</link><description>&lt;p&gt;
&#23545;&#20110;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#21464;&#21270;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance. (arXiv:2310.10385v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10385
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#21464;&#21270;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12289;&#35789;&#27719;&#37325;&#21472;&#21644;&#35821;&#35328;&#29305;&#24615;&#26159;&#24433;&#21709;&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;MNMT&#65289;&#20419;&#36827;&#20102;&#30693;&#35782;&#20998;&#20139;&#65292;&#20294;&#24448;&#24448;&#22312;&#38646;&#26679;&#26412;&#65288;ZS&#65289;&#32763;&#35793;&#36136;&#37327;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#25972;&#20307;ZS&#24615;&#33021;&#20302;&#19979;&#30340;&#21407;&#22240;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65306;ZS&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#36825;&#34920;&#26126;MNMT&#30340;ZS&#33021;&#21147;&#24182;&#19981;&#26159;&#22343;&#21248;&#22320;&#24046;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#20135;&#29983;&#20102;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;40&#31181;&#35821;&#35328;&#30340;1,560&#20010;&#35821;&#35328;&#26041;&#21521;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#24433;&#21709;ZS NMT&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;1&#65289;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;2&#65289;&#35789;&#27719;&#37325;&#21472;&#65292;3&#65289;&#35821;&#35328;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#26159;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#22240;&#32032;&#65292;&#35789;&#27719;&#37325;&#21472;&#22987;&#32456;&#24433;&#21709;ZS&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#29305;&#24615;&#65292;&#22914;&#35821;&#35328;&#23478;&#26063;&#21644;&#20070;&#20889;&#31995;&#32479;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low ZS performance, our work introduces a fresh perspective: the presence of high variations in ZS performance. This suggests that MNMT does not uniformly exhibit poor ZS capability; instead, certain translation directions yield reasonable results. Through systematic experimentation involving 1,560 language directions spanning 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target side translation capability 2) vocabulary overlap 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting ZS performance. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.09866</link><description>&lt;p&gt;
&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20316;&#20026;&#35768;&#22810;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#38382;&#39064;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#20173;&#23616;&#38480;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#26080;&#27861;&#28385;&#36275;&#36825;&#20123;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#25345;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#30340;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#21327;&#20316;&#35299;&#20915;&#19968;&#20010;MOO&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;FMOL&#26694;&#26550;&#20801;&#35768;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#39318;&#27425;&#23558;MOO&#24418;&#24335;&#21270;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#12290;&#23545;&#20110;&#36825;&#20010;FMOL&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;FMOO&#65289;&#31639;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#22810;&#26799;&#24230;&#19979;&#38477;&#24179;&#22343;&#65288;FMGDA&#65289;&#21644;&#32852;&#37030;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;Federated SGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
&lt;/p&gt;</description></item><item><title>MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.09833</link><description>&lt;p&gt;
MIR2:&#38754;&#21521;&#36890;&#36807;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#36827;&#34892;&#21487;&#35777;&#26126;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09833
&lt;/p&gt;
&lt;p&gt;
MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#23545;&#20110;&#26410;&#30693;&#30431;&#21451;&#30340;&#19981;&#30830;&#23450;&#25110;&#26368;&#22351;&#24773;&#20917;&#34892;&#21160;&#38656;&#35201;&#20855;&#22791;&#24377;&#24615;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;MARL&#20013;&#30340;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#25216;&#26415;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#30340;&#23545;&#25163;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#38590;&#20197;&#25805;&#20316;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#35797;&#22270;&#31616;&#21270;&#36825;&#31181;&#22797;&#26434;&#24615;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#20110;&#24754;&#35266;&#30340;&#31574;&#30053;&#12289;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#19981;&#36275;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#20154;&#31867;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#34892;&#20026;&#26102;&#33258;&#28982;&#32780;&#28982;&#22320;&#19981;&#38656;&#35201;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIR2&#65292;&#23427;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#35270;&#20026;&#19968;&#20010;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#21382;&#21490;&#21644;&#34892;&#21160;&#20043;&#38388;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#38544;&#21547;&#22320;&#26368;&#22823;&#21270;&#20102;&#40065;&#26834;&#24615;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust
&lt;/p&gt;</description></item><item><title>&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#20026;&#20248;&#21270;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26426;&#20250;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#20379;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.08748</link><description>&lt;p&gt;
&#36827;&#21270;&#21160;&#24577;&#20248;&#21270;&#19982;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Dynamic Optimization and Machine Learning. (arXiv:2310.08748v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08748
&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#20026;&#20248;&#21270;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26426;&#20250;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#20379;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;(EC)&#20316;&#20026;&#19968;&#31181;&#21463;&#33258;&#28982;&#28176;&#36827;&#21457;&#23637;&#26426;&#21046;&#21551;&#21457;&#30340;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;EC&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20572;&#28382;&#12289;&#22810;&#26679;&#24615;&#25439;&#22833;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#31181;&#32676;&#21021;&#22987;&#21270;&#21644;&#36807;&#26089;&#25910;&#25947;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#23398;&#20064;&#31639;&#27861;&#19982;&#36827;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#38598;&#25104;&#21033;&#29992;&#20102;EC&#31639;&#27861;&#22312;&#36845;&#20195;&#25628;&#32034;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;&#31867;&#20284;&#22320;&#65292;&#36827;&#21270;&#31639;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;(ML)&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#30456;&#20114;&#30340;&#65292;&#22240;&#20026;EC&#26041;&#27861;&#20026;&#20248;&#21270;&#22122;&#22768;&#12289;&#19981;&#20934;&#30830;&#21644;&#21160;&#24577;&#30446;&#26631;&#20989;&#25968;&#25152;&#25551;&#36848;&#30340;&#22797;&#26434;ML&#20219;&#21153;&#25552;&#20379;&#20102;&#26497;&#22909;&#30340;&#26426;&#20250;&#12290;&#36825;&#20123;&#28151;&#21512;&#25216;&#26415;&#34987;&#31216;&#20026;&#36827;&#21270;&#26426;&#22120;&#23398;&#20064;(EML)&#65292;&#24050;&#22312;ML&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2310.06827</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#26356;&#23569;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65288;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#12289;&#20250;&#35758;&#27010;&#36848;&#21644;&#20020;&#24202;&#25253;&#21578;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#20351;&#25152;&#26377;&#24517;&#35201;&#20449;&#24687;&#37117;&#22312;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20248;&#21270;LLMs&#20197;&#20943;&#23569;&#24187;&#35273;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#27599;&#20010;&#20248;&#21270;&#27493;&#39588;&#20013;&#26377;&#25928;&#35780;&#20272;&#24187;&#35273;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#20063;&#21487;&#20197;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;SynTra&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21512;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#26131;&#20110;&#35825;&#21457;&#21644;&#34913;&#37327;&#24187;&#35273;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#20219;&#21153;&#36827;&#34892;&#21069;&#32512;&#35843;&#20248;&#26469;&#20248;&#21270;LLM&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#24182;&#26368;&#32456;&#23558;&#31995;&#32479;&#28040;&#24687;&#36716;&#31227;&#21040;&#29616;&#23454;&#20013;&#38590;&#20197;&#20248;&#21270;&#30340;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#29616;&#23454;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#26816;&#32034;&#20219;&#21153;&#36827;&#34892;&#30417;&#30563;&#65292;SynTra&#20943;&#23569;&#20102;&#20004;&#20010;&#20855;&#26377;13B&#21442;&#25968;&#30340;LLMs&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05446</link><description>&lt;p&gt;
RetSeg: &#22522;&#20110;&#20445;&#30041;&#26426;&#21046;&#30340;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RetSeg: Retention-based Colorectal Polyps Segmentation Network. (arXiv:2310.05446v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30456;&#27604;&#65292;&#22312;&#24687;&#32905;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#32858;&#28966;&#20110;&#29305;&#23450;&#22270;&#20687;&#21306;&#22495;&#65292;ViTs&#22312;&#22788;&#29702;&#35270;&#35273;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#22797;&#26434;&#21307;&#23398;&#22270;&#20687;&#26102;&#23454;&#29616;&#20102;&#24378;&#22823;&#19988;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#21464;&#25442;&#22120;&#20013;&#22266;&#26377;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#36866;&#24212;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23610;&#23544;&#21644;&#20998;&#36776;&#29575;&#65292;&#20026;&#20256;&#32479;&#30340;CNNs&#25152;&#19981;&#20855;&#22791;&#30340;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21464;&#25442;&#22120;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#32780;&#38754;&#20020;&#30528;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#24182;&#34892;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#31350;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, intr
&lt;/p&gt;</description></item><item><title>BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03320</link><description>&lt;p&gt;
BioBridge: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03320
&lt;/p&gt;
&lt;p&gt;
BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;(FMs)&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;FMs&#20027;&#35201;&#20173;&#22788;&#20110;&#21333;&#27169;&#24577;&#29366;&#24577;&#65292;&#21363;&#29420;&#31435;&#35757;&#32451;&#24182;&#29992;&#20110;&#22788;&#29702;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#23567;&#20998;&#23376;&#32467;&#26500;&#25110;&#20020;&#24202;&#25968;&#25454;&#31561;&#21333;&#19968;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#29983;&#29289;&#21307;&#23398;FMs&#30340;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;BioBridge&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;(KG)&#26469;&#23398;&#20064;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#24213;&#23618;&#21333;&#27169;&#24577;FMs&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#26725;&#25509;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;FMs&#20197;&#24314;&#31435;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21487;&#20197;&#20987;&#36133;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65288;&#24179;&#22343;&#25552;&#39640;&#32422;76.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;BioBridge&#34920;&#29616;&#20986;&#39046;&#22495;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#30340;&#27169;&#24577;&#25110;&#20851;&#31995;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01210</link><description>&lt;p&gt;
&#23454;&#29616;&#31283;&#20581;&#30340;&#24515;&#33039;&#20998;&#21106;&#65306;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#33258;&#21160;&#21270;&#30340;&#24515;&#33039;&#20998;&#21106;&#21487;&#20197;&#24555;&#36895;&#12289;&#21487;&#37325;&#22797;&#22320;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#26816;&#26597;&#20013;&#25552;&#21462;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;U-Net&#32467;&#26500;&#26159;&#30446;&#21069;&#21307;&#23398;&#20998;&#21106;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#26102;&#20998;&#21106;&#24515;&#33039;&#32467;&#26500;&#65292;&#24182;&#19988;&#24179;&#22343;&#35823;&#24046;&#21487;&#19982;&#35266;&#27979;&#32773;&#38388;&#21464;&#24322;&#24615;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#35813;&#26550;&#26500;&#20173;&#28982;&#20250;&#29983;&#25104;&#35768;&#22810;&#35299;&#31163;&#24322;&#24120;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#39044;&#27979;&#20986;&#24863;&#20852;&#36259;&#32467;&#26500;&#30340;&#36718;&#24275;&#28857;&#65292;&#32780;&#19981;&#26159;&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#33039;&#35299;&#21078;&#23398;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#36825;&#28040;&#38500;&#20102;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;CAMUS&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#32467;&#26500;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#22312;&#20020;&#24202;HUNT4&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully automatic cardiac segmentation can be a fast and reproducible method to extract clinical measurements from an echocardiography examination. The U-Net architecture is the current state-of-the-art deep learning architecture for medical segmentation and can segment cardiac structures in real-time with average errors comparable to inter-observer variability. However, this architecture still generates large outliers that are often anatomically incorrect. This work uses the concept of graph convolutional neural networks that predict the contour points of the structures of interest instead of labeling each pixel. We propose a graph architecture that uses two convolutional rings based on cardiac anatomy and show that this eliminates anatomical incorrect multi-structure segmentations on the publicly available CAMUS dataset. Additionally, this work contributes with an ablation study on the graph convolutional architecture and an evaluation of clinical measurements on the clinical HUNT4 dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.00093</link><description>&lt;p&gt;
DataDAM: &#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
DataDAM: Efficient Dataset Distillation with Attention Matching. (arXiv:2310.00093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#22312;&#23613;&#37327;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#26356;&#22823;&#30495;&#23454;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#19981;&#33021;&#20687;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#37027;&#26679;&#20998;&#24067;&#21644;&#21306;&#20998;&#65292;&#32780;&#19988;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#31934;&#28860;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminate as well as the original training data, and they incur significant computational costs. Despite promising results, there still exists a significant performance gap between models trained on condensed synthetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs. Specifically, we learn synthetic images by matching the spa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39564;&#35777;&#21644;&#37325;&#26032;&#23454;&#29616;&#20316;&#32773;&#25552;&#20986;&#30340;&#29983;&#29289;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;P-NET&#30340;&#26041;&#27861;&#65292;&#37327;&#21270;&#20102;&#20351;&#29992;Reactome&#29983;&#29289;&#36890;&#36335;&#36827;&#34892;&#32593;&#32476;&#31232;&#30095;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#20182;&#31070;&#32463;&#26550;&#26500;&#21644;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#20102;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.16645</link><description>&lt;p&gt;
&#21487;&#37325;&#22797;&#24615;&#25253;&#21578;&#65306;&#22810;&#26679;&#30340;&#29983;&#29289;&#20449;&#24687;&#21270;&#31070;&#32463;&#32467;&#26500;&#23545;&#21069;&#21015;&#33146;&#30284;&#20998;&#23618;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures. (arXiv:2309.16645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16645
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39564;&#35777;&#21644;&#37325;&#26032;&#23454;&#29616;&#20316;&#32773;&#25552;&#20986;&#30340;&#29983;&#29289;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;P-NET&#30340;&#26041;&#27861;&#65292;&#37327;&#21270;&#20102;&#20351;&#29992;Reactome&#29983;&#29289;&#36890;&#36335;&#36827;&#34892;&#32593;&#32476;&#31232;&#30095;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#20182;&#31070;&#32463;&#26550;&#26500;&#21644;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#20102;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Elmarakeby&#31561;&#20154;&#30340;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29983;&#29289;&#20449;&#24687;&#21270;&#12289;&#31232;&#30095;&#36830;&#25509;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;P-NET&#65289;&#26469;&#27169;&#25311;&#21069;&#21015;&#33146;&#30284;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#20195;&#30721;&#21644;&#25105;&#20204;&#33258;&#24049;&#20351;&#29992;&#26356;&#29616;&#20195;&#21270;&#30340;&#24211;&#37325;&#26032;&#23454;&#29616;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;Elmarakeby&#31561;&#20154;&#30740;&#31350;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36890;&#36807;Reactome&#29983;&#29289;&#36890;&#36335;&#36827;&#34892;&#32593;&#32476;&#31232;&#30095;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#30830;&#35748;&#20854;&#23545;P-NET&#30340;&#21331;&#36234;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#29983;&#29289;&#20449;&#24687;&#32435;&#20837;&#32593;&#32476;&#30340;&#20854;&#20182;&#31070;&#32463;&#26550;&#26500;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#23581;&#35797;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#20020;&#24202;&#39044;&#27979;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In, Elmarakeby et al., "Biologically informed deep neural network for prostate cancer discovery", a feedforward neural network with biologically informed, sparse connections (P-NET) was presented to model the state of prostate cancer. We verified the reproducibility of the study conducted by Elmarakeby et al., using both their original codebase, and our own re-implementation using more up-to-date libraries. We quantified the contribution of network sparsification by Reactome biological pathways, and confirmed its importance to P-NET's superior performance. Furthermore, we explored alternative neural architectures and approaches to incorporating biological information into the networks. We experimented with three types of graph neural networks on the same training data, and investigated the clinical prediction agreement between different models. Our analyses demonstrated that deep neural networks with distinct architectures make incorrect predictions for individual patient that are pers
&lt;/p&gt;</description></item><item><title>COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14356</link><description>&lt;p&gt;
COCO-Counterfactuals:&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14356
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#20363;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#24050;&#35777;&#26126;&#23545;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#20363;&#22312;NLP&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29992;&#65292;&#20294;&#30001;&#20110;&#21019;&#24314;&#26368;&#23567;&#21453;&#20107;&#23454;&#21464;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#38590;&#24230;&#65292;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;COCO-Counterfactuals&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;MS-COCO&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#26631;&#39064;&#30340;&#37197;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;COCO-Counterfactuals&#22312;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13135</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#19981;&#33391;&#32467;&#26524;&#21644;&#24739;&#32773;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#22024;&#26434;&#21644;&#38388;&#27463;&#24615;&#65292;&#23454;&#38469;&#20013;&#39044;&#27979;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#36825;&#20123;&#25361;&#25112;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#22240;&#32032;&#35825;&#23548;&#30340;&#21464;&#21270;&#28857;&#65288;&#22914;&#33647;&#29289;&#20351;&#29992;&#65289;&#32780;&#21152;&#21095;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#25928;&#24212;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21463;&#27835;&#30103;&#24433;&#21709;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20351;&#29992;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#39044;&#27979;&#34880;&#31958;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#33647;&#21160;&#23398;&#32534;&#30721;&#22120;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36229;&#36807;&#22522;&#20934;&#32422;11&#65285;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36229;&#36807;8&#65285;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#20363;&#22914;&#21457;&#20986;&#20851;&#20110;&#24847;&#22806;&#27835;&#30103;&#21453;&#24212;&#30340;&#26089;&#26399;&#35686;&#21578;&#65292;&#25110;&#24110;&#21161;&#34920;&#24449;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.12488</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#31283;&#23450;&#24615;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;(GD)&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#25805;&#20316;&#31526;&#33539;&#25968;&#20250;&#22686;&#38271;&#65292;&#30452;&#21040;&#25509;&#36817;$2/\eta$&#65292;&#20043;&#21518;&#20250;&#22312;&#35813;&#20540;&#21608;&#22260;&#27874;&#21160;&#12290;&#26681;&#25454;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20108;&#27425;&#36924;&#36817;&#65292;$2/\eta$&#34987;&#31216;&#20026;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#30830;&#23450;&#20102;&#19968;&#20010;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#65292;SAM&#26159;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;GD&#21464;&#31181;&#12290;&#19982;GD&#19981;&#21516;&#65292;SAM&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;&#36890;&#36807;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SAM&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#30830;&#23450;&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;&#29289;&#29702;&#32593;&#32476;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#23398;&#20064;&#34892;&#20026;&#19982;&#26080;&#24207;&#21644;&#29627;&#29827;&#29366;&#31995;&#32479;&#20013;&#30340;&#32769;&#21270;&#21644;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#30456;&#20284;&#12290;&#23398;&#20064;&#21160;&#24577;&#31867;&#20284;&#20110;&#32769;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#37325;&#22797;&#26045;&#21152;&#21453;&#39304;&#36793;&#30028;&#21147;&#26469;&#25918;&#26494;&#24182;&#32534;&#30721;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2309.04382</link><description>&lt;p&gt;
&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#32039;&#24613;&#23398;&#20064;&#20316;&#20026;&#22522;&#20110;&#21453;&#39304;&#30340;&#29627;&#29827;&#26223;&#35266;&#20013;&#30340;&#32769;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Emergent learning in physical systems as feedback-based aging in a glassy landscape. (arXiv:2309.04382v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04382
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;&#29289;&#29702;&#32593;&#32476;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#23398;&#20064;&#34892;&#20026;&#19982;&#26080;&#24207;&#21644;&#29627;&#29827;&#29366;&#31995;&#32479;&#20013;&#30340;&#32769;&#21270;&#21644;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#30456;&#20284;&#12290;&#23398;&#20064;&#21160;&#24577;&#31867;&#20284;&#20110;&#32769;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#37325;&#22797;&#26045;&#21152;&#21453;&#39304;&#36793;&#30028;&#21147;&#26469;&#25918;&#26494;&#24182;&#32534;&#30721;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;&#29289;&#29702;&#32593;&#32476;&#26469;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#29289;&#29702;&#29305;&#24615;&#26159;&#30001;&#26435;&#37325;&#26356;&#26032;&#35268;&#21017;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36825;&#26679;&#30340;&#32593;&#32476;&#23398;&#20064;&#34892;&#20026;&#19982;&#26080;&#24207;&#21644;&#29627;&#29827;&#29366;&#31995;&#32479;&#20013;&#32769;&#21270;&#21644;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#24778;&#20154;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#21160;&#24577;&#31867;&#20284;&#20110;&#32769;&#21270;&#36807;&#31243;&#65292;&#31995;&#32479;&#22312;&#38754;&#23545;&#36755;&#20837;&#21147;&#30340;&#21453;&#39304;&#36793;&#30028;&#21147;&#30340;&#37325;&#22797;&#26045;&#21152;&#26102;&#25918;&#26494;&#24182;&#32534;&#30721;&#20102;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#35760;&#24518;&#12290;&#38543;&#30528;&#36825;&#31181;&#25918;&#26494;&#65292;&#30456;&#20851;&#38271;&#24230;&#22686;&#21152;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#30340;&#20998;&#37327;&#30340;&#20004;&#28857;&#30456;&#20851;&#20989;&#25968;&#26469;&#25351;&#31034;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#35823;&#24046;&#22343;&#26041;&#26681;&#30340;&#24179;&#26041;&#26681;&#20316;&#20026;&#26102;&#38388;&#30340;&#20989;&#25968;&#21576;&#38750;&#25351;&#25968;&#24418;&#24335;&#65292;&#36825;&#26159;&#29627;&#29827;&#31995;&#32479;&#30340;&#20856;&#22411;&#29305;&#24449;&#12290;&#36825;&#31181;&#29289;&#29702;&#35299;&#37322;&#34920;&#26126;&#36890;&#36807;&#23558;&#26356;&#35814;&#32454;&#30340;&#20449;&#24687;&#32534;&#30721;&#21040;&#36755;&#20837;&#21644;&#21453;&#39304;&#36793;&#30028;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
By training linear physical networks to learn linear transformations, we discern how their physical properties evolve due to weight update rules. Our findings highlight a striking similarity between the learning behaviors of such networks and the processes of aging and memory formation in disordered and glassy systems. We show that the learning dynamics resembles an aging process, where the system relaxes in response to repeated application of the feedback boundary forces in presence of an input force, thus encoding a memory of the input-output relationship. With this relaxation comes an increase in the correlation length, which is indicated by the two-point correlation function for the components of the network. We also observe that the square root of the mean-squared error as a function of epoch takes on a non-exponential form, which is a typical feature of glassy systems. This physical interpretation suggests that by encoding more detailed information into input and feedback boundar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.04272</link><description>&lt;p&gt;
&#23398;&#20064;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#65288;LQ&#65289;&#21338;&#24328;&#22312;&#26368;&#20248;&#25511;&#21046;&#20013;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#65288;i&#65289;&#39118;&#38505;&#25935;&#24863;&#25110;&#40065;&#26834;&#25511;&#21046;&#30340;&#21160;&#24577;&#21338;&#24328;&#24418;&#24335;&#65292;&#25110;&#32773;&#65288;ii&#65289;&#20316;&#20026;&#36830;&#32493;&#29366;&#24577;-&#25511;&#21046;&#31354;&#38388;&#20013;&#20004;&#20010;&#31454;&#20105;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#35774;&#32622;&#12290;&#19982;&#24191;&#27867;&#30740;&#31350;&#30340;&#21333;&#26234;&#33021;&#20307;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#19981;&#21516;&#65292;&#38646;&#21644;LQ&#21338;&#24328;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#32570;&#20047;&#24378;&#21046;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38750;&#20985;&#26368;&#23567;-&#26368;&#22823;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#25511;&#21046;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#27169;&#22411;&#26080;&#20851;&#35774;&#32622;&#20013;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#65292;&#20197;&#36798;&#21040;Nash&#22343;&#34913;&#30340;&#949;-&#37051;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#29702;&#24819;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476; (Spa-Net) &#23545;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#22788;&#29702;&#22810;&#31181;&#36755;&#20837;&#29289;&#20307;&#31867;&#22411;&#21644;&#20840;&#23616;&#20107;&#20214;&#29305;&#24449;&#12290;&#22312;&#39030;&#22840;&#20811;&#23545;&#30340;&#21322;&#36731;&#23376;&#34928;&#21464;&#21644;&#19982;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#20849;&#21516;&#20135;&#29983;&#30340;&#39030;&#22840;&#20811;&#23545;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.01886</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476;&#37325;&#24314;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of Unstable Heavy Particles Using Deep Symmetry-Preserving Attention Networks. (arXiv:2309.01886v2 [hep-ex] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01886
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476; (Spa-Net) &#23545;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#22788;&#29702;&#22810;&#31181;&#36755;&#20837;&#29289;&#20307;&#31867;&#22411;&#21644;&#20840;&#23616;&#20107;&#20214;&#29305;&#24449;&#12290;&#22312;&#39030;&#22840;&#20811;&#23545;&#30340;&#21322;&#36731;&#23376;&#34928;&#21464;&#21644;&#19982;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#20849;&#21516;&#20135;&#29983;&#30340;&#39030;&#22840;&#20811;&#23545;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24314;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;&#38656;&#35201;&#31934;&#23494;&#30340;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#23558;&#25506;&#27979;&#22120;&#29289;&#20307;&#20998;&#37197;&#32473;&#24213;&#23618;&#31890;&#23376;&#30340;&#22823;&#37327;&#21487;&#33021;&#25490;&#21015;&#12290;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#27880;&#24847;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476; (Spa-Net)&#65292;&#20808;&#21069;&#24050;&#24212;&#29992;&#20110;&#21482;&#20135;&#29983;&#24378;&#23376;&#21943;&#27880;&#30340;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#30340;&#39030;&#22840;&#20811;&#23545;&#34928;&#21464;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102; Spa-Net &#26550;&#26500;&#26469;&#32771;&#34385;&#22810;&#31181;&#36755;&#20837;&#29289;&#20307;&#31867;&#22411;&#65292;&#22914;&#36731;&#23376;&#65292;&#20197;&#21450;&#20840;&#23616;&#20107;&#20214;&#29305;&#24449;&#65292;&#22914;&#22833;&#21435;&#30340;&#27178;&#21521;&#21160;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#22238;&#24402;&#21644;&#20998;&#31867;&#36755;&#20986;&#20197;&#34917;&#20805;&#31890;&#23376;&#20998;&#37197;&#12290;&#25105;&#20204;&#22312;&#39030;&#22840;&#20811;&#23545;&#30340;&#21322;&#36731;&#23376;&#34928;&#21464;&#20197;&#21450;&#19982;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#20849;&#21516;&#20135;&#29983;&#30340;&#39030;&#22840;&#20811;&#23545;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102; Spa-Net &#25193;&#23637;&#33021;&#21147;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#33021;&#21147;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65306;ttH &#25628;&#32034;&#12289;&#39030;&#22840;&#20811;&#23545;&#30340;&#27979;&#37327;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to the underlying partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (Spa-Net), has been previously applied to top quark pair decays at the Large Hadron Collider which produce only hadronic jets. Here we extend the Spa-Net architecture to consider multiple input object types, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of Spa-Net in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: a search for ttH, a measurement of the 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24191;&#20041;&#30697;&#26041;&#27861;&#65288;SGMM&#65289;&#65292;&#29992;&#20110;&#20272;&#35745;&#21644;&#25512;&#26029;&#30697;&#38480;&#21046;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#22312;&#32447;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.13564</link><description>&lt;p&gt;
SGMM: &#24191;&#20041;&#30697;&#26041;&#27861;&#30340;&#38543;&#26426;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
SGMM: Stochastic Approximation to Generalized Method of Moments. (arXiv:2308.13564v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13564
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24191;&#20041;&#30697;&#26041;&#27861;&#65288;SGMM&#65289;&#65292;&#29992;&#20110;&#20272;&#35745;&#21644;&#25512;&#26029;&#30697;&#38480;&#21046;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#22312;&#32447;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31867;&#65292;&#38543;&#26426;&#24191;&#20041;&#30697;&#26041;&#27861;&#65288;SGMM&#65289;&#65292;&#29992;&#20110;&#20272;&#35745;&#21644;&#25512;&#26029;&#65288;&#36229;&#35782;&#21035;&#65289;&#30697;&#38480;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;SGMM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#26367;&#20195;&#20102;&#27969;&#34892;&#30340;Hansen&#65288;1982&#24180;&#65289;&#30340;&#65288;&#31163;&#32447;&#65289;GMM&#65292;&#24182;&#25552;&#20379;&#20102;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SGMM&#23545;&#20110;&#25928;&#29575;&#19981;&#39640;&#30340;&#22312;&#32447;2SLS&#21644;&#39640;&#25928;&#30340;SGMM&#20855;&#26377;&#20960;&#20046;&#30830;&#23450;&#30340;&#25910;&#25947;&#24615;&#21644;&#65288;&#20989;&#25968;&#65289;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Durbin-Wu-Hausman&#21644;Sargan-Hansen&#27979;&#35797;&#30340;&#22312;&#32447;&#29256;&#26412;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;SGMM&#26694;&#26550;&#20013;&#12290;&#24191;&#27867;&#30340;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#65292;SGMM&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#19982;&#26631;&#20934;&#65288;&#31163;&#32447;&#65289;GMM&#30456;&#21305;&#37197;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#21644;&#22312;&#32447;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of algorithms, Stochastic Generalized Method of Moments (SGMM), for estimation and inference on (overidentified) moment restriction models. Our SGMM is a novel stochastic approximation alternative to the popular Hansen (1982) (offline) GMM, and offers fast and scalable implementation with the ability to handle streaming datasets in real time. We establish the almost sure convergence, and the (functional) central limit theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo simulations show that as the sample size increases, the SGMM matches the standard (offline) GMM in terms of estimation accuracy and gains over computational efficiency, indicating its practical value for both large-scale and online datasets. We demonstrate the efficacy of our approach by a proof of concept using two we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12896</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#39029;&#20998;&#31867;&#65306;&#35774;&#35745;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#24615;&#36136;&#19978;&#65288;$X$&#65306;&#22810;&#36890;&#36947;&#12289;&#22810;&#39029;&#12289;&#22810;&#34892;&#19994;&#65307;$Y$&#65306;&#31867;&#21035;&#20998;&#24067;&#21644;&#26631;&#31614;&#38598;&#30340;&#22810;&#26679;&#24615;&#65289;&#21644;&#32771;&#34385;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65288;$f$&#65306;&#22810;&#39029;&#25991;&#26723;&#12289;&#39029;&#38754;&#27969;&#21644;&#25991;&#26723;&#25414;&#32465;&#20998;&#31867;&#65292;...&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20844;&#20849;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#24182;&#35268;&#33539;&#20102;&#24212;&#29992;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#28608;&#21457;&#20102;&#20197;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#20026;&#30446;&#26631;&#30340;&#20215;&#20540;&#12290;&#23545;&#25552;&#20986;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#65292;&#24182;&#38656;&#35201;&#26356;&#26032;&#20197;&#35780;&#20272;&#23454;&#38469;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#36825;&#20010;&#29616;&#23454;&#24773;&#20917;&#26816;&#26597;&#20063;&#21628;&#21505;&#26356;&#25104;&#29087;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#28085;&#30422;&#26657;&#20934;&#35780;&#20272;&#12289;&#25512;&#29702;&#22797;&#26434;&#24615;&#65288;&#26102;&#38388;-&#20869;&#23384;&#65289;&#21644;&#19968;&#31995;&#21015;&#29616;&#23454;&#20998;&#25955;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#35299;&#37322;&#27169;&#22411;&#20013;&#20559;&#35265;&#21306;&#22495;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#26469;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.11090</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Fairness and Explainability in Image Classification Using Optimal Transport. (arXiv:2308.11090v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#35299;&#37322;&#27169;&#22411;&#20013;&#20559;&#35265;&#21306;&#22495;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#26469;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#25191;&#27861;&#31561;&#39046;&#22495;&#20013;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#23545;&#28508;&#22312;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#35299;&#37322;&#26159;&#24314;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20449;&#20219;&#21644;&#38382;&#36131;&#24615;&#30340;&#20851;&#38190;&#12290;&#23613;&#31649;&#22312;&#27599;&#20010;&#39046;&#22495;&#20998;&#21035;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39046;&#22495;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#24212;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20262;&#29702;&#25968;&#25454;&#25366;&#25496;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#22810;&#27425;&#30740;&#31350;&#34920;&#26126;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#31639;&#27861;&#20250;&#23548;&#33268;&#26377;&#20559;&#24046;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22312;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#24456;&#23569;&#26377;&#23581;&#35797;&#35299;&#37322;&#27169;&#22411;&#20026;&#20160;&#20040;&#23384;&#22312;&#20559;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#26469;&#21457;&#29616;&#22270;&#20687;&#20013;&#26377;&#20559;&#24046;&#21306;&#22495;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#34920;&#26684;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;Wasserstein barycenters&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#35299;&#37322;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Fairness and the explainability of potentially unfair outcomes are crucial for establishing trust and accountability of Artificial Intelligence systems in domains such as healthcare and policing. Though significant advances have been made in each of the fields separately, achieving explainability in fairness applications remains challenging, particularly so in domains where deep neural networks are used. At the same time, ethical data-mining has become ever more relevant, as it has been shown countless times that fairness-unaware algorithms result in biased outcomes. Current approaches focus on mitigating biases in the outcomes of the model, but few attempts have been made to try to explain \emph{why} a model is biased. To bridge this gap, we propose a comprehensive approach that leverages optimal transport theory to uncover the causes and implications of biased regions in images, which easily extends to tabular data as well. Through the use of Wasserstein barycenters, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10364</link><description>&lt;p&gt;
SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;
&lt;/p&gt;
&lt;p&gt;
SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#33021;&#22815;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#20351;&#20854;&#25104;&#20026;&#29289;&#29702;&#31995;&#32479;&#27010;&#29575;&#24314;&#27169;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#32806;&#21512;&#26550;&#26500;&#26080;&#27861;&#36171;&#20104;&#25805;&#20316;&#21407;&#23376;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#27969;SE(3)&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27839;&#38468;&#21152;&#22686;&#24378;&#32500;&#24230;&#36827;&#34892;&#22352;&#26631;&#20998;&#21106;&#30340;&#32806;&#21512;&#27969;&#65292;&#20197;&#20445;&#25345;SE(3)&#21644;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;&#22312;&#27599;&#19968;&#23618;&#20013;&#65292;&#27969;&#23558;&#21407;&#23376;&#30340;&#20301;&#32622;&#26144;&#23556;&#21040;&#23398;&#20064;&#24471;&#21040;&#30340;SE(3)&#19981;&#21464;&#22522;&#19978;&#65292;&#25105;&#20204;&#22312;&#36820;&#22238;&#21040;&#21407;&#22987;&#22522;&#20043;&#21069;&#24212;&#29992;&#26631;&#20934;&#27969;&#21464;&#25442;&#65292;&#22914;&#21333;&#35843;&#20998;&#23376;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#27969;&#20445;&#25345;&#20102;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20135;&#29983;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#26399;&#26395;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#22312;DW4&#12289;LJ13&#21644;QM9&#20301;&#32622;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27969;&#19982;&#31561;&#21464;&#27969;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivari
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#34892;&#20026;&#31232;&#30095;&#24615;&#21644;&#34892;&#20026;&#31354;&#38388;&#38169;&#20301;&#31561;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#20248;&#36136;&#22810;&#26679;&#24615;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23454;&#39564;&#39046;&#22495;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;...</title><link>http://arxiv.org/abs/2308.05483</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#20013;&#30340;&#31232;&#30095;&#22870;&#21169;&#21644;&#31232;&#30095;&#20132;&#20114;&#19979;&#30340;&#20248;&#36136;&#22810;&#26679;&#24615;&#65306;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#25235;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics. (arXiv:2308.05483v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05483
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#34892;&#20026;&#31232;&#30095;&#24615;&#21644;&#34892;&#20026;&#31354;&#38388;&#38169;&#20301;&#31561;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#20248;&#36136;&#22810;&#26679;&#24615;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23454;&#39564;&#39046;&#22495;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#36136;&#22810;&#26679;&#24615;&#65288;Quality-Diversity&#65292;QD&#65289;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32473;&#23450;&#38382;&#39064;&#12290;&#22312;&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#20013;&#26368;&#21021;&#24320;&#21457;&#65292;&#22823;&#22810;&#25968;QD&#30740;&#31350;&#37117;&#26159;&#22312;&#26377;&#38480;&#30340;&#19968;&#32452;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#36816;&#21160;&#65292;&#20854;&#20013;&#36866;&#24212;&#24230;&#21644;&#34892;&#20026;&#20449;&#21495;&#26159;&#23494;&#38598;&#30340;&#12290;&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#31038;&#21306;&#30340;&#21162;&#21147;&#65292;&#20294;&#35813;&#20219;&#21153;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#25235;&#21462;&#22312;QD&#25991;&#29486;&#20013;&#38754;&#20020;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#65306;&#22870;&#21169;&#31232;&#30095;&#24615;&#65292;&#34892;&#20026;&#31232;&#30095;&#24615;&#21644;&#34892;&#20026;&#31354;&#38388;&#38169;&#20301;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;QD&#22914;&#20309;&#35299;&#20915;&#25235;&#21462;&#38382;&#39064;&#12290;&#22312;10&#20010;&#25235;&#21462;&#39046;&#22495;&#19978;&#36827;&#34892;&#20102;15&#31181;&#19981;&#21516;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#23545;&#24212;&#20110;2&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#22841;&#25345;&#22120;&#35774;&#32622;&#21644;5&#31181;&#26631;&#20934;&#29289;&#20307;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#21306;&#20998;&#31639;&#27861;&#30340;&#35780;&#20272;&#19982;&#20854;&#20869;&#37096;&#32452;&#20214;&#30340;&#35780;&#20272;&#65292;&#20197;&#20415;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity (QD) methods are algorithms that aim to generate a set of diverse and high-performing solutions to a given problem. Originally developed for evolutionary robotics, most QD studies are conducted on a limited set of domains - mainly applied to locomotion, where the fitness and the behavior signal are dense. Grasping is a crucial task for manipulation in robotics. Despite the efforts of many research communities, this task is yet to be solved. Grasping cumulates unprecedented challenges in QD literature: it suffers from reward sparsity, behavioral sparsity, and behavior space misalignment. The present work studies how QD can address grasping. Experiments have been conducted on 15 different methods on 10 grasping domains, corresponding to 2 different robot-gripper setups and 5 standard objects. An evaluation framework that distinguishes the evaluation of an algorithm from its internal components has also been proposed for a fair comparison. The obtained results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#32534;&#30721;&#36164;&#20135;&#23402;&#29983;&#32806;&#21512;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21516;&#21270;&#24863;&#27979;&#25968;&#25454;&#26469;&#25552;&#20379;&#23454;&#26102;&#30340;&#32467;&#26500;&#20581;&#24247;&#35786;&#26029;&#65292;&#19981;&#26029;&#26356;&#26032;&#25968;&#23383;&#23402;&#29983;&#29366;&#24577;&#24182;&#29992;&#20110;&#20248;&#21270;&#32500;&#25252;&#21644;&#31649;&#29702;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.01445</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A digital twin framework for civil engineering structures. (arXiv:2308.01445v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#32534;&#30721;&#36164;&#20135;&#23402;&#29983;&#32806;&#21512;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21516;&#21270;&#24863;&#27979;&#25968;&#25454;&#26469;&#25552;&#20379;&#23454;&#26102;&#30340;&#32467;&#26500;&#20581;&#24247;&#35786;&#26029;&#65292;&#19981;&#26029;&#26356;&#26032;&#25968;&#23383;&#23402;&#29983;&#29366;&#24577;&#24182;&#29992;&#20110;&#20248;&#21270;&#32500;&#25252;&#21644;&#31649;&#29702;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#27010;&#24565;&#20026;&#22303;&#26408;&#24037;&#31243;&#31995;&#32479;&#30340;&#22522;&#20110;&#26465;&#20214;&#21644;&#39044;&#27979;&#32500;&#25252;&#33539;&#24335;&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#38477;&#20302;&#29983;&#21629;&#21608;&#26399;&#25104;&#26412;&#12289;&#22686;&#21152;&#31995;&#32479;&#23433;&#20840;&#24615;&#21644;&#25552;&#39640;&#31995;&#32479;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#30340;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#29992;&#20110;&#22303;&#26408;&#24037;&#31243;&#32467;&#26500;&#30340;&#20581;&#24247;&#30417;&#27979;&#12289;&#32500;&#25252;&#21644;&#31649;&#29702;&#35268;&#21010;&#12290;&#36164;&#20135;&#23402;&#29983;&#32806;&#21512;&#21160;&#24577;&#31995;&#32479;&#37319;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#21487;&#20197;&#32771;&#34385;&#21040;&#25152;&#26377;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#29305;&#21035;&#26159;&#65292;&#37319;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#23545;&#26102;&#38388;&#37325;&#22797;&#30340;&#35266;&#27979;-&#20915;&#31574;&#27969;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23558;&#24863;&#27979;&#25968;&#25454;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21516;&#21270;&#65292;&#23454;&#26102;&#30340;&#32467;&#26500;&#20581;&#24247;&#35786;&#26029;&#24471;&#20197;&#25552;&#20379;&#12290;&#25968;&#23383;&#23402;&#29983;&#29366;&#24577;&#20197;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#26041;&#24335;&#19981;&#26029;&#26356;&#26032;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#21160;&#24577;&#20915;&#31574;&#19979;&#30340;&#32500;&#25252;&#21644;&#31649;&#29702;&#34892;&#21160;&#30340;&#26368;&#20248;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digital twin concept represents an appealing opportunity to advance condition-based and predictive maintenance paradigms for civil engineering systems, thus allowing reduced lifecycle costs, increased system safety, and increased system availability. This work proposes a predictive digital twin approach to the health monitoring, maintenance, and management planning of civil engineering structures. The asset-twin coupled dynamical system is encoded employing a probabilistic graphical model, which allows all relevant sources of uncertainty to be taken into account. In particular, the time-repeating observations-to-decisions flow is modeled using a dynamic Bayesian network. Real-time structural health diagnostics are provided by assimilating sensed data with deep learning models. The digital twin state is continually updated in a sequential Bayesian inference fashion. This is then exploited to inform the optimal planning of maintenance and management actions within a dynamic decision-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20511;&#37492;&#20102;&#22810;&#20010;&#25991;&#29486;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35745;&#31639;Wasserstein&#36317;&#31163;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#27604;&#20854;&#20182;&#31639;&#27861;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08507</link><description>&lt;p&gt;
&#20511;&#37492;&#29109;&#26368;&#20248;&#36755;&#36816;&#12289;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25991;&#29486;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients. (arXiv:2307.08507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20511;&#37492;&#20102;&#22810;&#20010;&#25991;&#29486;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35745;&#31639;Wasserstein&#36317;&#31163;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#27604;&#20854;&#20182;&#31639;&#27861;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#29109;&#26368;&#20248;&#36755;&#36816;&#12289;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#25193;&#23637;&#19988;&#21487;&#22312;GPU&#19978;&#24182;&#34892;&#35745;&#31639;&#65292;&#33021;&#22815;&#20197;&#26497;&#39640;&#30340;&#31934;&#24230;&#35745;&#31639;Wasserstein&#36317;&#31163;&#65292;&#20351;&#30456;&#23545;&#35823;&#24046;&#36798;&#21040;$10^{-8}$&#65292;&#24182;&#19988;&#27809;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#35777;&#19978;&#65292;&#19982;&#21253;&#25324;&#23545;&#25968;&#22495;&#31283;&#23450;Sinkhorn&#31639;&#27861;&#22312;&#20869;&#30340;&#22810;&#31181;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#36798;&#21040;&#39640;&#31934;&#24230;&#35299;&#65292;&#20855;&#26377;&#26356;&#30701;&#30340;&#22681;&#38047;&#26102;&#38388;&#12290;&#25105;&#20204;&#35814;&#32454;&#22320;&#20998;&#26512;&#20102;&#31639;&#27861;&#21644;&#38382;&#39064;&#21442;&#25968;&#65292;&#24182;&#22312;MNIST&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#21508;&#31181;&#26368;&#26032;&#30340;&#39640;&#32500;&#38382;&#39064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#25104;&#20026;&#20174;&#19994;&#20154;&#21592;&#26368;&#20248;&#36755;&#36816;&#24037;&#20855;&#21253;&#20013;&#26377;&#29992;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a novel algorithm for optimal transport by drawing from the entropic optimal transport, mirror descent and conjugate gradients literatures. Our scalable and GPU parallelizable algorithm is able to compute the Wasserstein distance with extreme precision, reaching relative error rates of $10^{-8}$ without numerical stability issues. Empirically, the algorithm converges to high precision solutions more quickly in terms of wall-clock time than a variety of algorithms including log-domain stabilized Sinkhorn's Algorithm. We provide careful ablations with respect to algorithm and problem parameters, and present benchmarking over upsampled MNIST images, comparing to various recent algorithms over high-dimensional problems. The results suggest that our algorithm can be a useful addition to the practitioner's optimal transport toolkit.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#21644;&#22120;&#26800;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#27425;&#20248;&#30340;&#21010;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.16879</link><description>&lt;p&gt;
&#25163;&#26415;&#38454;&#27573;&#21644;&#22120;&#26800;&#35782;&#21035;&#65306;&#22914;&#20309;&#35782;&#21035;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Surgical Phase and Instrument Recognition: How to identify appropriate Dataset Splits. (arXiv:2306.16879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#21644;&#22120;&#26800;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#27425;&#20248;&#30340;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#24320;&#21457;&#29992;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#21644;&#22120;&#26800;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25163;&#26415;&#24037;&#20316;&#27969;&#30340;&#22797;&#26434;&#24615;&#36136;&#65292;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#26159;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#65292;&#23558;&#25968;&#25454;&#20180;&#32454;&#21010;&#20998;&#20026;&#35757;&#32451;&#38598;&#12289;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#65292;&#20197;&#21450;&#36873;&#25321;&#21512;&#36866;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#36827;&#34892;&#25968;&#25454;&#38598;&#21010;&#20998;&#30340;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#25152;&#25552;&#20986;&#30340;&#21487;&#35270;&#21270;&#26694;&#26550;&#26377;&#21161;&#20110;&#35780;&#20272;&#25163;&#26415;&#24037;&#20316;&#27969;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#27425;&#20248;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#12290;&#30446;&#21069;&#65292;&#23427;&#25903;&#25345;&#25163;&#26415;&#38454;&#27573;&#21644;&#22120;&#26800;&#27880;&#37322;&#30340;&#21487;&#35270;&#21270;&#12290;&#32467;&#26524;&#65306;&#20026;&#20102;&#39564;&#35777;&#19987;&#29992;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Purpose: The development of machine learning models for surgical workflow and instrument recognition from temporal data represents a challenging task due to the complex nature of surgical workflows. In particular, the imbalanced distribution of data is one of the major challenges in the domain of surgical workflow recognition. In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as the selection of suitable evaluation metrics are crucial. Methods: In this work, we present an openly available web-based application that enables interactive exploration of dataset partitions. The proposed visual framework facilitates the assessment of dataset splits for surgical workflow recognition, especially with regard to identifying sub-optimal dataset splits. Currently, it supports visualization of surgical phase and instrument annotations. Results: In order to validate the dedicated interactive visualizations, we use a dataset split of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#29992;&#20998;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#21453;&#20107;&#23454;&#26597;&#35810;&#26469;&#27979;&#37327;&#21160;&#20316;&#23545;&#26410;&#26469;&#22870;&#21169;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#23545;&#22870;&#21169;&#25110;&#22870;&#21169;&#23545;&#35937;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#33719;&#24471;&#20102;&#20855;&#26377;&#26356;&#20302;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.16803</link><description>&lt;p&gt;
&#38271;&#26399;&#20449;&#29992;&#24402;&#22240;&#36890;&#36807;&#21453;&#20107;&#23454;&#36129;&#29486;&#20998;&#26512;&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis. (arXiv:2306.16803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#29992;&#20998;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#21453;&#20107;&#23454;&#26597;&#35810;&#26469;&#27979;&#37327;&#21160;&#20316;&#23545;&#26410;&#26469;&#22870;&#21169;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#23545;&#22870;&#21169;&#25110;&#22870;&#21169;&#23545;&#35937;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#33719;&#24471;&#20102;&#20855;&#26377;&#26356;&#20302;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22909;&#30340;&#20449;&#29992;&#24402;&#22240;&#26041;&#27861;&#26469;&#34913;&#37327;&#21160;&#20316;&#23545;&#26410;&#26469;&#22870;&#21169;&#30340;&#24433;&#21709;&#12290;&#22312;&#24724;&#26827;&#20449;&#29992;&#24402;&#22240;&#65288;HCA&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36129;&#29486;&#20998;&#26512;&#65288;COCOA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#29992;&#24402;&#22240;&#31639;&#27861;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#37327;&#21270;&#19968;&#20010;&#21453;&#20107;&#23454;&#26597;&#35810;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#20449;&#29992;&#20998;&#37197;&#65306;&#8220;&#22914;&#26524;&#20195;&#29702;&#36873;&#25321;&#21478;&#19968;&#20010;&#21160;&#20316;&#65292;&#23427;&#20173;&#28982;&#20250;&#33719;&#24471;&#36825;&#20010;&#22870;&#21169;&#21527;&#65311;&#8221;&#36890;&#36807;&#27979;&#37327;&#21160;&#20316;&#23545;&#33719;&#24471;&#21518;&#32493;&#22870;&#21169;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#22870;&#21169;&#29366;&#24577;&#27979;&#37327;&#36129;&#29486;&#65288;&#21363;HCA&#20013;&#25152;&#20570;&#30340;&#65289;&#20250;&#23548;&#33268;&#36129;&#29486;&#30340;&#38169;&#35823;&#20272;&#35745;&#65292;&#20351;&#24471;HCA&#22312;&#35768;&#22810;&#30456;&#20851;&#29615;&#22659;&#20013;&#21521;&#39640;&#26041;&#24046;&#30340;REINFORCE&#20272;&#35745;&#22120;&#36864;&#21270;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#23545;&#22870;&#21169;&#25110;&#25152;&#23398;&#20064;&#30340;&#22870;&#21169;&#23545;&#35937;&#30340;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#24471;&#21040;&#20855;&#26377;&#26356;&#20302;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#29305;&#23450;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
To make reinforcement learning more sample efficient, we need better credit assignment methods that measure an action's influence on future rewards. Building upon Hindsight Credit Assignment (HCA), we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual query: "Would the agent still have reached this reward if it had taken another action?". We show that measuring contributions w.r.t. rewarding states, as is done in HCA, results in spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or learned representations of the rewarding objects, resulting in gradient estimates with lower variance. We run experiments on a suite of problems specifically 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>PLASTIC&#31639;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#26631;&#31614;&#21487;&#22609;&#24615;&#65292;&#25552;&#39640;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10711</link><description>&lt;p&gt;
PLASTIC: &#25913;&#21892;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#36755;&#20837;&#21644;&#26631;&#31614;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. (arXiv:2306.10711v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10711
&lt;/p&gt;
&lt;p&gt;
PLASTIC&#31639;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#26631;&#31614;&#21487;&#22609;&#24615;&#65292;&#25552;&#39640;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#39640;&#26114;&#19988;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#12290;&#21407;&#21017;&#19978;&#65292;&#31163;&#31574;&#30053;RL&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#29615;&#22659;&#20132;&#20114;&#36827;&#34892;&#22810;&#27425;&#26356;&#26032;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22810;&#27425;&#26356;&#26032;&#24448;&#24448;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20043;&#21069;&#30340;&#20132;&#20114;&#65292;&#36825;&#34987;&#31216;&#20026;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#21487;&#22609;&#24615;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#35843;&#26597;&#12290;&#36755;&#20837;&#21487;&#22609;&#24615;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#23545;&#21464;&#21270;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#26631;&#31614;&#21487;&#22609;&#24615;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21270;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#21512;&#25104;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25439;&#22833;&#27010;&#35272;&#20013;&#23547;&#25214;&#26356;&#24179;&#28369;&#30340;&#26368;&#23567;&#20540;&#21487;&#20197;&#22686;&#24378;&#36755;&#20837;&#21487;&#22609;&#24615;&#65292;&#32780;&#32454;&#21270;&#30340;&#26799;&#24230;&#20256;&#25773;&#21487;&#20197;&#25552;&#39640;&#26631;&#31614;&#21487;&#22609;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PLASTIC&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#36825;&#20004;&#26041;&#38754;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, which is referred to as the loss of plasticity. Our study investigates the underlying causes of this phenomenon by dividing plasticity into two aspects. Input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity, whereas refined gradient propagation improves label plasticity. Leveraging these findings, we introduce the PLASTIC algorithm, which harmoniously combines techniques to address both
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29366;&#24577;&#36712;&#36857;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#25512;&#26029;&#26041;&#27861;&#21644;&#28385;&#36275;&#39640;&#32500;&#31995;&#32479;&#19982;&#38271;&#26102;&#38388;&#36328;&#24230;&#19979;&#36827;&#34892;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.10574</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Data Assimilation. (arXiv:2306.10574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29366;&#24577;&#36712;&#36857;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#25512;&#26029;&#26041;&#27861;&#21644;&#28385;&#36275;&#39640;&#32500;&#31995;&#32479;&#19982;&#38271;&#26102;&#38388;&#36328;&#24230;&#19979;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#20840;&#38754;&#30340;&#24418;&#24335;&#19979;&#65292;&#25968;&#25454;&#21516;&#21270;&#35299;&#20915;&#20102;&#37492;&#23450;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#21487;&#33021;&#29366;&#24577;&#36712;&#36857;&#30340;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#37322;&#23613;&#31649;&#23384;&#22312;&#22122;&#22768;&#25110;&#19981;&#23436;&#25972;&#35266;&#27979;&#30340;&#20869;&#23481;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#22522;&#20110;&#31890;&#23376;&#30340;&#21644;&#21487;&#21464;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#20381;&#36182;&#20110;&#36716;&#31227;&#21160;&#24577;&#36827;&#34892;&#25512;&#26029;&#65292;&#36825;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#25110;&#20855;&#26377;&#22797;&#26434;&#21160;&#24577;&#30340;&#39640;&#32500;&#31995;&#32479;&#20013;&#21464;&#24471;&#26840;&#25163;&#65292;&#22914;&#28023;&#27915;&#25110;&#22823;&#27668;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#26469;&#23454;&#29616;&#36712;&#36857;&#25512;&#26029;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#29366;&#24577;&#36712;&#36857;&#27169;&#22411;&#65292;&#36825;&#26159;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#27934;&#23519;&#65292;&#21363;&#20219;&#24847;&#38271;&#36712;&#36857;&#30340;&#24471;&#20998;&#21487;&#20197;&#20998;&#35299;&#20026;&#30701;&#37096;&#20998;&#30340;&#24471;&#20998;&#31995;&#21015;&#12290;&#22312;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#36816;&#29992;&#24471;&#20998;&#27169;&#22411;&#36827;&#34892;&#26080;&#33258;&#22238;&#24402;&#30340;&#25512;&#26029;&#65292;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#29366;&#24577;&#12290;&#19982;&#20247;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data assimilation, in its most comprehensive form, addresses the Bayesian inverse problem of identifying plausible state trajectories that explain noisy or incomplete observations of stochastic dynamical systems. Various approaches have been proposed to solve this problem, including particle-based and variational methods. However, most algorithms depend on the transition dynamics for inference, which becomes intractable for long time horizons or for high-dimensional systems with complex dynamics, such as oceans or atmospheres. In this work, we introduce score-based data assimilation for trajectory inference. We learn a score-based generative model of state trajectories based on the key insight that the score of an arbitrarily long trajectory can be decomposed into a series of scores over short segments. After training, inference is carried out using the score model, in a non-autoregressive manner by generating all states simultaneously. Quite distinctively, we decouple the observation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09739</link><description>&lt;p&gt;
&#23398;&#20064;&#21463;&#38480;&#21160;&#21147;&#23398;&#30340;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stabilized Neural Differential Equations for Learning Constrained Dynamics. (arXiv:2306.09739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#20174;&#25968;&#25454;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#25512;&#26029;&#20986;&#30340;&#21160;&#24577;&#31995;&#32479;&#20445;&#30041;&#24050;&#30693;&#32422;&#26463;&#26465;&#20214;&#65288;&#20363;&#22914;&#23432;&#24658;&#23450;&#24459;&#25110;&#23545;&#20801;&#35768;&#30340;&#31995;&#32479;&#29366;&#24577;&#30340;&#38480;&#21046;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#31283;&#23450;&#39033;&#65292;&#24403;&#28155;&#21152;&#21040;&#21407;&#22987;&#21160;&#24577;&#31995;&#32479;&#20013;&#26102;&#65292;&#21487;&#20197;&#23558;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25152;&#26377;&#24120;&#35265;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#27169;&#22411;&#20860;&#23481;&#24182;&#24191;&#27867;&#36866;&#29992;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;SNDE&#22312;&#25193;&#23637;&#21487;&#32435;&#20837;NODE&#35757;&#32451;&#30340;&#32422;&#26463;&#31867;&#22411;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07916</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#27169;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Identification of Nonlinear Latent Hierarchical Models. (arXiv:2306.07916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#28041;&#21450;&#29983;&#29289;&#25968;&#25454;&#12289;&#21307;&#23398;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#35821;&#35328;&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#24182;&#19988;&#20851;&#31995;&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#35266;&#27979;&#21464;&#37327;&#30001;&#19968;&#32452;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#26377;&#20123;&#28508;&#21464;&#37327;&#21487;&#33021;&#27809;&#26377;&#35266;&#23519;&#21040;&#30340;&#21518;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#65306;&#23545;&#20110;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#20801;&#35768;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#22810;&#26465;&#36335;&#24452;&#65292;&#36825;&#25918;&#23485;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#28508;&#21464;&#37327;&#26641;&#20551;&#35774;&#65307;&#23545;&#20110;&#32467;&#26500;&#20989;&#25968;&#65292;&#25105;&#20204;&#27809;&#26377;&#36827;&#34892;&#21442;&#25968;&#20551;&#35774;&#65292;&#22240;&#27492;&#21487;&#20197;&#20801;&#35768;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07462</link><description>&lt;p&gt;
&#20851;&#20110;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Removal-Based Feature Attributions. (arXiv:2306.07462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#37322;&#22522;&#20110;&#36755;&#20837;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#35768;&#22810;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26469;&#20998;&#37197;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25361;&#25112;&#20102;&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;&#36825;&#20123;&#26041;&#27861;&#23545;&#36755;&#20837;&#21644;&#27169;&#22411;&#25200;&#21160;&#25935;&#24863;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#40065;&#26834;&#24402;&#22240;&#26041;&#27861;&#21644;&#27169;&#22411;&#20462;&#25913;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24402;&#22240;&#40065;&#26834;&#24615;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Removal-Based&#24402;&#22240;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#36136;&#23578;&#26410;&#20840;&#38754;&#22320;&#24471;&#21040;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#23545;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38416;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#36755;&#20837;&#21644;&#27169;&#22411;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#23436;&#22909;&#21644;&#21463;&#25200;&#21160;&#30340;&#24402;&#22240;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To explain complex models based on their inputs, many feature attribution methods have been developed that assign importance scores to input features. However, some recent work challenges the robustness of feature attributions by showing that these methods are sensitive to input and model perturbations, while other work addresses this robustness issue by proposing robust attribution methods and model modifications. Nevertheless, previous work on attribution robustness has focused primarily on gradient-based feature attributions. In contrast, the robustness properties of removal-based attribution methods are not comprehensively well understood. To bridge this gap, we theoretically characterize the robustness of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and prove upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical experiments on synthetic and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-RandP&#30340;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#36807;&#31243;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#30340;&#22270;&#20687;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#25552;&#39640;&#20102;CIFAR-10&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.06076</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#36807;&#31243;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Image Classification by Learning Priors from Random Processes. (arXiv:2306.06076v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-RandP&#30340;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#36807;&#31243;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#30340;&#22270;&#20687;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#25552;&#39640;&#20102;CIFAR-10&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19981;&#21516;ially&#31169;&#26377;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#30001;&#20110;&#27599;&#20010;&#26679;&#26412;&#26799;&#24230;&#21098;&#36753;&#21644;&#22122;&#22768;&#28155;&#21152;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#26368;&#36817;&#37325;&#28857;&#26159;&#36890;&#36807;&#23558;&#22312;&#30495;&#23454;&#19990;&#30028;&#20844;&#20849;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;DP-SGD&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20174;&#30001;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#24182;&#23558;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#36716;&#31227;&#21040;&#31169;&#26377;&#25968;&#25454;&#26469;&#25913;&#36827;DP-SGD&#30340;&#38544;&#31169;-&#25928;&#29992;&#25240;&#34935;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DP-RandP&#65292;&#36825;&#26159;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#26041;&#27861;&#12290;&#22312;CIFAR10&#12289;CIFAR100&#21644;MedMNIST&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#38544;&#31169;&#39044;&#31639;&#949;&#8712;[1&#65292;8]&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#22312;&#949;=1&#26102;&#22312;CIFAR10&#19978;&#25253;&#21578;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#20174;60.6%&#25552;&#39640;&#21040;72.3%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/inspire-group/DP-RandP&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition. A recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data. In this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, and MedMNIST for a range of privacy budgets $\varepsilon \in [1, 8]$. In particular, we improve the previous best reported accuracy on CIFAR10 from $60.6 \%$ to $72.3 \%$ for $\varepsilon=1$. Our code is available at https://github.com/inspire-group/DP-RandP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Shapley&#20540;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#21487;&#20197;&#37327;&#21270;&#27599;&#20010;&#29305;&#24449;&#23545;&#20010;&#21035;&#27169;&#22411;&#36755;&#20986;&#26465;&#20214;&#29109;&#30340;&#36129;&#29486;&#65292;&#36866;&#29992;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#26816;&#27979;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27963;&#21160;&#29305;&#24449;&#20215;&#20540;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.05724</link><description>&lt;p&gt;
&#29992;&#20449;&#24687;&#29702;&#35770;&#30340;Shapley&#20540;&#35299;&#37322;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explaining Predictive Uncertainty with Information Theoretic Shapley Values. (arXiv:2306.05724v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Shapley&#20540;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#21487;&#20197;&#37327;&#21270;&#27599;&#20010;&#29305;&#24449;&#23545;&#20010;&#21035;&#27169;&#22411;&#36755;&#20986;&#26465;&#20214;&#29109;&#30340;&#36129;&#29486;&#65292;&#36866;&#29992;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#26816;&#27979;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27963;&#21160;&#29305;&#24449;&#20215;&#20540;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22797;&#26434;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#30340;$\textit{&#19981;&#30830;&#23450;&#24615;}$&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;Shapley&#20540;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#31867;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#37327;&#21270;&#27599;&#20010;&#29305;&#24449;&#23545;&#20010;&#21035;&#27169;&#22411;&#36755;&#20986;&#26465;&#20214;&#29109;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20462;&#25913;&#29305;&#24449;&#20989;&#25968;&#30340;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;Shapley&#20540;&#19982;&#20449;&#24687;&#35770;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20013;&#30340;&#22522;&#26412;&#37327;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#26377;&#35777;&#26126;&#20445;&#35777;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#29575;&#25511;&#21046;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#22312;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#26816;&#27979;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27963;&#21160;&#29305;&#24449;&#20215;&#20540;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the $\textit{uncertainty}$ of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature's contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement an efficient algorithm that performs well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05584</link><description>&lt;p&gt;
&#22810;&#20307;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation. (arXiv:2306.05584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#30340;&#30495;&#27491;&#36890;&#29992;&#26041;&#27861;&#23545;&#20110;&#29702;&#35299;&#20851;&#33410;&#29289;&#20307;&#21644;&#31227;&#21160;&#22330;&#26223;&#30340;&#19977;&#32500;&#24433;&#20687;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#20043;&#38388;&#23494;&#20999;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SE&#65288;3&#65289;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#21253;&#25324;&#20004;&#20010;&#36731;&#37327;&#32423;&#21644;&#30456;&#20114;&#36830;&#25509;&#30340;&#22836;&#37096;&#65292;&#20351;&#29992;&#28857;&#32423;&#19981;&#21464;&#29305;&#24449;&#21644;&#26469;&#33258;SE&#65288;3&#65289;&#31561;&#21464;&#29305;&#24449;&#30340;&#36816;&#21160;&#20272;&#35745;&#26469;&#39044;&#27979;&#20998;&#21106;&#25513;&#27169;&#65292;&#32780;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#22521;&#35757;&#31574;&#30053;&#21487;&#20197;&#22312;&#32447;&#25191;&#34892;&#65292;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#27969;&#65292;&#20998;&#21106;&#25513;&#27169;&#21644;&#21018;&#24615;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26469;&#21516;&#26102;&#20248;&#21270;&#20004;&#20010;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21482;&#26377;0.25M&#21442;&#25968;&#21644;0.92G FLOPs&#12290;
&lt;/p&gt;
&lt;p&gt;
A truly generalizable approach to rigid segmentation and motion estimation is fundamental to 3D understanding of articulated objects and moving scenes. In view of the tightly coupled relationship between segmentation and motion estimates, we present an SE(3) equivariant architecture and a training strategy to tackle this task in an unsupervised manner. Our architecture comprises two lightweight and inter-connected heads that predict segmentation masks using point-level invariant features and motion estimates from SE(3) equivariant features without the prerequisites of category information. Our unified training strategy can be performed online while jointly optimizing the two predictions by exploiting the interrelations among scene flow, segmentation mask, and rigid transformations. We show experiments on four datasets as evidence of the superiority of our method both in terms of model performance and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the best of ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04746</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26377;&#25928;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;: &#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65288;CSS&#65289;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#25991;&#26723;&#26469;&#35299;&#37322;&#31038;&#20250;&#21644;&#25919;&#27835;&#29616;&#35937;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;CSS&#30740;&#31350;&#20154;&#21592;&#39318;&#20808;&#33719;&#21462;&#25991;&#26723;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#20998;&#26512;&#26469;&#35299;&#37322;&#26631;&#31614;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#22312;&#35268;&#27169;&#19978;&#20415;&#23452;&#22320;&#27880;&#37322;&#25991;&#26723;&#26469;&#38477;&#20302;CSS&#30740;&#31350;&#25104;&#26412;&#65292;&#20294;&#36825;&#20123;&#26367;&#20195;&#26631;&#31614;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#21644;&#26377;&#20559;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;LLMs&#30340;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#35777;&#19982;CSS&#30740;&#31350;&#22522;&#26412;&#30456;&#20851;&#30340;&#32479;&#35745;&#23646;&#24615;-&#22914;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#22312;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#20013;&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#26367;&#20195;&#26631;&#31614;&#20250;&#23548;&#33268;&#23454;&#36136;&#24615;&#20559;&#24046;&#21644;&#26080;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#21363;&#20351;&#26367;&#20195;&#20934;&#30830;&#24615;&#39640;&#36798;80-90&#65285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;D-SSL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#27880;&#37322;&#19982;&#26377;&#38024;&#23545;&#24615;&#30340;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#26631;&#31614;&#33719;&#21462;&#30340;CSS&#30740;&#31350;&#25104;&#26412;&#38477;&#20302;80&#65285;&#65292;&#32780;&#19981;&#24433;&#21709;&#32479;&#35745;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#34920;&#26126;&#65292;&#19982;&#30452;&#25509;&#20351;&#29992;LLM&#39044;&#27979;&#26631;&#31614;&#30456;&#27604;&#65292;D-SSL&#21487;&#20197;&#23558;&#22238;&#24402;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#22810;&#36798;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24120;&#35265;&#30340;&#35780;&#20215;&#25351;&#26631;&#22914;FID&#31561;&#19981;&#33021;&#24456;&#22909;&#22320;&#20307;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#24863;&#30693;&#30495;&#23454;&#24615;&#65292;&#24314;&#35758;&#20351;&#29992;SwAV&#29305;&#24449;&#25552;&#21462;&#22120;&#32467;&#21512;FID&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.04675</link><description>&lt;p&gt;
&#25581;&#31034;&#29983;&#25104;&#27169;&#22411;&#35780;&#20215;&#24230;&#37327;&#30340;&#32570;&#38519;&#21450;&#20854;&#19981;&#20844;&#24179;&#23545;&#24453;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. (arXiv:2306.04675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04675
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24120;&#35265;&#30340;&#35780;&#20215;&#25351;&#26631;&#22914;FID&#31561;&#19981;&#33021;&#24456;&#22909;&#22320;&#20307;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#24863;&#30693;&#30495;&#23454;&#24615;&#65292;&#24314;&#35758;&#20351;&#29992;SwAV&#29305;&#24449;&#25552;&#21462;&#22120;&#32467;&#21512;FID&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#35768;&#22810;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#35821;&#20041;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#29702;&#35299;&#21644;&#25913;&#36827;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#24230;&#37327;&#12290;&#20351;&#29992;&#24515;&#29702;&#29289;&#29702;&#23398;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#23454;&#39564;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#20154;&#31867;&#24863;&#30693;&#22270;&#20687;&#30495;&#23454;&#24615;&#30340;&#27979;&#37327;&#65292;&#21457;&#29616;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#30340;&#24230;&#37327;&#33021;&#19982;&#20154;&#31867;&#35780;&#20272;&#24378;&#30456;&#20851;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#12289;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#35760;&#24518;&#33021;&#21147;&#30340;16&#20010;&#29616;&#20195;&#25351;&#26631;&#65292;&#21457;&#29616;&#20197;&#20154;&#31867;&#20026;&#22522;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#30340;&#24863;&#30693;&#30495;&#23454;&#24615;&#19981;&#21453;&#26144;&#22312;&#24120;&#35265;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;FID&#20013;&#12290;&#36825;&#31181;&#24046;&#24322;&#24182;&#19981;&#33021;&#36890;&#36807;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#37322;&#65292;&#23613;&#31649;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#36807;&#24230;&#20381;&#36182;&#20110;Inception-V3&#12290;&#36890;&#36807;&#30740;&#31350;&#26367;&#20195;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#38519;&#65292;&#21457;&#29616;&#20010;&#21035;&#24369;Downstream&#20219;&#21153;&#32534;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#26368;&#33021;&#35299;&#37322;&#22270;&#20687;&#30495;&#23454;&#24615;&#65292;&#24314;&#35758;&#22312;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#26102;&#20351;&#29992;SwAV&#29305;&#24449;&#25552;&#21462;&#22120;&#32467;&#21512;FID&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JWINS&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20165;&#36890;&#36807;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#20849;&#20139;&#37096;&#20998;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#26469;&#34917;&#20607;&#30001;&#31232;&#30095;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36890;&#20449;&#25130;&#26029;&#26469;&#20943;&#23569;&#36890;&#20449;&#29992;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;JWINS&#21487;&#20197;&#22312;&#21457;&#36865;&#26356;&#23569;&#30340;&#23383;&#33410;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#23436;&#20840;&#20849;&#20139;&#20998;&#24067;&#24335;&#23398;&#20064;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04377</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#20013;&#29992;&#26356;&#23569;&#30340;&#20195;&#20215;&#33719;&#24471;&#26356;&#22810;&#12290;&#65288;arXiv:2306.04377v1 [cs.DC]&#65289;
&lt;/p&gt;
&lt;p&gt;
Get More for Less in Decentralized Learning Systems. (arXiv:2306.04377v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JWINS&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20165;&#36890;&#36807;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#20849;&#20139;&#37096;&#20998;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#26469;&#34917;&#20607;&#30001;&#31232;&#30095;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36890;&#20449;&#25130;&#26029;&#26469;&#20943;&#23569;&#36890;&#20449;&#29992;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;JWINS&#21487;&#20197;&#22312;&#21457;&#36865;&#26356;&#23569;&#30340;&#23383;&#33410;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#23436;&#20840;&#20849;&#20139;&#20998;&#24067;&#24335;&#23398;&#20064;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#22240;&#20026;&#33021;&#22815;&#36991;&#20813;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#32780;&#20165;&#36890;&#36807;&#20132;&#27969;&#27169;&#22411;&#21442;&#25968;&#20445;&#25252;&#20102;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24222;&#22823;&#35268;&#27169;&#23545;&#20998;&#24067;&#24335;&#35757;&#32451;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#8212;&#8212;&#27599;&#20010;&#33410;&#28857;&#38656;&#35201;&#20132;&#25442;&#25968;&#21315;&#19975;&#20010;&#25968;&#25454;&#65292;&#23481;&#26131;&#23548;&#33268;&#32593;&#32476;&#36229;&#36127;&#33655;&#12290;&#26412;&#25991;&#25552;&#20986;JWINS&#65306;&#19968;&#31181;&#36890;&#20449;&#25928;&#29575;&#39640;&#19988;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#31232;&#30095;&#21270;&#26469;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#12290;JWINS&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#26469;&#38480;&#21046;&#22240;&#31232;&#30095;&#21270;&#23548;&#33268;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#36890;&#20449;&#20999;&#26029;&#26469;&#38477;&#20302;&#36890;&#20449;&#29992;&#37327;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;96&#20010;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33410;&#28857;&#30340;&#23454;&#29616;&#25928;&#26524;&#65292;&#35777;&#26126;JWINS&#21487;&#20197;&#22312;&#21457;&#36865;64&#65285;&#26356;&#23569;&#30340;&#23383;&#33410;&#26102;&#23454;&#29616;&#19982;&#23436;&#20840;&#20849;&#20139;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#30456;&#20284;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36890;&#20449;&#39044;&#31639;&#19979;&#65292;JWINS&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning (DL) systems have been gaining popularity because they avoid raw data sharing by communicating only model parameters, hence preserving data confidentiality. However, the large size of deep neural networks poses a significant challenge for decentralized training, since each node needs to exchange gigabytes of data, overloading the network. In this paper, we address this challenge with JWINS, a communication-efficient and fully decentralized learning system that shares only a subset of parameters through sparsification. JWINS uses wavelet transform to limit the information loss due to sparsification and a randomized communication cut-off that reduces communication usage without damaging the performance of trained models. We demonstrate empirically with 96 DL nodes on non-IID datasets that JWINS can achieve similar accuracies to full-sharing DL while sending up to 64% fewer bytes. Additionally, on low communication budgets, JWINS outperforms the state-of-the-art com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;&#20004;&#31181;&#20154;&#31867;&#27880;&#37322;&#32773;&#21487;&#20197;&#29992;&#20110;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#20998;&#31867;&#23398;&#12290;</title><link>http://arxiv.org/abs/2306.04125</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#20132;&#20114;: &#20154;&#31867;&#21644;&#33258;&#21160;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;&#20004;&#31181;&#20154;&#31867;&#27880;&#37322;&#32773;&#21487;&#20197;&#29992;&#20110;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#20998;&#31867;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20960;&#20046;&#25152;&#26377;&#22810;&#27169;&#24577;&#38382;&#39064;&#21644;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#22810;&#31181;&#24322;&#26500;&#21644;&#20114;&#32852;&#30340;&#20449;&#21495;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20026;&#20102;&#36827;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#27169;&#24577;&#21487;&#20197;&#23637;&#29616;&#30340;&#20132;&#20114;&#31867;&#22411;&#65306;&#27599;&#31181;&#27169;&#24577;&#22914;&#20309;&#21333;&#29420;&#25552;&#20379;&#23545;&#20219;&#21153;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#24403;&#23384;&#22312;&#20854;&#20182;&#27169;&#24577;&#26102;&#36825;&#20123;&#20449;&#24687;&#22914;&#20309;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#27880;&#37322;&#32773;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20004;&#31181;&#20998;&#31867;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65306;(1) &#37096;&#20998;&#26631;&#31614;&#65292;&#20854;&#20013;&#19981;&#21516;&#38543;&#26426;&#20998;&#37197;&#30340;&#27880;&#37322;&#32773;&#27880;&#37322;&#32473;&#23450;&#31532;&#19968;&#20010;&#12289;&#31532;&#20108;&#20010;&#21644;&#20004;&#20010;&#27169;&#24577;&#30340;&#26631;&#31614;&#65292;&#20197;&#21450;(2) &#21453;&#20107;&#23454;&#26631;&#31614;&#65292;&#20854;&#20013;&#21516;&#19968;&#27880;&#37322;&#32773;&#34987;&#35201;&#27714;&#22312;&#32473;&#20986;&#31532;&#19968;&#20010;&#27169;&#24577;&#20043;&#21069;&#27880;&#37322;&#26631;&#31614;&#65292;&#28982;&#21518;&#32473;&#20986;&#31532;&#20108;&#20010;&#27169;&#24577;&#65292;&#24182;&#35201;&#27714;&#20182;&#20204;&#26126;&#30830;&#22320;&#25512;&#29702;&#20182;&#20204;&#30340;&#31572;&#26696;&#22914;&#20309;&#25913;&#21464;&#65292;&#28982;&#21518;&#25552;&#20986;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#21478;&#19968;&#31181;&#20998;&#31867;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotator
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39640;&#26031;&#21464;&#20998;&#36807;&#31243;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#20855;&#26377;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#28508;&#22312;&#36807;&#31243;&#65292;&#27492;&#26041;&#27861;&#37319;&#29992;&#20855;&#26377;&#36830;&#32493;&#25351;&#25968;&#26063;&#25551;&#36848;&#30340;&#31639;&#27861;&#23454;&#29616;&#20984;&#20248;&#21270;&#65292;&#21487;&#20197;&#20195;&#26367;&#32531;&#24930;&#30340;&#20855;&#26377;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02066</link><description>&lt;p&gt;
&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Variational Gaussian Process Diffusion Processes. (arXiv:2306.02066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39640;&#26031;&#21464;&#20998;&#36807;&#31243;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#20855;&#26377;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#28508;&#22312;&#36807;&#31243;&#65292;&#27492;&#26041;&#27861;&#37319;&#29992;&#20855;&#26377;&#36830;&#32493;&#25351;&#25968;&#26063;&#25551;&#36848;&#30340;&#31639;&#27861;&#23454;&#29616;&#20984;&#20248;&#21270;&#65292;&#21487;&#20197;&#20195;&#26367;&#32531;&#24930;&#30340;&#20855;&#26377;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#36807;&#31243;&#26159;&#19968;&#31867;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#34920;&#29616;&#20016;&#23500;&#30340;&#27169;&#22411;&#65292;&#33258;&#28982;&#22320;&#20986;&#29616;&#22312;&#21160;&#24577;&#24314;&#27169;&#20219;&#21153;&#20013;&#12290;&#27010;&#29575;&#25512;&#29702;&#21644;&#29983;&#25104;&#27169;&#22411;&#19979;&#20855;&#26377;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#28508;&#22312;&#36807;&#31243;&#30340;&#23398;&#20064;&#37117;&#26159;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#21464;&#20998;&#25512;&#29702;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#39640;&#26031;&#36807;&#31243;&#25193;&#25955;&#36807;&#31243;&#30340;&#21442;&#25968;&#21270;&#65292;&#25351;&#20986;&#26041;&#27861;&#20013;&#30340;&#30149;&#24577;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#25351;&#25968;&#26063;&#25551;&#36848;&#30340;&#39640;&#26031;&#21464;&#20998;&#36807;&#31243;&#30340;&#26367;&#20195;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#29992;&#20984;&#20248;&#21270;&#30340;&#24555;&#36895;&#31639;&#27861;&#20195;&#26367;&#20855;&#26377;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32531;&#24930;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#31867;&#20284;&#20110;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#22909;&#30340;&#30446;&#26631;&#26469;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion processes are a class of stochastic differential equations (SDEs) providing a rich family of expressive models that arise naturally in dynamic modelling tasks. Probabilistic inference and learning under generative models with latent processes endowed with a non-linear diffusion process prior are intractable problems. We build upon work within variational inference approximating the posterior process as a linear diffusion process, point out pathologies in the approach, and propose an alternative parameterization of the Gaussian variational process using a continuous exponential family description. This allows us to trade a slow inference algorithm with fixed-point iterations for a fast algorithm for convex optimization akin to natural gradient descent, which also provides a better objective for the learning of model parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;TDA&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#20174;&#20013;&#21457;&#29616;&#20010;&#21035;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#24120;&#34987;&#22122;&#22768;&#25513;&#30422;&#65292;TDA&#21482;&#33021;&#29992;&#20110;&#35299;&#37322;&#23545;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#31283;&#23450;&#12289;&#29420;&#31435;&#20110;&#20854;&#20182;&#22122;&#22768;&#22240;&#32032;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.19765</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Perspective On Training Data Attribution. (arXiv:2305.19765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;TDA&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#20174;&#20013;&#21457;&#29616;&#20010;&#21035;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#24120;&#34987;&#22122;&#22768;&#25513;&#30422;&#65292;TDA&#21482;&#33021;&#29992;&#20110;&#35299;&#37322;&#23545;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#31283;&#23450;&#12289;&#29420;&#31435;&#20110;&#20854;&#20182;&#22122;&#22768;&#22240;&#32032;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65288;TDA&#65289;&#25216;&#26415;&#21487;&#25214;&#20986;&#24433;&#21709;&#27169;&#22411;&#23545;&#25152;&#20851;&#27880;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#39044;&#27979;&#30340;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20272;&#35745;&#20943;&#23569;&#25110;&#22686;&#21152;&#29305;&#23450;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;TDA&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#23558;&#23398;&#20064;&#30340;&#27169;&#22411;&#35270;&#20026;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;TDA&#20272;&#35745;&#20316;&#20026;&#38543;&#26426;&#21464;&#37327;&#12290;&#20174;&#36825;&#20010;&#26032;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#20010;&#21035;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#24120;&#20250;&#34987;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;SGD&#25209;&#27425;&#32452;&#21512;&#20135;&#29983;&#30340;&#22122;&#22768;&#25513;&#30422;&#12290;&#22522;&#20110;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;TDA&#21482;&#33021;&#21487;&#38752;&#22320;&#29992;&#20110;&#35299;&#37322;&#19968;&#20123;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#26159;&#31283;&#23450;&#30340;&#65292;&#29420;&#31435;&#20110;&#20854;&#20182;&#22122;&#22768;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#29420;&#31435;&#20110;&#22122;&#22768;&#30340;&#35757;&#32451;-&#27979;&#35797;&#25968;&#25454;&#26159;&#24456;&#32597;&#35265;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training data attribution (TDA) techniques find influential training data for the model's prediction on the test data of interest. They approximate the impact of down- or up-weighting a particular training sample. While conceptually useful, they are hardly applicable in practice, particularly because of their sensitivity to different model initialisation. In this paper, we introduce a Bayesian perspective on the TDA task, where the learned model is treated as a Bayesian posterior and the TDA estimates as random variables. From this novel viewpoint, we observe that the influence of an individual training sample is often overshadowed by the noise stemming from model initialisation and SGD batch composition. Based on this observation, we argue that TDA can only be reliably used for explaining model predictions that are consistently influenced by certain training data, independent of other noise factors. Our experiments demonstrate the rarity of such noise-independent training-test data pa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18654</link><description>&lt;p&gt;
&#20449;&#20208;&#19982;&#21629;&#36816;&#65306;Transformer&#22312;&#32452;&#21512;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18654
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21516;&#26102;&#22312;&#19968;&#20123;&#31616;&#21333;&#38382;&#39064;&#19978;&#20063;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#36825;&#24341;&#21457;&#20102;&#30097;&#38382;&#65306;&#36825;&#20123;&#38169;&#35823;&#26159;&#20598;&#28982;&#30340;&#65292;&#36824;&#26159;&#23427;&#20204;&#34920;&#26126;&#20102;&#26356;&#23454;&#36136;&#24615;&#30340;&#38480;&#21046;&#65311;&#20026;&#20102;&#25581;&#31034;Transformer&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#26497;&#38480; - &#22810;&#20301;&#25968;&#20056;&#27861;&#12289;&#36923;&#36753;&#32593;&#26684;&#35868;&#39064;&#21644;&#19968;&#20010;&#32463;&#20856;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290; &#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#27493;&#39588;&#65292;&#24182;&#23558;&#36825;&#20123;&#27493;&#39588;&#32508;&#21512;&#25104;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#32452;&#21512;&#22411;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#31639;&#22270;&#65292;&#20197;&#31995;&#32479;&#22320;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#25512;&#29702;&#27493;&#39588;&#20998;&#35299;&#20026;&#20013;&#38388;&#23376;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#36890;&#36807;&#23558;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#36716;&#21270;&#20026;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#26469;&#35299;&#20915;&#32452;&#21512;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17333</link><description>&lt;p&gt;
&#21482;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#30340;&#23384;&#20648;&#31354;&#38388;&#25968;&#37327;&#21464;&#24471;&#36807;&#39640;&#12290;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#29702;&#35770;&#19978;&#20165;&#20351;&#29992;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#23601;&#21487;&#20197;&#20272;&#35745;&#26799;&#24230;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#30340;&#36895;&#24230;&#38750;&#24120;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65288;MeZO&#65289;&#65292;&#23558;&#32463;&#20856;&#30340;ZO-SGD&#26041;&#27861;&#36866;&#24212;&#20110;&#21407;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#19968;&#24352;A100 80GB GPU&#65292;MeZO&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;300&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#20165;&#35757;&#32451;&#19968;&#20010;27&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#22411;&#65288;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#27169;&#22411;&#35268;&#27169;&#65288;&#39640;&#36798;66B&#65289;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#22810;&#39033;&#36873;&#25321;&#21644;&#29983;&#25104;&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;MeZO&#26126;&#26174;&#20248;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#32447;&#24615;PR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPOK&#65292;&#19968;&#31181;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16381</link><description>&lt;p&gt;
DPOK: &#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPOK&#65292;&#19968;&#31181;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#36825;&#20123;&#25216;&#26415;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#25429;&#25417;&#20219;&#21153;&#20013;&#20154;&#31867;&#20851;&#24515;&#30340;&#29305;&#24449;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#25913;&#36827;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#30456;&#23545;&#31616;&#21333;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22522;&#20110;&#22870;&#21169;&#24471;&#20998;&#30340;&#25298;&#32477;&#37319;&#26679;&#65289;&#65292;&#20294;&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#20219;&#21153;&#23450;&#20041;&#20026;RL&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#21453;&#39304;&#35757;&#32451;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DPOK&#38598;&#25104;&#20102;KL&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;RL&#24494;&#35843;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;KL&#27491;&#21017;&#21270;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DPOK&#36890;&#24120;&#20248;&#20110;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#20197;&#21069;&#30340;RL&#24494;&#35843;&#25216;&#26415;&#12290;DPOK&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;IS&#21644;FID&#24471;&#20998;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21453;&#20559;&#32622;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#26469;&#24674;&#22797;&#23384;&#22312;&#20559;&#35265;&#26679;&#26412;&#30340;&#30495;&#23454;&#29289;&#29702;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.15618</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#20256;&#36755;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65306;&#21453;&#20559;&#24046;&#65292;&#26465;&#20214;&#37319;&#26679;&#19979;&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models. (arXiv:2305.15618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15618
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21453;&#20559;&#32622;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#26469;&#24674;&#22797;&#23384;&#22312;&#20559;&#35265;&#26679;&#26412;&#30340;&#30495;&#23454;&#29289;&#29702;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#25104;&#23545;&#25968;&#25454;&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#20004;&#38454;&#27573;&#27010;&#29575;&#26694;&#26550;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#36890;&#36807;&#19968;&#20010;&#27010;&#29575;&#26144;&#23556;&#26469;&#23558;&#20302;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#20174;&#65288;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#30340;&#65289;&#31895;&#31890;&#24230;&#25968;&#20540;&#26041;&#26696;&#36716;&#25442;&#20026;&#19982;&#39640;&#20445;&#30495;&#24230;&#26041;&#26696;&#19968;&#33268;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20018;&#32852;&#20004;&#20010;&#36716;&#25442;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#19968;&#20010;&#30001;&#26368;&#20248;&#20256;&#36755;&#22270;&#23454;&#29616;&#30340;&#21453;&#20559;&#32622;&#27493;&#39588;&#65292;&#20197;&#21450;&#19968;&#20010;&#30001;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#19978;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#23558;&#26465;&#20214;&#37319;&#26679;&#21518;&#30340;&#27010;&#29575;&#20998;&#24067;&#32435;&#20837;&#35813;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#30830;&#23450;&#26465;&#20214;&#20998;&#24067;&#65292;&#24182;&#20174;&#23384;&#22312;&#20559;&#35265;&#30340;&#26679;&#26412;&#20013;&#30495;&#23454;&#22320;&#24674;&#22797;&#30456;&#20851;&#30340;&#29289;&#29702;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#29992;&#19968;&#32500;&#21644;&#20108;&#32500;&#27969;&#20307;&#27969;&#21160;&#38382;&#39064;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29992;&#65292;&#36825;&#20123;&#38382;&#39064;&#20195;&#34920;&#20102;&#22825;&#27668;&#21644;&#27668;&#20505;&#25968;&#20540;&#27169;&#25311;&#20013;&#30340;&#26680;&#24515;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#20013;&#29983;&#25104;&#30495;&#23454;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a two-stage probabilistic framework for statistical downscaling between unpaired data. Statistical downscaling seeks a probabilistic map to transform low-resolution data from a (possibly biased) coarse-grained numerical scheme to high-resolution data that is consistent with a high-fidelity scheme. Our framework tackles the problem by tandeming two transformations: a debiasing step that is performed by an optimal transport map, and an upsampling step that is achieved by a probabilistic diffusion model with \textit{a posteriori} conditional sampling. This approach characterizes a conditional distribution without the need for paired data, and faithfully recovers relevant physical statistics from biased samples. We demonstrate the utility of the proposed approach on one- and two-dimensional fluid flow problems, which are representative of the core difficulties present in numerical simulations of weather and climate. Our method produces realistic high-resolution outputs from lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#37325;&#26500;&#23431;&#23449;&#23398;&#21644;Pantheon&#32534;&#35793;&#20013;&#30340;Hubble&#22270;&#12290;&#36890;&#36807;&#25193;&#23637;ReFANN&#31639;&#27861;&#20197;&#22788;&#29702;&#38750;&#39640;&#26031;&#25968;&#25454;&#28857;&#21644;&#20855;&#26377;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#65292;&#36824;&#36827;&#34892;&#20102;&#38646;&#27979;&#35797;&#26469;&#39564;&#35777;&#23431;&#23449;&#23398;&#30340;&#21327;&#35843;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15499</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#37325;&#26500;&#23431;&#23449;&#23398;&#19982;Pantheon&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural network reconstruction of cosmology using the Pantheon compilation. (arXiv:2305.15499v2 [gr-qc] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#37325;&#26500;&#23431;&#23449;&#23398;&#21644;Pantheon&#32534;&#35793;&#20013;&#30340;Hubble&#22270;&#12290;&#36890;&#36807;&#25193;&#23637;ReFANN&#31639;&#27861;&#20197;&#22788;&#29702;&#38750;&#39640;&#26031;&#25968;&#25454;&#28857;&#21644;&#20855;&#26377;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#65292;&#36824;&#36827;&#34892;&#20102;&#38646;&#27979;&#35797;&#26469;&#39564;&#35777;&#23431;&#23449;&#23398;&#30340;&#21327;&#35843;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#20013;&#37325;&#26500;&#21704;&#21187;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;ReFANN&#26469;&#22788;&#29702;&#20855;&#26377;&#29420;&#31435;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25193;&#23637;&#23427;&#20197;&#21253;&#25324;&#38750;&#39640;&#26031;&#25968;&#25454;&#28857;&#20197;&#21450;&#20855;&#26377;&#21327;&#26041;&#24046;&#30697;&#38453;&#31561;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#24471;&#20986;&#30340;&#29616;&#26377;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#38646;&#27979;&#35797;&#20197;&#39564;&#35777;&#21644;&#35856;&#23431;&#23449;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we reconstruct the Hubble diagram using various data sets, including correlated ones, in Artificial Neural Networks (ANN). Using ReFANN, that was built for data sets with independent uncertainties, we expand it to include non-Guassian data points, as well as data sets with covariance matrices among others. Furthermore, we compare our results with the existing ones derived from Gaussian processes and we also perform null tests in order to test the validity of the concordance model of cosmology.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;</title><link>http://arxiv.org/abs/2305.15284</link><description>&lt;p&gt;
&#21487;&#22797;&#29616;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Replicable Reinforcement Learning. (arXiv:2305.15284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#12289;&#34892;&#20026;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#23548;&#33268;&#20102;&#31639;&#27861;&#26694;&#26550;&#30340;&#24418;&#25104;&#65292;&#21363;&#35201;&#27714;&#31639;&#27861;&#22312;&#20174;&#30456;&#21516;&#30340;&#24213;&#23618;&#20998;&#24067;&#25552;&#21462;&#30340;&#20004;&#20010;&#19981;&#21516;&#26679;&#26412;&#19978;&#36816;&#34892;&#26102;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65288;&#27010;&#29575;&#39640;&#65289;&#12290;&#34429;&#28982;&#20173;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#35768;&#22810;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#12289;&#37325;&#35201;&#39033;&#38382;&#39064;&#21644;&#20998;&#24067;&#27979;&#35797;&#65292;&#37117;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21487;&#35777;&#26126;&#21487;&#22797;&#29616;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#21487;&#22797;&#29616;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24182;&#34892;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#22797;&#21046;&#31639;&#27861;&#20197;&#21450;&#19968;&#20010;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#21487;&#35777;&#22797;&#21046;&#30340;R-max&#12290;&#36825;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#25209;&#37327;&#23398;&#20064;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22797;&#21046;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The replicability crisis in the social, behavioral, and data sciences has led to the formulation of algorithm frameworks for replicability -- i.e., a requirement that an algorithm produce identical outputs (with high probability) when run on two different samples from the same underlying distribution. While still in its infancy, provably replicable algorithms have been developed for many fundamental tasks in machine learning and statistics, including statistical query learning, the heavy hitters problem, and distribution testing. In this work we initiate the study of replicable reinforcement learning, providing a provably replicable algorithm for parallel value iteration, and a provably replicable version of R-max in the episodic setting. These are the first formal replicability results for control problems, which present different challenges for replication than batch learning settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#21644;&#29702;&#35770;&#20445;&#35777;&#30340;&#20915;&#31574;&#24863;&#30693;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#32852;&#21512;&#30446;&#26631;&#26469;&#35299;&#20915;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24182;&#19988;&#26080;&#35770;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#22914;&#20309;&#65292;&#35813;&#31639;&#27861;&#37117;&#20445;&#35777;&#21333;&#35843;&#31574;&#30053;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.15249</link><description>&lt;p&gt;
&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#21644;&#29702;&#35770;&#20445;&#35777;&#30340;&#20915;&#31574;&#24863;&#30693;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees. (arXiv:2305.15249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#21644;&#29702;&#35770;&#20445;&#35777;&#30340;&#20915;&#31574;&#24863;&#30693;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#32852;&#21512;&#30446;&#26631;&#26469;&#35299;&#20915;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24182;&#19988;&#26080;&#35770;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#22914;&#20309;&#65292;&#35813;&#31639;&#27861;&#37117;&#20445;&#35777;&#21333;&#35843;&#31574;&#30053;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21592;&#35780;&#35770;&#23478; (AC) &#26041;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064; (RL) &#20013;&#65292;&#24182;&#20174;&#20351;&#29992;&#20219;&#20309;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20316;&#20026;&#28436;&#21592;&#21644;&#22522;&#20110;&#20540;&#26041;&#27861;&#20316;&#20026;&#35780;&#35770;&#23478;&#30340;&#28789;&#27963;&#24615;&#20013;&#21463;&#30410;&#12290;&#35780;&#35770;&#23478;&#36890;&#24120;&#36890;&#36807;&#26368;&#23567;&#21270; TD &#35823;&#24046;&#26469;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#23454;&#29616;&#39640;&#22870;&#21169;&#30340;&#30495;&#23454;&#30446;&#26631;&#21487;&#33021;&#33073;&#38057;&#30340;&#23458;&#35266;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20915;&#31574;&#24863;&#30693;&#30340;&#32852;&#21512;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#31181;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#30446;&#26631;&#26469;&#35774;&#35745;&#19968;&#20010;&#36890;&#29992;&#30340; AC &#31639;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#20219;&#20309;&#20989;&#25968;&#36924;&#36817;&#12290;&#25105;&#20204;&#26126;&#30830;&#34920;&#24449;&#20102;&#22312;&#36873;&#25321;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#24471;&#31639;&#27861;&#20445;&#35777;&#21333;&#35843;&#31574;&#30053;&#25913;&#36827;&#30340;&#26465;&#20214;&#12290;&#23454;&#20363;&#21270;&#36890;&#29992;&#31639;&#27861;&#23558;&#23548;&#33268;&#28041;&#21450;&#26368;&#22823;&#21270;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968; (&#31867;&#20284;&#20110; TRPO&#12289;PPO) &#30340;&#28436;&#21592;&#21644;&#28041;&#21450;&#26368;&#23567;&#21270;&#19968;&#20010;&#23494;&#20999;&#30456;&#20851;&#30446;&#26631;&#30340;&#35780;&#35770;&#23478;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#35777;&#26126;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-critic (AC) methods are widely used in reinforcement learning (RL) and benefit from the flexibility of using any policy gradient method as the actor and value-based method as the critic. The critic is usually trained by minimizing the TD error, an objective that is potentially decorrelated with the true goal of achieving a high reward with the actor. We address this mismatch by designing a joint objective for training the actor and critic in a decision-aware fashion. We use the proposed objective to design a generic, AC algorithm that can easily handle any function approximation. We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization. Instantiating the generic algorithm results in an actor that involves maximizing a sequence of surrogate functions (similar to TRPO, PPO) and a critic that involves minimizing a closely connected objective. Using simple 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;BIG-bench&#23454;&#39564;&#35760;&#24405;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23547;&#25214;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#20197;&#26368;&#22823;&#31243;&#24230;&#24674;&#22797;&#25972;&#20010;&#38598;&#21512;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14947</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#21487;&#39044;&#27979;&#24615;&#22914;&#20309;&#65311;&#23545;BIG-bench&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. (arXiv:2305.14947v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;BIG-bench&#23454;&#39564;&#35760;&#24405;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23547;&#25214;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#20197;&#26368;&#22823;&#31243;&#24230;&#24674;&#22797;&#25972;&#20010;&#38598;&#21512;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#21487;&#39044;&#27979;&#24615;&#65306;&#22312;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#12289;&#21442;&#25968;&#25968;&#37327;&#12289;&#20219;&#21153;&#25968;&#37327;&#21644;&#19978;&#19979;&#25991;&#31034;&#20363;&#25968;&#37327;&#30340;&#36807;&#21435;&#23454;&#39564;&#35760;&#24405;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#33021;&#21542;&#20934;&#30830;&#39044;&#27979;LLM&#22312;&#26032;&#23454;&#39564;&#37197;&#32622;&#19978;&#30340;&#24615;&#33021;&#65311;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#23545;LLM&#29992;&#25143;&#65288;&#20363;&#22914;&#65292;&#20915;&#23450;&#23581;&#35797;&#21738;&#20123;&#27169;&#22411;&#65289;&#12289;&#24320;&#21457;&#32773;&#65288;&#20363;&#22914;&#65292;&#20248;&#20808;&#35780;&#20272;&#20195;&#34920;&#24615;&#20219;&#21153;&#65289;&#21644;&#30740;&#31350;&#31038;&#21306;&#65288;&#20363;&#22914;&#65292;&#35782;&#21035;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#30340;&#38590;&#20197;&#39044;&#27979;&#30340;&#33021;&#21147;&#65289;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#25105;&#20204;&#22312;BIG-bench&#30340;&#23454;&#39564;&#35760;&#24405;&#19978;&#30740;&#31350;&#20102;&#24615;&#33021;&#39044;&#27979;&#38382;&#39064;&#12290;&#22312;&#38543;&#26426;&#30340;&#35757;&#32451;-&#27979;&#35797;&#20998;&#31163;&#20013;&#65292;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#39044;&#27979;&#22120;&#30340;R^2&#24471;&#20998;&#36229;&#36807;95%&#65292;&#34920;&#26126;&#23454;&#39564;&#35760;&#24405;&#20013;&#23384;&#22312;&#21487;&#23398;&#20064;&#30340;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23547;&#25214;&#8220;small-bench&#8221;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;BIG-bench&#20219;&#21153;&#20013;&#23547;&#25214;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#20174;&#20013;&#26368;&#22823;&#31243;&#24230;&#22320;&#24674;&#22797;&#25972;&#20010;&#38598;&#21512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g., identifying hard-to-predict capabilities that warrant further investigation).  We study the performance prediction problem on experiment records from BIG-bench. On a random train-test split, an MLP-based predictor achieves an $R^2$ score greater than 95%, indicating the presence of learnable patterns within the experiment records. We then formulate the problem of searching for "small-bench," an informative subset of BIG-bench tasks from which the performance on the full set can be maximally recovered. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2305.14928</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#38752;&#30340;&#20551;&#26032;&#38395;&#32531;&#35299;&#65306;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#31038;&#20250;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23578;&#26410;&#25214;&#21040;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#27880;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#26080;&#27861;&#23436;&#32654;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26356;&#23454;&#29992;&#30340;&#24037;&#20855;&#26469;&#35780;&#20272;&#20449;&#24687;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;GPT-4&#22312;&#22810;&#20010;&#35774;&#23450;&#21644;&#35821;&#35328;&#20013;&#21487;&#20197;&#32988;&#36807;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#32034;&#27867;&#21270;&#65292;&#25581;&#31034;&#20102;GPT-4&#21644;RoBERTa-large&#22312;&#22833;&#25928;&#27169;&#24335;&#19978;&#30340;&#24046;&#24322;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#19981;&#21487;&#33021;&#30340;&#20363;&#23376;&#24182;&#26174;&#33879;&#25913;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#65292;&#28201;&#24230;&#65292;&#25552;&#31034;&#65292;&#29256;&#26412;&#25511;&#21046;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#30340;&#32467;&#26524;&#65292;&#27599;&#20010;&#32467;&#26524;&#37117;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20855;&#26377;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#21644;&#21487;&#34892;&#24615;&#26631;&#31614;&#30340;LIAR-New&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14627</link><description>&lt;p&gt;
&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#20449;&#24687;&#23547;&#25214;&#24037;&#20855;&#65292;&#20294;&#29983;&#25104;&#30340;&#36755;&#20986;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#26412;&#25991;&#26088;&#22312;&#23454;&#29616;LLMs&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ALCE&#65292;&#36825;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#12290;ALCE&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#21644;&#26816;&#32034;&#35821;&#26009;&#24211;&#65292;&#24182;&#35201;&#27714;&#24314;&#31435;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#26816;&#32034;&#25903;&#25345;&#35777;&#25454;&#24182;&#29983;&#25104;&#24102;&#26377;&#24341;&#25991;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#27839;&#30528;&#27969;&#30021;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#24341;&#25991;&#36136;&#37327;&#19977;&#20010;&#32500;&#24230;&#26500;&#24314;&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#21644;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#31995;&#32479;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;--&#20363;&#22914;&#65292;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#21457;&#23637;&#33021;&#22815;&#29983;&#25104;&#21487;&#39564;&#35777;&#21644;&#21487;&#20449;&#36182;&#36755;&#20986;&#30340;LLMs&#25552;&#20379;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -for example,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;CF-GNN&#65289;&#65292;&#36890;&#36807;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#25193;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#23545;GNN&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26377;&#25928;&#20272;&#35745;&#12290;CF-GNN&#29983;&#25104;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#21487;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#35206;&#30422;&#27010;&#29575;&#20445;&#35777;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;/&#21306;&#38388;&#38271;&#24230;&#30340;&#25299;&#25169;&#24847;&#35782;&#36755;&#20986;&#26657;&#27491;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14535</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#31526;&#21512;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#19978;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification over Graph with Conformalized Graph Neural Networks. (arXiv:2305.14535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;CF-GNN&#65289;&#65292;&#36890;&#36807;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#25193;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#23545;GNN&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26377;&#25928;&#20272;&#35745;&#12290;CF-GNN&#29983;&#25104;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#21487;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#35206;&#30422;&#27010;&#29575;&#20445;&#35777;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;/&#21306;&#38388;&#38271;&#24230;&#30340;&#25299;&#25169;&#24847;&#35782;&#36755;&#20986;&#26657;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;GNN&#32570;&#20047;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#25104;&#26412;&#26174;&#33879;&#30340;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#37096;&#32626;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#24615;GNN&#65288;CF-GNN&#65289;&#65292;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#25193;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#20197;&#33719;&#24471;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#32473;&#23450;&#22270;&#20013;&#30340;&#23454;&#20307;&#65292;CF-GNN&#29983;&#25104;&#19968;&#20010;&#39044;&#27979;&#38598;/&#21306;&#38388;&#65292;&#20197;&#20808;&#39564;&#35206;&#30422;&#27010;&#29575;&#65288;&#20363;&#22914;90%&#65289;&#30340;&#26041;&#24335;&#20445;&#35777;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25490;&#21015;&#19981;&#21464;&#26465;&#20214;&#65292;&#20351;&#24471;CP&#22312;&#22270;&#25968;&#25454;&#19978;&#25104;&#31435;&#65292;&#24182;&#25552;&#20379;&#20102;&#27979;&#35797;&#26102;&#38388;&#35206;&#30422;&#29575;&#30340;&#31934;&#30830;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#38500;&#20102;&#26377;&#25928;&#30340;&#35206;&#30422;&#65292;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;/&#21306;&#38388;&#38271;&#24230;&#23545;&#20110;&#23454;&#38469;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#31526;&#21512;&#24615;&#24471;&#20998;&#21644;&#32593;&#32476;&#32467;&#26500;&#20043;&#38388;&#23384;&#22312;&#20851;&#38190;&#32852;&#31995;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20855;&#26377;&#25299;&#25169;&#24847;&#35782;&#30340;&#36755;&#20986;&#26657;&#27491;&#27169;&#22411;&#26469;&#23398;&#20064;&#26356;&#26032;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32416;&#27491;&#34920;&#31034;&#20559;&#35265;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#21453;&#20107;&#23454;&#22686;&#24378;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21453;&#20107;&#23454;&#22686;&#24378;&#30456;&#27604;&#20110;&#26410;&#20462;&#27491;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#20559;&#35265;&#26657;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#27169;&#22411;&#20998;&#26512;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#19982;&#30495;&#23454;&#21453;&#20107;&#23454;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.14083</link><description>&lt;p&gt;
&#23545;&#20110;&#34920;&#31034;&#20559;&#35265;&#19979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Augmentation for Multimodal Learning Under Presentation Bias. (arXiv:2305.14083v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32416;&#27491;&#34920;&#31034;&#20559;&#35265;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#21453;&#20107;&#23454;&#22686;&#24378;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21453;&#20107;&#23454;&#22686;&#24378;&#30456;&#27604;&#20110;&#26410;&#20462;&#27491;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#20559;&#35265;&#26657;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#27169;&#22411;&#20998;&#26512;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#19982;&#30495;&#23454;&#21453;&#20107;&#23454;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#26631;&#31614;&#36890;&#24120;&#26159;&#20174;&#31995;&#32479;&#24076;&#26395;&#40723;&#21169;&#30340;&#29992;&#25143;&#34892;&#20026;&#20013;&#24471;&#20986;&#30340;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#38543;&#30528;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#29305;&#24449;&#30340;&#25552;&#20379;&#65292;&#38656;&#35201;&#35757;&#32451;&#26032;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#26410;&#26469;&#29992;&#25143;&#34892;&#20026;&#30340;&#20559;&#35265;&#65292;&#36827;&#32780;&#23548;&#33268;&#26631;&#31614;&#20013;&#30340;&#34920;&#31034;&#20559;&#35265;&#65292;&#36825;&#25439;&#23475;&#20102;&#35757;&#32451;&#26032;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#26041;&#27861;&#65292;&#21363;&#21453;&#20107;&#23454;&#22686;&#24378;&#65292;&#36890;&#36807;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#26469;&#32416;&#27491;&#34920;&#31034;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#30456;&#27604;&#26410;&#20462;&#27491;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#20559;&#35265;&#26657;&#27491;&#26041;&#27861;&#65292;&#21453;&#20107;&#23454;&#22686;&#24378;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#27169;&#22411;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#19982;&#30495;&#23454;&#21453;&#20107;&#23454;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13786</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#12298;&#24863;&#30693;&#27979;&#35797;&#65306;&#22810;&#27169;&#24577;&#35270;&#39057;&#27169;&#22411;&#30340;&#35786;&#26029;&#22522;&#20934;&#12299;
&lt;/p&gt;
&lt;p&gt;
Perception Test: A Diagnostic Benchmark for Multimodal Video Models. (arXiv:2305.13786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#8212;&#8212;&#24863;&#30693;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914; Flamingo&#12289;BEiT-3 &#25110; GPT-4&#65289;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#25216;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20391;&#37325;&#20110;&#35745;&#31639;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#12289;&#26816;&#27979;&#25110;&#36319;&#36394;&#65289;&#19981;&#21516;&#65292;&#24863;&#30693;&#27979;&#35797;&#20391;&#37325;&#20110;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#36328;&#36234;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25512;&#29702;&#31867;&#22411;&#65288;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#65289;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#32780;&#39640;&#25928;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#25110;&#26377;&#38480;&#24494;&#35843;&#19979;&#25361;&#36873;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#20123;&#30446;&#30340;&#65292;&#24863;&#30693;&#27979;&#35797;&#20171;&#32461;&#20102;11.6k&#31181;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#65292;&#24179;&#22343;&#38271;&#24230;&#20026;23&#31186;&#65292;&#26088;&#22312;&#23637;&#31034;&#24863;&#30693;&#19978;&#26377;&#36259;&#30340;&#24773;&#22659;&#65292;&#30001;&#20840;&#29699;&#32422;100&#21517;&#21442;&#19982;&#32773;&#25293;&#25668;&#12290;&#36825;&#20123;&#35270;&#39057;&#23494;&#38598;&#22320;&#24102;&#26377;&#20845;&#31181;&#26631;&#31614;&#65288;&#22810;&#39033;&#36873;&#25321;&#21644;&#22522;&#20110;&#35270;&#39057;&#38382;&#39064;&#22238;&#31572;&#65292;&#23545;&#35937;a&#65289;
&lt;/p&gt;
&lt;p&gt;
We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#20998;&#25968;&#22270;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#31070;&#32463;&#22270;ODE&#26694;&#26550;&#26469;&#25551;&#36848;&#38750;&#23616;&#37096;&#21160;&#21147;&#23398;&#65292;&#20801;&#35768;&#22312;&#36828;&#22788;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#27010;&#29575;&#30340;&#36828;&#36317;&#31163;&#36339;&#36291;&#65292;&#36827;&#32780;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.13084</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#36807;&#24230;&#24179;&#28369;&#30340;&#20998;&#25968;&#22270;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fractional Graph Laplacian Approach to Oversmoothing. (arXiv:2305.13084v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#20998;&#25968;&#22270;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#31070;&#32463;&#22270;ODE&#26694;&#26550;&#26469;&#25551;&#36848;&#38750;&#23616;&#37096;&#21160;&#21147;&#23398;&#65292;&#20801;&#35768;&#22312;&#36828;&#22788;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#27010;&#29575;&#30340;&#36828;&#36317;&#31163;&#36339;&#36291;&#65292;&#36827;&#32780;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#65292;GNN&#32463;&#24120;&#38590;&#20197;&#25429;&#25417;&#22270;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36807;&#24230;&#24179;&#28369;&#30340;&#27010;&#24565;&#20174;&#26080;&#21521;&#22270;&#25512;&#24191;&#21040;&#26377;&#21521;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#26377;&#21521;&#23545;&#31216;&#24402;&#19968;&#21270;&#25289;&#26222;&#25289;&#26031;&#26469;&#25193;&#23637;Dirichlet&#33021;&#37327;&#30340;&#27010;&#24565;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31070;&#32463;&#22270;ODE&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25968;&#22270;&#25289;&#26222;&#25289;&#26031;&#31070;&#32463;ODE&#65292;&#25551;&#36848;&#20102;&#38750;&#23616;&#37096;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#36828;&#22788;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#27010;&#29575;&#30340;&#36828;&#36317;&#31163;&#36339;&#36291;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22270;&#30340;Dirichlet&#33021;&#37327;&#25910;&#25947;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22270;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown state-of-the-art performances in various applications. However, GNNs often struggle to capture long-range dependencies in graphs due to oversmoothing. In this paper, we generalize the concept of oversmoothing from undirected to directed graphs. To this aim, we extend the notion of Dirichlet energy by considering a directed symmetrically normalized Laplacian. As vanilla graph convolutional networks are prone to oversmooth, we adopt a neural graph ODE framework. Specifically, we propose fractional graph Laplacian neural ODEs, which describe non-local dynamics. We prove that our approach allows propagating information between distant nodes while maintaining a low probability of long-distance jumps. Moreover, we show that our method is more flexible with respect to the convergence of the graph's Dirichlet energy, thereby mitigating oversmoothing. We conduct extensive experiments on synthetic and real-world graphs, both directed and undirected, demons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12162</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;DSIC&#20223;&#23556;&#26497;&#22823;&#20215;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23547;&#25214;&#32463;&#39564;&#19978;&#39640;&#25910;&#20837;&#30340;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#22810;&#29289;&#21697;&#25293;&#21334;&#24773;&#26223;&#30340;&#24037;&#20316;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;RegretNet&#31867;&#21644;&#20223;&#23556;&#26497;&#22823;&#20215;&#65288;AMAs&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#33021;&#20005;&#26684;&#20445;&#35777;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#65292;&#32780;&#21518;&#32773;&#22240;&#20026;&#20998;&#37197;&#20505;&#36873;&#20154;&#25968;&#36807;&#22810;&#32780;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMenuNet&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20174;&#20986;&#20215;&#20154;&#21644;&#29289;&#21697;&#34920;&#31034;&#20013;&#26500;&#36896;AMA&#21442;&#25968;&#65288;&#29978;&#33267;&#21253;&#25324;&#20998;&#37197;&#33756;&#21333;&#65289;&#12290;&#30001;&#20110;AMA&#30340;&#23646;&#24615;&#65292;AMenuNet&#22987;&#32456;&#26159;DSIC&#21644;&#20010;&#20154;&#29702;&#24615;&#65288;IR&#65289;&#30340;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#26469;&#22686;&#24378;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;AMenuNet&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#19981;&#21463;&#25293;&#21334;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;AMenuNet&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.06986</link><description>&lt;p&gt;
&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#21487;&#35777;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. (arXiv:2305.06986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#12290;&#28145;&#24230;&#32593;&#32476;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#30340;&#33021;&#21147;&#23545;&#20854;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20173;&#28982;&#19981;&#22815;&#28165;&#26224;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#20027;&#35201;&#23616;&#38480;&#20110;&#20004;&#23618;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#35777;&#26126;&#30340;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36890;&#36807;&#36880;&#23618;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#19977;&#23618;&#32593;&#32476;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#23427;&#19978;&#30028;&#20102;&#30446;&#26631;&#20855;&#26377;&#29305;&#23450;&#23618;&#27425;&#32467;&#26500;&#26102;&#23454;&#29616;&#20302;&#27979;&#35797;&#38169;&#35823;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#20363;&#21270;&#21040;&#29305;&#23450;&#30340;&#32479;&#35745;&#23398;&#23398;&#20064;&#35774;&#32622;&#20013;&#8212;&#8212;&#21333;&#25351;&#25968;&#27169;&#22411;&#21644;&#20108;&#27425;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06563</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;(STDI)&#26159;&#25968;&#25454;&#39537;&#21160;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#19981;&#21487;&#36991;&#20813;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#20132;&#36890;&#25968;&#25454;&#20013;&#20272;&#35745;&#20002;&#22833;&#25968;&#25454;&#12290;&#30001;&#20110;&#20132;&#36890;&#25968;&#25454;&#20855;&#26377;&#22810;&#32500;&#21644;&#26102;&#31354;&#24615;&#36136;&#65292;&#25105;&#20204;&#23558;&#20002;&#22833;&#25968;&#25454;&#22635;&#20805;&#35270;&#20026;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#20851;&#20110;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340; STDI &#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#24320;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#26680;&#24352;&#37327;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#22635;&#20805;&#24615;&#33021;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#26412;&#25991;&#37325;&#26032;&#26500;&#36896;&#20102;3/4&#38454;&#27721;&#20811;&#23572;&#24352;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;(maniRTD)&#27169;&#22411;&#29992;&#20110;STDI&#12290;&#26126;&#30830;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#32500;&#24310;&#36831;&#23884;&#20837;&#21464;&#25442;&#23558;&#20256;&#24863;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#34920;&#31034;&#20026;3/4&#38454;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;ManiRTD&#20351;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#20351;&#29992;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.05276</link><description>&lt;p&gt;
&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#20852;&#36259;&#12290;&#37319;&#26679;&#39057;&#29575;&#36828;&#20302;&#20110;&#22240;&#26524;&#24433;&#21709;&#39057;&#29575;&#26159;&#27492;&#31867;&#25512;&#26029;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#35201;&#20040;&#23616;&#38480;&#20110;&#32447;&#24615;&#24773;&#20917;&#65292;&#35201;&#20040;&#26080;&#27861;&#24314;&#31435;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#21442;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#24819;&#26159;&#65292;&#23376;&#37319;&#26679;&#30340;&#25361;&#25112;&#20027;&#35201;&#26469;&#33258;&#20110;&#8220;&#26410;&#35266;&#23519;&#21040;&#8221;&#30340;&#26102;&#38388;&#27493;&#65292;&#22240;&#27492;&#24212;&#20351;&#29992;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#35774;&#35745;&#30340;&#24037;&#20855;&#22788;&#29702;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#29305;&#21035;&#36866;&#21512;&#65292;&#22240;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#30340;&#20195;&#29702;&#21464;&#37327;&#33258;&#28982;&#26159;&#22312;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#27493;&#19978;&#26412;&#36523;&#12290;&#26681;&#25454;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#32467;&#26500;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01210</link><description>&lt;p&gt;
ChatGPT&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20005;&#26684;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32508;&#21512;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#34987;&#38271;&#26399;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#30452;&#25509;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#20013;&#29992;&#25143;&#30340;&#24847;&#22270;&#29983;&#25104;&#20195;&#30721;&#12290;&#20195;&#30721;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31574;&#21010;&#22909;&#30340;&#32508;&#21512;&#38382;&#39064;&#21644;&#21508;&#31181;&#36755;&#20837;/&#36755;&#20986;&#27979;&#35797;&#29992;&#20363;&#65292;&#34987;&#29992;&#26469;&#34913;&#37327;&#21508;&#31181;LLMs&#22312;&#20195;&#30721;&#32508;&#21512;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#27979;&#35797;&#29992;&#20363;&#22312;&#23436;&#20840;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#26041;&#38754;&#65292;&#25968;&#37327;&#21644;&#36136;&#37327;&#37117;&#21487;&#33021;&#26377;&#25152;&#38480;&#21046;&#12290;&#36825;&#31181;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#38480;&#21046;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;LLMs&#26102;&#20195;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvalPlus&#8212;&#8212;&#19968;&#20010;&#35780;&#20272;LLM-synthesized&#20195;&#30721;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#20005;&#26684;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#12290;EvalPlus&#25509;&#21463;&#22522;&#30784;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#36755;&#20837;&#29983;&#25104;&#27493;&#39588;&#65292;&#20351;&#29992;LLM-based&#21644;&#22522;&#20110;&#21464;&#24322;&#30340;&#26041;&#27861;&#29983;&#25104;&#21644;&#22810;&#26679;&#21270;&#22823;&#37327;&#26032;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#24605;&#32771;&#12289;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00833</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Learning to Reason and Memorize with Self-Notes. (arXiv:2305.00833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#24605;&#32771;&#12289;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#19981;&#33021;&#20445;&#30041;&#20197;&#20379;&#23558;&#26469;&#20351;&#29992;&#30340;&#20808;&#21069;&#25512;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#20801;&#35768;&#27169;&#22411;&#36827;&#34892;&#33258;&#27880;&#35760;&#12290;&#19982;&#26368;&#36817;&#30340;&#24605;&#32500;&#38142;&#25110;&#33609;&#31295;&#26412;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#38543;&#26102;&#20559;&#31163;&#36755;&#20837;&#19978;&#19979;&#25991;&#26469;&#26126;&#30830;&#24605;&#32771;&#21644;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#38405;&#35835;&#19978;&#19979;&#25991;&#26102;&#21363;&#26102;&#25512;&#29702;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#35760;&#24518;&#24182;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#32455;&#36755;&#20837;&#25991;&#26412;&#30340;&#33258;&#27880;&#35760;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#24605;&#32500;&#38142;&#21644;&#33609;&#31295;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12579</link><description>&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#26159;&#27867;&#21270;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20248;&#21270;&#26102;&#23545;&#24212;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#32447;&#24615;&#36817;&#20284;&#20989;&#25968;&#26469;&#27169;&#25311;&#36712;&#36857;&#20449;&#24687;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#20016;&#23500;&#36712;&#36857;&#20449;&#24687;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27867;&#21270;&#19978;&#30028;&#20381;&#36182;&#20110;&#23398;&#20064;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#35757;&#32451;&#38598;&#30340;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#19981;&#21516;&#35757;&#32451;&#27493;&#39588;&#12289;&#23398;&#20064;&#29575;&#21644;&#26631;&#31614;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#27867;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
&lt;/p&gt;</description></item><item><title>Calder&#243;n&#26041;&#27861;&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;EIT&#25104;&#20687;&#31639;&#27861;&#65292;&#20294;&#22270;&#20687;&#27169;&#31946;&#19988;&#20302;&#20272;&#30005;&#23548;&#29575;&#20540;&#12290;&#35813;&#35770;&#25991;&#22522;&#20110;U-net&#27169;&#22411;&#23545;Calder&#243;n&#26041;&#27861;&#30340;&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#30005;&#23548;&#29575;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09074</link><description>&lt;p&gt;
&#28145;&#24230;Calder&#243;n&#26041;&#27861;&#30340;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Electrical Impedance Tomography with Deep Calder\'on Method. (arXiv:2304.09074v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09074
&lt;/p&gt;
&lt;p&gt;
Calder&#243;n&#26041;&#27861;&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;EIT&#25104;&#20687;&#31639;&#27861;&#65292;&#20294;&#22270;&#20687;&#27169;&#31946;&#19988;&#20302;&#20272;&#30005;&#23548;&#29575;&#20540;&#12290;&#35813;&#35770;&#25991;&#22522;&#20110;U-net&#27169;&#22411;&#23545;Calder&#243;n&#26041;&#27861;&#30340;&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#30005;&#23548;&#29575;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;(EIT)&#26159;&#19968;&#31181;&#21033;&#29992;&#22312;&#23545;&#35937;&#34920;&#38754;&#19978;&#27979;&#37327;&#30340;&#30005;&#27969;&#23494;&#24230;/&#30005;&#21387;&#25968;&#25454;&#30340;&#38750;&#20405;&#20837;&#24335;&#21307;&#23398;&#25104;&#20687;&#26041;&#24335;&#12290;Calderon&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#30456;&#23545;&#36739;&#26032;&#30340;EIT&#25104;&#20687;&#31639;&#27861;&#65292;&#23427;&#26159;&#38750;&#36845;&#20195;&#30340;&#12289;&#24555;&#36895;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#37325;&#24314;&#22797;&#20540;&#30005;&#38459;&#25239;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27491;&#21017;&#21270;&#36890;&#36807;&#20302;&#36890;&#28388;&#27874;&#21644;&#32447;&#24615;&#21270;&#65292;&#37325;&#24314;&#30340;&#22270;&#20687;&#36973;&#21463;&#20005;&#37325;&#30340;&#27169;&#31946;&#21644;&#20302;&#20272;&#30830;&#20999;&#30340;&#30005;&#23548;&#29575;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Calder&#243;n&#26041;&#27861;&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;&#21363;U-net&#65289;&#36890;&#36807;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;U-net&#26469;&#21518;&#22788;&#29702;&#30001;Calder&#243;n&#26041;&#27861;&#29983;&#25104;&#30340;EIT&#22270;&#20687;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#26356;&#20934;&#30830;&#30340;&#30005;&#23548;&#29575;&#20272;&#35745;&#12290;&#25105;&#20204;&#27169;&#25311;&#33016;&#37096;&#37197;&#32622;&#65292;&#36890;&#36807;Calder&#243;n&#26041;&#27861;&#29983;&#25104;&#30005;&#27969;&#23494;&#24230;/&#30005;&#21387;&#36793;&#30028;&#27979;&#37327;&#21644;&#30456;&#24212;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical impedance tomography (EIT) is a noninvasive medical imaging modality utilizing the current-density/voltage data measured on the surface of the subject. Calder\'on's method is a relatively recent EIT imaging algorithm that is non-iterative, fast, and capable of reconstructing complex-valued electric impedances. However, due to the regularization via low-pass filtering and linearization, the reconstructed images suffer from severe blurring and underestimation of the exact conductivity values. In this work, we develop an enhanced version of Calder\'on's method, using convolution neural networks (i.e., U-net) via a postprocessing step. Specifically, we learn a U-net to postprocess the EIT images generated by Calder\'on's method so as to have better resolutions and more accurate estimates of conductivity values. We simulate chest configurations with which we generate the current-density/voltage boundary measurements and the corresponding reconstructed images by Calder\'on's metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#31934;&#30830;&#22320;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;&#35745;&#31639;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#65292;&#20854;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06174</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#29289;&#20307;&#24863;&#30693;&#31561;&#21464;&#22522;&#20803;&#21453;&#24212;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#30830;&#36807;&#28193;&#24577;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model. (arXiv:2304.06174v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#31934;&#30830;&#22320;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;&#35745;&#31639;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#65292;&#20854;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#28193;&#24577;&#25628;&#32034;&#22312;&#21270;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#29992;&#20110;&#38416;&#26126;&#21453;&#24212;&#26426;&#29702;&#21644;&#25506;&#32034;&#21453;&#24212;&#32593;&#32476;&#12290;&#20294;&#25628;&#32034;&#31934;&#30830;&#30340;&#19977;&#32500;&#36807;&#28193;&#24577;&#32467;&#26500;&#38656;&#35201;&#22823;&#37327;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#65292;&#22240;&#20026;&#21183;&#33021;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#28385;&#36275;&#29983;&#25104;&#21453;&#24212;&#29289;&#12289;&#36807;&#28193;&#24577;&#21644;&#29983;&#25104;&#29289;&#19977;&#31181;&#32467;&#26500;&#30340;&#25152;&#26377;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#24050;&#30693;&#21453;&#24212;&#29289;&#21644;&#29983;&#25104;&#29289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#32553;&#30701;&#20102;&#35745;&#31639;&#26102;&#38388;&#12290;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#12290;&#36890;&#36807;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230; (2.6 kcal/mol)&#65292;&#24182;&#19988;&#21482;&#38656;&#23545; 14% &#30340;&#32467;&#26524;&#36827;&#34892;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transition state (TS) search is key in chemistry for elucidating reaction mechanisms and exploring reaction networks. The search for accurate 3D TS structures, however, requires numerous computationally intensive quantum chemistry calculations due to the complexity of potential energy surfaces. Here, we developed an object-aware SE(3) equivariant diffusion model that satisfies all physical symmetries and constraints for generating pairs of structures, i.e., reactant, TS, and product, in an elementary reaction. Provided reactant and product, this model generates a TS structure in seconds instead of the hours required when performing quantum chemistry-based optimizations. The generated TS structures achieve an average error of 0.13 A root mean square deviation compared to true TS. With a confidence scoring model for uncertainty quantification, we approach an accuracy required for reaction rate estimation (2.6 kcal/mol) by only performing quantum chemistry-based optimizations on 14% of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#19979;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#21644;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.02912</link><description>&lt;p&gt;
&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#30340;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classification of Superstatistical Features in High Dimensions. (arXiv:2304.02912v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#19979;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#21644;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#20855;&#26377;&#19968;&#33324;&#20013;&#24515;&#28857;&#30340;&#20004;&#20010;&#25968;&#25454;&#20113;&#30340;&#28151;&#21512;&#36827;&#34892;&#20102;&#23398;&#20064;&#65292;&#20551;&#35774;&#20855;&#26377;&#36890;&#29992;&#30340;&#20984;&#25439;&#22833;&#21644;&#20984;&#27491;&#21017;&#21270;&#12290;&#27599;&#20010;&#25968;&#25454;&#20113;&#26159;&#36890;&#36807;&#20174;&#21487;&#33021;&#26159;&#19981;&#21487;&#25968;&#30340;&#39640;&#26031;&#20998;&#24067;&#21472;&#21152;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#33719;&#24471;&#30340;&#65292;&#20854;&#26041;&#24046;&#20855;&#26377;&#36890;&#29992;&#30340;&#27010;&#29575;&#23494;&#24230;$\varrho$&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#21253;&#25324;&#27809;&#26377;&#21327;&#26041;&#24046;&#30340;&#24130;&#24459;&#23614;&#37096;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#24471;&#20272;&#35745;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#20197;&#21450;&#20998;&#31163;&#36716;&#25442;&#19982;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterise the learning of a mixture of two clouds of data points with generic centroids via empirical risk minimisation in the high dimensional regime, under the assumptions of generic convex loss and convex regularisation. Each cloud of data points is obtained by sampling from a possibly uncountable superposition of Gaussian distributions, whose variance has a generic probability density $\varrho$. Our analysis covers therefore a large family of data distributions, including the case of power-law-tailed distributions with no covariance. We study the generalisation performance of the obtained estimator, we analyse the role of regularisation, and the dependence of the separability transition on the distribution scale parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00199</link><description>&lt;p&gt;
&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#27969;&#34892;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24341;&#20837;&#20110;[Nurbekyan et al.&#65292;2020]&#30340;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#22270;&#20687;&#25968;&#25454;&#30340;&#27969;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#34920;&#31034;&#31867;&#20284;&#36816;&#21160;&#25110;&#21464;&#24418;&#29616;&#35937;&#30340;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;&#22522;&#20110;&#36816;&#36755;&#30340;&#36317;&#31163;&#21644;&#29305;&#24449;&#30340;&#30740;&#31350;&#22823;&#24133;&#22686;&#21152;&#12290;&#20107;&#23454;&#19978;&#65292;&#22266;&#23450;&#20301;&#32622;&#27604;&#36739;&#24378;&#24230;&#36890;&#24120;&#26080;&#27861;&#26174;&#31034;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;[Nurbekyan et al.&#65292;2020]&#20013;&#24320;&#21457;&#30340;&#26080;&#30896;&#25758;&#22270;&#21644;&#36317;&#31163;&#31867;&#20284;&#20110;&#26368;&#20248;&#20256;&#36755;(OT)&#22270;&#30340;&#20960;&#20309;&#29305;&#24449;&#20294;&#30001;&#20110;&#26080;&#38656;&#20248;&#21270;&#65292;&#35745;&#31639;&#25104;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#26412;&#25991;&#35777;&#26126;&#26080;&#30896;&#25758;&#36317;&#31163;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21644;&#35013;&#22791;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21521;&#37327;&#20043;&#38388;&#30340;&#31561;&#36317;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#20197;&#21450;OT&#21644;&#32447;&#24615;OT&#22270;&#65292;&#19968;&#33324;&#26469;&#35828;&#19981;&#33021;&#20026;&#26059;&#36716;&#25552;&#20379;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
&lt;/p&gt;</description></item><item><title>BERT4ETH&#26159;&#19968;&#20010;&#29992;&#20110;&#20197;&#22826;&#22346;&#27450;&#35784;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#19977;&#31181;&#23454;&#29992;&#26377;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#38024;&#23545;&#39640;&#24230;&#37325;&#22797;&#12289;&#20559;&#26012;&#20998;&#24067;&#21644;&#24322;&#26500;&#20197;&#22826;&#22346;&#20132;&#26131;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2303.18138</link><description>&lt;p&gt;
BERT4ETH&#65306;&#29992;&#20110;&#20197;&#22826;&#22346;&#27450;&#35784;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection. (arXiv:2303.18138v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18138
&lt;/p&gt;
&lt;p&gt;
BERT4ETH&#26159;&#19968;&#20010;&#29992;&#20110;&#20197;&#22826;&#22346;&#27450;&#35784;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#19977;&#31181;&#23454;&#29992;&#26377;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#38024;&#23545;&#39640;&#24230;&#37325;&#22797;&#12289;&#20559;&#26012;&#20998;&#24067;&#21644;&#24322;&#26500;&#20197;&#22826;&#22346;&#20132;&#26131;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;&#22826;&#22346;&#19978;&#21508;&#31181;&#27450;&#35784;&#34892;&#20026;&#30340;&#28608;&#22686;&#65292;&#20445;&#25252;&#26131;&#21463;&#25915;&#20987;&#29992;&#25143;&#20813;&#21463;&#21463;&#23475;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#30740;&#31350;&#20165;&#20381;&#36182;&#20110;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#26377;&#20154;&#35748;&#20026;&#23427;&#20204;&#21487;&#33021;&#19981;&#36866;&#21512;&#22788;&#29702;&#39640;&#24230;&#37325;&#22797;&#12289;&#20559;&#26012;&#20998;&#24067;&#21644;&#24322;&#26500;&#20197;&#22826;&#22346;&#20132;&#26131;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BERT4ETH&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#21508;&#31181;&#27450;&#35784;&#34892;&#20026;&#12290;BERT4ETH&#20855;&#26377;Transformer&#30340;&#20248;&#31168;&#24314;&#27169;&#33021;&#21147;&#65292;&#33021;&#22815;&#25429;&#25417;&#20197;&#22826;&#22346;&#20132;&#26131;&#20013;&#22266;&#26377;&#30340;&#21160;&#24577;&#39034;&#24207;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#37325;&#22797;&#24615;&#12289;&#20943;&#36731;&#20559;&#26012;&#21644;&#24314;&#27169;&#24322;&#26500;&#24615;&#31561;&#19977;&#31181;&#23454;&#29992;&#26377;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#20026;&#20197;&#22826;&#22346;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;BERT4ETH&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26368;&#22823;&#21270;&#35838;&#31243;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#26435;&#37325;&#65292;&#35753;&#27169;&#22411;&#19987;&#27880;&#20110;&#33021;&#22815;&#34920;&#31034;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#27169;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#24615;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19987;&#23478;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#22823;&#29109;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2303.15349</link><description>&lt;p&gt;
&#20449;&#24687;&#26368;&#22823;&#21270;&#35838;&#31243;&#65306;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#27169;&#20223;&#22810;&#26679;&#25216;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Information Maximizing Curriculum: A Curriculum-Based Approach for Imitating Diverse Skills. (arXiv:2303.15349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26368;&#22823;&#21270;&#35838;&#31243;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#26435;&#37325;&#65292;&#35753;&#27169;&#22411;&#19987;&#27880;&#20110;&#33021;&#22815;&#34920;&#31034;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#27169;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#24615;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19987;&#23478;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#22823;&#29109;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#20294;&#24403;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#26102;&#65292;&#30001;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#24322;&#24615;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#22823;&#22810;&#25968;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#26368;&#22823;&#20284;&#28982;&#65288;ML&#65289;&#30446;&#26631;&#26469;&#23398;&#20064;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#20294;&#30001;&#20110;ML&#30446;&#26631;&#30340;&#27169;&#24179;&#22343;&#29305;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#25110;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#26368;&#22823;&#21270;&#35838;&#31243;&#65292;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#26435;&#37325;&#65292;&#24182;&#40723;&#21169;&#27169;&#22411;&#19987;&#27880;&#20110;&#23427;&#33021;&#22815;&#34920;&#31034;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#24573;&#30053;&#19981;&#33021;&#34920;&#31034;&#30340;&#27169;&#24577;&#25968;&#25454;&#26469;&#26377;&#25928;&#32531;&#35299;&#27169;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#28085;&#30422;&#25152;&#26377;&#27169;&#24577;&#24182;&#33021;&#22815;&#23454;&#29616;&#22810;&#26679;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#31574;&#30053;&#65292;&#20854;&#20013;&#27599;&#20010;&#28151;&#21512;&#25104;&#20998;&#20026;&#33258;&#24049;&#36873;&#25321;&#19968;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#12290;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#32422;&#26463;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning uses data for training policies to solve complex tasks. However, when the training data is collected from human demonstrators, it often leads to multimodal distributions because of the variability in human actions. Most imitation learning methods rely on a maximum likelihood (ML) objective to learn a parameterized policy, but this can result in suboptimal or unsafe behavior due to the mode-averaging property of the ML objective. In this work, we propose Information Maximizing Curriculum, a curriculum-based approach that assigns a weight to each data point and encourages the model to specialize in the data it can represent, effectively mitigating the mode-averaging problem by allowing the model to ignore data from modes it cannot represent. To cover all modes and thus, enable diverse behavior, we extend our approach to a mixture of experts (MoE) policy, where each mixture component selects its own subset of the training data for learning. A novel, maximum entropy-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12799</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#22270;&#20687;&#65306;&#29992;&#35270;&#35273;transformer&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#19981;&#35268;&#21017;&#25277;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39640;&#24230;&#23450;&#21046;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#24615;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#23427;&#20204;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#39640;&#31232;&#30095;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#35843;&#25972;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#20197;&#25191;&#34892;&#19982;&#22270;&#20687;&#20998;&#31867;&#30456;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20551;&#35774;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#28508;&#22312;&#22320;&#25193;&#23637;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#20256;&#24863;&#22120;&#35774;&#32622;&#20013;&#65292;&#21363;&#22312;&#27979;&#35797;&#26399;&#38388;&#23631;&#34109;&#21464;&#37327;&#30340;&#23376;&#38598;&#20013;&#65292;&#24615;&#33021;&#27604;&#26368;&#20339;&#22522;&#20934;&#25552;&#39640;&#20102;&#39640;&#36798;11&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
&lt;/p&gt;</description></item><item><title>FedML-HE&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#21516;&#24577;&#21152;&#23494;&#30340;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#24615;&#21152;&#23494;&#25935;&#24863;&#21442;&#25968;&#26469;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#20379;&#21487;&#23450;&#21046;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2303.10837</link><description>&lt;p&gt;
FedML-HE:&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System. (arXiv:2303.10837v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10837
&lt;/p&gt;
&lt;p&gt;
FedML-HE&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#21516;&#24577;&#21152;&#23494;&#30340;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#24615;&#21152;&#23494;&#25935;&#24863;&#21442;&#25968;&#26469;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#20379;&#21487;&#23450;&#21046;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#32780;&#19981;&#26159;&#26412;&#22320;&#25968;&#25454;&#65292;&#22312;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#20135;&#29983;&#20102;&#65292;&#22240;&#20026;&#26381;&#21153;&#31471;&#19978;&#32858;&#21512;&#30340;&#26412;&#22320;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#36870;&#21521;&#25915;&#20987;&#25581;&#31034;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#12290;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22914;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#22240;&#27492;&#25104;&#20026;FL&#35757;&#32451;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#23613;&#31649;HE&#20855;&#26377;&#38544;&#31169;&#20248;&#21183;&#65292;&#20294;&#20854;&#24212;&#29992;&#21463;&#21040;&#19981;&#23454;&#38469;&#30340;&#24320;&#38144;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#23545;&#22522;&#30784;&#27169;&#22411;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedML-HE&#65292;&#31532;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;HE&#23433;&#20840;&#27169;&#22411;&#32858;&#21512;&#30340;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;FedML-HE&#25552;&#20986;&#20102;&#36873;&#25321;&#24615;&#21152;&#23494;&#25935;&#24863;&#21442;&#25968;&#65292;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#23450;&#21046;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#20248;&#21270;&#30340;&#31995;&#32479;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#24320;&#38144;&#38477;&#20302;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;ResNet-50&#20943;&#23569;&#20102;&#32422;10&#20493;&#65292;&#32780;B&#27169;&#22411;&#20943;&#23569;&#20102;&#32422;40&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as the aggregated local models on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present FedML-HE, the first practical federated learning system with efficient HE-based secure model aggregation. FedML-HE proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing customizable privacy preservation. Our optimized system demonstrates considerable overhead reduction, particularly for large foundation models (e.g., ~10x reduction for ResNet-50, and up to ~40x reduction for B
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.10834</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10834
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#31361;&#26174;&#20102;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#25972;&#21512;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Latent Slot Diffusion (LSD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#30446;&#26631;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23558;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#20197;&#23545;&#35937;&#27133;&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#27169;&#22411;&#65307;&#20854;&#27425;&#65292;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#20687;&#25991;&#26412;&#36825;&#26679;&#30340;&#30417;&#30563;&#27880;&#37322;&#32780;&#33021;&#22815;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#23545;&#35937;&#20013;&#24515;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#39318;&#27425;&#22312;FFHQ&#25968;&#25454;&#38598;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
&lt;/p&gt;</description></item><item><title>InCA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20219;&#20309;&#28608;&#27963;&#23618;&#36827;&#34892;&#20132;&#20114;&#24335;&#20851;&#27880;&#12290;&#19982;&#20854;&#20182;&#24418;&#24335;&#30340;&#36866;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;InCA&#30340;&#24615;&#33021;&#25509;&#36817;&#20840;&#38754;&#24494;&#35843;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20165;&#20026;&#22522;&#32447;&#30340;51%&#12290;</title><link>http://arxiv.org/abs/2303.04105</link><description>&lt;p&gt;
&#20320;&#30340;&#34920;&#31034;&#22312;&#32593;&#32476;&#20013;&#65306;&#21487;&#32452;&#21512;&#21644;&#24182;&#34892;&#36866;&#24212;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Your representations are in the network: composable and parallel adaptation for large scale models. (arXiv:2303.04105v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04105
&lt;/p&gt;
&lt;p&gt;
InCA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20219;&#20309;&#28608;&#27963;&#23618;&#36827;&#34892;&#20132;&#20114;&#24335;&#20851;&#27880;&#12290;&#19982;&#20854;&#20182;&#24418;&#24335;&#30340;&#36866;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;InCA&#30340;&#24615;&#33021;&#25509;&#36817;&#20840;&#38754;&#24494;&#35843;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20165;&#20026;&#22522;&#32447;&#30340;51%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;InCA&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20219;&#20309;&#28608;&#27963;&#23618;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#20851;&#27880;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;InCA&#20351;&#29992;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#25552;&#21462;&#22810;&#20010;&#28608;&#27963;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#22806;&#37096;&#20132;&#21449;&#27880;&#24847;&#21147;&#36866;&#37197;&#22120;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21644;&#32452;&#21512;&#25110;&#36873;&#25321;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#36873;&#25321;&#19968;&#20010;&#26368;&#39640;&#20998;&#30340;&#36866;&#37197;&#22120;&#65292;InCA&#30340;&#24615;&#33021;&#20063;&#21487;&#20197;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#23218;&#32654;&#65292;&#32780;&#25104;&#26412;&#20165;&#30456;&#24403;&#20110;&#20165;&#24494;&#35843;&#26368;&#21518;&#19968;&#23618;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#19982;&#39044;&#35757;&#32451;ViT-L/16&#27169;&#22411;&#22823;&#23567;&#30456;&#27604;&#20165;&#20026;1.3%&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#25506;&#38024;&#65292;&#22312;&#24179;&#22343;11&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#25509;&#36817;&#20840;&#38754;&#24494;&#35843;&#30340;&#21442;&#29031;&#65292;&#24182;&#19988;&#35745;&#31639;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;&#22522;&#32447;&#30340;51%&#12290;&#19982;&#20854;&#20182;&#24418;&#24335;&#30340;&#39640;&#25928;&#36866;&#24212;&#19981;&#21516;&#65292;InCA&#19981;&#38656;&#35201;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#20445;&#25345;&#20854;&#25191;&#34892;&#19981;&#21464;&#12290;InCA&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;fine-tuning&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose InCA, a lightweight method for transfer learning that cross-attends to any activation layer of a pre-trained model. During training, InCA uses a single forward pass to extract multiple activations, which are passed to external cross-attention adapters, trained anew and combined or selected for downstream tasks. We show that, even when selecting a single top-scoring adapter, InCA achieves performance comparable to full fine-tuning, at a cost comparable to fine-tuning just the last layer. For example, with a cross-attention probe 1.3% the size of a pre-trained ViT-L/16 model, we achieve performance within 0.2% of the full fine-tuning paragon at a computational training cost of 51% of the baseline, on average across 11 downstream classification. Unlike other forms of efficient adaptation, InCA does not require backpropagating through the pre-trained model, thus leaving its execution unaltered at both training and inference. The versatility of InCA is best illustrated in fine-gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;-GlucoSynth&#65292;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#26469;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#65292;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#23433;&#20840;&#30340;&#21516;&#26102;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;.</title><link>http://arxiv.org/abs/2303.01621</link><description>&lt;p&gt;
GlucoSynth&#65306;&#29983;&#25104;&#24046;&#20998;&#31169;&#26377;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces. (arXiv:2303.01621v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;-GlucoSynth&#65292;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#26469;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#65292;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#23433;&#20840;&#30340;&#21516;&#26102;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#31169;&#26377;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#30340;&#38382;&#39064;&#65292;&#36825;&#20010;&#20219;&#21153;&#21487;&#25512;&#24191;&#21040;&#35768;&#22810;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#34880;&#31958;&#25968;&#25454;&#30340;&#20808;&#22825;&#29305;&#24449;&#65292;&#20063;&#26080;&#27861;&#22312;&#19981;&#20005;&#37325;&#38477;&#20302;&#21512;&#25104;&#25968;&#25454;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20219;&#20309;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GlucoSynth&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;GAN&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#32771;&#34385;&#26102;&#24207;&#21160;&#24577;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#36712;&#36857;&#20013;motif&#65288;&#34880;&#31958;&#20107;&#20214;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;120&#19975;&#26465;&#34880;&#31958;&#36712;&#36857;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65307;GlucoSynth&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synt
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU&#32593;&#32476;&#20013;&#26799;&#24230;&#27969;&#30340;&#38544;&#24335;&#20559;&#24046;&#23545;&#27867;&#21270;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26799;&#24230;&#27969;&#20542;&#21521;&#20110;&#27867;&#21270;&#33021;&#21147;&#24378;&#20294;&#23545;&#25239;&#24615;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36825;&#31181;&#20559;&#24046;&#36824;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01456</link><description>&lt;p&gt;
&#38544;&#21547;&#20559;&#35265;&#30340;&#21452;&#20995;&#21073;&#65306;ReLU&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#19982;&#40065;&#26834;&#24615;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks. (arXiv:2303.01456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU&#32593;&#32476;&#20013;&#26799;&#24230;&#27969;&#30340;&#38544;&#24335;&#20559;&#24046;&#23545;&#27867;&#21270;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26799;&#24230;&#27969;&#20542;&#21521;&#20110;&#27867;&#21270;&#33021;&#21147;&#24378;&#20294;&#23545;&#25239;&#24615;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36825;&#31181;&#20559;&#24046;&#36824;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26799;&#24230;&#27969;&#30340;&#38544;&#24335;&#20559;&#24046;&#23545;ReLU&#32593;&#32476;&#20013;&#27867;&#21270;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#25968;&#25454;&#30001;&#31751;&#32452;&#25104;&#19988;&#31751;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#23567;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21457;&#29616;&#22312;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#65292;&#26799;&#24230;&#27969;&#22312;&#20559;&#21521;&#27867;&#21270;&#33021;&#21147;&#24378;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21516;&#26102;&#20063;&#23545;&#23567;&#35268;&#27169;&#23545;&#25239;&#24615;&#20363;&#23376;&#39640;&#24230;&#33030;&#24369;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21363;&#20351;&#22312;&#32593;&#32476;&#21442;&#25968;&#36828;&#36828;&#22810;&#20313;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#25104;&#31435;&#12290;&#23613;&#31649;&#22312;&#36825;&#31181;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35774;&#32622;&#20013;&#26377;&#28508;&#22312;&#30340;&#26377;&#23475;&#36807;&#25311;&#21512;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#26799;&#24230;&#27969;&#30340;&#38544;&#24335;&#20559;&#24046;&#21487;&#20197;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#38544;&#24335;&#20559;&#24046;&#20063;&#20250;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#23481;&#26131;&#21463;&#21040;&#23567;&#30340;&#23545;&#25239;&#24615;$\ell_2$&#25200;&#21160;&#65289;&#65292;&#23613;&#31649;&#20063;&#23384;&#22312;&#33021;&#22815;&#25311;&#21512;&#25968;&#25454;&#30340;&#40065;&#26834;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the implications of the implicit bias of gradient flow on generalization and adversarial robustness in ReLU networks. We focus on a setting where the data consists of clusters and the correlations between cluster means are small, and show that in two-layer ReLU networks gradient flow is biased towards solutions that generalize well, but are highly vulnerable to adversarial examples. Our results hold even in cases where the network has many more parameters than training examples. Despite the potential for harmful overfitting in such overparameterized settings, we prove that the implicit bias of gradient flow prevents it. However, the implicit bias also leads to non-robust solutions (susceptible to small adversarial $\ell_2$-perturbations), even though robust networks that fit the data exist.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10586</link><description>&lt;p&gt;
&#20998;&#24067;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#22312;&#23569;&#37327;&#26631;&#31614;&#19978;&#20114;&#30456;&#21463;&#30410;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#8212;&#8212;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#65292;&#35813;&#31574;&#30053;&#24314;&#31435;&#22312;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#19978;&#12290;DPT&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#20266;&#26631;&#31614;&#65307;&#20351;&#29992;&#36825;&#20123;&#20266;&#26631;&#31614;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#20266;&#22270;&#20687;&#65307;&#24182;&#20351;&#29992;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#28151;&#21512;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;DPT&#22987;&#32456;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#30340;SOTA&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;DPT&#30340;Fr\'echet Inception Distance&#65288;FID&#65289;&#24471;&#20998;&#20998;&#21035;&#20026;3.08&#25110;2.52&#65292;&#36229;&#36807;&#20102;&#20855;&#26377;&#23436;&#25972;&#26631;&#31614;&#30340;&#24378;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;IDDPM&#65292;CDM&#65292;ADM&#21644;LDM&#65289;&#12290;&#27492;&#22806;&#65292;DPT&#22312;ImageNet&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#31454;&#20105;&#24615;&#30340;&#21322;&#30417;&#30563;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#39030;&#32423;1&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36873;&#25321;&#20999;&#21106;&#24179;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#20999;&#21106;&#36873;&#25321;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09166</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25972;&#25968;&#35268;&#21010;&#20013;&#30340;&#20999;&#21106;&#24179;&#38754;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Cutting Planes in Integer Programming: A Survey. (arXiv:2302.09166v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36873;&#25321;&#20999;&#21106;&#24179;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#20999;&#21106;&#36873;&#25321;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36873;&#25321;&#20999;&#21106;&#24179;&#38754;&#65288;&#25110;&#20999;&#21106;&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#23613;&#31649;&#23384;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#20999;&#21106;&#31867;&#21035;&#65292;&#20294;&#22312;&#20998;&#25903;&#23450;&#30028;&#26641;&#30340;&#32473;&#23450;&#33410;&#28857;&#30340;&#32447;&#24615;&#35268;&#21010;&#25918;&#26494;&#20013;&#36873;&#25321;&#19968;&#32452;&#35201;&#28155;&#21152;&#30340;&#20999;&#21106;&#24179;&#38754;&#30340;&#20219;&#21153;&#36804;&#20170;&#20026;&#27490;&#26080;&#27861;&#24471;&#21040;&#27491;&#24335;&#21644;&#21551;&#21457;&#24335;&#35299;&#27861;&#12290;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#26469;&#35782;&#21035;&#21152;&#36895;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#23454;&#20363;&#30340;&#26377;&#21069;&#26223;&#30340;&#20999;&#21106;&#65292;&#20026;&#25913;&#36827;&#20999;&#21106;&#36873;&#25321;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#20027;&#39064;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25968;&#25454;&#25910;&#38598;&#12289;&#35780;&#20272;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25991;&#29486;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#35797;&#22270;&#37327;&#21270;&#24050;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#26368;&#21518;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We survey recent work on machine learning (ML) techniques for selecting cutting planes (or cuts) in mixed-integer linear programming (MILP). Despite the availability of various classes of cuts, the task of choosing a set of cuts to add to the linear programming (LP) relaxation at a given node of the branch-and-bound (B&amp;B) tree has defied both formal and heuristic solutions to date. ML offers a promising approach for improving the cut selection process by using data to identify promising cuts that accelerate the solution of MILP instances. This paper presents an overview of the topic, highlighting recent advances in the literature, common approaches to data collection, evaluation, and ML model architectures. We analyze the empirical results in the literature in an attempt to quantify the progress that has been made and conclude by suggesting avenues for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.07491</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#19982;&#26102;&#38388;&#21644;&#32467;&#26500;&#24378;&#24230;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment. (arXiv:2302.07491v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#23398;&#20064;&#26088;&#22312;&#29983;&#25104;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#65292;&#21516;&#26102;&#21253;&#21547;&#21160;&#24577;&#20449;&#24687;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#38745;&#24577;&#22270;&#19981;&#21516;&#65292;&#26102;&#38388;&#22270;&#36890;&#24120;&#20197;&#36830;&#32493;&#26102;&#38388;&#19978;&#30340;&#33410;&#28857;&#20132;&#20114;&#24207;&#21015;&#30340;&#24418;&#24335;&#32452;&#32455;&#65292;&#32780;&#19981;&#26159;&#37051;&#25509;&#30697;&#38453;&#12290;&#22823;&#22810;&#25968;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22312;&#26102;&#38388;&#19978;&#32452;&#21512;&#21382;&#21490;&#20449;&#24687;&#26469;&#24314;&#27169;&#24403;&#21069;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#32771;&#34385;&#20102;&#19968;&#38454;&#26102;&#38388;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#37325;&#35201;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;S2T&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph learning aims to generate high-quality representations for graph-based tasks along with dynamic information, which has recently drawn increasing attention. Unlike the static graph, a temporal graph is usually organized in the form of node interaction sequences over continuous time instead of an adjacency matrix. Most temporal graph learning methods model current interactions by combining historical information over time. However, such methods merely consider the first-order temporal information while ignoring the important high-order structural information, leading to sub-optimal performance. To solve this issue, by extracting both temporal and structural information to learn more informative node representations, we propose a self-supervised method termed S2T for temporal graph learning. Note that the first-order temporal information and the high-order structural information are combined in different ways by the initial node representations to calculate two conditional 
&lt;/p&gt;</description></item><item><title>&#35780;&#20998;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#22833;&#36133;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#30340;&#26657;&#20934;&#21327;&#35758;&#21487;&#20197;&#25552;&#39640;&#29616;&#26377;&#20462;&#21098;&#31639;&#27861;&#22312;&#35813;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06960</link><description>&lt;p&gt;
&#25968;&#25454;&#20462;&#21098;&#21644;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65306;&#22522;&#20110;&#35780;&#20998;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Data pruning and neural scaling laws: fundamental limitations of score-based algorithms. (arXiv:2302.06960v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06960
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20998;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#22833;&#36133;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#30340;&#26657;&#20934;&#21327;&#35758;&#21487;&#20197;&#25552;&#39640;&#29616;&#26377;&#20462;&#21098;&#31639;&#27861;&#22312;&#35813;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#24120;&#29992;&#20110;&#20943;&#23569;&#20248;&#21270;&#36807;&#31243;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#25968;&#25454;&#20462;&#21098;&#20173;&#28982;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#20248;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;&#20102;&#19981;&#21040;&#25968;&#25454;&#30340;30&#65285;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#21387;&#32553;&#21306;&#22495;&#26368;&#36817;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#25968;&#25454;&#20462;&#21098;&#22312;&#25552;&#39640;&#25152;&#35859;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#20013;&#30340;&#20316;&#29992;&#65307;&#22312;[Sorscher et al.]&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#25165;&#33021;&#20987;&#36133;&#26679;&#26412;&#21183;&#24459;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#35780;&#20998;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#38469;&#19978;&#23637;&#31034;&#20102;&#20026;&#20160;&#20040;&#36825;&#26679;&#30340;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#22833;&#36133;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#25454;&#20462;&#21098;&#30340;&#8220;&#27809;&#26377;&#20813;&#36153;&#21320;&#39184;&#8221;&#23450;&#29702;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#25552;&#20986;&#20102;&#26657;&#20934;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;&#20462;&#21098;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of $30\%$ or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; in [Sorscher et al.], the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law.  In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate ``No Free Lunch" theorems for data pruning and present calibration protocols that enhance the performance of existing pruning algorithms in this high compression regime using randomization.
&lt;/p&gt;</description></item><item><title>&#28040;&#24687;&#20256;&#36882;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#21512;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#38754;&#23545;6G&#31995;&#32479;&#37096;&#32626;&#22823;&#37327;&#22825;&#32447;&#26102;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06896</link><description>&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#30340;&#26032;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Message Passing Meets Graph Neural Networks: A New Paradigm for Massive MIMO Systems. (arXiv:2302.06896v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06896
&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#21512;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#38754;&#23545;6G&#31995;&#32479;&#37096;&#32626;&#22823;&#37327;&#22825;&#32447;&#26102;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;5G&#31995;&#32479;&#30340;&#26680;&#24515;&#25216;&#26415;&#20043;&#19968;&#65292;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#24341;&#20837;&#20102;&#24040;&#22823;&#30340;&#23481;&#37327;&#25552;&#21319;&#20197;&#21450;&#38750;&#24120;&#39640;&#30340;&#27874;&#26463;&#36171;&#24418;&#21644;&#31354;&#38388;&#22797;&#29992;&#22686;&#30410;&#12290;&#22312;&#20026;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#24320;&#21457;&#39640;&#25928;&#30340;&#29289;&#29702;&#23618;&#31639;&#27861;&#26102;&#65292;&#28040;&#24687;&#20256;&#36882;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#32773;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38382;&#39064;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#22823;&#24133;&#22686;&#21152;&#65292;&#29616;&#26377;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;6G&#31995;&#32479;&#65292;&#20854;&#20013;&#23558;&#37096;&#32626;&#22823;&#37327;&#22825;&#32447;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#65292;&#21363;AMP-GNN&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#25910;&#21457;&#22120;&#35774;&#35745;&#65292;&#32771;&#34385;&#21040;AMP&#31639;&#27861;&#30340;&#20302;&#22797;&#26434;&#24230;&#21644;GNN&#30340;&#36866;&#24212;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AMP-GNN&#32593;&#32476;&#30340;&#32467;&#26500;&#36890;&#36807;&#23637;&#24320;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#31639;&#27861;&#21644;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;
&lt;/p&gt;
&lt;p&gt;
As one of the core technologies for 5G systems, massive multiple-input multiple-output (MIMO) introduces dramatic capacity improvements along with very high beamforming and spatial multiplexing gains. When developing efficient physical layer algorithms for massive MIMO systems, message passing is one promising candidate owing to the superior performance. However, as their computational complexity increases dramatically with the problem size, the state-of-the-art message passing algorithms cannot be directly applied to future 6G systems, where an exceedingly large number of antennas are expected to be deployed. To address this issue, we propose a model-driven deep learning (DL) framework, namely the AMP-GNN for massive MIMO transceiver design, by considering the low complexity of the AMP algorithm and adaptability of GNNs. Specifically, the structure of the AMP-GNN network is customized by unfolding the approximate message passing (AMP) algorithm and introducing a graph neural network (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>CodeBERTScore&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#30340;&#20195;&#30721;&#20197;&#21450;&#20854;&#25152;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2302.05527</link><description>&lt;p&gt;
CodeBERTScore: &#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#27169;&#22411;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. (arXiv:2302.05527v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05527
&lt;/p&gt;
&lt;p&gt;
CodeBERTScore&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#30340;&#20195;&#30721;&#20197;&#21450;&#20854;&#25152;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#27169;&#22411;&#65288;NL-&gt;Code&#65289;&#30340;&#23835;&#36215;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#38271;&#34920;&#36798;&#24335;&#21644;&#35821;&#21477;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#21333;&#29420;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#26159;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#21487;&#38752;&#22320;&#35780;&#20272;&#23427;&#20204;&#25152;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeBERTScore&#65306;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#24314;&#31435;&#22312;BERTScore (Zhang et al., 2020) &#30340;&#22522;&#30784;&#19978;&#12290;&#19982;BERTScore&#21482;&#23545;&#29983;&#25104;&#30340;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#19981;&#21516;&#65292;CodeBERTScore&#36824;&#23545;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#24314;&#27169;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#20854;&#25152;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23545;CodeBERTScore&#22312;&#22235;&#31181;&#32534;&#31243;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;CodeBERTScore&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#30456;&#20851;&#24615;&#27604;&#25152;&#26377;&#29616;&#26377;&#25351;&#26631;&#37117;&#26356;&#39640;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CodeBERTScore&#35780;&#20998;&#26356;&#39640;&#30340;&#29983;&#25104;&#20195;&#30721;&#26356;&#26377;&#21487;&#33021;&#34987;&#20154;&#31867;&#38738;&#30544;&#65292;&#24182;&#19988;&#22312;&#25191;&#34892;&#26102;&#26356;&#21487;&#33021;&#27491;&#30830;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the rise of neural natural-language-to-code models (NL-&gt;Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#25620;&#25235;&#24378;&#24230;&#30340;&#22810;&#27169;&#24577;&#24863;&#24212;&#25163;&#29615;&#65292;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#21644;&#25509;&#35302;&#40614;&#20811;&#39118;&#31561;&#35774;&#22791;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36816;&#29992;&#65292;&#23454;&#29616;&#20102;&#23545;&#25620;&#25235;&#24378;&#24230;&#30340;&#20272;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20020;&#24202;&#30456;&#20851;&#37492;&#21035;&#19978;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.03813</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#25620;&#25235;&#24378;&#24230;&#30340;&#22810;&#27169;&#24577;&#24863;&#24212;&#25163;&#29615;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Sensing Ring for Quantification of Scratch Intensity. (arXiv:2302.03813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#25620;&#25235;&#24378;&#24230;&#30340;&#22810;&#27169;&#24577;&#24863;&#24212;&#25163;&#29615;&#65292;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#21644;&#25509;&#35302;&#40614;&#20811;&#39118;&#31561;&#35774;&#22791;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36816;&#29992;&#65292;&#23454;&#29616;&#20102;&#23545;&#25620;&#25235;&#24378;&#24230;&#30340;&#20272;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20020;&#24202;&#30456;&#20851;&#37492;&#21035;&#19978;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#21307;&#30103;&#29366;&#20917;&#65292;&#23458;&#35266;&#27979;&#37327;&#24930;&#24615;&#30233;&#30162;&#38750;&#24120;&#24517;&#35201;&#20197;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#27700;&#24179;&#12290;&#23613;&#31649;&#21487;&#31359;&#25140;&#35774;&#22791;&#24050;&#32463;&#26174;&#31034;&#20986;&#26816;&#27979;&#25620;&#25235;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#26080;&#27861;&#20272;&#35745;&#25620;&#25235;&#24378;&#24230;&#65292;&#20174;&#32780;&#26080;&#27861;&#20840;&#38754;&#20102;&#35299;&#30233;&#30162;&#23545;&#20010;&#20307;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#25620;&#25235;&#24378;&#24230;&#20197;&#21450;&#26816;&#27979;&#25620;&#25235;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#25163;&#29615;&#35774;&#22791;&#23454;&#29616;&#65292;&#21253;&#25324;&#19968;&#20010;&#21152;&#36895;&#24230;&#35745;&#21644;&#19968;&#20010;&#25509;&#35302;&#40614;&#20811;&#39118;&#65292;&#19968;&#20010;&#21387;&#21147;&#25935;&#24863;&#30340;&#24179;&#26495;&#29992;&#20110;&#25429;&#33719;&#30495;&#23454;&#24378;&#24230;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22238;&#24402;&#65292;&#23558;&#25620;&#25235;&#24378;&#24230;&#26144;&#23556;&#21040;0-10&#36830;&#32493;&#21051;&#24230;&#30340;0-600&#27627;&#29926;&#65288;mW&#65289;&#21151;&#29575;&#21051;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#30041;&#19968;&#20307;&#22806;&#20132;&#21449;&#39564;&#35777;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;20&#21517;&#20010;&#20307;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;14&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20020;&#24202;&#30456;&#20851;&#37492;&#21035;&#19978;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
An objective measurement of chronic itch is necessary for improvements in patient care for numerous medical conditions. While wearables have shown promise for scratch detection, they are currently unable to estimate scratch intensity, preventing a comprehensive understanding of the effect of itch on an individual. In this work, we present a framework for the estimation of scratch intensity in addition to the detection of scratch. This is accomplished with a multimodal ring device, consisting of an accelerometer and a contact microphone, a pressure-sensitive tablet for capturing ground truth intensity values, and machine learning algorithms for regression of scratch intensity on a 0-600 milliwatts (mW) power scale that can be mapped to a 0-10 continuous scale. We evaluate the performance of our algorithms on 20 individuals using leave one subject out cross-validation and using data from 14 additional participants, we show that our algorithms achieve clinically-relevant discrimination of
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32593;&#26684;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#23432;&#24658;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21253;&#25324;&#26102;&#38388;&#30456;&#20851;&#30340;Fokker-Planck&#26041;&#31243;&#21644;Wasserstein&#26799;&#24230;&#27969;&#12290;&#36890;&#36807;&#33258;&#27965;&#30340;&#36895;&#24230;&#21305;&#37197;&#26041;&#27861;&#21644;&#36845;&#20195;&#24418;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32469;&#36807;&#20102;&#35745;&#31639;&#38556;&#30861;&#65292;&#24182;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13737</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#27010;&#29575;&#27969;&#36895;&#24230;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Velocity Matching of Probability Flows. (arXiv:2301.13737v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13737
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32593;&#26684;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#23432;&#24658;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21253;&#25324;&#26102;&#38388;&#30456;&#20851;&#30340;Fokker-Planck&#26041;&#31243;&#21644;Wasserstein&#26799;&#24230;&#27969;&#12290;&#36890;&#36807;&#33258;&#27965;&#30340;&#36895;&#24230;&#21305;&#37197;&#26041;&#27861;&#21644;&#36845;&#20195;&#24418;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32469;&#36807;&#20102;&#35745;&#31639;&#38556;&#30861;&#65292;&#24182;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32593;&#26684;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#19968;&#31867;&#21253;&#25324;&#26102;&#38388;&#30456;&#20851;&#30340;Fokker-Planck&#26041;&#31243;&#21644;Wasserstein&#26799;&#24230;&#27969;&#22312;&#20869;&#30340;&#23432;&#24658;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#20027;&#35201;&#35266;&#23519;&#26159;PDE&#35299;&#30340;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#22330;&#38656;&#35201;&#26159;&#33258;&#27965;&#30340;&#65306;&#23427;&#24517;&#39035;&#28385;&#36275;&#19968;&#20010;&#21253;&#21547;&#30456;&#21516;&#36895;&#24230;&#22330;&#30340;&#27010;&#29575;&#27969;&#30340;&#19981;&#21160;&#28857;&#26041;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#24418;&#24335;&#21644;&#26377;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#32469;&#36807;&#20102;&#20855;&#26377;&#24378;&#22823;&#23454;&#35777;&#24615;&#33021;&#30340;&#37325;&#22823;&#35745;&#31639;&#38556;&#30861;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#26368;&#23567;&#21270;&#19981;&#21160;&#28857;&#26041;&#31243;&#30340;&#27531;&#24046;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21463;&#26102;&#38388;&#25110;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#38480;&#21046;&#65292;&#28085;&#30422;&#20102;&#26356;&#24191;&#27867;&#30340;PDEs&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#36827;&#34892;&#25193;&#23637;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#35299;&#26512;&#35299;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#35299;&#26512;&#35299;&#65292;&#24182;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#21462;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a discretization-free scalable framework for solving a large class of mass-conserving partial differential equations (PDEs), including the time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The main observation is that the time-varying velocity field of the PDE solution needs to be self-consistent: it must satisfy a fixed-point equation involving the probability flow characterized by the same velocity field. Instead of directly minimizing the residual of the fixed-point equation with neural parameterization, we use an iterative formulation with a biased gradient estimator that bypasses significant computational obstacles with strong empirical performance. Compared to existing approaches, our method does not suffer from temporal or spatial discretization, covers a wider range of PDEs, and scales to high dimensions. Experimentally, our method recovers analytical solutions accurately when they are available and achieves superior performance in high dimensi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#23558;&#20854;&#20998;&#31867;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#21644;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#30340;&#24615;&#33021;&#38480;&#21046;&#21644;&#21051;&#30011;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#36824;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#25506;&#31350;&#20102;&#22312;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2301.11781</link><description>&lt;p&gt;
&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65306;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions. (arXiv:2301.11781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#23558;&#20854;&#20998;&#31867;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#21644;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#30340;&#24615;&#33021;&#38480;&#21046;&#21644;&#21051;&#30011;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#36824;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#25506;&#31350;&#20102;&#22312;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26576;&#20123;&#20154;&#32676;&#20013;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22312;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#36873;&#25321;&#21644;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#27495;&#35270;&#26469;&#28304;&#20998;&#20026;&#20004;&#31867;&#65306;&#20598;&#28982;&#24615;&#27495;&#35270;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#65292;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#21363;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#22312;&#23436;&#20840;&#20102;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#38480;&#21046;&#26469;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#24067;&#33713;&#20811;&#38886;&#23572;&#23545;&#27604;&#32479;&#35745;&#23454;&#39564;&#30340;&#32467;&#26524;&#26469;&#21051;&#30011;&#20598;&#28982;&#24615;&#27495;&#35270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35748;&#30693;&#24615;&#27495;&#35270;&#23450;&#20041;&#20026;&#22312;&#24212;&#29992;&#20844;&#24179;&#32422;&#26463;&#26102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19982;&#20598;&#28982;&#24615;&#27495;&#35270;&#25152;&#38480;&#23450;&#30340;&#30028;&#38480;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#35843;&#26597;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate th
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#38598;&#25104;&#32852;&#21512;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#22522;&#30784;&#23398;&#20064;&#32773;&#21246;&#32467;&#65292;&#22240;&#27492;&#30452;&#25509;&#20248;&#21270;&#25972;&#20010;&#38598;&#25104;&#30340;&#25439;&#22833;&#24456;&#23569;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.11323</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#30340;&#32852;&#21512;&#35757;&#32451;&#22240;&#23398;&#20064;&#32773;&#21246;&#32467;&#32780;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Joint Training of Deep Ensembles Fails Due to Learner Collusion. (arXiv:2301.11323v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11323
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#38598;&#25104;&#32852;&#21512;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#22522;&#30784;&#23398;&#20064;&#32773;&#21246;&#32467;&#65292;&#22240;&#27492;&#30452;&#25509;&#20248;&#21270;&#25972;&#20010;&#38598;&#25104;&#30340;&#25439;&#22833;&#24456;&#23569;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#24050;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#19968;&#31181;&#25552;&#39640;&#24615;&#33021;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#38598;&#25104;&#31639;&#27861;&#29420;&#31435;&#25110;&#39034;&#24207;&#22320;&#35757;&#32451;&#23427;&#20204;&#30340;&#22522;&#30784;&#23398;&#20064;&#32773;&#65292;&#30446;&#30340;&#26159;&#20248;&#21270;&#23427;&#20204;&#30340;&#32852;&#21512;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#20013;&#65292;&#25105;&#20204;&#26377;&#26426;&#20250;&#30452;&#25509;&#20248;&#21270;&#30495;&#27491;&#30340;&#30446;&#26631;&#65306;&#25972;&#20010;&#38598;&#25104;&#30340;&#32852;&#21512;&#24615;&#33021;&#12290;&#28982;&#32780;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#30452;&#25509;&#26368;&#23567;&#21270;&#38598;&#25104;&#25439;&#22833;&#12290;&#30456;&#21453;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#26159;&#29420;&#31435;&#22320;&#35757;&#32451;&#20010;&#21035;&#27169;&#22411;&#65292;&#24182;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#38598;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#20570;&#30340;&#21407;&#22240;&#26159;&#24456;&#22909;&#30340; - &#38598;&#25104;&#25439;&#22833;&#30340;&#32852;&#21512;&#20248;&#21270;&#20250;&#23548;&#33268;&#36864;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#38598;&#25104;&#30446;&#26631;&#20998;&#35299;&#20026;&#22522;&#30784;&#23398;&#20064;&#32773;&#30340;&#24378;&#24230;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#32852;&#21512;&#20248;&#21270;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#30784;&#23398;&#20064;&#32773;&#21246;&#32467;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembles of machine learning models have been well established as a powerful method of improving performance over a single model. Traditionally, ensembling algorithms train their base learners independently or sequentially with the goal of optimizing their joint performance. In the case of deep ensembles of neural networks, we are provided with the opportunity to directly optimize the true objective: the joint performance of the ensemble as a whole. Surprisingly, however, directly minimizing the loss of the ensemble appears to rarely be applied in practice. Instead, most previous research trains individual models independently with ensembling performed post hoc. In this work, we show that this is for good reason - joint optimization of ensemble loss results in degenerate behavior. We approach this problem by decomposing the ensemble objective into the strength of the base learners and the diversity between them. We discover that joint optimization results in a phenomenon in which base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2212.10764</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#25490;&#21517;&#30340;&#21015;&#34920;&#32423;&#21035;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#23558;&#22312;&#65288;&#25968;&#25454;&#20016;&#23500;&#65289;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#65288;&#36164;&#28304;&#26377;&#38480;&#65289;&#30446;&#26631;&#39046;&#22495;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#21305;&#37197;&#24182;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#22312;&#25490;&#21517;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#21364;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#20960;&#31181;&#23454;&#29616;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#25490;&#21517;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23457;&#26597;&#20043;&#21069;&#30340;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#23454;&#26045;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#39033;&#30446;&#32423;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32858;&#21512;&#30340;&#25152;&#26377;&#21015;&#34920;&#20013;&#23545;&#36827;&#34892;&#25490;&#21517;&#30340;&#39033;&#30446;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#21015;&#34920;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#21015;&#34920;&#30340;&#32467;&#26500;&#24212;&#35813;&#34987;&#21033;&#29992;&#65292;&#22240;&#20026;&#23427;&#26159;&#25490;&#21517;&#38382;&#39064;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#24230;&#37327;&#26159;&#22312;&#21015;&#34920;&#19978;&#23450;&#20041;&#21644;&#35745;&#31639;&#30340;&#65292;&#32780;&#19981;&#26159;&#22312;&#39033;&#30446;&#26412;&#36523;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10564</link><description>&lt;p&gt;
&#26080;&#35270;&#35273;&#22522;&#32447;&#30340;&#22810;&#27169;&#24335;&#35821;&#27861;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37197;&#23545;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20449;&#21495;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65288;&#22914;MSCOCO&#65289;&#20013;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;LLM&#30340;C-PCFG&#65288;LC-PCFG&#65289;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#27861;&#24402;&#32435;&#24615;&#33021;&#12290;&#19982;&#24102;&#22270;&#20687;&#30340;&#35821;&#27861;&#24402;&#32435;&#30456;&#27604;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;7.9&#20010;&#28857;&#65292;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;85&#65285;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;1.7&#20493;&#12290;&#22312;&#19977;&#20010;&#36741;&#21161;&#35270;&#39057;&#30340;&#35821;&#27861;&#24402;&#32435;&#22522;&#20934;&#20013;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26368;&#22810;7.7&#20010;&#28857;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;8.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2212.02648</link><description>&lt;p&gt;
Spuriosity Rankings: &#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20381;&#36182;&#25152;&#24341;&#36215;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#36827;&#34892;&#26114;&#36149;&#30340;&#25913;&#21464;&#65292;&#32780;&#26159;&#26356;&#22909;&#22320;&#21033;&#29992;&#24050;&#26377;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#36890;&#36807;&#21487;&#35299;&#37322;&#32593;&#32476;&#30340;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#31867;&#20869;&#25490;&#24207;&#65292;&#20197;&#34913;&#37327;&#20854;&#34394;&#20551;&#24615;&#65288;&#21363;&#24120;&#35265;&#34394;&#20551;&#32447;&#32034;&#30340;&#23384;&#22312;&#31243;&#24230;&#65289;&#12290;&#36890;&#36807;&#34394;&#20551;&#24615;&#25490;&#21517;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65288;&#21363;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#65289;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#29978;&#33267;&#21487;&#20197;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#20998;&#31867;&#22836;&#37096;&#65292;&#20197;&#26497;&#23569;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#26469;&#26377;&#25928;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#26356;&#20844;&#27491;&#22788;&#29702;&#65292;&#26080;&#35770;&#34394;&#20551;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#22312;ImageNet&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27880;&#37322;&#20102;5000&#20010;&#31867;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65288;&#20854;&#20013;630&#20010;&#26159;&#34394;&#20551;&#30340;&#65289;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;325k&#20010;&#36719;&#20998;&#21106;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#30340;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#26469;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20998;&#31867;&#22120;&#22312;&#26410;&#30693;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01792</link><description>&lt;p&gt;
&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification by sparse additive models. (arXiv:2212.01792v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01792
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#30340;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#26469;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20998;&#31867;&#22120;&#22312;&#26410;&#30693;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#20110;&#20998;&#31867;&#30340;&#38750;&#21442;&#25968;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#65288;SpAM&#65289;&#12290;SpAM&#20998;&#31867;&#22120;&#30340;&#35774;&#35745;&#22522;&#20110;&#26368;&#23567;&#21270;logistic&#25439;&#22833;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#23637;&#24320;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#26356;&#19968;&#33324;&#30340;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#65288;&#20363;&#22914;&#65292;&#20613;&#37324;&#21494;&#25110;&#23567;&#27874;&#65289;&#12290;&#25152;&#24471;&#30340;&#20998;&#31867;&#22120;&#23545;&#26410;&#30693;&#30340;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#20855;&#26377;&#22266;&#26377;&#30340;&#33258;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#31232;&#30095;&#32452;&#21463;&#38480;&#29305;&#24449;&#20540;&#26465;&#20214;&#19979;&#65292;&#31232;&#30095;&#32452;Lasso&#20998;&#31867;&#22120;&#22312;&#25972;&#20010;&#35299;&#26512;&#12289;Sobolev&#21644;Besov&#31867;&#33539;&#22260;&#20869;&#20960;&#20046;&#26159;&#26368;&#23567;&#21270;&#26497;&#23567;&#65288;&#21152;&#19978;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#32780;&#31232;&#30095;&#32452;Slope&#20998;&#31867;&#22120;&#22312;&#31232;&#30095;&#21644;&#36866;&#24230;&#31264;&#23494;&#35774;&#23450;&#19979;&#36798;&#21040;&#20102;&#30830;&#20999;&#30340;&#26368;&#23567;&#21270;&#26497;&#23567;&#38454;&#25968;&#65288;&#19981;&#21547;&#39069;&#22806;&#30340;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#35813;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#22312;&#23454;&#38469;&#25968;&#25454;&#20363;&#23376;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (nonparametric) sparse additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso and more general sparse group Slope-type penalties on the coefficients of univariate components' expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifiers are inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition the sparse group Lasso classifier is nearly-minimax (up to log-factors) within the entire range of analytic, Sobolev and Besov classes while the sparse group Slope classifier achieves the exact minimax order (without the extra log-factors) for sparse and moderately dense setups. The performance of the proposed classifier is illustrated on the real-data example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32469;&#36807;&#22312;DNN&#25511;&#21046;&#31995;&#32479;&#20013;&#23545;DNN&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;DNN&#20013;&#25554;&#20837;&#19968;&#20010;&#25277;&#35937;&#23618;&#65292;&#23558;&#23454;&#25968;&#25277;&#35937;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#36827;&#32780;&#23454;&#29616;&#23545;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#40657;&#30418;&#21487;&#36798;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2211.11127</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25277;&#35937;&#30340;&#35757;&#32451;&#39535;&#26381;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Taming Reachability Analysis of DNN-Controlled Systems via Abstraction-Based Training. (arXiv:2211.11127v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32469;&#36807;&#22312;DNN&#25511;&#21046;&#31995;&#32479;&#20013;&#23545;DNN&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;DNN&#20013;&#25554;&#20837;&#19968;&#20010;&#25277;&#35937;&#23618;&#65292;&#23558;&#23454;&#25968;&#25277;&#35937;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#36827;&#32780;&#23454;&#29616;&#23545;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#40657;&#30418;&#21487;&#36798;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#20351;&#24471;&#39564;&#35777;&#32593;&#32476;&#26412;&#36523;&#21644;&#25176;&#31649;DNN&#25511;&#21046;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#38754;&#20020;&#30456;&#21516;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#23545;DNN&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#25928;&#29575;&#20302;&#19979;&#24182;&#19988;&#36807;&#24230;&#20272;&#35745;&#36739;&#22823;&#65292;&#24182;&#19988;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;DNNs&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#32469;&#36807;&#22312;&#21487;&#36798;&#24615;&#20998;&#26512;&#20013;&#23545;DNNs&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#25277;&#35937;&#23618;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;DNNs&#65292;&#35813;&#25277;&#35937;&#23618;&#23558;&#23454;&#25968;&#25277;&#35937;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;&#36827;&#34892;&#35757;&#32451;&#12290;&#25554;&#20837;&#30340;&#25277;&#35937;&#23618;&#30830;&#20445;&#21306;&#38388;&#34920;&#31034;&#30340;&#20540;&#22312;&#35757;&#32451;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#32593;&#32476;&#19981;&#21487;&#21306;&#20998;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#40657;&#30418;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20854;&#20013;&#21482;&#23545;&#35757;&#32451;&#36807;&#30340;DNN&#36827;&#34892;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intrinsic complexity of deep neural networks (DNNs) makes it challenging to verify not only the networks themselves but also the hosting DNN-controlled systems. Reachability analysis of these systems faces the same challenge. Existing approaches rely on over-approximating DNNs using simpler polynomial models. However, they suffer from low efficiency and large overestimation, and are restricted to specific types of DNNs. This paper presents a novel abstraction-based approach to bypass the crux of over-approximating DNNs in reachability analysis. Specifically, we extend conventional DNNs by inserting an additional abstraction layer, which abstracts a real number to an interval for training. The inserted abstraction layer ensures that the values represented by an interval are indistinguishable to the network for both training and decision-making. Leveraging this, we devise the first black-box reachability analysis approach for DNN-controlled systems, where trained DNNs are only querie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#26041;&#27861;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#37051;&#25509;&#30697;&#38453;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;GNN&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#24615;&#33021;&#30456;&#24403;&#30340;&#29616;&#35937;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2210.09809</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#65292;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#26041;&#27861;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#37051;&#25509;&#30697;&#38453;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;GNN&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#24615;&#33021;&#30456;&#24403;&#30340;&#29616;&#35937;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#36890;&#36807;&#20351;&#29992;&#8220;&#22270;&#21367;&#31215;&#8221;&#26469;&#32858;&#21512;&#30456;&#37051;&#33410;&#28857;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#26550;&#26500;&#65288;&#20363;&#22914;&#28145;&#24230;&#21644;&#28608;&#27963;&#20989;&#25968;&#65289;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#27599;&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#21367;&#31215;&#24050;&#25104;&#20026;&#20027;&#27969;&#36873;&#25321;&#65292;&#20854;&#20013;&#23545;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#23545;&#31216;&#24402;&#19968;&#21270;&#26159;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#34892;&#24402;&#19968;&#21270;&#30340;&#37051;&#25509;&#30697;&#38453;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#23613;&#31649;GNN&#30340;&#20351;&#29992;&#38750;&#24120;&#24191;&#27867;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#20851;&#20110;&#36825;&#20123;&#21367;&#31215;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#26080;&#27861;&#35299;&#37322;&#36825;&#31181;&#34892;&#20026;&#12290;&#21516;&#26679;&#65292;&#32447;&#24615;GNN&#30340;&#24615;&#33021;&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#30340;&#24615;&#33021;&#30456;&#24403;&#30340;&#32463;&#39564;&#35266;&#23519;&#20063;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20449;&#24687;&#35770;&#24230;&#37327;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#22810;&#26679;&#24615;&#65292;&#21457;&#29616;&#20854;&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35760;&#24518;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26410;&#26631;&#35760;&#30340;&#20998;&#24067;&#20869;&#37096;&#35745;&#31639;&#31070;&#32463;&#28608;&#27963;&#26102;&#65292;&#20449;&#24687;&#30340;&#32452;&#32455;&#20063;&#21487;&#20197;&#25351;&#21521;&#20004;&#31181;&#24418;&#24335;&#30340;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2210.09404</link><description>&lt;p&gt;
&#20449;&#24687;&#24230;&#37327;&#21453;&#26144;&#35760;&#24518;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Measures of Information Reflect Memorization Patterns. (arXiv:2210.09404v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20449;&#24687;&#35770;&#24230;&#37327;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#22810;&#26679;&#24615;&#65292;&#21457;&#29616;&#20854;&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35760;&#24518;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26410;&#26631;&#35760;&#30340;&#20998;&#24067;&#20869;&#37096;&#35745;&#31639;&#31070;&#32463;&#28608;&#27963;&#26102;&#65292;&#20449;&#24687;&#30340;&#32452;&#32455;&#20063;&#21487;&#20197;&#25351;&#21521;&#20004;&#31181;&#24418;&#24335;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21033;&#29992;&#19982;&#30446;&#26631;&#26631;&#31614;&#20849;&#29616;&#30340;&#38169;&#35823;&#20551;&#35937;&#65288;&#25110;&#25463;&#24452;&#65289;&#26469;&#23637;&#31034;&#21551;&#21457;&#24335;&#35760;&#24518;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#20250;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#20135;&#29983;&#22522;&#20110;&#31034;&#20363;&#30340;&#35760;&#24518;&#12290;&#36825;&#20123;&#35760;&#24518;&#31867;&#22411;&#38459;&#30861;&#20102;&#32593;&#32476;&#22312;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26816;&#27979;&#27492;&#31867;&#35760;&#24518;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#23450;&#21046;&#30340;&#27979;&#35797;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#8212;&#8212;&#24182;&#38543;&#21518;&#23637;&#31034;&#20102;&#8212;&#8212;&#19981;&#21516;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#24230;&#37327;&#37327;&#21270;&#31070;&#32463;&#28608;&#27963;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#28085;&#30422;&#20102;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#25903;&#25345;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#30340;&#32452;&#32455;&#25351;&#21521;&#20102;&#36825;&#20004;&#31181;&#24418;&#24335;&#30340;&#35760;&#24518;&#65292;&#21363;&#20351;&#26159;&#22312;&#26410;&#26631;&#35760;&#30340;&#20998;&#24067;&#20869;&#37096;&#35745;&#31639;&#30340;&#31070;&#32463;&#28608;&#27963;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize -- and subsequently show -- that the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis on experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabelled in-distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.14790</link><description>&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#38598;&#26041;&#24046;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#36825;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#31232;&#30095;&#24615;&#21644;&#27491;&#20132;&#24615;&#32422;&#26463;&#30340;&#20984;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#19968;&#20010;&#31232;&#30095;&#20027;&#25104;&#20998;&#24182;&#32553;&#20943;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20294;&#22312;&#23547;&#25214;&#22810;&#20010;&#30456;&#20114;&#27491;&#20132;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#25152;&#24471;&#35299;&#30340;&#27491;&#20132;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#21152;&#24378;&#19978;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#26469;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
&lt;/p&gt;</description></item><item><title>EffEval&#26159;&#19968;&#31181;&#23545;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#25928;&#29575;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;TinyBERT&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;CPU&#21152;&#36895;&#27604;GPU&#26356;&#26174;&#33879;&#65292;WMD&#36817;&#20284;&#27809;&#26377;&#25552;&#39640;&#25928;&#29575;&#20294;&#38477;&#20302;&#20102;&#36136;&#37327;&#65292;&#36866;&#37197;&#22120;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#26631;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.09593</link><description>&lt;p&gt;
EffEval:&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#25928;&#29575;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics. (arXiv:2209.09593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09593
&lt;/p&gt;
&lt;p&gt;
EffEval&#26159;&#19968;&#31181;&#23545;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#25928;&#29575;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;TinyBERT&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;CPU&#21152;&#36895;&#27604;GPU&#26356;&#26174;&#33879;&#65292;WMD&#36817;&#20284;&#27809;&#26377;&#25552;&#39640;&#25928;&#29575;&#20294;&#38477;&#20302;&#20102;&#36136;&#37327;&#65292;&#36866;&#37197;&#22120;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#26631;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25928;&#29575;&#26159;&#20419;&#36827;&#21253;&#23481;&#24615;&#21644;&#20943;&#23569;&#29615;&#22659;&#25104;&#26412;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;LLM&#26102;&#20195;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#30340;&#25928;&#29575;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#29992;&#36731;&#37327;&#32423;&#26367;&#20195;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;transformers&#65292;&#24182;&#22312;LLM&#34920;&#31034;&#20043;&#19978;&#37319;&#29992;&#32447;&#24615;&#21644;&#20108;&#27425;&#36817;&#20284;&#30340;&#23545;&#40784;&#31639;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#20010;&#65288;&#26080;&#21442;&#32771;&#21644;&#26377;&#21442;&#32771;&#65289;&#25351;&#26631;&#22312;&#19977;&#20010;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#26816;&#26597;&#20102;16&#20010;&#36731;&#37327;&#32423;transformers&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#26469;&#30740;&#31350;COMET&#31561;&#25351;&#26631;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;a&#65289;TinyBERT&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#65288;b&#65289;CPU&#21152;&#36895;&#27604;GPU&#26356;&#26174;&#33879;&#65292;&#65288;c&#65289;WMD&#36817;&#20284;&#27809;&#26377;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#38477;&#20302;&#20102;&#36136;&#37327;&#65292;&#65288;d&#65289;&#36866;&#37197;&#22120;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#65288;&#20851;&#20110;&#21453;&#21521;&#20256;&#25773;&#36895;&#24230;&#21644;&#20869;&#23384;&#35201;&#27714;&#65289;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#26631;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter alternatives and employing linear and quadratic approximations for alignment algorithms on top of LLM representations. We evaluate six (reference-free and reference-based) metrics across three MT datasets and examine 16 lightweight transformers. In addition, we look into the training efficiency of metrics like COMET by utilizing adapters. Our results indicate that (a) TinyBERT provides the optimal balance between quality and efficiency, (b) CPU speed-ups are more substantial than those on GPU; (c) WMD approximations yield no efficiency gains while reducing quality and (d) adapters enhance training efficiency (regarding backward pass speed and memory requirements) as well as, in some cases, metric qualit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#23384;&#22312;&#20559;&#24046;&#30340;&#27491;&#21521;&#26080;&#26631;&#35760;&#25968;&#25454;&#12290;&#36890;&#36807;&#36991;&#20813;&#20551;&#35774;&#20542;&#21521;&#24471;&#20998;&#20989;&#25968;&#20026;&#24120;&#25968;&#65292;&#20316;&#32773;&#20849;&#21516;&#20272;&#35745;&#21518;&#39564;&#27010;&#29575;&#21644;&#20542;&#21521;&#24471;&#20998;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#26696;&#30340;&#26041;&#27861;&#30456;&#27604;&#26159;&#21487;&#27604;&#36739;&#25110;&#26356;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2209.07787</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#36923;&#36753;&#22238;&#24402;&#30340;&#26377;&#20559;&#27491;&#26080;&#26631;&#35760;&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Double logistic regression approach to biased positive-unlabeled data. (arXiv:2209.07787v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#23384;&#22312;&#20559;&#24046;&#30340;&#27491;&#21521;&#26080;&#26631;&#35760;&#25968;&#25454;&#12290;&#36890;&#36807;&#36991;&#20813;&#20551;&#35774;&#20542;&#21521;&#24471;&#20998;&#20989;&#25968;&#20026;&#24120;&#25968;&#65292;&#20316;&#32773;&#20849;&#21516;&#20272;&#35745;&#21518;&#39564;&#27010;&#29575;&#21644;&#20542;&#21521;&#24471;&#20998;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#26696;&#30340;&#26041;&#27861;&#30456;&#27604;&#26159;&#21487;&#27604;&#36739;&#25110;&#26356;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21521;&#21644;&#26080;&#26631;&#35760;&#23398;&#20064;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#33258;&#28982;&#20135;&#29983;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#22312;&#20110;&#20551;&#35774;&#20542;&#21521;&#24471;&#20998;&#20989;&#25968;&#26159;&#24120;&#25968;&#65288;SCAR&#20551;&#35774;&#65289;&#65292;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#20551;&#35774;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20849;&#21516;&#20272;&#35745;&#21518;&#39564;&#27010;&#29575;&#21644;&#20542;&#21521;&#24471;&#20998;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#24403;&#20004;&#20010;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#21442;&#25968;&#24418;&#24335;&#26102;&#65288;&#20363;&#22914;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#30340;&#36923;&#36753;&#20989;&#25968;&#65289;&#65292;&#30456;&#24212;&#30340;&#21442;&#25968;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#26041;&#27861;&#65306;&#32852;&#21512;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#21644;&#22522;&#20110;&#20004;&#20010;Fisher&#19968;&#33268;&#34920;&#36798;&#24335;&#30340;&#20132;&#26367;&#26368;&#22823;&#21270;&#30340;&#31532;&#20108;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#26696;&#30340;&#29616;&#26377;&#26041;&#27861;&#21487;&#27604;&#36739;&#25110;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive and unlabelled learning is an important problem which arises naturally in many applications. The significant limitation of almost all existing methods lies in assuming that the propensity score function is constant (SCAR assumption), which is unrealistic in many practical situations. Avoiding this assumption, we consider parametric approach to the problem of joint estimation of posterior probability and propensity score functions. We show that under mild assumptions when both functions have the same parametric form (e.g. logistic with different parameters) the corresponding parameters are identifiable. Motivated by this, we propose two approaches to their estimation: joint maximum likelihood method and the second approach based on alternating maximization of two Fisher consistent expressions. Our experimental results show that the proposed methods are comparable or better than the existing methods based on Expectation-Maximisation scheme.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#30340;&#20998;&#24067;&#24335;&#26041;&#26696;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#20998;&#24067;&#24335;OMP&#26041;&#26696;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#20449;&#22122;&#27604;&#19979;&#23454;&#29616;&#32447;&#24615;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#24182;&#33021;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2209.07230</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;OMP&#30340;&#24674;&#22797;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Recovery Guarantees for Distributed-OMP. (arXiv:2209.07230v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#30340;&#20998;&#24067;&#24335;&#26041;&#26696;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#20998;&#24067;&#24335;OMP&#26041;&#26696;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#20449;&#22122;&#27604;&#19979;&#23454;&#29616;&#32447;&#24615;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#24182;&#33021;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;(OMP)&#30340;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#29305;&#21035;&#36866;&#29992;&#20110;&#26377;&#35745;&#31639;&#21644;&#36890;&#20449;&#38480;&#21046;&#30340;&#26411;&#31471;&#26426;&#22120;&#36830;&#25509;&#21040;&#20013;&#22830;&#34701;&#21512;&#20013;&#24515;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#20998;&#24067;&#24335;OMP&#26041;&#26696;&#33021;&#22815;&#20197;&#19982;&#31232;&#30095;&#24230;&#32447;&#24615;&#21644;&#32500;&#24230;&#23545;&#25968;&#25104;&#27604;&#20363;&#30340;&#36890;&#20449;&#24674;&#22797;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#20449;&#22122;&#27604;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#20010;&#26426;&#22120;&#20063;&#26080;&#27861;&#26816;&#27979;&#21040;&#25903;&#25345;&#26102;&#65292;&#36825;&#20173;&#28982;&#25104;&#31435;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#24067;&#24335;OMP&#26041;&#26696;&#19982;&#26356;&#35745;&#31639;&#23494;&#38598;&#30340;&#26041;&#27861;&#31454;&#20105;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributed schemes for high-dimensional sparse linear regression, based on orthogonal matching pursuit (OMP). Such schemes are particularly suited for settings where a central fusion center is connected to end machines, that have both computation and communication limitations. We prove that under suitable assumptions, distributed-OMP schemes recover the support of the regression vector with communication per machine linear in its sparsity and logarithmic in the dimension. Remarkably, this holds even at low signal-to-noise-ratios, where individual machines are unable to detect the support. Our simulations show that distributed-OMP schemes are competitive with more computationally intensive methods, and in some cases even outperform them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25512;&#23548;&#20102;&#26368;&#20248;&#21464;&#25442;&#24182;&#35777;&#26126;&#20102;&#24403;&#20854;&#21487;&#35266;&#27979;&#26102;&#38745;&#27490;&#28857;&#30340;&#21807;&#19968;&#24615;&#65292;&#20174;&#32780;&#20026;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#21160;&#24577;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26368;&#20248;&#24615;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2209.05042</link><description>&lt;p&gt;
&#20851;&#20110;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#30340;&#20248;&#21270;&#26223;&#35266;: &#22522;&#20110;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator. (arXiv:2209.05042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25512;&#23548;&#20102;&#26368;&#20248;&#21464;&#25442;&#24182;&#35777;&#26126;&#20102;&#24403;&#20854;&#21487;&#35266;&#27979;&#26102;&#38745;&#27490;&#28857;&#30340;&#21807;&#19968;&#24615;&#65292;&#20174;&#32780;&#20026;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#21160;&#24577;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26368;&#20248;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25910;&#25947;&#21462;&#20915;&#20110;&#22522;&#30784;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#20248;&#21270;&#26223;&#35266;&#12290;&#36890;&#36807;&#20998;&#26512;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#36825;&#20123;&#31639;&#27861;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#22823;&#22810;&#21482;&#32771;&#34385;&#38745;&#24577;&#20840;&#29366;&#24577;&#25110;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#65288;&#25511;&#21046;&#22120;&#65289;&#30340;&#20248;&#21270;&#26223;&#35266;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#65288;&#31616;&#31216; dLQR&#65289;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#20854;&#20248;&#21270;&#26223;&#35266;&#30456;&#23545;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102; dLQR &#25104;&#26412;&#22914;&#20309;&#38543;&#21160;&#24577;&#25511;&#21046;&#22120;&#30340;&#22352;&#26631;&#21464;&#25442;&#32780;&#21464;&#21270;&#65292;&#28982;&#21518;&#25512;&#23548;&#20102;&#32473;&#23450;&#21487;&#35266;&#25511;&#31283;&#23450;&#25511;&#21046;&#22120;&#30340;&#26368;&#20248;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#26680;&#24515;&#32467;&#26524;&#26159; dLQR &#22312;&#21487;&#35266;&#27979;&#26102;&#38745;&#27490;&#28857;&#30340;&#21807;&#19968;&#24615;&#65292;&#36825;&#20026;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#21160;&#24577;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26368;&#20248;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of policy gradient algorithms hinges on the optimization landscape of the underlying optimal control problem. Theoretical insights into these algorithms can often be acquired from analyzing those of linear quadratic control. However, most of the existing literature only considers the optimization landscape for static full-state or output feedback policies (controllers). We investigate the more challenging case of dynamic output-feedback policies for linear quadratic regulation (abbreviated as dLQR), which is prevalent in practice but has a rather complicated optimization landscape. We first show how the dLQR cost varies with the coordinate transformation of the dynamic controller and then derive the optimal transformation for a given observable stabilizing controller. One of our core results is the uniqueness of the stationary point of dLQR when it is observable, which provides an optimality certificate for solving dynamic controllers using policy gradient methods. More
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#25913;&#36827;&#23545;&#35937;&#26816;&#27979;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#31649;&#32447;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#30340;&#25913;&#36827;&#21644;&#23545;&#22270;&#20687;&#38590;&#24230;&#35780;&#20998;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#29983;&#25104;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#35937;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.02564</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#23545;&#35937;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Progressive Domain Adaptation with Contrastive Learning for Object Detection in the Satellite Imagery. (arXiv:2209.02564v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#25913;&#36827;&#23545;&#35937;&#26816;&#27979;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#31649;&#32447;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#30340;&#25913;&#36827;&#21644;&#23545;&#22270;&#20687;&#38590;&#24230;&#35780;&#20998;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#29983;&#25104;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#35937;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#22312;&#21355;&#26143;&#21644;&#26080;&#20154;&#26426;&#22270;&#20687;&#19978;&#24212;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#35782;&#21035;&#23567;&#32780;&#23494;&#38598;&#30340;&#23545;&#35937;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#30001;&#20110;&#25293;&#25668;&#30340;&#22320;&#38754;&#21306;&#22495;&#21644;&#37319;&#38598;&#26465;&#20214;&#30340;&#39640;&#21464;&#24322;&#24615;&#65292;&#23548;&#33268;&#33322;&#25293;&#22270;&#20687;&#20013;&#30340;&#20869;&#23481;&#21464;&#24322;&#24615;&#22823;&#12290;&#21478;&#19968;&#20010;&#21407;&#22240;&#26159;&#33322;&#25293;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#25968;&#37327;&#21644;&#22823;&#23567;&#19982;&#28040;&#36153;&#32773;&#25968;&#25454;&#22823;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#29289;&#20307;&#26816;&#27979;&#31649;&#32447;&#65292;&#36890;&#36807;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#12289;&#36328;&#23618;&#37096;&#20998;&#32593;&#32476;&#12289;&#22522;&#20110;&#28909;&#22270;&#30340;&#21306;&#22495;&#20505;&#36873;&#32593;&#32476;&#20197;&#21450;&#36890;&#36807;&#26032;&#39062;&#30340;&#22270;&#20687;&#38590;&#24230;&#35780;&#20998;&#26469;&#36866;&#24212;&#25972;&#20307;&#28966;&#28857;&#25439;&#22833;&#24230;&#37327;&#30340;&#23545;&#35937;&#23450;&#20301;&#21644;&#35782;&#21035;&#65292;&#25913;&#36827;&#20102;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#22312;&#33322;&#25293;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20351;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#32452;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#20943;&#36731;&#23545;&#35937;&#35782;&#21035;&#25928;&#26524;&#30340;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art object detection methods applied to satellite and drone imagery largely fail to identify small and dense objects. One reason is the high variability of content in the overhead imagery due to the terrestrial region captured and the high variability of acquisition conditions. Another reason is that the number and size of objects in aerial imagery are very different than in the consumer data. In this work, we propose a small object detection pipeline that improves the feature extraction process by spatial pyramid pooling, cross-stage partial networks, heatmap-based region proposal network, and object localization and identification through a novel image difficulty score that adapts the overall focal loss measure based on the image difficulty. Next, we propose novel contrastive learning with progressive domain adaptation to produce domain-invariant features across aerial datasets using local and global components. We show we can alleviate the degradation of object identifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;R\'enyi&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#20013;&#36890;&#36807;&#27927;&#29260;&#25552;&#20986;&#20102;&#26356;&#24378;&#38544;&#31169;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#29702;&#35770;&#21644;&#25968;&#20540;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2208.04591</link><description>&lt;p&gt;
R\'enyi&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#20013;&#36890;&#36807;&#27927;&#29260;&#23454;&#29616;&#26356;&#24378;&#38544;&#31169;&#25918;&#22823;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Stronger Privacy Amplification by Shuffling for R\'enyi and Approximate Differential Privacy. (arXiv:2208.04591v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;R\'enyi&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#20013;&#36890;&#36807;&#27927;&#29260;&#25552;&#20986;&#20102;&#26356;&#24378;&#38544;&#31169;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#29702;&#35770;&#21644;&#25968;&#20540;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#27927;&#29260;&#27169;&#22411;&#20316;&#20026;&#26631;&#20934;&#26412;&#22320;&#21644;&#38598;&#20013;&#27169;&#22411;&#20043;&#38388;&#30340;&#19968;&#31181;&#20013;&#38388;&#20449;&#20219;&#27169;&#22411;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#26159;&#65292;&#38543;&#26426;&#27927;&#29260;&#26412;&#22320;&#38543;&#26426;&#21270;&#25968;&#25454;&#21487;&#20197;&#25918;&#22823;&#24046;&#20998;&#38544;&#31169;&#30340;&#20445;&#35777;&#12290;&#36825;&#31181;&#25918;&#22823;&#25928;&#26524;&#24847;&#21619;&#30528;&#23545;&#20110;&#21311;&#21517;&#36129;&#29486;&#25968;&#25454;&#30340;&#31995;&#32479;&#32780;&#35328;&#65292;&#38544;&#31169;&#20445;&#35777;&#20250;&#26356;&#21152;&#24378;&#22823;&#12290;&#26412;&#30740;&#31350;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#25913;&#36827;&#20102;&#27927;&#29260;&#23548;&#33268;&#30340;&#38544;&#31169;&#25918;&#22823;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;LDP&#38543;&#26426;&#21270;&#22120;&#27927;&#29260;&#36755;&#20986;&#30340;R\'enyi&#24046;&#20998;&#38544;&#31169;&#21442;&#25968;&#30340;&#28176;&#36817;&#26368;&#20248;&#20998;&#26512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#38544;&#31169;&#25918;&#22823;&#25928;&#26524;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#25913;&#36827;&#20102;[FMT20]&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;&#25152;&#26377;&#21442;&#25968;&#35774;&#32622;&#19979;&#24471;&#21040;&#20102;&#26356;&#32039;&#23494;&#30340;&#25968;&#20540;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shuffle model of differential privacy has gained significant interest as an intermediate trust model between the standard local and central models [EFMRTT19; CSUZZ19]. A key result in this model is that randomly shuffling locally randomized data amplifies differential privacy guarantees. Such amplification implies substantially stronger privacy guarantees for systems in which data is contributed anonymously [BEMMRLRKTS17].  In this work, we improve the state of the art privacy amplification by shuffling results both theoretically and numerically. Our first contribution is the first asymptotically optimal analysis of the R\'enyi differential privacy parameters for the shuffled outputs of LDP randomizers. Our second contribution is a new analysis of privacy amplification by shuffling. This analysis improves on the techniques of [FMT20] and leads to tighter numerical bounds in all parameter settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#21010;&#26102;&#38388;&#20869;&#26080;&#27861;&#22788;&#29702;&#30340;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102; $O(L+\sqrt{T})$ &#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2207.04550</link><description>&lt;p&gt;
&#24102;&#26377;&#20002;&#22833;&#38144;&#21806;&#21644;&#19981;&#30830;&#23450;&#20379;&#24212;&#30340;&#24211;&#23384;&#31995;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Order for Inventory Systems with Lost Sales and Uncertain Supplies. (arXiv:2207.04550v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#21010;&#26102;&#38388;&#20869;&#26080;&#27861;&#22788;&#29702;&#30340;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102; $O(L+\sqrt{T})$ &#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#35745;&#21010;&#26102;&#38388; $T$ &#20869;&#20855;&#26377;&#23548;&#21521;&#26102;&#38388; $L$ &#30340;&#38543;&#26426;&#20002;&#22833;&#38144;&#21806;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#12290;&#30001;&#20110;&#38543;&#26426;&#20135;&#37327;/&#20135;&#33021;&#31561;&#21407;&#22240;&#65292;&#20379;&#24212;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#26159;&#35746;&#21333;&#25968;&#37327;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#26088;&#22312;&#26368;&#23567;&#21270; $T$ &#26399;&#25104;&#26412;&#65292;&#20294;&#21363;&#20351;&#22312;&#38656;&#27714;&#21644;&#20379;&#24212;&#20998;&#24067;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;&#35745;&#31639;&#26080;&#27861;&#22788;&#29702;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#38656;&#27714;&#21644;&#20379;&#24212;&#20998;&#24067;&#37117;&#26410;&#30693;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403; $L\geq\log(T)$ &#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102; $O(L+\sqrt{T})$ &#30340;&#21518;&#24724;&#65288;&#21363;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#26412;&#21644; $T$ &#26399;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65289;&#12290;&#25105;&#20204;&#26159;&#36890;&#36807; 1&#65289;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#26412;&#27604;&#23436;&#20840;&#20449;&#24687;&#19979;&#19968;&#20010;&#26368;&#20248;&#24120;&#25968;&#35746;&#21333;&#31574;&#30053;&#39640;&#33267;&#22810; $O(L+\sqrt{T})$&#65288;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65289;; 2&#65289;&#21033;&#29992;&#20854;&#24050;&#30693;&#30340;&#24615;&#33021;&#20445;&#35777;&#20174;&#29616;&#26377;&#30340;&#25991;&#23398;&#20013;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a stochastic lost-sales inventory control system with a lead time $L$ over a planning horizon $T$. Supply is uncertain, and is a function of the order quantity (due to random yield/capacity, etc). We aim to minimize the $T$-period cost, a problem that is known to be computationally intractable even under known distributions of demand and supply. In this paper, we assume that both the demand and supply distributions are unknown and develop a computationally efficient online learning algorithm. We show that our algorithm achieves a regret (i.e. the performance gap between the cost of our algorithm and that of an optimal policy over $T$ periods) of $O(L+\sqrt{T})$ when $L\geq\log(T)$. We do so by 1) showing our algorithm cost is higher by at most $O(L+\sqrt{T})$ for any $L\geq 0$ compared to an optimal constant-order policy under complete information (a well-known and widely-used algorithm) and 2) leveraging its known performance guarantee from the existing literature. To the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.02016</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35748;&#20026;&#22312;&#29615;&#22659;&#25200;&#21160;&#19979;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22312;&#20540;&#20989;&#25968;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#31561;&#20215;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#36716;&#25442;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23613;&#31649;&#27491;&#21017;&#21270;-&#40065;&#26834;&#24615;&#36716;&#25442;&#22240;&#20854;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#22120;&#65288;USR&#65289;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#20989;&#25968;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#23454;&#29616;&#12290;&#29305;&#21035;&#26159;&#65292;USR&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;RL&#26694;&#26550;&#20013;&#12290;&#20026;&#20102;&#22788;&#29702;&#26410;&#30693;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20989;&#25968;&#29983;&#25104;&#30340;&#26032;&#39062;&#23545;&#25239;&#26041;&#27861;&#26469;&#29983;&#25104;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#65288;RWRL&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;USR&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i
&lt;/p&gt;</description></item><item><title>Mixup is a popular data augmentation method that synthesizes extra samples. This paper explores the potential of Mixup to generate in-domain samples as universum negatives in supervised contrastive learning, providing high-quality hard negatives and reducing the need for large batch sizes. The proposed UniCon incorporates Mixup strategy to generate Mixup-induced universum as negatives.</title><link>http://arxiv.org/abs/2204.10695</link><description>&lt;p&gt;
&#21463;Universum&#23398;&#20064;&#21551;&#21457;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Universum-inspired Supervised Contrastive Learning. (arXiv:2204.10695v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10695
&lt;/p&gt;
&lt;p&gt;
Mixup is a popular data augmentation method that synthesizes extra samples. This paper explores the potential of Mixup to generate in-domain samples as universum negatives in supervised contrastive learning, providing high-quality hard negatives and reducing the need for large batch sizes. The proposed UniCon incorporates Mixup strategy to generate Mixup-induced universum as negatives.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;Mixup&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#21512;&#25104;&#20102;&#22823;&#37327;&#30340;&#26679;&#26412;&#12290;&#23613;&#31649; Mixup &#22312;&#29702;&#35770;&#19978;&#20381;&#36182;&#20110;&#25968;&#25454;&#23646;&#24615;&#65292;&#20294;&#25454;&#25253;&#36947;&#65292;&#23427;&#20316;&#20026;&#19968;&#31181;&#35268;&#21017;&#21270;&#22120;&#21644;&#26657;&#20934;&#22120;&#22312;&#28145;&#24230;&#27169;&#22411;&#35757;&#32451;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20026;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#36129;&#29486;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#20351;&#29992;&#24102;&#26377;&#35838;&#22806;&#26679;&#26412;&#36741;&#21161;&#30446;&#26631;&#20219;&#21153;&#30340; Universum &#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#34987;&#24191;&#27867;&#24573;&#35270;&#30340;&#35282;&#24230;&#23545; Mixup &#36827;&#34892;&#20102;&#30740;&#31350; - &#21033;&#29992;&#20854;&#29983;&#25104;&#19981;&#23646;&#20110;&#30446;&#26631;&#31867;&#21035;&#30340;&#22495;&#20869;&#26679;&#26412;&#65292;&#21363; universum&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807; Mixup &#20135;&#29983;&#30340; universum &#21487;&#20197;&#20316;&#20026;&#39640;&#36136;&#37327;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#65292;&#26497;&#22823;&#22320;&#20943;&#36731;&#20102;&#23545;&#22823;&#25209;&#27425;&#22823;&#23567;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#38656;&#35201;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;Universum&#23398;&#20064;&#21551;&#21457;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;UniCon&#65289;&#65292;&#23427;&#23558;Mixup&#31574;&#30053;&#24212;&#29992;&#20110;&#20135;&#29983;Mixup&#35825;&#23548;&#30340;universum&#20316;&#20026;&#36127;&#21521;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an effective data augmentation method, Mixup synthesizes an extra amount of samples through linear interpolations. Despite its theoretical dependency on data properties, Mixup reportedly performs well as a regularizer and calibrator contributing reliable robustness and generalization to deep model training. In this paper, inspired by Universum Learning which uses out-of-class samples to assist the target tasks, we investigate Mixup from a largely under-explored perspective - the potential to generate in-domain samples that belong to none of the target classes, that is, universum. We find that in the framework of supervised contrastive learning, Mixup-induced universum can serve as surprisingly high-quality hard negatives, greatly relieving the need for large batch sizes in contrastive learning. With these findings, we propose Universum-inspired supervised Contrastive learning (UniCon), which incorporates Mixup strategy to generate Mixup-induced universum as universum negatives and p
&lt;/p&gt;</description></item><item><title>DAMNETS&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#35774;&#35745;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2203.15009</link><description>&lt;p&gt;
DAMNETS&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DAMNETS: A Deep Autoregressive Model for Generating Markovian Network Time Series. (arXiv:2203.15009v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15009
&lt;/p&gt;
&lt;p&gt;
DAMNETS&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#35774;&#35745;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;&#20063;&#31216;&#20026;&#21160;&#24577;&#22270;&#65289;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#32463;&#27982;&#23398;&#31561;&#39046;&#22495;&#65292;&#20854;&#20013;&#22797;&#26434;&#30340;&#22522;&#20110;&#22270;&#30340;&#21160;&#24577;&#26159;&#26680;&#24515;&#30740;&#31350;&#23545;&#35937;&#12290;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#20197;&#21450;&#34920;&#31034;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#36793;&#38469;&#32593;&#32476;&#32467;&#26500;&#30340;&#38656;&#35201;&#65292;&#35774;&#35745;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DAMNETS&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#25152;&#26377;&#26679;&#26412;&#36136;&#37327;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models for network time series (also known as dynamic graphs) have tremendous potential in fields such as epidemiology, biology and economics, where complex graph-based dynamics are core objects of study. Designing flexible and scalable generative models is a very challenging task due to the high dimensionality of the data, as well as the need to represent temporal dependencies and marginal network structure. Here we introduce DAMNETS, a scalable deep generative model for network time series. DAMNETS outperforms competing methods on all of our measures of sample quality, over both real and synthetic data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Label Hierarchy Transition (LHT)&#26694;&#26550;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#25913;&#36827;&#23618;&#27425;&#20998;&#31867;&#12290;LHT&#26694;&#26550;&#20027;&#35201;&#21253;&#25324;&#36716;&#25442;&#32593;&#32476;&#21644;&#28151;&#28102;&#25439;&#22833;&#20004;&#20010;&#37096;&#20998;&#65292;&#36890;&#36807;&#26174;&#24335;&#23398;&#20064;&#26631;&#31614;&#23618;&#27425;&#36716;&#25442;&#30697;&#38453;&#21644;&#40723;&#21169;&#20998;&#31867;&#32593;&#32476;&#22788;&#29702;&#28151;&#28102;&#24773;&#20917;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.02353</link><description>&lt;p&gt;
&#26631;&#31614;&#23618;&#32423;&#36716;&#25442;&#65306;&#28145;&#20837;&#30740;&#31350;&#31867;&#23618;&#27425;&#32467;&#26500;&#20197;&#22686;&#24378;&#28145;&#24230;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Label Hierarchy Transition: Delving into Class Hierarchies to Enhance Deep Classifiers. (arXiv:2112.02353v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Label Hierarchy Transition (LHT)&#26694;&#26550;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#25913;&#36827;&#23618;&#27425;&#20998;&#31867;&#12290;LHT&#26694;&#26550;&#20027;&#35201;&#21253;&#25324;&#36716;&#25442;&#32593;&#32476;&#21644;&#28151;&#28102;&#25439;&#22833;&#20004;&#20010;&#37096;&#20998;&#65292;&#36890;&#36807;&#26174;&#24335;&#23398;&#20064;&#26631;&#31614;&#23618;&#27425;&#36716;&#25442;&#30697;&#38453;&#21644;&#40723;&#21169;&#20998;&#31867;&#32593;&#32476;&#22788;&#29702;&#28151;&#28102;&#24773;&#20917;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#20998;&#31867;&#26088;&#22312;&#23558;&#23545;&#35937;&#25353;&#29031;&#31867;&#21035;&#30340;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#25490;&#24207;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23558;&#20854;&#35299;&#32806;&#20026;&#19968;&#31995;&#21015;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#26469;&#22788;&#29702;&#23618;&#27425;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#19981;&#21516;&#23618;&#32423;&#20043;&#38388;&#21508;&#20010;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#19968;&#27010;&#29575;&#26694;&#26550;Label Hierarchy Transition (LHT)&#65292;&#20197;&#24212;&#23545;&#23618;&#27425;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;LHT&#26694;&#26550;&#30001;&#19968;&#20010;&#36716;&#25442;&#32593;&#32476;&#21644;&#19968;&#20010;&#28151;&#28102;&#25439;&#22833;&#32452;&#25104;&#12290;&#36716;&#25442;&#32593;&#32476;&#19987;&#27880;&#20110;&#26174;&#24335;&#23398;&#20064;&#26631;&#31614;&#23618;&#27425;&#36716;&#25442;&#30697;&#38453;&#65292;&#36825;&#26377;&#21161;&#20110;&#26377;&#25928;&#22320;&#32534;&#30721;&#23884;&#20837;&#22312;&#31867;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#28151;&#28102;&#25439;&#22833;&#40723;&#21169;&#20998;&#31867;&#32593;&#32476;&#23398;&#20064;&#26356;&#22909;&#22320;&#22788;&#29702;&#31867;&#21035;&#20043;&#38388;&#30340;&#28151;&#28102;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical classification aims to sort the object into a hierarchical structure of categories. For example, a bird can be categorized according to a three-level hierarchy of order, family, and species. Existing methods commonly address hierarchical classification by decoupling it into a series of multi-class classification tasks. However, such a multi-task learning strategy fails to fully exploit the correlation among various categories across different levels of the hierarchy. In this paper, we propose Label Hierarchy Transition (LHT), a unified probabilistic framework based on deep learning, to address the challenges of hierarchical classification. The LHT framework consists of a transition network and a confusion loss. The transition network focuses on explicitly learning the label hierarchy transition matrices, which has the potential to effectively encode the underlying correlations embedded within class hierarchies. The confusion loss encourages the classification network to le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33647;&#25928;&#22266;&#25351;&#32441;&#21644;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861; Pharmacoprint&#65292;&#36890;&#36807;&#32534;&#30721;&#20998;&#23376;&#30340;&#33647;&#25928;&#22266;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#33647;&#29289;&#35774;&#35745;&#24037;&#20855;&#65292;&#20248;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#20998;&#23376;&#25351;&#32441;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.01339</link><description>&lt;p&gt;
Pharmacoprint -- &#19968;&#31181;&#23558;&#33647;&#25928;&#22266;&#25351;&#32441;&#21644;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#20026;&#35745;&#31639;&#36741;&#21161;&#33647;&#29289;&#35774;&#35745;&#24037;&#20855;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pharmacoprint -- a combination of pharmacophore fingerprint and artificial intelligence as a tool for computer-aided drug design. (arXiv:2110.01339v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33647;&#25928;&#22266;&#25351;&#32441;&#21644;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861; Pharmacoprint&#65292;&#36890;&#36807;&#32534;&#30721;&#20998;&#23376;&#30340;&#33647;&#25928;&#22266;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#33647;&#29289;&#35774;&#35745;&#24037;&#20855;&#65292;&#20248;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#20998;&#23376;&#25351;&#32441;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#25351;&#32441;&#21644;&#33647;&#25928;&#22266;&#24314;&#27169;&#26159;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#20102;&#33267;&#23569;&#20108;&#21313;&#24180;&#30340;&#26041;&#27861;&#23398;&#65306;&#20174;&#30456;&#20284;&#24615;&#25628;&#32034;&#21040;&#26426;&#22120;&#23398;&#20064;&#12290;&#38543;&#30528;&#20307;&#22806;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#31181;&#31216;&#20026;&#33647;&#25928;&#22266;&#25351;&#32441;&#30340;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#33647;&#25928;&#22266;&#25351;&#32441; Pharmacoprint&#65292;&#23427;&#32534;&#30721;&#20102;&#20998;&#23376;&#30340;&#33647;&#25928;&#22266;&#29305;&#24449;&#30340;&#23384;&#22312;&#12289;&#31867;&#22411;&#21644;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#32447;&#24615;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#23545; Pharmacoprint &#36827;&#34892;&#20102;&#20998;&#31867;&#23454;&#39564;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#20998;&#23376;&#25351;&#32441;&#65288;&#21363; Estate&#12289;MACCS&#12289;PubChem&#12289;&#20122;&#32467;&#26500;&#12289;Klekotha-Roth&#12289;CDK&#12289;&#25193;&#23637;&#21644; GraphOnly&#65289;&#21644; ChemAxon &#33647;&#25928;&#22266;&#29305;&#24449;&#25351;&#32441;&#12290;Pharmacoprint &#21253;&#21547;&#20102; 39973 &#20010;&#20301;&#65307;&#20351;&#29992;&#20102;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#38477;&#32500;&#22788;&#29702;&#65292;
&lt;/p&gt;
&lt;p&gt;
Structural fingerprints and pharmacophore modeling are methodologies that have been used for at least two decades in various fields of cheminformatics: from similarity searching to machine learning (ML). Advances in silico techniques consequently led to combining both these methodologies into a new approach known as pharmacophore fingerprint. Herein, we propose a high-resolution, pharmacophore fingerprint called Pharmacoprint that encodes the presence, types, and relationships between pharmacophore features of a molecule. Pharmacoprint was evaluated in classification experiments by using ML algorithms (logistic regression, support vector machines, linear support vector machines, and neural networks) and outperformed other popular molecular fingerprints (i.e., Estate, MACCS, PubChem, Substructure, Klekotha-Roth, CDK, Extended, and GraphOnly) and ChemAxon Pharmacophoric Features fingerprint. Pharmacoprint consisted of 39973 bits; several methods were applied for dimensionality reduction,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#38236;&#20687;&#19978;&#21319;&#30340;&#26222;&#36866;&#26694;&#26550;(FMA-PG)&#65292;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#65292;&#24182;&#19988;&#19981;&#21463;&#31574;&#30053;&#21442;&#25968;&#21270;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2108.05828</link><description>&lt;p&gt;
&#19968;&#31867;&#31283;&#23450;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#26367;&#20195;&#20989;&#25968;&#30340;&#26222;&#36866;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A general class of surrogate functions for stable and efficient reinforcement learning. (arXiv:2108.05828v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#38236;&#20687;&#19978;&#21319;&#30340;&#26222;&#36866;&#26694;&#26550;(FMA-PG)&#65292;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#65292;&#24182;&#19988;&#19981;&#21463;&#31574;&#30053;&#21442;&#25968;&#21270;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#26367;&#20195;&#20989;&#25968;&#65292;&#22823;&#22810;&#25968;&#27809;&#26377;&#24378;&#26377;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;TRPO&#12289;PPO&#25110;MPO&#31561;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#19981;&#26159;&#35774;&#35745;&#21478;&#19968;&#20010;&#26367;&#20195;&#20989;&#25968;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#38236;&#20687;&#19978;&#21319;&#30340;&#26222;&#36866;&#26694;&#26550;&#65288;FMA-PG&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#25972;&#22871;&#26367;&#20195;&#20989;&#25968;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#26367;&#20195;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#20445;&#35777;&#31574;&#30053;&#25913;&#36827;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#29616;&#26377;&#26367;&#20195;&#20989;&#25968;&#25152;&#27809;&#26377;&#30340;&#29305;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#20123;&#20445;&#35777;&#19981;&#21463;&#31574;&#30053;&#21442;&#25968;&#21270;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;FMA-PG&#30340;&#29305;&#23450;&#23454;&#20363;&#24674;&#22797;&#20102;&#37325;&#35201;&#30340;&#23454;&#29616;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#21069;&#21521;&#21644;&#21453;&#21521;KL&#25955;&#24230;&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20855;&#26377;&#39069;&#22806;&#29702;&#24819;&#24615;&#36136;&#30340;TRPO&#21464;&#31181;&#12290;&#36890;&#36807;&#22312;&#31616;&#21333;&#36125;&#21494;&#26031;&#38382;&#39064;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;FMA-PG&#20135;&#29983;&#30340;&#31639;&#27861;&#23454;&#20363;&#12290;&#35813;&#26694;&#26550;&#20063;&#25903;&#25345;&#20854;&#20182;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions. We construct surrogate functions that enable policy improvement guarantees, a property not shared by most existing surrogate functions. Crucially, these guarantees hold regardless of the choice of policy parameterization. Moreover, a particular instantiation of FMA-PG recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) resulting in a variant of TRPO with additional desirable properties. Via experiments on simple bandit problems, we evaluate the algorithms instantiated by FMA-PG. The proposed framework also su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#20998;&#24067;&#30456;&#23545;&#20110;&#26368;&#20248;&#36755;&#36816;&#24230;&#37327;&#30340;&#37325;&#24515;&#30340;&#31639;&#27861;&#65292;&#22312;Bures-Wasserstein&#27969;&#24418;&#19978;&#35777;&#26126;&#20102;&#26032;&#30340;&#27979;&#22320;&#20984;&#24615;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#36845;&#20195;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#26080;&#32500;&#25910;&#25947;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;Riemannian GD &#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2106.08502</link><description>&lt;p&gt;
Bures-Wasserstein&#27969;&#24418;&#19978;&#30340;&#24179;&#22343;&#65306;&#26799;&#24230;&#19979;&#38477;&#30340;&#26080;&#32500;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent. (arXiv:2106.08502v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#20998;&#24067;&#30456;&#23545;&#20110;&#26368;&#20248;&#36755;&#36816;&#24230;&#37327;&#30340;&#37325;&#24515;&#30340;&#31639;&#27861;&#65292;&#22312;Bures-Wasserstein&#27969;&#24418;&#19978;&#35777;&#26126;&#20102;&#26032;&#30340;&#27979;&#22320;&#20984;&#24615;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#36845;&#20195;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#26080;&#32500;&#25910;&#25947;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;Riemannian GD &#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#20998;&#24067;&#30456;&#23545;&#20110;&#26368;&#20248;&#36755;&#36816;&#24230;&#37327;&#30340;&#37325;&#24515;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#23613;&#31649;&#30446;&#26631;&#26159;&#27979;&#22320;&#19981;&#20984;&#30340;&#65292;&#20294;&#27979;&#22320;&#26799;&#24230;&#19979;&#38477;&#22312;&#32463;&#39564;&#19978;&#24555;&#36895;&#25910;&#25947;&#65292;&#23454;&#38469;&#19978;&#27604;&#35832;&#22914;&#27431;&#20960;&#37324;&#24503;&#26799;&#24230;&#19979;&#38477;&#21644;SDP&#27714;&#35299;&#22120;&#31561;&#29616;&#25104;&#26041;&#27861;&#26356;&#24555;&#12290;&#36825;&#19982;Riemannian GD&#30340;&#24050;&#30693;&#30340;&#26368;&#20339;&#29702;&#35770;&#32467;&#26524;&#24418;&#25104;&#20102;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#26159;&#20197;&#25351;&#25968;&#26041;&#24335;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26032;&#30340;&#27979;&#22320;&#20984;&#24615;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#36845;&#20195;&#25511;&#21046;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#26080;&#32500;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36824;&#20351;&#24471;&#23545;&#20004;&#31181;&#30456;&#20851;&#30340;&#24179;&#22343;&#27010;&#24565; - &#29109;&#27491;&#21017;&#21270;&#30340;&#37325;&#24515;&#21644;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;Riemannian GD&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study first-order optimization algorithms for computing the barycenter of Gaussian distributions with respect to the optimal transport metric. Although the objective is geodesically non-convex, Riemannian GD empirically converges rapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP solvers. This stands in stark contrast to the best-known theoretical results for Riemannian GD, which depend exponentially on the dimension. In this work, we prove new geodesic convexity results which provide stronger control of the iterates, yielding a dimension-free convergence rate. Our techniques also enable the analysis of two related notions of averaging, the entropically-regularized barycenter and the geometric median, providing the first convergence guarantees for Riemannian GD for these problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#20272;&#35745;&#27969;&#24418;Helmholtzian&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;1-Laplacian&#26500;&#24314;&#20102;&#22270;Helmholtzian&#20316;&#20026;&#36830;&#32493;&#31639;&#23376;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#65292;&#24182;&#21033;&#29992;Helmholtz-Hodge&#23450;&#29702;&#23545;&#27969;&#21644;&#21521;&#37327;&#22330;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#27969;&#36827;&#34892;&#24179;&#28369;&#12289;&#39044;&#27979;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2103.07626</link><description>&lt;p&gt;
Helmholtzian&#29305;&#24449;&#22270;&#65306;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#21457;&#29616;&#25299;&#25169;&#29305;&#24449;&#21644;&#36793;&#32536;&#27969;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Helmholtzian Eigenmap: Topological feature discovery &amp; edge flow learning from point cloud data. (arXiv:2103.07626v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#20272;&#35745;&#27969;&#24418;Helmholtzian&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;1-Laplacian&#26500;&#24314;&#20102;&#22270;Helmholtzian&#20316;&#20026;&#36830;&#32493;&#31639;&#23376;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#65292;&#24182;&#21033;&#29992;Helmholtz-Hodge&#23450;&#29702;&#23545;&#27969;&#21644;&#21521;&#37327;&#22330;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#27969;&#36827;&#34892;&#24179;&#28369;&#12289;&#39044;&#27979;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;Helmholtzian&#65288;1-Laplacian&#65289;&#31639;&#23376;$ \Delta_1 $&#23558;Laplace-Beltrami&#31639;&#23376;&#20248;&#38597;&#22320;&#24191;&#20041;&#21270;&#21040;&#27969;&#24418;$ \mathcal M $&#19978;&#30340;&#21521;&#37327;&#22330;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21152;&#26435;1-Laplacian $ \mathcal L_1 $&#20174;&#28857;&#20113;&#25968;&#25454;&#20272;&#35745;&#27969;&#24418;Helmholtzian&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#24341;&#20837;&#21644;&#30740;&#31350;&#20102;&#39640;&#38454;Laplacian&#65292;&#20294;&#26412;&#24037;&#20316;&#26159;&#39318;&#27425;&#25552;&#20986;&#20102;&#20174;&#21333;&#32431;&#22797;&#21512;&#29289;&#26500;&#24314;&#30340;&#22270;Helmholtzian&#20316;&#20026;&#36830;&#32493;&#31639;&#23376;&#22312;&#38750;&#21442;&#25968;&#35774;&#32622;&#20013;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#20855;&#22791;&#20851;&#20110;$ \mathcal M $&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#20449;&#24687;&#30340;Helmholtzian&#26159;&#36890;&#36807;Helmholtz-Hodge&#23450;&#29702;&#23545;$ \mathcal M $&#19978;&#30340;&#27969;&#21644;&#21521;&#37327;&#22330;&#36827;&#34892;&#20998;&#26512;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;$ \mathcal L_1 $&#20801;&#35768;&#23545;&#27969;&#36827;&#34892;&#24179;&#28369;&#12289;&#39044;&#27979;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#38750;&#24179;&#20961;&#25299;&#25169;&#32467;&#26500;&#30340;&#22823;&#37327;&#21512;&#25104;&#21644;&#30495;&#23454;&#28857;&#20113;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#36825;&#20123;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold Helmholtzian (1-Laplacian) operator $\Delta_1$ elegantly generalizes the Laplace-Beltrami operator to vector fields on a manifold $\mathcal M$. In this work, we propose the estimation of the manifold Helmholtzian from point cloud data by a weighted 1-Laplacian $\mathcal L_1$. While higher order Laplacians have been introduced and studied, this work is the first to present a graph Helmholtzian constructed from a simplicial complex as a consistent estimator for the continuous operator in a non-parametric setting. Equipped with the geometric and topological information about $\mathcal M$, the Helmholtzian is a useful tool for the analysis of flows and vector fields on $\mathcal M$ via the Helmholtz-Hodge theorem. In addition, the $\mathcal L_1$ allows the smoothing, prediction, and feature extraction of the flows. We demonstrate these possibilities on substantial sets of synthetic and real point cloud datasets with non-trivial topological structures; and provide theoretical r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#21512;&#25104;&#24178;&#39044;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#35266;&#23519;&#21040;&#27599;&#20010;&#21333;&#20803;&#26368;&#22810;&#20004;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#27599;&#20010;&#21333;&#20803;&#23545;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#39044;&#26399;&#28508;&#22312;&#32467;&#26524;&#65292;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;</title><link>http://arxiv.org/abs/2006.07691</link><description>&lt;p&gt;
&#21512;&#25104;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Synthetic Interventions. (arXiv:2006.07691v6 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.07691
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#21512;&#25104;&#24178;&#39044;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#35266;&#23519;&#21040;&#27599;&#20010;&#21333;&#20803;&#26368;&#22810;&#20004;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#27599;&#20010;&#21333;&#20803;&#23545;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#39044;&#26399;&#28508;&#22312;&#32467;&#26524;&#65292;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#25317;&#26377;$N$&#20010;&#24322;&#36136;&#21333;&#20803;&#65288;&#20363;&#22914;&#20010;&#20307;&#25110;&#23376;&#32676;&#20307;&#65289;&#21644;$D$&#20010;&#24178;&#39044;&#25514;&#26045;&#65288;&#20363;&#22914;&#31038;&#20250;&#32463;&#27982;&#25919;&#31574;&#65289;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#27599;&#20010;&#21333;&#20803;&#23545;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#39044;&#26399;&#28508;&#22312;&#32467;&#26524;&#65292;&#24635;&#20849;&#26377;$N \times D$&#20010;&#22240;&#26524;&#21442;&#25968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#8212;&#8212;&#21512;&#25104;&#24178;&#39044;&#65288;SI&#65289;&#65292;&#20197;&#25512;&#26029;&#36825;$N \times D$&#20010;&#22240;&#26524;&#21442;&#25968;&#65292;&#21516;&#26102;&#20165;&#35266;&#23519;&#27599;&#20010;&#21333;&#20803;&#22312;&#26368;&#22810;&#20004;&#20010;&#24178;&#39044;&#25514;&#26045;&#19979;&#30340;&#24773;&#20917;&#65292;&#19982;$D$&#26080;&#20851;&#12290;&#24403;&#20010;&#24615;&#21270;&#27700;&#24179;&#22686;&#21152;&#26102;&#65292;&#36825;&#23558;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#19968;&#20010;&#26032;&#30340;&#24352;&#37327;&#22240;&#23376;&#27169;&#22411;&#19979;&#65292;&#36328;&#21333;&#20803;&#12289;&#32467;&#26524;&#21644;&#24178;&#39044;&#25514;&#26045;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;$N \times D$&#20010;&#22240;&#26524;&#21442;&#25968;&#30340;&#35782;&#21035;&#32467;&#26524;&#65292;&#24182;&#22312;&#38468;&#21152;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#25105;&#20204;&#20272;&#35745;&#20540;&#30340;&#26377;&#38480;&#26679;&#26412;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#36824;&#20801;&#35768;&#23384;&#22312;&#20915;&#23450;&#24178;&#39044;&#20998;&#37197;&#26041;&#24335;&#30340;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a setting with $N$ heterogeneous units (e.g., individuals, sub-populations) and $D$ interventions (e.g., socio-economic policies). Our goal is to learn the expected potential outcome associated with every intervention on every unit, totaling $N \times D$ causal parameters. Towards this, we present a causal framework, synthetic interventions (SI), to infer these $N \times D$ causal parameters while only observing each of the $N$ units under at most two interventions, independent of $D$. This can be significant as the number of interventions, i.e., level of personalization, grows. Under a novel tensor factor model across units, outcomes, and interventions, we prove an identification result for each of these $N \times D$ causal parameters, establish finite-sample consistency of our estimator along with asymptotic normality under additional conditions. Importantly, our estimator also allows for latent confounders that determine how interventions are assigned. The estimator is furt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#35775;&#20102;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#25351;&#31034;&#20449;&#21495;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#31574;&#30053;&#32593;&#32476;&#20849;&#20139;&#21442;&#25968;&#30340;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#31574;&#30053;&#25110;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24322;&#26500;&#35266;&#27979;&#21644;&#34892;&#21160;&#31354;&#38388;&#23398;&#20064;&#20013;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2005.13625</link><description>&lt;p&gt;
&#37325;&#35775;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning. (arXiv:2005.13625v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.13625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#35775;&#20102;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#25351;&#31034;&#20449;&#21495;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#31574;&#30053;&#32593;&#32476;&#20849;&#20139;&#21442;&#25968;&#30340;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#31574;&#30053;&#25110;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24322;&#26500;&#35266;&#27979;&#21644;&#34892;&#21160;&#31354;&#38388;&#23398;&#20064;&#20013;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20849;&#20139;&#26159;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#24120;&#29992;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#29420;&#31435;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#24182;&#19988;&#25152;&#26377;&#31574;&#30053;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#20849;&#20139;&#21516;&#19968;&#31574;&#30053;&#32593;&#32476;&#65292;&#23427;&#20204;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#30340;&#31574;&#30053;&#25110;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#35266;&#27979;&#20013;&#28155;&#21152;&#26234;&#33021;&#20307;&#29305;&#23450;&#30340;&#25351;&#31034;&#20449;&#21495;&#65288;&#31216;&#20026;&#8220;&#26234;&#33021;&#20307;&#25351;&#31034;&#8221;&#65289;&#26469;&#36827;&#34892;&#23454;&#39564;&#24615;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#20307;&#25351;&#31034;&#30340;&#23616;&#38480;&#22312;&#20110;&#65292;&#22914;&#26524;&#19981;&#36827;&#34892;&#20462;&#25913;&#65292;&#23427;&#26080;&#27861;&#24212;&#29992;&#20110;&#34892;&#21160;&#31354;&#38388;&#21644;/&#25110;&#35266;&#27979;&#31354;&#38388;&#19981;&#21516;&#36136;&#30340;&#29615;&#22659;&#12290;&#26412;&#30740;&#31350;&#27491;&#24335;&#23450;&#20041;&#20102;&#26234;&#33021;&#20307;&#25351;&#31034;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#39318;&#27425;&#23454;&#29616;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#25193;&#23637;&#21442;&#25968;&#20849;&#20139;&#21040;&#24322;&#26500;&#35266;&#27979;&#21644;&#34892;&#21160;&#31354;&#38388;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#24182;&#23545;&#27604;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter sharing, where each agent independently learns a policy with fully shared parameters between all policies, is a popular baseline method for multi-agent deep reinforcement learning. Unfortunately, since all agents share the same policy network, they cannot learn different policies or tasks. This issue has been circumvented experimentally by adding an agent-specific indicator signal to observations, which we term "agent indication". Agent indication is limited, however, in that without modification it does not allow parameter sharing to be applied to environments where the action spaces and/or observation spaces are heterogeneous. This work formalizes the notion of agent indication and proves that it enables convergence to optimal policies for the first time. Next, we formally introduce methods to extend parameter sharing to learning in heterogeneous observation and action spaces, and prove that these methods allow for convergence to optimal policies. Finally, we experimentally
&lt;/p&gt;</description></item><item><title>Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2002.01368</link><description>&lt;p&gt;
&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#31867;&#21035;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#65288;Open-LACU&#65289;
&lt;/p&gt;
&lt;p&gt;
Open-set learning with augmented category by exploiting unlabeled data (Open-LACU). (arXiv:2002.01368v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.01368
&lt;/p&gt;
&lt;p&gt;
Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#24335;&#35782;&#21035;&#65288;OSR&#65289;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#23581;&#35797;&#20197;&#21512;&#25104;&#21333;&#20010;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27599;&#27425;&#23581;&#35797;&#37117;&#36829;&#21453;&#20102;&#24320;&#25918;&#38598;&#23450;&#20041;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#22312;&#26410;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#26032;&#39062;&#30340;&#31867;&#21035;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#25512;&#24191;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#32972;&#26223;&#31867;&#21035;&#21644;&#26410;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#26410;&#30693;&#31867;&#21035;&#12290;&#36890;&#36807;&#20998;&#31867;&#36825;&#20004;&#31181;&#26032;&#39062;&#31867;&#21035;&#30340;&#26041;&#24335;&#65292;Open-LACU&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#24182;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several efforts have been made to synthesize semi-supervised learning (SSL) and open set recognition (OSR) within a single training policy. However, each attempt violated the definition of an open set by incorporating novel categories within the unlabeled training set. Although such \textit{observed} novel categories are undoubtedly prevalent in application-grade datasets, they should not be conflated with the OSR-defined \textit{unobserved} novel categories, which only emerge during testing. This study proposes a new learning policy wherein classifiers generalize between observed and unobserved novel categories. Specifically, our open-set learning with augmented category by exploiting unlabeled data (Open-LACU) policy defines a background category for observed novel categories and an unknown category for unobserved novel categories. By separating these novel category types, Open-LACU promotes cost-efficient training by eliminating the need to label every category and ensures safe clas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25197;&#26354;&#27010;&#29575;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;UCB&#31639;&#27861;&#20026;&#22522;&#30784;&#12289;&#32771;&#34385;&#20102;&#22870;&#21169;&#25197;&#26354;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/1611.10283</link><description>&lt;p&gt;
&#21152;&#26435;&#36172;&#21338;&#26426;&#25110;&#32773;&#65306;&#36172;&#21338;&#26426;&#22914;&#20309;&#23398;&#20064;&#39044;&#26399;&#20043;&#22806;&#30340;&#25197;&#26354;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Weighted bandits or: How bandits learn distorted values that are not expected. (arXiv:1611.10283v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1611.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25197;&#26354;&#27010;&#29575;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;UCB&#31639;&#27861;&#20026;&#22522;&#30784;&#12289;&#32771;&#34385;&#20102;&#22870;&#21169;&#25197;&#26354;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#29992;&#20110;&#35299;&#37322;&#24120;&#35265;&#20559;&#31163;&#20256;&#32479;&#39044;&#26399;&#20215;&#20540;&#20559;&#22909;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24102;&#26377;&#25197;&#26354;&#27010;&#29575;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65306;&#32463;&#20856;&#30340;K&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#21442;&#25968;&#21270;&#36172;&#21338;&#26426;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;&#23545;&#20110;K&#33218;&#36172;&#21338;&#26426;&#20197;&#21450;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#21040;&#19978;&#32622;&#20449;&#30028;(UCB)&#31639;&#27861;&#21551;&#21457;&#12289;&#21253;&#21547;&#22870;&#21169;&#25197;&#26354;&#24182;&#19988;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;K&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#39044;&#26399;&#21518;&#24724;&#30340;&#19978;&#30028;&#65292;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#20248;&#21270;&#39034;&#24207;&#12290;&#23545;&#20110;&#32447;&#24615;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#21518;&#24724;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#26159;&#27425;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the reward distributions: the classic $K$-armed bandit and the linearly parameterized bandit settings. We consider the aforementioned problems in the regret minimization as well as best arm identification framework for multi-armed bandits. For the regret minimization setting in $K$-armed as well as linear bandit problems, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate reward distortions, and exhibit sublinear regret. For the $K$-armed bandit setting, we derive an upper bound on the expected regret for our proposed algorithm, and then we prove a matching lower bound to establish the order-optimality of our algorithm. For the linearly parameterized setting, our algorithm achieves a regret upper bound that is of the 
&lt;/p&gt;</description></item></channel></rss>